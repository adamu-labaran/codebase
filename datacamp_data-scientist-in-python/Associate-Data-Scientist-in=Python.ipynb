{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "certification exaams preparation\n",
    "\n",
    "# INTERMEDIATE IMPORTING DATA IN PYTHON\n",
    "As a data scientist, you will need to clean data, wrangle and munge it, visualize it, build predictive models and interpret these models. Before you can do so, however, you will need to know how to get data into Python. In the prequel to this course, you learned many ways to import data into Python: from flat files such as .txt and .csv; from files native to other software such as Excel spreadsheets, Stata, SAS, and MATLAB files; and from relational databases such as SQLite and PostgreSQL. In this course, you'll extend this knowledge base by learning to import data from the web and by pulling data from Application Programming Interfaces— APIs—such as the Twitter streaming API, which allows us to stream real-time tweets.\n",
    "\n",
    "# CHAPTER ONE\n",
    "# Importing data from the Internet\n",
    "The web is a rich source of data from which you can extract various types of insights and findings. In this chapter, you will learn how to get data from the web, whether it is stored in files or in HTML. You'll also learn the basics of scraping and parsing web data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Save file locally\n",
    "urlretrieve(url, 'winequality-red.csv')\n",
    "\n",
    "# Read file into a DataFrame and print its head\n",
    "df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explanation\n",
    "This code snippet performs the following tasks:\n",
    "\n",
    "1. Importing Necessary Packages\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "\n",
    "# urllib.request: This is a module that provides various functions to work with URLs. The urlretrieve function is used here to download a file from a specified URL.\n",
    "\n",
    "# pandas: This is a powerful data manipulation library in Python. The pd alias is commonly used when importing pandas to shorten the code needed to reference it.\n",
    "\n",
    "2. Assigning the URL of the File\n",
    "# url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# url: A variable that stores the URL of the dataset file you want to download. This dataset is a CSV file containing information about red wine quality.\n",
    "\n",
    "3. Saving the File Locally\n",
    "\n",
    "# urlretrieve(url, 'winequality-red.csv')\n",
    "\n",
    "# urlretrieve(url, 'winequality-red.csv'): Downloads the file from the specified URL and saves it locally with the filename 'winequality-red.csv'.\n",
    "\n",
    "# The first argument (url) is the location of the file on the web, and the second argument ('winequality-red.csv') is the local filename under which the file will be saved.\n",
    "\n",
    "4. Reading the File into a DataFrame\n",
    "\n",
    "# df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "# pd.read_csv('winequality-red.csv', sep=';'): Reads the downloaded CSV file into a pandas DataFrame.\n",
    "# 'winequality-red.csv': The filename of the CSV file to be read.\n",
    "# sep=';': Specifies that the delimiter used in the CSV file is a semicolon (;) instead of the default comma (,). This is # necessary because the data in the CSV file is separated by semicolons.\n",
    "\n",
    "5. Printing the First Few Rows of the DataFrame\n",
    "\n",
    "# print(df.head())\n",
    "# df.head(): Displays the first five rows of the DataFrame. This is useful to quickly inspect the contents of the DataFrame and  verify that the data has been loaded correctly.\n",
    "\n",
    "# Summary\n",
    "# The code downloads a dataset of red wine quality from the internet, saves it locally as a CSV file, reads the file into a pandas DataFrame, and prints the first five rows of the data. This process allows you to work with the data locally, inspect it, and then perform further data analysis or manipulation using pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the function urlretrieve from the subpackage urllib.request.\n",
    "Assign the URL of the file to the variable url.\n",
    "Use the function urlretrieve() to save the file locally as 'winequality-red.csv'.\n",
    "Execute the remaining code to load 'winequality-red.csv' in a pandas DataFrame and to print its head to the shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n"
     ]
    }
   ],
   "source": [
    "# Import package\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Save file locally\n",
    "r = urlretrieve(url, 'winequality-red.csv')\n",
    "\n",
    "# Read file into a DataFrame and print its head\n",
    "df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Read file into a DataFrame: df\n",
    "df = pd.read_csv(url, sep=';')\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "To open and read a flat file from the web directly into a pandas DataFrame without saving it locally, you can use the pd.read_csv() function provided by pandas. This function is typically used to read CSV files, but it can also handle files that are hosted online.\n",
    "\n",
    "Here's a step-by-step breakdown:\n",
    "\n",
    "URL of the File: The file you're interested in is located on a web server, and you have its URL. In this case, the URL is 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'.\n",
    "\n",
    "Using pd.read_csv(): Instead of downloading the file and saving it on your computer, you can directly pass the URL of the file to the pd.read_csv() function. Pandas will handle the process of fetching the file from the web and loading it into a DataFrame.\n",
    "\n",
    "Example Code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the file\n",
    "url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "df = pd.read_csv(url, sep=',')  # sep=',' indicates that the file is comma-separated\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "url: This variable stores the URL of the file.\n",
    "pd.read_csv(url, sep=','): The read_csv function reads the file from the provided URL. The sep=',' argument specifies that the file is comma-separated (which is the default for CSV files). You can change this separator depending on the structure of your file (e.g., sep=';' for semicolon-separated values).\n",
    "df.head(): This function call displays the first few rows of the DataFrame so you can quickly inspect the data.\n",
    "Result: After running this code, you will have the data from the web file loaded into a pandas DataFrame named df. You can then perform any data analysis or manipulations you need on this DataFrame.\n",
    "\n",
    "This approach is particularly useful when you want to quickly load and analyze data from an online source without the extra step of saving the file locally first.\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Plot first column of df\n",
    "df.iloc[:, 0].hist()\n",
    "plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here's a detailed explanation of each line of the code:\n",
    "\n",
    "1. Import packages\n",
    "python\n",
    "Copy code\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt: This line imports the pyplot module from the matplotlib library, a powerful plotting library in Python. The pyplot module provides a MATLAB-like interface for creating plots and visualizations. The as plt part gives a shorter alias (plt) for easier reference in the code.\n",
    "\n",
    "import pandas as pd: This line imports the pandas library, which is a powerful tool for data manipulation and analysis. Like with matplotlib, the as pd part provides a shorter alias (pd), which is commonly used in pandas documentation and examples.\n",
    "\n",
    "2. Assign URL of file\n",
    "python\n",
    "Copy code\n",
    "url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "url = ...: This line assigns the URL of the CSV file to the variable url. This URL points to a dataset containing information about the quality of red wine. By storing the URL in a variable, you can easily reference it later in your code.\n",
    "3. Read file into a DataFrame\n",
    "python\n",
    "Copy code\n",
    "df = pd.read_csv(url, sep=';')\n",
    "df = pd.read_csv(url, sep=';'): This line reads the CSV file from the web into a pandas DataFrame.\n",
    "pd.read_csv(url, sep=';'): The read_csv() function reads a comma-separated values (CSV) file into a DataFrame. Here, the URL (url) is passed as the first argument, telling pandas to fetch the file from that location.\n",
    "sep=';': This argument specifies that the values in the CSV file are separated by semicolons (;). This is necessary because not all CSV files use commas as separators—some use other characters like semicolons, tabs, or spaces.\n",
    "4. Print the head of the DataFrame\n",
    "python\n",
    "Copy code\n",
    "print(df.head())\n",
    "print(df.head()): This line prints the first five rows of the DataFrame df. The head() method is a quick way to inspect the top rows of the DataFrame to ensure that the data has been loaded correctly and to get a sense of its structure.\n",
    "5. Plot the first column of df\n",
    "python\n",
    "Copy code\n",
    "df.iloc[:, 0].hist()\n",
    "df.iloc[:, 0].hist(): This line creates a histogram of the first column in the DataFrame.\n",
    "df.iloc[:, 0]: The iloc method allows for selecting rows and columns by integer-location based indexing. Here, : means \"select all rows,\" and 0 means \"select the first column.\" This extracts the entire first column from the DataFrame.\n",
    ".hist(): This method creates a histogram, which is a type of plot that shows the distribution of values in the selected data. The histogram will show how the values of the first column (which represents 'fixed acidity') are distributed.\n",
    "6. Label the x-axis\n",
    "python\n",
    "Copy code\n",
    "plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')\n",
    "plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)'): This line labels the x-axis of the plot with the string 'fixed acidity (g(tartaric acid)/dm$^3$)'. The xlabel() function is used to set the label for the x-axis. The part in parentheses describes the units of the 'fixed acidity' measurement.\n",
    "7. Label the y-axis\n",
    "python\n",
    "Copy code\n",
    "plt.ylabel('count')\n",
    "plt.ylabel('count'): This line labels the y-axis with the string 'count'. The ylabel() function sets the label for the y-axis, indicating that the y-axis represents the number of occurrences (or counts) of each value in the histogram.\n",
    "8. Display the plot\n",
    "python\n",
    "Copy code\n",
    "plt.show()\n",
    "plt.show(): This line displays the plot that was created. The show() function renders the plot in a window (or inline in a Jupyter notebook), allowing you to visually inspect the distribution of the data.\n",
    "Overall, this code reads a dataset from the web, loads it into a pandas DataFrame, prints the first few rows of the data, and then creates and displays a histogram of the first column in the dataset, complete with labeled axes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['1700', '1900'])\n",
      "\n",
      "Head of sheet '1700':\n",
      "                 country       1700\n",
      "0            Afghanistan  34.565000\n",
      "1  Akrotiri and Dhekelia  34.616667\n",
      "2                Albania  41.312000\n",
      "3                Algeria  36.720000\n",
      "4         American Samoa -14.307000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assign the URL of the file to the variable url\n",
    "url = \"https://assets.datacamp.com/course/importing_data_into_r/latitude.xls\"  \n",
    "# Replace with the actual URL of your Excel file\n",
    "\n",
    "# Read in all sheets of the Excel file into a dictionary xls\n",
    "xls = pd.read_excel(url, sheet_name=None)  # Pass None to read all sheets\n",
    "\n",
    "# Print the names of the sheets in the Excel spreadsheet\n",
    "# print(\"Sheet names:\", list(xls.keys()))\n",
    "print(xls.keys())\n",
    "\n",
    "# Print the head of the first sheet using the sheet name '1700'\n",
    "try:\n",
    "  first_sheet_name = \"1700\"\n",
    "  print(f\"\\nHead of sheet '{first_sheet_name}':\")\n",
    "  print(xls[first_sheet_name].head())\n",
    "except KeyError:\n",
    "  print(f\"Sheet named '{first_sheet_name}' not found in the Excel file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Non-Flat Files from the Web\n",
    "While you've already learned how to load flat files (like CSV files) directly from the web into a pandas DataFrame using pd.read_csv(), pandas also provides functions for loading other types of files, such as Excel spreadsheets. The process is very similar, and you can do it without saving the file locally first.\n",
    "\n",
    "Loading an Excel Spreadsheet with pd.read_excel()\n",
    "The pd.read_excel() function is specifically designed to handle Excel files (both .xls and .xlsx formats). You can use it to directly load an Excel file from the web into a DataFrame.\n",
    "\n",
    "Here's a breakdown of how this works:\n",
    "\n",
    "URL of the File:\n",
    "\n",
    "The file you're interested in is an Excel spreadsheet, and its URL is 'https://assets.datacamp.com/course/importing_data_into_r/latitude.xls'.\n",
    "Using pd.read_excel():\n",
    "\n",
    "You can load this file directly into a pandas DataFrame by passing the URL to pd.read_excel(). This function will fetch the Excel file from the web and parse it into a DataFrame.\n",
    "Example Code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the Excel file\n",
    "url = 'https://assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n",
    "\n",
    "# Load the Excel file into a DataFrame\n",
    "df = pd.read_excel(url)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "url = ...: This stores the URL of the Excel file in a variable.\n",
    "pd.read_excel(url): This line reads the Excel file from the provided URL into a DataFrame named df. Unlike pd.read_csv(), which reads CSV files, pd.read_excel() is tailored for Excel files.\n",
    "print(df.head()): This prints the first few rows of the DataFrame so you can inspect the loaded data.\n",
    "Key Points:\n",
    "Versatility: Just like pd.read_csv() for CSV files, pd.read_excel() is designed to handle Excel files. This makes pandas a versatile tool for importing various types of data directly from the web.\n",
    "No Local Saving Needed: You don't need to save the file locally before loading it into your DataFrame. This can streamline your workflow, especially when working with data from online sources.\n",
    "Other Functions:\n",
    "pd.read_json(): For JSON files.\n",
    "pd.read_html(): For reading HTML tables directly from a webpage.\n",
    "pd.read_sql(): For reading data from SQL databases.\n",
    "Pandas provides these functions to handle a wide range of data formats, allowing you to import data into your analysis pipeline with ease, no matter the source.\n",
    "\n",
    "Here's a detailed explanation of each line of the code:\n",
    "\n",
    "1. Import the pandas library\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "import pandas as pd: This line imports the pandas library, which is essential for data manipulation and analysis in Python. The alias pd is commonly used to refer to pandas throughout the code.\n",
    "2. Assign the URL of the file to the variable url\n",
    "python\n",
    "Copy code\n",
    "url = \"https://assets.datacamp.com/course/importing_data_into_r/latitude.xls\"\n",
    "url = ...: This line assigns the URL of the Excel file to the variable url. This URL points to the Excel file you want to read into your program. By storing the URL in a variable, you can easily reference it in the next steps.\n",
    "3. Read in all sheets of the Excel file into a dictionary xls\n",
    "python\n",
    "Copy code\n",
    "xls = pd.read_excel(url, sheet_name=None)\n",
    "pd.read_excel(url, sheet_name=None): This line reads the Excel file from the provided URL into a dictionary called xls.\n",
    "url: The URL pointing to the Excel file is passed as the first argument.\n",
    "sheet_name=None: The sheet_name parameter is set to None, which instructs pandas to read all sheets from the Excel file. When you pass None, the result is a dictionary where the keys are the sheet names and the values are DataFrames corresponding to each sheet.\n",
    "4. Print the names of the sheets in the Excel spreadsheet\n",
    "python\n",
    "Copy code\n",
    "print(xls.keys())\n",
    "xls.keys(): This method returns the keys of the dictionary xls, which correspond to the names of the sheets in the Excel file.\n",
    "print(xls.keys()): This line prints the names of the sheets in the Excel file. Each sheet name is a key in the xls dictionary.\n",
    "Optional Line Explanation:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# print(\"Sheet names:\", list(xls.keys()))\n",
    "print(\"Sheet names:\", list(xls.keys())): This commented-out line is an alternative way to print the sheet names, explicitly converting the keys to a list and adding a descriptive message. The effect is the same as the previous line but with additional context.\n",
    "5. Print the head of the first sheet using the sheet name '1700'\n",
    "python\n",
    "Copy code\n",
    "try:\n",
    "    first_sheet_name = \"1700\"\n",
    "    print(f\"\\nHead of sheet '{first_sheet_name}':\")\n",
    "    print(xls[first_sheet_name].head())\n",
    "except KeyError:\n",
    "    print(f\"Sheet named '{first_sheet_name}' not found in the Excel file.\")\n",
    "try:: This begins a try-except block, which is used to handle potential errors that might occur when accessing a specific sheet.\n",
    "first_sheet_name = \"1700\": This line assigns the string \"1700\" to the variable first_sheet_name. This variable is intended to store the name of the sheet you want to inspect.\n",
    "print(f\"\\nHead of sheet '{first_sheet_name}':\"): This line prints a message indicating that the head (first few rows) of the sheet named \"1700\" will be displayed.\n",
    "print(xls[first_sheet_name].head()): This line accesses the DataFrame corresponding to the sheet named \"1700\" from the xls dictionary and prints its first few rows using the .head() method. The .head() method by default shows the first 5 rows of the DataFrame.\n",
    "Exception Handling:\n",
    "\n",
    "except KeyError:: This part of the block will execute if a KeyError occurs, which happens if the specified sheet name does not exist in the Excel file.\n",
    "print(f\"Sheet named '{first_sheet_name}' not found in the Excel file.\"): If the sheet name \"1700\" is not found in the Excel file, this line will print a message indicating that the specified sheet could not be located.\n",
    "Summary:\n",
    "Importing Data: The code imports the pandas library, reads all sheets from an Excel file into a dictionary, and handles cases where a specific sheet might not exist.\n",
    "Inspecting Data: The sheet names are printed, and the first few rows of a specific sheet (if it exists) are displayed.\n",
    "Error Handling: The code is wrapped in a try-except block to handle situations where the specified sheet is not present in the Excel file.\n",
    "\n",
    "\n",
    "\n",
    "The error you're seeing is due to the fact that pandas requires an external library called xlrd to read .xls files. This library is not installed by default, which is why you're encountering the ModuleNotFoundError.\n",
    "\n",
    "Solution: Install xlrd\n",
    "To resolve this issue, you need to install the xlrd library. You can do this using pip, the Python package manager.\n",
    "\n",
    "Open a terminal or command prompt:\n",
    "\n",
    "On Windows: You can open Command Prompt or PowerShell.\n",
    "On macOS or Linux: You can use Terminal.\n",
    "Run the following command:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "pip install xlrd\n",
    "This command will install the xlrd library, which will allow pandas to read .xls files.\n",
    "\n",
    "Verify Installation:\n",
    "\n",
    "After installing, you can try running your code again. The error should be resolved.\n",
    "Explanation of the Error\n",
    "ModuleNotFoundError: No module named 'xlrd': This part of the error indicates that Python is unable to find the xlrd module, which is necessary for reading .xls files.\n",
    "ImportError: Missing optional dependency 'xlrd'. Install xlrd >= 2.0.1 for xls Excel support: Pandas tries to use xlrd when reading .xls files. Since it's not installed, it raises this ImportError, suggesting that you install xlrd with version 2.0.1 or higher.\n",
    "Additional Notes\n",
    ".xls vs .xlsx: If you are working with .xlsx files, pandas does not require xlrd (it uses openpyxl instead). However, for .xls files (an older Excel format), xlrd is needed.\n",
    "Installing via Conda: If you are using Anaconda or Miniconda, you can also install xlrd using:\n",
    "bash\n",
    "Copy code\n",
    "conda install -c anaconda xlrd\n",
    "After installing xlrd, your code should run without issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'http.client.HTTPResponse'>\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# This packages the request: request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Print the datatype of response\n",
    "print(type(response))\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "could you explain this \n",
    "Performing HTTP requests in Python using urllib\n",
    "Now that you know the basics behind HTTP GET requests, it's time to perform some of your own. In this interactive exercise, you will ping our very own DataCamp servers to perform a GET request to extract information from the first coding exercise of this course, \"https://campus.datacamp.com/courses/1606/4135?ex=2\".\n",
    "ChatGPT\n",
    "Performing HTTP Requests in Python Using urllib\n",
    "When you want to interact with web resources in Python, such as accessing web pages, downloading files, or interacting with APIs, you can use the urllib module. This module provides a way to send HTTP requests, handle responses, and work with URLs.\n",
    "\n",
    "Understanding HTTP GET Requests\n",
    "HTTP GET Request: This is the simplest and most common type of HTTP request. When you enter a URL in your web browser, you are making a GET request to the server to retrieve the content at that URL. In Python, you can perform the same type of request using urllib.\n",
    "Example: Performing a GET Request to DataCamp Servers\n",
    "Let's break down how to perform an HTTP GET request using urllib to extract information from a specific URL, such as the one provided by DataCamp.\n",
    "\n",
    "Steps to Perform an HTTP GET Request:\n",
    "Import the urllib module:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from urllib import request\n",
    "from urllib import request: This line imports the request submodule from urllib. The request submodule contains functions and classes that help you send requests and handle responses.\n",
    "Define the URL:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "url = ...: This line assigns the URL you want to request data from to the variable url.\n",
    "Send the HTTP GET Request:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "response = request.urlopen(url)\n",
    "response = request.urlopen(url): This line sends a GET request to the server at the specified URL. The server's response is stored in the response variable. The urlopen() function opens the URL and returns an HTTPResponse object, which contains the data from the server.\n",
    "Read the Response Content:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "html = response.read()\n",
    "html = response.read(): This line reads the content of the response from the server. The read() method returns the content as a byte object, which you can store in the html variable.\n",
    "Decode the Response Content (optional but often necessary):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "html = html.decode('utf-8')\n",
    "html = html.decode('utf-8'): Since the content is returned as a byte object, you may need to decode it into a string. The 'utf-8' argument specifies the encoding used to decode the bytes.\n",
    "Print the HTML Content:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(html)\n",
    "print(html): This line prints the decoded HTML content to the console. This is the raw HTML of the page you requested.\n",
    "Complete Example Code:\n",
    "python\n",
    "Copy code\n",
    "from urllib import request\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# Send the HTTP GET request\n",
    "response = request.urlopen(url)\n",
    "\n",
    "# Read and decode the response content\n",
    "html = response.read().decode('utf-8')\n",
    "\n",
    "# Print the HTML content\n",
    "print(html)\n",
    "Summary:\n",
    "Importing urllib: You use the request submodule from urllib to make the HTTP request.\n",
    "Sending a GET Request: urlopen(url) sends a GET request to the server at the specified URL.\n",
    "Handling the Response: The server's response is read and decoded to obtain the HTML content.\n",
    "Viewing the Result: You can print the HTML content or further process it to extract specific information.\n",
    "Practical Use Cases:\n",
    "Scraping Data: You can use this method to scrape data from web pages.\n",
    "Interacting with APIs: You can send GET requests to APIs to retrieve data.\n",
    "Downloading Files: You can download files from the web by requesting the file URL and saving the response content.\n",
    "I need explanation on each and every line of code\n",
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# This packages the request: request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Print the datatype of response\n",
    "print(type(response))\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()\n",
    "\n",
    "ChatGPT\n",
    "Certainly! Let's go through each line of this code and explain what it does.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "Explanation:\n",
    "from urllib.request import urlopen, Request: This line imports two functions, urlopen and Request, from the urllib.request module.\n",
    "urlopen: A function that opens a URL and returns a file-like object containing the response data.\n",
    "Request: A class used to create a request object, which encapsulates the details of the HTTP request (like the URL, headers, and method).\n",
    "python\n",
    "Copy code\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "Explanation:\n",
    "url = ...: This line assigns the URL you want to request data from to the variable url.\n",
    "The URL \"https://campus.datacamp.com/courses/1606/4135?ex=2\" points to a specific page on DataCamp's website. This is the resource you want to access.\n",
    "python\n",
    "Copy code\n",
    "# This packages the request: request\n",
    "request = Request(url)\n",
    "Explanation:\n",
    "request = Request(url): This line creates a Request object using the specified url.\n",
    "The Request object packages the URL and any associated data (like headers or HTTP methods) into a format that can be used by the urlopen function to make the HTTP request.\n",
    "Here, you're just specifying the URL, so the request will be a basic GET request.\n",
    "python\n",
    "Copy code\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "Explanation:\n",
    "response = urlopen(request): This line sends the HTTP GET request to the server using the urlopen function and the request object you created earlier.\n",
    "The server processes this request and sends back a response, which is captured and stored in the variable response.\n",
    "The response variable now contains the server's response, which includes the data (like HTML content) and metadata (like HTTP headers).\n",
    "python\n",
    "Copy code\n",
    "# Print the datatype of response\n",
    "print(type(response))\n",
    "Explanation:\n",
    "print(type(response)): This line prints the data type of the response object.\n",
    "The response object is typically of type <class 'http.client.HTTPResponse'>, which indicates that it is an HTTP response object.\n",
    "This object behaves similarly to a file, allowing you to read its content using methods like read().\n",
    "python\n",
    "Copy code\n",
    "# Be polite and close the response!\n",
    "response.close()\n",
    "Explanation:\n",
    "response.close(): This line closes the response object.\n",
    "Closing the response is important because it frees up system resources associated with the open connection.\n",
    "Just like closing a file after reading from it, it's good practice to close the response after you're done with it.\n",
    "Summary of What the Code Does:\n",
    "Imports necessary functions: urlopen to open a URL and Request to package a URL into a request object.\n",
    "Specifies the URL you want to access.\n",
    "Packages the request into a Request object.\n",
    "Sends the request to the server and stores the server's response.\n",
    "Prints the type of the response object to verify what kind of object was returned.\n",
    "Closes the response to free up resources.\n",
    "This code is a simple example of how to perform an HTTP GET request in Python using the urllib module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!doctype html><html lang=\"en\"><head><meta charset=\"UTF-8\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"57x57\" href=\"/campus/apple-touch-icon-57x57.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"114x114\" href=\"/campus/apple-touch-icon-114x114.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"72x72\" href=\"/campus/apple-touch-icon-72x72.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"144x144\" href=\"/campus/apple-touch-icon-144x144.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"60x60\" href=\"/campus/apple-touch-icon-60x60.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"120x120\" href=\"/campus/apple-touch-icon-120x120.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"76x76\" href=\"/campus/apple-touch-icon-76x76.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"152x152\" href=\"/campus/apple-touch-icon-152x152.png\"><link rel=\"icon\" type=\"image/png\" href=\"/campus/favicon.ico\"><link rel=\"icon\" type=\"image/png\" href=\"/campus/favicon-196x196.png\" sizes=\"196x196\"><link rel=\"icon\" type=\"image/png\" href=\"/campus/favicon-96x96.png\" sizes=\"96x96\"><link rel=\"icon\" type=\"image/png\" href=\"/campus/favicon-32x32.png\" sizes=\"32x32\"><link rel=\"icon\" type=\"image/png\" href=\"/campus/favicon-16x16.png\" sizes=\"16x16\"><link rel=\"icon\" type=\"image/png\" href=\"/campus/favicon-128.png\" sizes=\"128x128\"><meta name=\"application-name\" content=\"DataCamp\"><meta name=\"msapplication-TileColor\" content=\"#FFFFFF\"><meta name=\"msapplication-TileImage\" content=\"/campus/mstile-144x144.png\"><meta name=\"msapplication-square70x70logo\" content=\"/campus/mstile-70x70.png\"><meta name=\"msapplication-square150x150logo\" content=\"/campus/mstile-150x150.png\"><meta name=\"msapplication-wide310x150logo\" content=\"/campus/mstile-310x150.png\"><meta name=\"msapplication-square310x310logo\" content=\"/campus/mstile-310x310.png\"><script>!function(n,t,e,r){function a(){return t&&t.now?t.now():null}e.version||(e._events=[],e._errors=[],e._metadata={},e._urlGroup=null,window.RM=e,e.install=function(t){e._options=t;var r=n.createElement(\"script\");r.async=!0,r.crossOrigin=\"anonymous\",r.src=\"https://cdn.requestmetrics.com/agent/current/rm.js\";var a=n.getElementsByTagName(\"script\")[0];a.parentNode.insertBefore(r,a)},e.identify=function(n,t){e._userId=n,e._identifyOptions=t},e.sendEvent=function(n,t){e._events.push({eventName:n,metadata:t,time:a()})},e.setUrlGroup=function(n){e._urlGroup=n},e.track=function(n,t){e._errors.push({error:n,metadata:t,time:a()})},e.addMetadata=function(n){e._metadata=Object.assign(e._metadata,n)})}(document,window.performance,window.RM||{}),window.RM.install({token:\"h4zx2kc:w3sn5gv\"})</script><link href=\"/campus/static/css/vendors~main~c09866d3.3e2ac45e.chunk.css\" rel=\"stylesheet\"><link href=\"/campus/static/css/vendors~main~1f20a385.ead2d232.chunk.css\" rel=\"stylesheet\"><link href=\"/campus/static/css/vendors~main~678f84af.41148f02.chunk.css\" rel=\"stylesheet\"><link href=\"/campus/static/css/main~c714bc7b.0ef44ef2.chunk.css\" rel=\"stylesheet\"><title data-react-helmet=\"true\">Importing flat files from the web: your turn! | Python</title><link data-react-helmet=\"true\" rel=\"canonical\" href=\"https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2\"><link data-react-helmet=\"true\" rel=\"preload\" href=\"https://dcmfe.datacamp.com/assets/fonts/StudioFeixenSansRegular-english-v2.woff2\" as=\"font\" crossorigin=\"anonymous\"><link data-react-helmet=\"true\" rel=\"preload\" href=\"https://dcmfe.datacamp.com/assets/fonts/StudioFeixenSans-Semibold-english.woff2\" as=\"font\" crossorigin=\"anonymous\"><link data-react-helmet=\"true\" rel=\"preload\" href=\"https://dcmfe.datacamp.com/assets/fonts/StudioFeixenSansRegular-latin-v2.woff2\" as=\"font\" crossorigin=\"anonymous\"><link data-react-helmet=\"true\" rel=\"preload\" href=\"https://dcmfe.datacamp.com/assets/fonts/JetBrainsMono-english.woff2\" as=\"font\" crossorigin=\"anonymous\"><link data-react-helmet=\"true\" rel=\"preload\" href=\"https://dcmfe.datacamp.com/assets/fonts/JetBrainsMono-rest.woff2\" as=\"font\" crossorigin=\"anonymous\"><link data-react-helmet=\"true\" rel=\"preconnect\" href=\"https://campus-api.datacamp.com\"><link data-react-helmet=\"true\" rel=\"dns-prefetch\" href=\"https://campus-api.datacamp.com\"><link data-react-helmet=\"true\" rel=\"preconnect\" href=\"https://projector.datacamp.com\"><link data-react-helmet=\"true\" rel=\"dns-prefetch\" href=\"https://projector.datacamp.com\"><link data-react-helmet=\"true\" rel=\"preconnect\" href=\"https://assets.datacamp.com\"><link data-react-helmet=\"true\" rel=\"dns-prefetch\" href=\"https://assets.datacamp.com\"><meta data-react-helmet=\"true\" http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\"><meta data-react-helmet=\"true\" name=\"viewport\" content=\"width=device-width, initial-scale=1, maximum-scale=1\"><meta data-react-helmet=\"true\" name=\"fragment\" content=\"!\"><meta data-react-helmet=\"true\" name=\"keywords\" content=\"R, Python, Data analysis, interactive, learning\"><meta data-react-helmet=\"true\" name=\"description\" content=\"Here is an example of Importing flat files from the web: your turn!: You are about to import your first file from the web! The flat file you will import will be &apos;winequality-red.\"><meta data-react-helmet=\"true\" name=\"twitter:card\" content=\"summary\"><meta data-react-helmet=\"true\" name=\"twitter:site\" content=\"@DataCamp\"><meta data-react-helmet=\"true\" name=\"twitter:title\" content=\"Importing flat files from the web: your turn! | Python\"><meta data-react-helmet=\"true\" name=\"twitter:description\" content=\"Here is an example of Importing flat files from the web: your turn!: You are about to import your first file from the web! The flat file you will import will be &apos;winequality-red.\"><meta data-react-helmet=\"true\" name=\"twitter:creator\" content=\"@DataCamp\"><meta data-react-helmet=\"true\" name=\"twitter:image:src\" content=\"/public/assets/images/var/twitter_share.png\"><meta data-react-helmet=\"true\" name=\"twitter:domain\" content=\"www.datacamp.com\"><meta data-react-helmet=\"true\" property=\"og:title\" content=\"Importing flat files from the web: your turn! | Python\"><meta data-react-helmet=\"true\" property=\"og:image\" content=\"/public/assets/images/var/linkedin_share.png\"><meta data-react-helmet=\"true\" name=\"google-signin-clientid\" content=\"892114885437-01a7plbsu1b2vobuhvnckmmanhb58h3a.apps.googleusercontent.com\"><meta data-react-helmet=\"true\" name=\"google-signin-scope\" content=\"email profile\"><meta data-react-helmet=\"true\" name=\"google-signin-cookiepolicy\" content=\"single_host_origin\"><meta content=\"en\" http-equiv=\"content-language\"><link href=\"https://campus.datacamp.com/courses/1606/4135?ex=2\" hreflang=\"en\" rel=\"alternate\"><link href=\"https://campus.datacamp.com/es/courses/1606/4135?ex=2\" hreflang=\"es\" rel=\"alternate\"><link href=\"https://campus.datacamp.com/pt/courses/1606/4135?ex=2\" hreflang=\"pt\" rel=\"alternate\"><link href=\"https://campus.datacamp.com/courses/1606/4135?ex=2\" hreflang=\"x-default\" rel=\"alternate\"></head><body><script>window.PRELOADED_STATE = \"[&quot;~#iR&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;StateRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;backendSession&quot;,[&quot;^ &quot;,&quot;status&quot;,[&quot;^ &quot;,&quot;code&quot;,&quot;none&quot;,&quot;text&quot;,&quot;&quot;],&quot;lastSubmittedCode&quot;,null,&quot;lastSubmittedCommand&quot;,null,&quot;isInitSession&quot;,false,&quot;message&quot;,null,&quot;sessionId&quot;,null],&quot;backendSessionJsonRpc&quot;,[&quot;^ &quot;,&quot;^2&quot;,&quot;initial&quot;],&quot;boot&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;BootStateRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;bootState&quot;,&quot;PRE_BOOTED&quot;,&quot;error&quot;,null]]],&quot;chapter&quot;,[&quot;~#iOM&quot;,[&quot;current&quot;,[&quot;^?&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,1,&quot;number_of_videos&quot;,3,&quot;slug&quot;,&quot;importing-data-from-the-internet-1&quot;,&quot;last_updated_on&quot;,&quot;11/08/2022&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,12,&quot;free_preview&quot;,true,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter1.pdf&quot;,&quot;title&quot;,&quot;Importing data from the Internet&quot;,&quot;xp&quot;,1050,&quot;id&quot;,4135,&quot;exercises&quot;,[&quot;~#iL&quot;,[[&quot;^?&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Importing flat files from the web&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=1&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=3&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Importing non-flat files from the web&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=4&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;HTTP requests to import files from the web&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=5&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=6&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=7&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=8&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Scraping the web in Python&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=9&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=10&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=11&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=12&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]]]],&quot;description&quot;,&quot;The web is a rich source of data from which you can extract various types of insights and findings. In this chapter, you will learn how to get data from the web, whether it is stored in files or in HTML. You&#39;ll also learn the basics of scraping and parsing web data.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]]]],&quot;codeExplanation&quot;,[&quot;^ &quot;,&quot;^A&quot;,[&quot;^ &quot;,&quot;type&quot;,&quot;initial&quot;]],&quot;contentAuthorization&quot;,[&quot;^ &quot;],&quot;datawarehouseSession&quot;,[&quot;^ &quot;,&quot;^2&quot;,&quot;initial&quot;],&quot;course&quot;,[&quot;^?&quot;,[&quot;difficulty_level&quot;,2,&quot;reduced_outline&quot;,null,&quot;course_resources&quot;,[&quot;^@&quot;,[]],&quot;marketing_video&quot;,&quot;&quot;,&quot;tier&quot;,null,&quot;mobile_enabled&quot;,true,&quot;author_field&quot;,null,&quot;chapters&quot;,[&quot;^@&quot;,[[&quot;^?&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,1,&quot;number_of_videos&quot;,3,&quot;slug&quot;,&quot;importing-data-from-the-internet-1&quot;,&quot;last_updated_on&quot;,&quot;11/08/2022&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,12,&quot;free_preview&quot;,true,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter1.pdf&quot;,&quot;title&quot;,&quot;Importing data from the Internet&quot;,&quot;xp&quot;,1050,&quot;id&quot;,4135,&quot;exercises&quot;,[&quot;^@&quot;,[[&quot;^?&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Importing flat files from the web&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=1&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=3&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Importing non-flat files from the web&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=4&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;HTTP requests to import files from the web&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=5&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=6&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=7&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=8&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Scraping the web in Python&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=9&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=10&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=11&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=12&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]]]],&quot;description&quot;,&quot;The web is a rich source of data from which you can extract various types of insights and findings. In this chapter, you will learn how to get data from the web, whether it is stored in files or in HTML. You&#39;ll also learn the basics of scraping and parsing web data.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]],[&quot;^?&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,2,&quot;number_of_videos&quot;,2,&quot;slug&quot;,&quot;interacting-with-apis-to-import-data-from-the-web-2&quot;,&quot;last_updated_on&quot;,&quot;11/08/2022&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,9,&quot;free_preview&quot;,null,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter2.pdf&quot;,&quot;title&quot;,&quot;Interacting with APIs to import data from the web&quot;,&quot;xp&quot;,650,&quot;id&quot;,4136,&quot;exercises&quot;,[&quot;^@&quot;,[[&quot;^?&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Introduction to APIs and JSONs&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=1&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Pop quiz: What exactly is a JSON?&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=2&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Loading and exploring a JSON&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=3&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;MultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Pop quiz: Exploring your JSON&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=4&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;APIs and interacting with the world wide web&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=5&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Pop quiz: What&#39;s an API?&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=6&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;API requests&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=7&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;JSON\\xe2\\x80\\x93from the web to Python&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=8&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Checking out the Wikipedia API&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=9&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]]]],&quot;description&quot;,&quot;In this chapter, you will gain a deeper understanding of how to import data from the web. You will learn the basics of extracting data from APIs, gain insight on the importance of APIs, and practice extracting data by diving into the OMDB and Library of Congress APIs.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]],[&quot;^?&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,3,&quot;number_of_videos&quot;,2,&quot;slug&quot;,&quot;diving-deep-into-the-twitter-api&quot;,&quot;last_updated_on&quot;,&quot;11/08/2022&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,7,&quot;free_preview&quot;,null,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter3.pdf&quot;,&quot;title&quot;,&quot;Diving  deep into the Twitter API&quot;,&quot;xp&quot;,600,&quot;id&quot;,4140,&quot;exercises&quot;,[&quot;^@&quot;,[[&quot;^?&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;The Twitter API and Authentication&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=1&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Streaming tweets&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=2&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Load and explore your Twitter data&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=3&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Twitter data to DataFrame&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=4&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;A little bit of Twitter text analysis&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=5&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Plotting your Twitter data&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=6&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]],[&quot;^?&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Final Thoughts&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=7&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null]]]],&quot;description&quot;,&quot;In this chapter, you will consolidate your knowledge of interacting with APIs in a deep dive into the Twitter streaming API. You&#39;ll learn how to stream real-time Twitter data, and how to analyze and visualize it.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]]]],&quot;time_needed&quot;,null,&quot;author_image&quot;,&quot;https://assets.datacamp.com/production/course_1606/author_images/author_image_course_1606_20200310-1-lgdj4c?1583853939&quot;,&quot;tracks&quot;,[&quot;^@&quot;,[[&quot;^?&quot;,[&quot;path&quot;,&quot;/tracks/data-engineer-in-python&quot;,&quot;title_with_subtitle&quot;,&quot;Data Engineer in Python&quot;]],[&quot;^?&quot;,[&quot;path&quot;,&quot;/tracks/data-scientist-in-python&quot;,&quot;title_with_subtitle&quot;,&quot;Data Scientist in Python&quot;]],[&quot;^?&quot;,[&quot;path&quot;,&quot;/tracks/importing-cleaning-data-with-python&quot;,&quot;title_with_subtitle&quot;,&quot;Importing &amp; Cleaning Data  with Python&quot;]]]],&quot;runtime_config&quot;,null,&quot;lti_only&quot;,false,&quot;image_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/thumb/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;topic_id&quot;,18,&quot;slug&quot;,&quot;intermediate-importing-data-in-python&quot;,&quot;last_updated_on&quot;,&quot;12/08/2024&quot;,&quot;audio_recorders&quot;,[&quot;^@&quot;,[]],&quot;paid&quot;,true,&quot;collaborators&quot;,[&quot;^@&quot;,[[&quot;^?&quot;,[&quot;avatar_url&quot;,&quot;https://assets.datacamp.com/authors/avatars/000/000/058/square/francis-photo.jpg?1705506690&quot;,&quot;full_name&quot;,&quot;Francisco Castro&quot;]]]],&quot;difficulty_level_hardcoded&quot;,null,&quot;time_needed_in_hours&quot;,2,&quot;technology_id&quot;,2,&quot;university&quot;,null,&quot;archived_at&quot;,null,&quot;state&quot;,&quot;live&quot;,&quot;content_area&quot;,&quot;Data Science and Analytics&quot;,&quot;author_bio&quot;,null,&quot;is_labeled_as_new&quot;,null,&quot;should_cache&quot;,true,&quot;sharing_links&quot;,[&quot;^?&quot;,[&quot;twitter&quot;,&quot;http://bit.ly/1eWTMJh&quot;,&quot;facebook&quot;,&quot;http://bit.ly/1iS42Do&quot;]],&quot;instructors&quot;,[&quot;^@&quot;,[[&quot;^?&quot;,[&quot;id&quot;,301837,&quot;marketing_biography&quot;,&quot;Data Scientist&quot;,&quot;biography&quot;,&quot;Hugo is a data scientist, educator, writer and podcaster formerly at DataCamp. His main interests are promoting data &amp; AI literacy, helping to spread data skills through organizations and society and doing amateur stand up comedy in NYC. If you want to know what he likes to talk about, definitely check out &lt;a href=\\\\\\\\&quot;https://www.datacamp.com/community/podcast\\\\\\\\&quot;&gt;DataFramed&lt;/a&gt;, the DataCamp podcast, which he hosted and produced.&quot;,&quot;avatar_url&quot;,&quot;https://assets.datacamp.com/authors/avatars/000/000/006/square/hugoaboutpic.jpg?1705506415&quot;,&quot;full_name&quot;,&quot;Hugo Bowne-Anderson&quot;,&quot;instructor_path&quot;,&quot;/instructors/hugobowne&quot;]]]],&quot;translated_course_id&quot;,1606,&quot;seo_title&quot;,&quot;Intermediate Importing Data in Python&quot;,&quot;industry_ids&quot;,[&quot;^@&quot;,[]],&quot;title&quot;,&quot;Intermediate Importing Data in Python&quot;,&quot;xp&quot;,2300,&quot;image_thumbnail_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/thumb_home/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;short_description&quot;,&quot;Improve your Python data importing skills and learn to work with web and API data.&quot;,&quot;nb_of_subscriptions&quot;,174762,&quot;long_description&quot;,null,&quot;seo_description&quot;,&quot;Learn how to import data into Python from sources like the web and by pulling data from APIs, such as the Twitter streaming API to stream real-time tweets.&quot;,&quot;type&quot;,&quot;datacamp&quot;,&quot;link&quot;,&quot;https://www.datacamp.com/courses/intermediate-importing-data-in-python&quot;,&quot;case_study&quot;,null,&quot;id&quot;,1606,&quot;datasets&quot;,[&quot;^@&quot;,[[&quot;^?&quot;,[&quot;asset_url&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/b422ace2fceada7b569e0ba3e8d833fddc684c4d/latitude.xls&quot;,&quot;name&quot;,&quot;Latitudes (XLS)&quot;]],[&quot;^?&quot;,[&quot;asset_url&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/3ef452f83a91556ea4284624b969392c0506fb33/tweets3.txt&quot;,&quot;name&quot;,&quot;Tweets&quot;]],[&quot;^?&quot;,[&quot;asset_url&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/013936d2700e2d00207ec42100d448c23692eb6f/winequality-red.csv&quot;,&quot;name&quot;,&quot;Red wine quality&quot;]]]],&quot;description&quot;,&quot;As a data scientist, you will need to clean data, wrangle and munge it, visualize it, build predictive models and interpret these models. Before you can do so, however, you will need to know how to get data into Python. In the prequel to this course, you learned many ways to import data into Python: from flat files such as .txt and .csv; from files native to other software such as Excel spreadsheets, Stata, SAS, and MATLAB files; and from relational databases such as SQLite and PostgreSQL. In this course, you&#39;ll extend this knowledge base by learning to import data from the web and by pulling data from Application Programming Interfaces\\xe2\\x80\\x94 APIs\\xe2\\x80\\x94such as the Twitter streaming API, which allows us to stream real-time tweets.&quot;,&quot;prerequisites&quot;,[&quot;^@&quot;,[[&quot;^?&quot;,[&quot;path&quot;,&quot;/courses/introduction-to-importing-data-in-python&quot;,&quot;title&quot;,&quot;Introduction to Importing Data in Python&quot;]]]],&quot;original_image_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/original/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;programming_language&quot;,&quot;python&quot;,&quot;external_slug&quot;,&quot;intermediate-importing-data-in-python&quot;]],&quot;exercises&quot;,[&quot;^?&quot;,[&quot;current&quot;,1,&quot;all&quot;,[&quot;^@&quot;,[[&quot;^?&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;externalId&quot;,990668,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^@&quot;,[]],&quot;runtime_config&quot;,null,&quot;number&quot;,1,&quot;video_hls&quot;,null,&quot;randomNumber&quot;,0.6885205858522894,&quot;chapter_id&quot;,4135,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^@&quot;,[]],&quot;attachments&quot;,null,&quot;exercise_image&quot;,null,&quot;version&quot;,&quot;v0&quot;,&quot;title&quot;,&quot;Importing flat files from the web&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,990668,&quot;projector_key&quot;,&quot;course_1606_59604c018a6e132016cd26144a12fee0&quot;,&quot;video_link&quot;,null,&quot;programming_language&quot;,null,&quot;key&quot;,&quot;e36457c7ed&quot;,&quot;course_id&quot;,1606]],[&quot;^?&quot;,[&quot;sample_code&quot;,&quot;# Import package\\\\\\\\nfrom ____ import ____\\\\\\\\n\\\\\\\\n# Import pandas\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Save file locally\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read file into a DataFrame and print its head\\\\\\\\ndf = pd.read_csv(&#39;winequality-red.csv&#39;, sep=&#39;;&#39;)\\\\\\\\nprint(df.head())&quot;,&quot;sct&quot;,&quot;Ex().has_import(\\\\\\\\&quot;urllib.request.urlretrieve\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;pandas\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlretrieve\\\\\\\\&quot;).multi(\\\\\\\\n  check_args(0).has_equal_value(),\\\\\\\\n  check_args(1).has_equal_value()\\\\\\\\n)\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;df\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;pandas.read_csv\\\\\\\\&quot;).multi(\\\\\\\\n    check_args(0).has_equal_value(),\\\\\\\\n    check_args(1).has_equal_value()\\\\\\\\n  )\\\\\\\\n)\\\\\\\\nEx().has_printout(0)\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the function &lt;code&gt;urlretrieve&lt;/code&gt; from the subpackage &lt;code&gt;urllib.request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the function &lt;code&gt;urlretrieve()&lt;/code&gt; to save the file locally as &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Execute the remaining code to load &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; in a pandas DataFrame and to print its head to the shell.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42707,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a function &lt;code&gt;y&lt;/code&gt; from a subpackage &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;from x import y&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;This one&#39;s a long URL. Make sure you typed it in correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; to import (in the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and the &lt;em&gt;filename&lt;/em&gt; for saving the file locally as the second argument to &lt;code&gt;urlretrieve()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to change the code for loading &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; and printing its head.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^@&quot;,[]],&quot;runtime_config&quot;,null,&quot;number&quot;,2,&quot;user&quot;,[&quot;^?&quot;,[&quot;isHintShown&quot;,false,&quot;usedAiFeatures&quot;,[&quot;^?&quot;,[&quot;aiIncorrectSubmissions&quot;,false,&quot;aiErrorExplanations&quot;,false]],&quot;lastRunCode&quot;,null,&quot;editorTabs&quot;,[&quot;^?&quot;,[&quot;files/script.py&quot;,[&quot;^?&quot;,[&quot;title&quot;,&quot;script.py&quot;,&quot;isSolution&quot;,false,&quot;props&quot;,[&quot;^?&quot;,[&quot;active&quot;,true,&quot;isClosable&quot;,false,&quot;code&quot;,null,&quot;extra&quot;,[&quot;^?&quot;,[]]]]]]]],&quot;outputMarkdownTabs&quot;,[&quot;^?&quot;,[]],&quot;markdown&quot;,[&quot;^?&quot;,[&quot;titles&quot;,[&quot;^@&quot;,[&quot;Knit PDF&quot;,&quot;Knit HTML&quot;]],&quot;activeTitle&quot;,&quot;Knit HTML&quot;]],&quot;currentXp&quot;,100,&quot;graphicalTabs&quot;,[&quot;^?&quot;,[&quot;plot&quot;,[&quot;^?&quot;,[&quot;extraClass&quot;,&quot;animation--flash&quot;,&quot;title&quot;,&quot;Plots&quot;,&quot;props&quot;,[&quot;^?&quot;,[&quot;sources&quot;,[&quot;^@&quot;,[]],&quot;currentIndex&quot;,0]],&quot;dimension&quot;,[&quot;^?&quot;,[&quot;isRealSize&quot;,false,&quot;width&quot;,1,&quot;height&quot;,1]]]],&quot;html&quot;,[&quot;^?&quot;,[&quot;extraClass&quot;,&quot;animation--flash&quot;,&quot;title&quot;,&quot;HTML Viewer&quot;,&quot;props&quot;,[&quot;^?&quot;,[&quot;sources&quot;,[&quot;^@&quot;,[]],&quot;currentIndex&quot;,0]]]]]],&quot;feedbackMessages&quot;,[&quot;^@&quot;,[]],&quot;lastSubmittedCode&quot;,null,&quot;ltiStatus&quot;,[&quot;^?&quot;,[]],&quot;lastSubmitActiveEditorTab&quot;,null,&quot;consoleSqlTabs&quot;,[&quot;^?&quot;,[&quot;query_result&quot;,[&quot;^?&quot;,[&quot;extraClass&quot;,&quot;&quot;,&quot;title&quot;,&quot;query result&quot;,&quot;props&quot;,[&quot;^?&quot;,[&quot;active&quot;,true,&quot;isNotView&quot;,true,&quot;message&quot;,&quot;No query executed yet...&quot;]]]]]],&quot;consoleTabs&quot;,[&quot;^?&quot;,[&quot;console&quot;,[&quot;^?&quot;,[&quot;title&quot;,&quot;IPython Shell&quot;,&quot;props&quot;,[&quot;^?&quot;,[&quot;active&quot;,true]],&quot;dimension&quot;,[&quot;^?&quot;,[&quot;cols&quot;,400]]]],&quot;slides&quot;,[&quot;^?&quot;,[&quot;title&quot;,&quot;Slides&quot;,&quot;props&quot;,[&quot;^?&quot;,[&quot;active&quot;,false]]]]]],&quot;inputMarkdownTabs&quot;,[&quot;^?&quot;,[]],&quot;consoleObjectViewTabs&quot;,[&quot;^?&quot;,[]]]],&quot;randomNumber&quot;,0.9822140964849919,&quot;assignment&quot;,&quot;&lt;p&gt;You are about to import your first file from the web! The flat file you will import will be &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; from the University of California, Irvine&#39;s &lt;a href=\\\\\\\\&quot;https://archive.ics.uci.edu/ml/index.php\\\\\\\\&quot;&gt;Machine Learning repository&lt;/a&gt;. The flat file contains tabular data of physiochemical properties of red wine, such as pH, alcohol content and citric acid content, along with wine quality rating.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the file is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;\\\\\\\\n&lt;p&gt;After you import it, you&#39;ll check your working directory to confirm that it is there and then you&#39;ll load it into a &lt;code&gt;pandas&lt;/code&gt; DataFrame.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^@&quot;,[]],&quot;attachments&quot;,null,&quot;exercise_image&quot;,null,&quot;title&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import package\\\\\\\\nfrom urllib.request import urlretrieve\\\\\\\\n\\\\\\\\n# Import pandas\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n\\\\\\\\n# Save file locally\\\\\\\\nurlretrieve(url, &#39;winequality-red.csv&#39;)\\\\\\\\n\\\\\\\\n# Read file into a DataFrame and print its head\\\\\\\\ndf = pd.read_csv(&#39;winequality-red.csv&#39;, sep=&#39;;&#39;)\\\\\\\\nprint(df.head())&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42707,&quot;programming_language&quot;,null]],[&quot;^?&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\nimport matplotlib.pyplot as plt\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read file into a DataFrame: df\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the head of the DataFrame\\\\\\\\nprint(____)\\\\\\\\n\\\\\\\\n# Plot first column of df\\\\\\\\ndf.iloc[:, 0].hist()\\\\\\\\nplt.xlabel(&#39;fixed acidity (g(tartaric acid)/dm$^3$)&#39;)\\\\\\\\nplt.ylabel(&#39;count&#39;)\\\\\\\\nplt.show()\\\\\\\\n&quot;,&quot;sct&quot;,&quot;Ex().has_import(\\\\\\\\&quot;matplotlib.pyplot\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;pandas\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;df\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;pandas.read_csv\\\\\\\\&quot;).multi(\\\\\\\\n    check_args(0).has_equal_value(),\\\\\\\\n    check_args(1).has_equal_value()\\\\\\\\n  )\\\\\\\\n)\\\\\\\\nEx().has_printout(0)\\\\\\\\nEx().has_equal_ast(code=\\\\\\\\&quot;df.iloc[:, 0].hist\\\\\\\\&quot;, incorrect_msg=\\\\\\\\&quot;Please do not change the code to plot the histogram.\\\\\\\\&quot;)\\\\\\\\nEx().check_function(\\\\\\\\&quot;matplotlib.pyplot.show\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Read file into a DataFrame &lt;code&gt;df&lt;/code&gt; using &lt;code&gt;pd.read_csv()&lt;/code&gt;, recalling that the separator in the file is &lt;code&gt;&#39;;&#39;&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the head of the DataFrame &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Execute the rest of the code to plot histogram of the first feature in the DataFrame &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42708,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Make sure you typed the URL correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and the &lt;em&gt;separator&lt;/em&gt; as the second argument to &lt;code&gt;pd.read_csv()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The &lt;em&gt;head&lt;/em&gt; of a DataFrame can be accessed by using &lt;code&gt;head()&lt;/code&gt; on the DataFrame.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to change any of the code for plotting the histograms.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^@&quot;,[]],&quot;runtime_config&quot;,null,&quot;number&quot;,3,&quot;randomNumber&quot;,0.9313110883135951,&quot;assignment&quot;,&quot;&lt;p&gt;You have just imported a file from the web, saved it locally and loaded it into a DataFrame. If you just wanted to load a file from the web into a DataFrame without first saving it locally, you can do that easily using &lt;code&gt;pandas&lt;/code&gt;. In particular, you can use the function &lt;code&gt;pd.read_csv()&lt;/code&gt; with the URL as the first argument and the separator &lt;code&gt;sep&lt;/code&gt; as the second argument.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the file, once again, is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;&quot;,&quot;feedbacks&quot;,[&quot;^@&quot;,[]],&quot;attachments&quot;,null,&quot;exercise_image&quot;,null,&quot;title&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nimport matplotlib.pyplot as plt\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n\\\\\\\\n# Read file into a DataFrame: df\\\\\\\\ndf = pd.read_csv(url, sep=&#39;;&#39;)\\\\\\\\n\\\\\\\\n# Print the head of the DataFrame\\\\\\\\nprint(df.head())\\\\\\\\n\\\\\\\\n# Plot first column of df\\\\\\\\ndf.iloc[:, 0].hist()\\\\\\\\nplt.xlabel(&#39;fixed acidity (g(tartaric acid)/dm$^3$)&#39;)\\\\\\\\nplt.ylabel(&#39;count&#39;)\\\\\\\\nplt.show()\\\\\\\\n&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42708,&quot;programming_language&quot;,null]],[&quot;^?&quot;,[&quot;sample_code&quot;,&quot;# Import package\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read in all sheets of Excel file: xls\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the sheetnames to the shell\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the head of the first sheet (using its name, NOT its index)\\\\\\\\n\\\\\\\\n&quot;,&quot;sct&quot;,&quot;Ex().has_import(&#39;pandas&#39;)\\\\\\\\nEx().check_correct(\\\\\\\\n    has_printout(0),\\\\\\\\n    multi(\\\\\\\\n        check_correct(\\\\\\\\n            check_object(&#39;xls&#39;).is_instance(dict),\\\\\\\\n            check_correct(\\\\\\\\n                check_function(&#39;pandas.read_excel&#39;).multi(\\\\\\\\n                    check_args(0).has_equal_value(),\\\\\\\\n                    check_args(&#39;sheet_name&#39;).has_equal_value()\\\\\\\\n                ),\\\\\\\\n                check_object(&#39;url&#39;).has_equal_value()\\\\\\\\n            )\\\\\\\\n        )\\\\\\\\n    )\\\\\\\\n)\\\\\\\\nEx().has_printout(1)\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Read the file in &lt;code&gt;url&lt;/code&gt; into a dictionary &lt;code&gt;xls&lt;/code&gt; using &lt;code&gt;pd.read_excel()&lt;/code&gt; recalling that, in order to import all sheets you need to pass &lt;code&gt;None&lt;/code&gt; to the argument &lt;code&gt;sheet_name&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the names of the sheets in the Excel spreadsheet; these will be the keys of the dictionary &lt;code&gt;xls&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the head of the first sheet &lt;em&gt;using the sheet name, not the index of the sheet&lt;/em&gt;! The sheet name is &lt;code&gt;&#39;1700&#39;&lt;/code&gt;&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42709,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Make sure you typed in the URL correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and &lt;code&gt;sheet_name&lt;/code&gt; with its corresponding value as the second argument to &lt;code&gt;pd.read_excel()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The &lt;em&gt;keys&lt;/em&gt; of a dictionary can be accessed by using &lt;code&gt;keys()&lt;/code&gt; on the dictionary.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access a sheet using the format: &lt;em&gt;dictionary&lt;/em&gt;&lt;strong&gt;[&lt;/strong&gt;&lt;em&gt;sheet name or index&lt;/em&gt;&lt;strong&gt;]&lt;/strong&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^@&quot;,[]],&quot;runtime_config&quot;,null,&quot;number&quot;,4,&quot;randomNumber&quot;,0.051626335915768706,&quot;assignment&quot;,&quot;&lt;p&gt;Congrats! You&#39;ve just loaded a flat file from the web into a DataFrame without first saving it locally using the &lt;code&gt;pandas&lt;/code&gt; function &lt;code&gt;pd.read_csv()&lt;/code&gt;. This function is super cool because it has close relatives that allow you to load all types of files, not only flat ones. In this interactive exercise, you&#39;ll use &lt;code&gt;pd.read_excel()&lt;/code&gt; to import an Excel spreadsheet.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the spreadsheet is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;https://assets.datacamp.com/course/importing_data_into_r/latitude.xls&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;\\\\\\\\n&lt;p&gt;Your job is to use &lt;code&gt;pd.read_excel()&lt;/code&gt; to read in all of its sheets, print the sheet names and then print the head of the first sheet &lt;em&gt;using its name, not its index&lt;/em&gt;.&lt;/p&gt;\\\\\\\\n&lt;p&gt;Note that the output of &lt;code&gt;pd.read_excel()&lt;/code&gt; is a Python dictionary with sheet names as keys and corresponding DataFrames as corresponding values.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^@&quot;,[]],&quot;attachments&quot;,null,&quot;exercise_image&quot;,null,&quot;title&quot;,&quot;Importing non-flat files from the web&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import package\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;https://assets.datacamp.com/course/importing_data_into_r/latitude.xls&#39;\\\\\\\\n\\\\\\\\n# Read in all sheets of Excel file: xls\\\\\\\\nxls = pd.read_excel(url, sheet_name=None)\\\\\\\\n\\\\\\\\n# Print the sheetnames to the shell\\\\\\\\nprint(xls.keys())\\\\\\\\n\\\\\\\\n# Print the head of the first sheet (using its name, NOT its index)\\\\\\\\nprint(xls[&#39;1700&#39;].head())&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42709,&quot;programming_language&quot;,null]],[&quot;^?&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;externalId&quot;,990669,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^@&quot;,[]],&quot;runtime_config&quot;,null,&quot;number&quot;,5,&quot;video_hls&quot;,null,&quot;randomNumber&quot;,0.576175861463001,&quot;chapter_id&quot;,4135,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^@&quot;,[]],&quot;attachments&quot;,null,&quot;exercise_image&quot;,null,&quot;version&quot;,&quot;v0&quot;,&quot;title&quot;,&quot;HTTP requests to import files from the web&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,990669,&quot;projector_key&quot;,&quot;course_1606_9d15ae176be1800b996f7869a82b8087&quot;,&quot;video_link&quot;,null,&quot;programming_language&quot;,null,&quot;key&quot;,&quot;e480d1fdcf&quot;,&quot;course_id&quot;,1606]],[&quot;^?&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\n\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request: request\\\\\\\\n\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the datatype of response\\\\\\\\nprint(type(response))\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()\\\\\\\\n&quot;,&quot;sct&quot;,&quot;\\\\\\\\n# Test: import urlopen, Request\\\\\\\\nimport_msg = \\\\\\\\&quot;Did you correctly import the required packages?\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;, missing_msg=predef_msg).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\n\\\\\\\\n# Test: call to Request() and &#39;request&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;request\\\\\\\\&quot;)\\\\\\\\n  \\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;response\\\\\\\\&quot;),\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(0)\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.close\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the functions &lt;code&gt;urlopen&lt;/code&gt; and &lt;code&gt;Request&lt;/code&gt; from the subpackage &lt;code&gt;urllib.request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the url &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt; using the function &lt;code&gt;Request()&lt;/code&gt; and assign it to &lt;code&gt;request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Send the request and catch the response in the variable &lt;code&gt;response&lt;/code&gt; with  the function &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Run the rest of the code to see the datatype of &lt;code&gt;response&lt;/code&gt; and to close the connection!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42711,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import two functions in one line, import the first function as usual and add a comma &lt;code&gt;,&lt;/code&gt; followed by the second function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (already in the &lt;code&gt;url&lt;/code&gt; object defined) as an argument to &lt;code&gt;Request()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass &lt;code&gt;request&lt;/code&gt; as an argument to &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for printing the datatype of &lt;code&gt;response&lt;/code&gt; and closing the connection.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^@&quot;,[]],&quot;runtime_config&quot;,null,&quot;number&quot;,6,&quot;randomNumber&quot;,0.2948355478242206,&quot;assignment&quot;,&quot;&lt;p&gt;Now that you know the basics behind HTTP GET requests, it&#39;s time to perform some of your own. In this interactive exercise, you will ping our very own DataCamp servers to perform a GET request to extract information from the first coding exercise of this course, &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt;.&lt;/p&gt;\\\\\\\\n&lt;p&gt;In the next exercise, you&#39;ll extract the HTML itself. Right now, however, you are going to package and send the request and then catch the response.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^@&quot;,[]],&quot;attachments&quot;,null,&quot;exercise_image&quot;,null,&quot;title&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request: request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\nresponse = urlopen(request)\\\\\\\\n\\\\\\\\n# Print the datatype of response\\\\\\\\nprint(type(response))\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()\\\\\\\\n&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42711,&quot;programming_language&quot;,null]],[&quot;^?&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extract the response: html\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\n\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()&quot;,&quot;sct&quot;,&quot;\\\\\\\\n# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to Request() and &#39;request&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;request\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;response\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.read\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;html\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to print()\\\\\\\\nEx().check_function(&#39;print&#39;).check_args(0).has_equal_ast()\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.close\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Send the request and catch the response in the variable &lt;code&gt;response&lt;/code&gt; with the function &lt;code&gt;urlopen()&lt;/code&gt;, as in the previous exercise.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the response using the &lt;code&gt;read()&lt;/code&gt; method and store the result in the variable &lt;code&gt;html&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the string &lt;code&gt;html&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to perform all of the above and to close the response: be tidy!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42712,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass &lt;code&gt;request&lt;/code&gt; as an argument to &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Apply the method &lt;code&gt;read()&lt;/code&gt; to the response object &lt;code&gt;response&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Simply pass &lt;code&gt;html&lt;/code&gt; to the &lt;code&gt;print()&lt;/code&gt; function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for closing the response.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^@&quot;,[]],&quot;runtime_config&quot;,null,&quot;number&quot;,7,&quot;randomNumber&quot;,0.5669930325167178,&quot;assignment&quot;,&quot;&lt;p&gt;You have just packaged and sent a GET request to &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt; and then caught the response. You saw that such a response is a &lt;code&gt;http.client.HTTPResponse&lt;/code&gt; object. The question remains: what can you do with this response?&lt;/p&gt;\\\\\\\\n&lt;p&gt;Well, as it came from an HTML page, you could &lt;em&gt;read&lt;/em&gt; it to extract the HTML and, in fact, such a &lt;code&gt;http.client.HTTPResponse&lt;/code&gt; object has an associated &lt;code&gt;read()&lt;/code&gt; method. In this exercise, you&#39;ll build on your previous great work to extract the response and print the HTML.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^@&quot;,[]],&quot;attachments&quot;,null,&quot;exercise_image&quot;,null,&quot;title&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\nresponse = urlopen(request)\\\\\\\\n\\\\\\\\n# Extract the response: html\\\\\\\\nhtml = response.read()\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(html)\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42712,&quot;programming_language&quot;,null]],[&quot;^?&quot;,[&quot;sample_code&quot;,&quot;# Import package\\\\\\\\n\\\\\\\\n\\\\\\\\n# Specify the url: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Packages the request, send the request and catch the response: r\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extract the response: text\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(text)&quot;,&quot;sct&quot;,&quot;\\\\\\\\n# Test: import requests\\\\\\\\nEx().has_import(\\\\\\\\&quot;requests\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: &#39;text&#39; variable\\\\\\\\nEx().has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `text`?\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;text\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().check_function(&#39;print&#39;).check_args(0).has_equal_ast()\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the package &lt;code&gt;requests&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of interest to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the URL, send the request and catch the response with a single function &lt;code&gt;requests.get()&lt;/code&gt;, assigning the response to the variable &lt;code&gt;r&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; to return the HTML of the webpage as a string; store the result in a variable &lt;code&gt;text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print the HTML of the webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42713,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a package &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;import x&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Did you type in the URL correctly?&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as an argument to &lt;code&gt;requests.get()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; by executing &lt;code&gt;r.text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for printing the HTML of the webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^@&quot;,[]],&quot;runtime_config&quot;,null,&quot;number&quot;,8,&quot;randomNumber&quot;,0.1033626497550677,&quot;assignment&quot;,&quot;&lt;p&gt;Now that you&#39;ve got your head and hands around making HTTP requests using the urllib package, you&#39;re going to figure out how to do the same using the higher-level requests library. You&#39;ll once again be pinging DataCamp servers for their &lt;code&gt;\\\\\\\\&quot;http://www.datacamp.com/teach/documentation\\\\\\\\&quot;&lt;/code&gt; page.&lt;/p&gt;\\\\\\\\n&lt;p&gt;Note that unlike in the previous exercises using urllib, you don&#39;t have to close the connection when using requests!&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^@&quot;,[]],&quot;attachments&quot;,null,&quot;exercise_image&quot;,null,&quot;title&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import package\\\\\\\\nimport requests\\\\\\\\n\\\\\\\\n# Specify the url: url\\\\\\\\nurl = \\\\\\\\&quot;http://www.datacamp.com/teach/documentation\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# Packages the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response: text\\\\\\\\ntext = r.text\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(text)&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42713,&quot;programming_language&quot;,null]],[&quot;^?&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;externalId&quot;,990670,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^@&quot;,[]],&quot;runtime_config&quot;,null,&quot;number&quot;,9,&quot;video_hls&quot;,null,&quot;randomNumber&quot;,0.8648626013681029,&quot;chapter_id&quot;,4135,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^@&quot;,[]],&quot;attachments&quot;,null,&quot;exercise_image&quot;,null,&quot;version&quot;,&quot;v0&quot;,&quot;title&quot;,&quot;Scraping the web in Python&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,990670,&quot;projector_key&quot;,&quot;course_1606_9d1f8a331d1200c7e1bdbfcaf3a7a491&quot;,&quot;video_link&quot;,null,&quot;programming_language&quot;,null,&quot;key&quot;,&quot;da43858012&quot;,&quot;course_id&quot;,1606]],[&quot;^?&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom ____ import ____\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\n\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Prettify the BeautifulSoup object: pretty_soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the response\\\\\\\\nprint(pretty_soup)&quot;,&quot;sct&quot;,&quot;# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;requests\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: import BeautifulSoup\\\\\\\\nimport_msg = \\\\\\\\&quot;Did you correctly import the required packages?\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n\\\\\\\\n# Test: &#39;html_doc&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `html_doc`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to BeautifulSoup() and &#39;soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: call to prettify() and &#39;pretty_soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;pretty_soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;soup.prettify\\\\\\\\&quot;)\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the function &lt;code&gt;BeautifulSoup&lt;/code&gt; from the package &lt;code&gt;bs4&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of interest to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the URL, send the request and catch the response with a single function &lt;code&gt;requests.get()&lt;/code&gt;, assigning the response to the variable &lt;code&gt;r&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; to return the HTML of the webpage as a string; store the result in a variable &lt;code&gt;html_doc&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Create a BeautifulSoup object &lt;code&gt;soup&lt;/code&gt; from the resulting HTML using the function &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;prettify()&lt;/code&gt; on &lt;code&gt;soup&lt;/code&gt; and assign the result to &lt;code&gt;pretty_soup&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print to prettified HTML to your shell!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42715,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a function &lt;code&gt;y&lt;/code&gt; from a package &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;from x import y&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Check the URL to make sure that you typed it in correctly.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as an argument to &lt;code&gt;requests.get()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; by executing &lt;code&gt;r.text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the extracted &lt;em&gt;HTML&lt;/em&gt; as an argument to &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;To use the &lt;code&gt;prettify()&lt;/code&gt; method on the BeautifulSoup object &lt;code&gt;soup&lt;/code&gt;, execute &lt;code&gt;soup.prettify()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code to print the prettified HTML.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^@&quot;,[]],&quot;runtime_config&quot;,null,&quot;number&quot;,10,&quot;randomNumber&quot;,0.22794282309824188,&quot;assignment&quot;,&quot;&lt;p&gt;In this interactive exercise, you&#39;ll learn how to use the BeautifulSoup package to &lt;em&gt;parse&lt;/em&gt;, &lt;em&gt;prettify&lt;/em&gt; and &lt;em&gt;extract&lt;/em&gt; information from HTML. You&#39;ll scrape the data from the webpage of Guido van Rossum, Python&#39;s very own &lt;a href=\\\\\\\\&quot;https://en.wikipedia.org/wiki/Benevolent_dictator_for_life\\\\\\\\&quot;&gt;Benevolent Dictator for Life&lt;/a&gt;. In the following exercises, you&#39;ll prettify the HTML and then extract the text and the hyperlinks.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of interest is &lt;code&gt;url = &#39;https://www.python.org/~guido/&#39;&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^@&quot;,[]],&quot;attachments&quot;,null,&quot;exercise_image&quot;,null,&quot;title&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Prettify the BeautifulSoup object: pretty_soup\\\\\\\\npretty_soup = soup.prettify()\\\\\\\\n\\\\\\\\n# Print the response\\\\\\\\nprint(pretty_soup)&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42715,&quot;programming_language&quot;,null]],[&quot;^?&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Get the title of Guido&#39;s webpage: guido_title\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage to the shell\\\\\\\\n\\\\\\\\n\\\\\\\\n# Get Guido&#39;s text: guido_text\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print Guido&#39;s text to the shell\\\\\\\\nprint(guido_text)&quot;,&quot;sct&quot;,&quot;# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;requests\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: import BeautifulSoup\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n\\\\\\\\n# Test: &#39;html_doc&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `html_doc`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to BeautifulSoup() and &#39;soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: &#39;guido_title&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;guido_title\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;soup.title\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `soup.title` to create `guido_title`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to print()\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\n# Test: call to soup.get_text() and &#39;guido_text&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;guido_text\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;soup.get_text\\\\\\\\&quot;)\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(1)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;In the sample code, the HTML response object &lt;code&gt;html_doc&lt;/code&gt; has already been created: your first task is to Soupify it using the function &lt;code&gt;BeautifulSoup()&lt;/code&gt; and to assign the resulting soup to the variable &lt;code&gt;soup&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the title from the HTML soup &lt;code&gt;soup&lt;/code&gt; using the attribute &lt;code&gt;title&lt;/code&gt; and assign the result to &lt;code&gt;guido_title&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the title of Guido&#39;s webpage to the shell using the &lt;code&gt;print()&lt;/code&gt; function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the text from the HTML soup &lt;code&gt;soup&lt;/code&gt; using the method &lt;code&gt;get_text()&lt;/code&gt; and assign to &lt;code&gt;guido_text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print the text from Guido&#39;s webpage to the shell.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42716,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;HTML response object&lt;/em&gt; as an argument to &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;title&lt;/code&gt; attribute of the object &lt;code&gt;soup&lt;/code&gt; by executing &lt;code&gt;soup.title&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The object that contains the title of Guido&#39;s webpage is &lt;code&gt;guido_title&lt;/code&gt;; pass this as an argument to &lt;code&gt;print()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;get_text()&lt;/code&gt; on the HTML soup &lt;code&gt;soup&lt;/code&gt; by executing &lt;code&gt;soup.get_text()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code to print the text from Guido&#39;s webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^@&quot;,[]],&quot;runtime_config&quot;,null,&quot;number&quot;,11,&quot;randomNumber&quot;,0.6888273169376504,&quot;assignment&quot;,&quot;&lt;p&gt;As promised, in the following exercises, you&#39;ll learn the basics of extracting information from HTML soup. In this exercise, you&#39;ll figure out how to extract the text from the BDFL&#39;s webpage, along with printing the webpage&#39;s title.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^@&quot;,[]],&quot;attachments&quot;,null,&quot;exercise_image&quot;,null,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Get the title of Guido&#39;s webpage: guido_title\\\\\\\\nguido_title = soup.title\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage to the shell\\\\\\\\nprint(guido_title)\\\\\\\\n\\\\\\\\n# Get Guido&#39;s text: guido_text\\\\\\\\nguido_text = soup.get_text()\\\\\\\\n\\\\\\\\n# Print Guido&#39;s text to the shell\\\\\\\\nprint(guido_text)&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42716,&quot;programming_language&quot;,null]],[&quot;^?&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage\\\\\\\\nprint(soup.title)\\\\\\\\n\\\\\\\\n# Find all &#39;a&#39; tags (which define hyperlinks): a_tags\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the URLs to the shell\\\\\\\\nfor ____ in ____:\\\\\\\\n    ____&quot;,&quot;sct&quot;,&quot;predef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\&quot;requests\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\nEx().check_correct(\\\\\\\\n    check_object(\\\\\\\\&quot;a_tags\\\\\\\\&quot;),\\\\\\\\n    check_function(\\\\\\\\&quot;soup.find_all\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n)\\\\\\\\nEx().check_for_loop().multi(\\\\\\\\n        check_iter().has_equal_value(incorrect_msg = \\\\\\\\&quot;You have to iterate over `a_tags`\\\\\\\\&quot;),\\\\\\\\n        check_body().set_context(&#39;&lt;a href=\\\\\\\\&quot;pics.html\\\\\\\\&quot;&gt;&lt;img border=\\\\\\\\&quot;0\\\\\\\\&quot; src=\\\\\\\\&quot;images/IMG_2192.jpg\\\\\\\\&quot;/&gt;&lt;/a&gt;&#39;).check_function(\\\\\\\\&quot;print\\\\\\\\&quot;).check_args(0).check_function(\\\\\\\\&quot;link.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n    )\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;find_all()&lt;/code&gt; to find all hyperlinks in &lt;code&gt;soup&lt;/code&gt;, remembering that hyperlinks are defined by the HTML tag &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt; but passed to &lt;code&gt;find_all()&lt;/code&gt; without angle brackets; store the result in the variable &lt;code&gt;a_tags&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The variable &lt;code&gt;a_tags&lt;/code&gt; is a results set: your job now is to enumerate over it, using a &lt;code&gt;for&lt;/code&gt; loop and to print the actual URLs of the hyperlinks; to do this, for every element &lt;code&gt;link&lt;/code&gt; in &lt;code&gt;a_tags&lt;/code&gt;, you want to &lt;code&gt;print()&lt;/code&gt; &lt;code&gt;link.get(&#39;href&#39;)&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42717,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;HTML tag&lt;/em&gt; to find (without the angle brackets &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt;) as a string argument to &lt;code&gt;find_all()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Recall that the &lt;code&gt;for&lt;/code&gt; loop recipe is: &lt;code&gt;for&lt;/code&gt; &lt;em&gt;loop variable&lt;/em&gt; &lt;code&gt;in&lt;/code&gt; &lt;em&gt;results set&lt;/em&gt;&lt;code&gt;:&lt;/code&gt;. Don&#39;t forget to pass &lt;code&gt;link.get(&#39;href&#39;)&lt;/code&gt; as an argument to &lt;code&gt;print()&lt;/code&gt; inside the &lt;code&gt;for&lt;/code&gt; loop body.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^@&quot;,[]],&quot;runtime_config&quot;,null,&quot;number&quot;,12,&quot;randomNumber&quot;,0.22284468403192004,&quot;assignment&quot;,&quot;&lt;p&gt;In this exercise, you&#39;ll figure out how to extract the URLs of the hyperlinks from the BDFL&#39;s webpage. In the process, you&#39;ll become close friends with the soup method &lt;code&gt;find_all()&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^@&quot;,[]],&quot;attachments&quot;,null,&quot;exercise_image&quot;,null,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage\\\\\\\\nprint(soup.title)\\\\\\\\n\\\\\\\\n# Find all &#39;a&#39; tags (which define hyperlinks): a_tags\\\\\\\\na_tags = soup.find_all(&#39;a&#39;)\\\\\\\\n\\\\\\\\n# Print the URLs to the shell\\\\\\\\nfor link in a_tags:\\\\\\\\n    print(link.get(&#39;href&#39;))&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42717,&quot;programming_language&quot;,null]]]],&quot;canRateChapter&quot;,false,&quot;isChapterCompleted&quot;,false]],&quot;learningMode&quot;,&quot;course&quot;,&quot;learningRecap&quot;,null,&quot;location&quot;,[&quot;^?&quot;,[&quot;current&quot;,[&quot;^?&quot;,[&quot;pathname&quot;,&quot;/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1&quot;,&quot;query&quot;,[&quot;^?&quot;,[&quot;ex&quot;,&quot;2&quot;]]]],&quot;language&quot;,&quot;en&quot;,&quot;canonical&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2&quot;,&quot;before&quot;,[&quot;^?&quot;,[&quot;pathname&quot;,&quot;/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1&quot;,&quot;query&quot;,[&quot;^?&quot;,[&quot;ex&quot;,&quot;2&quot;]]]]]],&quot;mobilePopup&quot;,[&quot;^?&quot;,[]],&quot;onboardingMilestones&quot;,[&quot;^ &quot;,&quot;isStarted&quot;,false,&quot;isActive&quot;,true,&quot;step&quot;,0],&quot;notes&quot;,[&quot;^ &quot;,&quot;workspaceNotes&quot;,null,&quot;workspaceTemplate&quot;,[&quot;^ &quot;,&quot;_tag&quot;,&quot;template&quot;,&quot;id&quot;,3046,&quot;createdAt&quot;,&quot;2022-12-01T15:10:45.124Z&quot;,&quot;updatedAt&quot;,&quot;2022-12-05T06:31:46.338Z&quot;,&quot;key&quot;,&quot;course-dataset-intermediate-importing-data-in-python&quot;,&quot;language&quot;,&quot;Python&quot;,&quot;languageVersion&quot;,null,&quot;title&quot;,&quot;Intermediate Importing Data in Python&quot;,&quot;description&quot;,&quot;Explore the datasets from the course, Intermediate Importing Data in Python.&quot;,&quot;listed&quot;,false,&quot;latestVersion&quot;,&quot;d9783ad9a9e677a4f583a5bd9ed5b9d5987a2859&quot;,&quot;communitySlug&quot;,null,&quot;category&quot;,null,&quot;templateGroupKey&quot;,null,&quot;previewPublicationId&quot;,&quot;ade0176d-8e1d-436c-b7c1-44c3f4f1df8f&quot;,&quot;labels&quot;,[&quot;course-dataset&quot;],&quot;courseId&quot;,1606,&quot;integrationIds&quot;,[],&quot;publicationScreenshot&quot;,null]],&quot;output&quot;,[&quot;^ &quot;,&quot;lastErrorMessage&quot;,null,&quot;^17&quot;,[]],&quot;preFetchedData&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;PreFetchedDataStateRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;^&gt;&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;PreFetchedRequestRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;^2&quot;,&quot;SUCCESS&quot;,&quot;data&quot;,[&quot;^ &quot;,&quot;id&quot;,4135,&quot;title_meta&quot;,null,&quot;^W&quot;,&quot;Importing data from the Internet&quot;,&quot;^X&quot;,&quot;The web is a rich source of data from which you can extract various types of insights and findings. In this chapter, you will learn how to get data from the web, whether it is stored in files or in HTML. You&#39;ll also learn the basics of scraping and parsing web data.&quot;,&quot;number&quot;,1,&quot;slug&quot;,&quot;importing-data-from-the-internet-1&quot;,&quot;nb_exercises&quot;,12,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;last_updated_on&quot;,&quot;11/08/2022&quot;,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter1.pdf&quot;,&quot;free_preview&quot;,true,&quot;xp&quot;,1050,&quot;number_of_videos&quot;,3,&quot;^F&quot;,[[&quot;^ &quot;,&quot;^B&quot;,&quot;VideoExercise&quot;,&quot;^W&quot;,&quot;Importing flat files from the web&quot;,&quot;aggregate_xp&quot;,50,&quot;^1&lt;&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=1&quot;,&quot;exercise_image&quot;,null,&quot;programming_language&quot;,null,&quot;runtime_config&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=3&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Importing non-flat files from the web&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=4&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;VideoExercise&quot;,&quot;^W&quot;,&quot;HTTP requests to import files from the web&quot;,&quot;^1E&quot;,50,&quot;^1&lt;&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=5&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=6&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=7&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=8&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;VideoExercise&quot;,&quot;^W&quot;,&quot;Scraping the web in Python&quot;,&quot;^1E&quot;,50,&quot;^1&lt;&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=9&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=10&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=11&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=12&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null]]]]]],&quot;^E&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;PreFetchedRequestRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;^2&quot;,&quot;SUCCESS&quot;,&quot;^1:&quot;,[&quot;^ &quot;,&quot;id&quot;,1606,&quot;^W&quot;,&quot;Intermediate Importing Data in Python&quot;,&quot;^X&quot;,&quot;As a data scientist, you will need to clean data, wrangle and munge it, visualize it, build predictive models and interpret these models. Before you can do so, however, you will need to know how to get data into Python. In the prequel to this course, you learned many ways to import data into Python: from flat files such as .txt and .csv; from files native to other software such as Excel spreadsheets, Stata, SAS, and MATLAB files; and from relational databases such as SQLite and PostgreSQL. In this course, you&#39;ll extend this knowledge base by learning to import data from the web and by pulling data from Application Programming Interfaces\\xe2\\x80\\x94 APIs\\xe2\\x80\\x94such as the Twitter streaming API, which allows us to stream real-time tweets.&quot;,&quot;short_description&quot;,&quot;Improve your Python data importing skills and learn to work with web and API data.&quot;,&quot;author_field&quot;,null,&quot;author_bio&quot;,null,&quot;author_image&quot;,&quot;https://assets.datacamp.com/production/course_1606/author_images/author_image_course_1606_20200310-1-lgdj4c?1583853939&quot;,&quot;nb_of_subscriptions&quot;,174762,&quot;^1=&quot;,&quot;intermediate-importing-data-in-python&quot;,&quot;image_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/thumb/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;image_thumbnail_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/thumb_home/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;^1A&quot;,&quot;12/08/2024&quot;,&quot;link&quot;,&quot;https://www.datacamp.com/courses/intermediate-importing-data-in-python&quot;,&quot;should_cache&quot;,true,&quot;^B&quot;,&quot;datacamp&quot;,&quot;difficulty_level&quot;,2,&quot;state&quot;,&quot;live&quot;,&quot;university&quot;,null,&quot;sharing_links&quot;,[&quot;^ &quot;,&quot;twitter&quot;,&quot;http://bit.ly/1eWTMJh&quot;,&quot;facebook&quot;,&quot;http://bit.ly/1iS42Do&quot;],&quot;marketing_video&quot;,&quot;&quot;,&quot;^1G&quot;,&quot;python&quot;,&quot;paid&quot;,true,&quot;time_needed&quot;,null,&quot;xp&quot;,2300,&quot;topic_id&quot;,18,&quot;technology_id&quot;,2,&quot;reduced_outline&quot;,null,&quot;^1H&quot;,null,&quot;lti_only&quot;,false,&quot;instructors&quot;,[[&quot;^ &quot;,&quot;id&quot;,301837,&quot;marketing_biography&quot;,&quot;Data Scientist&quot;,&quot;biography&quot;,&quot;Hugo is a data scientist, educator, writer and podcaster formerly at DataCamp. His main interests are promoting data &amp; AI literacy, helping to spread data skills through organizations and society and doing amateur stand up comedy in NYC. If you want to know what he likes to talk about, definitely check out &lt;a href=\\\\\\\\&quot;https://www.datacamp.com/community/podcast\\\\\\\\&quot;&gt;DataFramed&lt;/a&gt;, the DataCamp podcast, which he hosted and produced.&quot;,&quot;avatar_url&quot;,&quot;https://assets.datacamp.com/authors/avatars/000/000/006/square/hugoaboutpic.jpg?1705506415&quot;,&quot;full_name&quot;,&quot;Hugo Bowne-Anderson&quot;,&quot;instructor_path&quot;,&quot;/instructors/hugobowne&quot;]],&quot;collaborators&quot;,[[&quot;^ &quot;,&quot;^26&quot;,&quot;https://assets.datacamp.com/authors/avatars/000/000/058/square/francis-photo.jpg?1705506690&quot;,&quot;^27&quot;,&quot;Francisco Castro&quot;]],&quot;datasets&quot;,[[&quot;^ &quot;,&quot;asset_url&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/b422ace2fceada7b569e0ba3e8d833fddc684c4d/latitude.xls&quot;,&quot;name&quot;,&quot;Latitudes (XLS)&quot;],[&quot;^ &quot;,&quot;^2;&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/3ef452f83a91556ea4284624b969392c0506fb33/tweets3.txt&quot;,&quot;^2&lt;&quot;,&quot;Tweets&quot;],[&quot;^ &quot;,&quot;^2;&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/013936d2700e2d00207ec42100d448c23692eb6f/winequality-red.csv&quot;,&quot;^2&lt;&quot;,&quot;Red wine quality&quot;]],&quot;tracks&quot;,[[&quot;^ &quot;,&quot;path&quot;,&quot;/tracks/data-engineer-in-python&quot;,&quot;title_with_subtitle&quot;,&quot;Data Engineer in Python&quot;],[&quot;^ &quot;,&quot;^2&gt;&quot;,&quot;/tracks/data-scientist-in-python&quot;,&quot;^2?&quot;,&quot;Data Scientist in Python&quot;],[&quot;^ &quot;,&quot;^2&gt;&quot;,&quot;/tracks/importing-cleaning-data-with-python&quot;,&quot;^2?&quot;,&quot;Importing &amp; Cleaning Data  with Python&quot;]],&quot;prerequisites&quot;,[[&quot;^ &quot;,&quot;^2&gt;&quot;,&quot;/courses/introduction-to-importing-data-in-python&quot;,&quot;^W&quot;,&quot;Introduction to Importing Data in Python&quot;]],&quot;time_needed_in_hours&quot;,2,&quot;seo_title&quot;,&quot;Intermediate Importing Data in Python&quot;,&quot;seo_description&quot;,&quot;Learn how to import data into Python from sources like the web and by pulling data from APIs, such as the Twitter streaming API to stream real-time tweets.&quot;,&quot;archived_at&quot;,null,&quot;original_image_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/original/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;external_slug&quot;,&quot;intermediate-importing-data-in-python&quot;,&quot;mobile_enabled&quot;,true,&quot;case_study&quot;,null,&quot;difficulty_level_hardcoded&quot;,null,&quot;long_description&quot;,null,&quot;industry_ids&quot;,[],&quot;audio_recorders&quot;,[],&quot;content_area&quot;,&quot;Data Science and Analytics&quot;,&quot;is_labeled_as_new&quot;,null,&quot;tier&quot;,null,&quot;chapters&quot;,[[&quot;^ &quot;,&quot;id&quot;,4135,&quot;^1;&quot;,null,&quot;^W&quot;,&quot;Importing data from the Internet&quot;,&quot;^X&quot;,&quot;The web is a rich source of data from which you can extract various types of insights and findings. In this chapter, you will learn how to get data from the web, whether it is stored in files or in HTML. You&#39;ll also learn the basics of scraping and parsing web data.&quot;,&quot;^1&lt;&quot;,1,&quot;^1=&quot;,&quot;importing-data-from-the-internet-1&quot;,&quot;^1&gt;&quot;,12,&quot;^1?&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^1@&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^1A&quot;,&quot;11/08/2022&quot;,&quot;^1B&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter1.pdf&quot;,&quot;^1C&quot;,true,&quot;xp&quot;,1050,&quot;^1D&quot;,3,&quot;^F&quot;,[[&quot;^ &quot;,&quot;^B&quot;,&quot;VideoExercise&quot;,&quot;^W&quot;,&quot;Importing flat files from the web&quot;,&quot;^1E&quot;,50,&quot;^1&lt;&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=1&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=3&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Importing non-flat files from the web&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=4&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;VideoExercise&quot;,&quot;^W&quot;,&quot;HTTP requests to import files from the web&quot;,&quot;^1E&quot;,50,&quot;^1&lt;&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=5&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=6&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=7&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=8&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;VideoExercise&quot;,&quot;^W&quot;,&quot;Scraping the web in Python&quot;,&quot;^1E&quot;,50,&quot;^1&lt;&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=9&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=10&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=11&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=12&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null]]],[&quot;^ &quot;,&quot;id&quot;,4136,&quot;^1;&quot;,null,&quot;^W&quot;,&quot;Interacting with APIs to import data from the web&quot;,&quot;^X&quot;,&quot;In this chapter, you will gain a deeper understanding of how to import data from the web. You will learn the basics of extracting data from APIs, gain insight on the importance of APIs, and practice extracting data by diving into the OMDB and Library of Congress APIs.&quot;,&quot;^1&lt;&quot;,2,&quot;^1=&quot;,&quot;interacting-with-apis-to-import-data-from-the-web-2&quot;,&quot;^1&gt;&quot;,9,&quot;^1?&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^1@&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^1A&quot;,&quot;11/08/2022&quot;,&quot;^1B&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter2.pdf&quot;,&quot;^1C&quot;,null,&quot;xp&quot;,650,&quot;^1D&quot;,2,&quot;^F&quot;,[[&quot;^ &quot;,&quot;^B&quot;,&quot;VideoExercise&quot;,&quot;^W&quot;,&quot;Introduction to APIs and JSONs&quot;,&quot;^1E&quot;,50,&quot;^1&lt;&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=1&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;^W&quot;,&quot;Pop quiz: What exactly is a JSON?&quot;,&quot;^1E&quot;,50,&quot;^1&lt;&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=2&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Loading and exploring a JSON&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=3&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;MultipleChoiceExercise&quot;,&quot;^W&quot;,&quot;Pop quiz: Exploring your JSON&quot;,&quot;^1E&quot;,50,&quot;^1&lt;&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=4&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;VideoExercise&quot;,&quot;^W&quot;,&quot;APIs and interacting with the world wide web&quot;,&quot;^1E&quot;,50,&quot;^1&lt;&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=5&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;^W&quot;,&quot;Pop quiz: What&#39;s an API?&quot;,&quot;^1E&quot;,50,&quot;^1&lt;&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=6&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;API requests&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=7&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;JSON\\xe2\\x80\\x93from the web to Python&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=8&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Checking out the Wikipedia API&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=9&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null]]],[&quot;^ &quot;,&quot;id&quot;,4140,&quot;^1;&quot;,null,&quot;^W&quot;,&quot;Diving  deep into the Twitter API&quot;,&quot;^X&quot;,&quot;In this chapter, you will consolidate your knowledge of interacting with APIs in a deep dive into the Twitter streaming API. You&#39;ll learn how to stream real-time Twitter data, and how to analyze and visualize it.&quot;,&quot;^1&lt;&quot;,3,&quot;^1=&quot;,&quot;diving-deep-into-the-twitter-api&quot;,&quot;^1&gt;&quot;,7,&quot;^1?&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^1@&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^1A&quot;,&quot;11/08/2022&quot;,&quot;^1B&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter3.pdf&quot;,&quot;^1C&quot;,null,&quot;xp&quot;,600,&quot;^1D&quot;,2,&quot;^F&quot;,[[&quot;^ &quot;,&quot;^B&quot;,&quot;VideoExercise&quot;,&quot;^W&quot;,&quot;The Twitter API and Authentication&quot;,&quot;^1E&quot;,50,&quot;^1&lt;&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=1&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Streaming tweets&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=2&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Load and explore your Twitter data&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=3&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Twitter data to DataFrame&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=4&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;A little bit of Twitter text analysis&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=5&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^W&quot;,&quot;Plotting your Twitter data&quot;,&quot;^1E&quot;,100,&quot;^1&lt;&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=6&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null],[&quot;^ &quot;,&quot;^B&quot;,&quot;VideoExercise&quot;,&quot;^W&quot;,&quot;Final Thoughts&quot;,&quot;^1E&quot;,50,&quot;^1&lt;&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=7&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null]]]],&quot;course_resources&quot;,[],&quot;translated_course_id&quot;,1606]]]],&quot;^F&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;PreFetchedRequestRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;^2&quot;,&quot;SUCCESS&quot;,&quot;^1:&quot;,[[&quot;^ &quot;,&quot;id&quot;,990668,&quot;^B&quot;,&quot;VideoExercise&quot;,&quot;assignment&quot;,null,&quot;^W&quot;,&quot;Importing flat files from the web&quot;,&quot;sample_code&quot;,&quot;&quot;,&quot;instructions&quot;,null,&quot;^1&lt;&quot;,1,&quot;sct&quot;,&quot;&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;attachments&quot;,null,&quot;xp&quot;,50,&quot;possible_answers&quot;,[],&quot;feedbacks&quot;,[],&quot;question&quot;,&quot;&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null,&quot;video_link&quot;,null,&quot;video_hls&quot;,null,&quot;aspect_ratio&quot;,56.25,&quot;projector_key&quot;,&quot;course_1606_59604c018a6e132016cd26144a12fee0&quot;,&quot;key&quot;,&quot;e36457c7ed&quot;,&quot;^U&quot;,&quot;python&quot;,&quot;course_id&quot;,1606,&quot;chapter_id&quot;,4135,&quot;version&quot;,&quot;v0&quot;,&quot;randomNumber&quot;,0.6885205858522894,&quot;externalId&quot;,990668],[&quot;^ &quot;,&quot;id&quot;,42707,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^2S&quot;,&quot;&lt;p&gt;You are about to import your first file from the web! The flat file you will import will be &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; from the University of California, Irvine&#39;s &lt;a href=\\\\\\\\&quot;https://archive.ics.uci.edu/ml/index.php\\\\\\\\&quot;&gt;Machine Learning repository&lt;/a&gt;. The flat file contains tabular data of physiochemical properties of red wine, such as pH, alcohol content and citric acid content, along with wine quality rating.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the file is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;\\\\\\\\n&lt;p&gt;After you import it, you&#39;ll check your working directory to confirm that it is there and then you&#39;ll load it into a &lt;code&gt;pandas&lt;/code&gt; DataFrame.&lt;/p&gt;&quot;,&quot;^W&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;^2T&quot;,&quot;# Import package\\\\\\\\nfrom ____ import ____\\\\\\\\n\\\\\\\\n# Import pandas\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Save file locally\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read file into a DataFrame and print its head\\\\\\\\ndf = pd.read_csv(&#39;winequality-red.csv&#39;, sep=&#39;;&#39;)\\\\\\\\nprint(df.head())&quot;,&quot;^2U&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the function &lt;code&gt;urlretrieve&lt;/code&gt; from the subpackage &lt;code&gt;urllib.request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the function &lt;code&gt;urlretrieve()&lt;/code&gt; to save the file locally as &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Execute the remaining code to load &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; in a pandas DataFrame and to print its head to the shell.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1&lt;&quot;,2,&quot;sct&quot;,&quot;Ex().has_import(\\\\\\\\&quot;urllib.request.urlretrieve\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;pandas\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlretrieve\\\\\\\\&quot;).multi(\\\\\\\\n  check_args(0).has_equal_value(),\\\\\\\\n  check_args(1).has_equal_value()\\\\\\\\n)\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;df\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;pandas.read_csv\\\\\\\\&quot;).multi(\\\\\\\\n    check_args(0).has_equal_value(),\\\\\\\\n    check_args(1).has_equal_value()\\\\\\\\n  )\\\\\\\\n)\\\\\\\\nEx().has_printout(0)\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^2V&quot;,&quot;&quot;,&quot;^2W&quot;,&quot;# Import package\\\\\\\\nfrom urllib.request import urlretrieve\\\\\\\\n\\\\\\\\n# Import pandas\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n\\\\\\\\n# Save file locally\\\\\\\\nurlretrieve(url, &#39;winequality-red.csv&#39;)\\\\\\\\n\\\\\\\\n# Read file into a DataFrame and print its head\\\\\\\\ndf = pd.read_csv(&#39;winequality-red.csv&#39;, sep=&#39;;&#39;)\\\\\\\\nprint(df.head())&quot;,&quot;^2X&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a function &lt;code&gt;y&lt;/code&gt; from a subpackage &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;from x import y&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;This one&#39;s a long URL. Make sure you typed it in correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; to import (in the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and the &lt;em&gt;filename&lt;/em&gt; for saving the file locally as the second argument to &lt;code&gt;urlretrieve()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to change the code for loading &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; and printing its head.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^2Y&quot;,null,&quot;xp&quot;,100,&quot;^2Z&quot;,[],&quot;^2[&quot;,[],&quot;^30&quot;,&quot;&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null,&quot;^U&quot;,&quot;python&quot;,&quot;^38&quot;,0.9822140964849919,&quot;^39&quot;,42707],[&quot;^ &quot;,&quot;id&quot;,42708,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^2S&quot;,&quot;&lt;p&gt;You have just imported a file from the web, saved it locally and loaded it into a DataFrame. If you just wanted to load a file from the web into a DataFrame without first saving it locally, you can do that easily using &lt;code&gt;pandas&lt;/code&gt;. In particular, you can use the function &lt;code&gt;pd.read_csv()&lt;/code&gt; with the URL as the first argument and the separator &lt;code&gt;sep&lt;/code&gt; as the second argument.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the file, once again, is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;&quot;,&quot;^W&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;^2T&quot;,&quot;# Import packages\\\\\\\\nimport matplotlib.pyplot as plt\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read file into a DataFrame: df\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the head of the DataFrame\\\\\\\\nprint(____)\\\\\\\\n\\\\\\\\n# Plot first column of df\\\\\\\\ndf.iloc[:, 0].hist()\\\\\\\\nplt.xlabel(&#39;fixed acidity (g(tartaric acid)/dm$^3$)&#39;)\\\\\\\\nplt.ylabel(&#39;count&#39;)\\\\\\\\nplt.show()\\\\\\\\n&quot;,&quot;^2U&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Read file into a DataFrame &lt;code&gt;df&lt;/code&gt; using &lt;code&gt;pd.read_csv()&lt;/code&gt;, recalling that the separator in the file is &lt;code&gt;&#39;;&#39;&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the head of the DataFrame &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Execute the rest of the code to plot histogram of the first feature in the DataFrame &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1&lt;&quot;,3,&quot;sct&quot;,&quot;Ex().has_import(\\\\\\\\&quot;matplotlib.pyplot\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;pandas\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;df\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;pandas.read_csv\\\\\\\\&quot;).multi(\\\\\\\\n    check_args(0).has_equal_value(),\\\\\\\\n    check_args(1).has_equal_value()\\\\\\\\n  )\\\\\\\\n)\\\\\\\\nEx().has_printout(0)\\\\\\\\nEx().has_equal_ast(code=\\\\\\\\&quot;df.iloc[:, 0].hist\\\\\\\\&quot;, incorrect_msg=\\\\\\\\&quot;Please do not change the code to plot the histogram.\\\\\\\\&quot;)\\\\\\\\nEx().check_function(\\\\\\\\&quot;matplotlib.pyplot.show\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^2V&quot;,&quot;&quot;,&quot;^2W&quot;,&quot;# Import packages\\\\\\\\nimport matplotlib.pyplot as plt\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n\\\\\\\\n# Read file into a DataFrame: df\\\\\\\\ndf = pd.read_csv(url, sep=&#39;;&#39;)\\\\\\\\n\\\\\\\\n# Print the head of the DataFrame\\\\\\\\nprint(df.head())\\\\\\\\n\\\\\\\\n# Plot first column of df\\\\\\\\ndf.iloc[:, 0].hist()\\\\\\\\nplt.xlabel(&#39;fixed acidity (g(tartaric acid)/dm$^3$)&#39;)\\\\\\\\nplt.ylabel(&#39;count&#39;)\\\\\\\\nplt.show()\\\\\\\\n&quot;,&quot;^2X&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Make sure you typed the URL correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and the &lt;em&gt;separator&lt;/em&gt; as the second argument to &lt;code&gt;pd.read_csv()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The &lt;em&gt;head&lt;/em&gt; of a DataFrame can be accessed by using &lt;code&gt;head()&lt;/code&gt; on the DataFrame.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to change any of the code for plotting the histograms.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^2Y&quot;,null,&quot;xp&quot;,100,&quot;^2Z&quot;,[],&quot;^2[&quot;,[],&quot;^30&quot;,&quot;&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null,&quot;^U&quot;,&quot;python&quot;,&quot;^38&quot;,0.9313110883135951,&quot;^39&quot;,42708],[&quot;^ &quot;,&quot;id&quot;,42709,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^2S&quot;,&quot;&lt;p&gt;Congrats! You&#39;ve just loaded a flat file from the web into a DataFrame without first saving it locally using the &lt;code&gt;pandas&lt;/code&gt; function &lt;code&gt;pd.read_csv()&lt;/code&gt;. This function is super cool because it has close relatives that allow you to load all types of files, not only flat ones. In this interactive exercise, you&#39;ll use &lt;code&gt;pd.read_excel()&lt;/code&gt; to import an Excel spreadsheet.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the spreadsheet is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;https://assets.datacamp.com/course/importing_data_into_r/latitude.xls&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;\\\\\\\\n&lt;p&gt;Your job is to use &lt;code&gt;pd.read_excel()&lt;/code&gt; to read in all of its sheets, print the sheet names and then print the head of the first sheet &lt;em&gt;using its name, not its index&lt;/em&gt;.&lt;/p&gt;\\\\\\\\n&lt;p&gt;Note that the output of &lt;code&gt;pd.read_excel()&lt;/code&gt; is a Python dictionary with sheet names as keys and corresponding DataFrames as corresponding values.&lt;/p&gt;&quot;,&quot;^W&quot;,&quot;Importing non-flat files from the web&quot;,&quot;^2T&quot;,&quot;# Import package\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read in all sheets of Excel file: xls\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the sheetnames to the shell\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the head of the first sheet (using its name, NOT its index)\\\\\\\\n\\\\\\\\n&quot;,&quot;^2U&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Read the file in &lt;code&gt;url&lt;/code&gt; into a dictionary &lt;code&gt;xls&lt;/code&gt; using &lt;code&gt;pd.read_excel()&lt;/code&gt; recalling that, in order to import all sheets you need to pass &lt;code&gt;None&lt;/code&gt; to the argument &lt;code&gt;sheet_name&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the names of the sheets in the Excel spreadsheet; these will be the keys of the dictionary &lt;code&gt;xls&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the head of the first sheet &lt;em&gt;using the sheet name, not the index of the sheet&lt;/em&gt;! The sheet name is &lt;code&gt;&#39;1700&#39;&lt;/code&gt;&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1&lt;&quot;,4,&quot;sct&quot;,&quot;Ex().has_import(&#39;pandas&#39;)\\\\\\\\nEx().check_correct(\\\\\\\\n    has_printout(0),\\\\\\\\n    multi(\\\\\\\\n        check_correct(\\\\\\\\n            check_object(&#39;xls&#39;).is_instance(dict),\\\\\\\\n            check_correct(\\\\\\\\n                check_function(&#39;pandas.read_excel&#39;).multi(\\\\\\\\n                    check_args(0).has_equal_value(),\\\\\\\\n                    check_args(&#39;sheet_name&#39;).has_equal_value()\\\\\\\\n                ),\\\\\\\\n                check_object(&#39;url&#39;).has_equal_value()\\\\\\\\n            )\\\\\\\\n        )\\\\\\\\n    )\\\\\\\\n)\\\\\\\\nEx().has_printout(1)\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)&quot;,&quot;^2V&quot;,&quot;&quot;,&quot;^2W&quot;,&quot;# Import package\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;https://assets.datacamp.com/course/importing_data_into_r/latitude.xls&#39;\\\\\\\\n\\\\\\\\n# Read in all sheets of Excel file: xls\\\\\\\\nxls = pd.read_excel(url, sheet_name=None)\\\\\\\\n\\\\\\\\n# Print the sheetnames to the shell\\\\\\\\nprint(xls.keys())\\\\\\\\n\\\\\\\\n# Print the head of the first sheet (using its name, NOT its index)\\\\\\\\nprint(xls[&#39;1700&#39;].head())&quot;,&quot;^2X&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Make sure you typed in the URL correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and &lt;code&gt;sheet_name&lt;/code&gt; with its corresponding value as the second argument to &lt;code&gt;pd.read_excel()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The &lt;em&gt;keys&lt;/em&gt; of a dictionary can be accessed by using &lt;code&gt;keys()&lt;/code&gt; on the dictionary.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access a sheet using the format: &lt;em&gt;dictionary&lt;/em&gt;&lt;strong&gt;[&lt;/strong&gt;&lt;em&gt;sheet name or index&lt;/em&gt;&lt;strong&gt;]&lt;/strong&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^2Y&quot;,null,&quot;xp&quot;,100,&quot;^2Z&quot;,[],&quot;^2[&quot;,[],&quot;^30&quot;,&quot;&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null,&quot;^U&quot;,&quot;python&quot;,&quot;^38&quot;,0.051626335915768706,&quot;^39&quot;,42709],[&quot;^ &quot;,&quot;id&quot;,990669,&quot;^B&quot;,&quot;VideoExercise&quot;,&quot;^2S&quot;,null,&quot;^W&quot;,&quot;HTTP requests to import files from the web&quot;,&quot;^2T&quot;,&quot;&quot;,&quot;^2U&quot;,null,&quot;^1&lt;&quot;,5,&quot;sct&quot;,&quot;&quot;,&quot;^2V&quot;,&quot;&quot;,&quot;^2W&quot;,&quot;&quot;,&quot;^2X&quot;,null,&quot;^2Y&quot;,null,&quot;xp&quot;,50,&quot;^2Z&quot;,[],&quot;^2[&quot;,[],&quot;^30&quot;,&quot;&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null,&quot;^31&quot;,null,&quot;^32&quot;,null,&quot;^33&quot;,56.25,&quot;^34&quot;,&quot;course_1606_9d15ae176be1800b996f7869a82b8087&quot;,&quot;key&quot;,&quot;e480d1fdcf&quot;,&quot;^U&quot;,&quot;python&quot;,&quot;^35&quot;,1606,&quot;^36&quot;,4135,&quot;^37&quot;,&quot;v0&quot;,&quot;^38&quot;,0.576175861463001,&quot;^39&quot;,990669],[&quot;^ &quot;,&quot;id&quot;,42711,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^2S&quot;,&quot;&lt;p&gt;Now that you know the basics behind HTTP GET requests, it&#39;s time to perform some of your own. In this interactive exercise, you will ping our very own DataCamp servers to perform a GET request to extract information from the first coding exercise of this course, &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt;.&lt;/p&gt;\\\\\\\\n&lt;p&gt;In the next exercise, you&#39;ll extract the HTML itself. Right now, however, you are going to package and send the request and then catch the response.&lt;/p&gt;&quot;,&quot;^W&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;^2T&quot;,&quot;# Import packages\\\\\\\\n\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request: request\\\\\\\\n\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the datatype of response\\\\\\\\nprint(type(response))\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()\\\\\\\\n&quot;,&quot;^2U&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the functions &lt;code&gt;urlopen&lt;/code&gt; and &lt;code&gt;Request&lt;/code&gt; from the subpackage &lt;code&gt;urllib.request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the url &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt; using the function &lt;code&gt;Request()&lt;/code&gt; and assign it to &lt;code&gt;request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Send the request and catch the response in the variable &lt;code&gt;response&lt;/code&gt; with  the function &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Run the rest of the code to see the datatype of &lt;code&gt;response&lt;/code&gt; and to close the connection!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1&lt;&quot;,6,&quot;sct&quot;,&quot;\\\\\\\\n# Test: import urlopen, Request\\\\\\\\nimport_msg = \\\\\\\\&quot;Did you correctly import the required packages?\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;, missing_msg=predef_msg).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\n\\\\\\\\n# Test: call to Request() and &#39;request&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;request\\\\\\\\&quot;)\\\\\\\\n  \\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;response\\\\\\\\&quot;),\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(0)\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.close\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^2V&quot;,&quot;&quot;,&quot;^2W&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request: request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\nresponse = urlopen(request)\\\\\\\\n\\\\\\\\n# Print the datatype of response\\\\\\\\nprint(type(response))\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()\\\\\\\\n&quot;,&quot;^2X&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import two functions in one line, import the first function as usual and add a comma &lt;code&gt;,&lt;/code&gt; followed by the second function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (already in the &lt;code&gt;url&lt;/code&gt; object defined) as an argument to &lt;code&gt;Request()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass &lt;code&gt;request&lt;/code&gt; as an argument to &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for printing the datatype of &lt;code&gt;response&lt;/code&gt; and closing the connection.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^2Y&quot;,null,&quot;xp&quot;,100,&quot;^2Z&quot;,[],&quot;^2[&quot;,[],&quot;^30&quot;,&quot;&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null,&quot;^U&quot;,&quot;python&quot;,&quot;^38&quot;,0.2948355478242206,&quot;^39&quot;,42711],[&quot;^ &quot;,&quot;id&quot;,42712,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^2S&quot;,&quot;&lt;p&gt;You have just packaged and sent a GET request to &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt; and then caught the response. You saw that such a response is a &lt;code&gt;http.client.HTTPResponse&lt;/code&gt; object. The question remains: what can you do with this response?&lt;/p&gt;\\\\\\\\n&lt;p&gt;Well, as it came from an HTML page, you could &lt;em&gt;read&lt;/em&gt; it to extract the HTML and, in fact, such a &lt;code&gt;http.client.HTTPResponse&lt;/code&gt; object has an associated &lt;code&gt;read()&lt;/code&gt; method. In this exercise, you&#39;ll build on your previous great work to extract the response and print the HTML.&lt;/p&gt;&quot;,&quot;^W&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;^2T&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extract the response: html\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\n\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()&quot;,&quot;^2U&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Send the request and catch the response in the variable &lt;code&gt;response&lt;/code&gt; with the function &lt;code&gt;urlopen()&lt;/code&gt;, as in the previous exercise.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the response using the &lt;code&gt;read()&lt;/code&gt; method and store the result in the variable &lt;code&gt;html&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the string &lt;code&gt;html&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to perform all of the above and to close the response: be tidy!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1&lt;&quot;,7,&quot;sct&quot;,&quot;\\\\\\\\n# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to Request() and &#39;request&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;request\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;response\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.read\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;html\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to print()\\\\\\\\nEx().check_function(&#39;print&#39;).check_args(0).has_equal_ast()\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.close\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^2V&quot;,&quot;&quot;,&quot;^2W&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\nresponse = urlopen(request)\\\\\\\\n\\\\\\\\n# Extract the response: html\\\\\\\\nhtml = response.read()\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(html)\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()&quot;,&quot;^2X&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass &lt;code&gt;request&lt;/code&gt; as an argument to &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Apply the method &lt;code&gt;read()&lt;/code&gt; to the response object &lt;code&gt;response&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Simply pass &lt;code&gt;html&lt;/code&gt; to the &lt;code&gt;print()&lt;/code&gt; function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for closing the response.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^2Y&quot;,null,&quot;xp&quot;,100,&quot;^2Z&quot;,[],&quot;^2[&quot;,[],&quot;^30&quot;,&quot;&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null,&quot;^U&quot;,&quot;python&quot;,&quot;^38&quot;,0.5669930325167178,&quot;^39&quot;,42712],[&quot;^ &quot;,&quot;id&quot;,42713,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^2S&quot;,&quot;&lt;p&gt;Now that you&#39;ve got your head and hands around making HTTP requests using the urllib package, you&#39;re going to figure out how to do the same using the higher-level requests library. You&#39;ll once again be pinging DataCamp servers for their &lt;code&gt;\\\\\\\\&quot;http://www.datacamp.com/teach/documentation\\\\\\\\&quot;&lt;/code&gt; page.&lt;/p&gt;\\\\\\\\n&lt;p&gt;Note that unlike in the previous exercises using urllib, you don&#39;t have to close the connection when using requests!&lt;/p&gt;&quot;,&quot;^W&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;^2T&quot;,&quot;# Import package\\\\\\\\n\\\\\\\\n\\\\\\\\n# Specify the url: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Packages the request, send the request and catch the response: r\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extract the response: text\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(text)&quot;,&quot;^2U&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the package &lt;code&gt;requests&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of interest to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the URL, send the request and catch the response with a single function &lt;code&gt;requests.get()&lt;/code&gt;, assigning the response to the variable &lt;code&gt;r&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; to return the HTML of the webpage as a string; store the result in a variable &lt;code&gt;text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print the HTML of the webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1&lt;&quot;,8,&quot;sct&quot;,&quot;\\\\\\\\n# Test: import requests\\\\\\\\nEx().has_import(\\\\\\\\&quot;requests\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: &#39;text&#39; variable\\\\\\\\nEx().has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `text`?\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;text\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().check_function(&#39;print&#39;).check_args(0).has_equal_ast()\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^2V&quot;,&quot;&quot;,&quot;^2W&quot;,&quot;# Import package\\\\\\\\nimport requests\\\\\\\\n\\\\\\\\n# Specify the url: url\\\\\\\\nurl = \\\\\\\\&quot;http://www.datacamp.com/teach/documentation\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# Packages the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response: text\\\\\\\\ntext = r.text\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(text)&quot;,&quot;^2X&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a package &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;import x&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Did you type in the URL correctly?&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as an argument to &lt;code&gt;requests.get()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; by executing &lt;code&gt;r.text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for printing the HTML of the webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^2Y&quot;,null,&quot;xp&quot;,100,&quot;^2Z&quot;,[],&quot;^2[&quot;,[],&quot;^30&quot;,&quot;&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null,&quot;^U&quot;,&quot;python&quot;,&quot;^38&quot;,0.1033626497550677,&quot;^39&quot;,42713],[&quot;^ &quot;,&quot;id&quot;,990670,&quot;^B&quot;,&quot;VideoExercise&quot;,&quot;^2S&quot;,null,&quot;^W&quot;,&quot;Scraping the web in Python&quot;,&quot;^2T&quot;,&quot;&quot;,&quot;^2U&quot;,null,&quot;^1&lt;&quot;,9,&quot;sct&quot;,&quot;&quot;,&quot;^2V&quot;,&quot;&quot;,&quot;^2W&quot;,&quot;&quot;,&quot;^2X&quot;,null,&quot;^2Y&quot;,null,&quot;xp&quot;,50,&quot;^2Z&quot;,[],&quot;^2[&quot;,[],&quot;^30&quot;,&quot;&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null,&quot;^31&quot;,null,&quot;^32&quot;,null,&quot;^33&quot;,56.25,&quot;^34&quot;,&quot;course_1606_9d1f8a331d1200c7e1bdbfcaf3a7a491&quot;,&quot;key&quot;,&quot;da43858012&quot;,&quot;^U&quot;,&quot;python&quot;,&quot;^35&quot;,1606,&quot;^36&quot;,4135,&quot;^37&quot;,&quot;v0&quot;,&quot;^38&quot;,0.8648626013681029,&quot;^39&quot;,990670],[&quot;^ &quot;,&quot;id&quot;,42715,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^2S&quot;,&quot;&lt;p&gt;In this interactive exercise, you&#39;ll learn how to use the BeautifulSoup package to &lt;em&gt;parse&lt;/em&gt;, &lt;em&gt;prettify&lt;/em&gt; and &lt;em&gt;extract&lt;/em&gt; information from HTML. You&#39;ll scrape the data from the webpage of Guido van Rossum, Python&#39;s very own &lt;a href=\\\\\\\\&quot;https://en.wikipedia.org/wiki/Benevolent_dictator_for_life\\\\\\\\&quot;&gt;Benevolent Dictator for Life&lt;/a&gt;. In the following exercises, you&#39;ll prettify the HTML and then extract the text and the hyperlinks.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of interest is &lt;code&gt;url = &#39;https://www.python.org/~guido/&#39;&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;^W&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;^2T&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom ____ import ____\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\n\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Prettify the BeautifulSoup object: pretty_soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the response\\\\\\\\nprint(pretty_soup)&quot;,&quot;^2U&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the function &lt;code&gt;BeautifulSoup&lt;/code&gt; from the package &lt;code&gt;bs4&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of interest to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the URL, send the request and catch the response with a single function &lt;code&gt;requests.get()&lt;/code&gt;, assigning the response to the variable &lt;code&gt;r&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; to return the HTML of the webpage as a string; store the result in a variable &lt;code&gt;html_doc&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Create a BeautifulSoup object &lt;code&gt;soup&lt;/code&gt; from the resulting HTML using the function &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;prettify()&lt;/code&gt; on &lt;code&gt;soup&lt;/code&gt; and assign the result to &lt;code&gt;pretty_soup&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print to prettified HTML to your shell!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1&lt;&quot;,10,&quot;sct&quot;,&quot;# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;requests\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: import BeautifulSoup\\\\\\\\nimport_msg = \\\\\\\\&quot;Did you correctly import the required packages?\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n\\\\\\\\n# Test: &#39;html_doc&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `html_doc`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to BeautifulSoup() and &#39;soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: call to prettify() and &#39;pretty_soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;pretty_soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;soup.prettify\\\\\\\\&quot;)\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^2V&quot;,&quot;&quot;,&quot;^2W&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Prettify the BeautifulSoup object: pretty_soup\\\\\\\\npretty_soup = soup.prettify()\\\\\\\\n\\\\\\\\n# Print the response\\\\\\\\nprint(pretty_soup)&quot;,&quot;^2X&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a function &lt;code&gt;y&lt;/code&gt; from a package &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;from x import y&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Check the URL to make sure that you typed it in correctly.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as an argument to &lt;code&gt;requests.get()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; by executing &lt;code&gt;r.text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the extracted &lt;em&gt;HTML&lt;/em&gt; as an argument to &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;To use the &lt;code&gt;prettify()&lt;/code&gt; method on the BeautifulSoup object &lt;code&gt;soup&lt;/code&gt;, execute &lt;code&gt;soup.prettify()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code to print the prettified HTML.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^2Y&quot;,null,&quot;xp&quot;,100,&quot;^2Z&quot;,[],&quot;^2[&quot;,[],&quot;^30&quot;,&quot;&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null,&quot;^U&quot;,&quot;python&quot;,&quot;^38&quot;,0.22794282309824188,&quot;^39&quot;,42715],[&quot;^ &quot;,&quot;id&quot;,42716,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^2S&quot;,&quot;&lt;p&gt;As promised, in the following exercises, you&#39;ll learn the basics of extracting information from HTML soup. In this exercise, you&#39;ll figure out how to extract the text from the BDFL&#39;s webpage, along with printing the webpage&#39;s title.&lt;/p&gt;&quot;,&quot;^W&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;^2T&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Get the title of Guido&#39;s webpage: guido_title\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage to the shell\\\\\\\\n\\\\\\\\n\\\\\\\\n# Get Guido&#39;s text: guido_text\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print Guido&#39;s text to the shell\\\\\\\\nprint(guido_text)&quot;,&quot;^2U&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;In the sample code, the HTML response object &lt;code&gt;html_doc&lt;/code&gt; has already been created: your first task is to Soupify it using the function &lt;code&gt;BeautifulSoup()&lt;/code&gt; and to assign the resulting soup to the variable &lt;code&gt;soup&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the title from the HTML soup &lt;code&gt;soup&lt;/code&gt; using the attribute &lt;code&gt;title&lt;/code&gt; and assign the result to &lt;code&gt;guido_title&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the title of Guido&#39;s webpage to the shell using the &lt;code&gt;print()&lt;/code&gt; function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the text from the HTML soup &lt;code&gt;soup&lt;/code&gt; using the method &lt;code&gt;get_text()&lt;/code&gt; and assign to &lt;code&gt;guido_text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print the text from Guido&#39;s webpage to the shell.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1&lt;&quot;,11,&quot;sct&quot;,&quot;# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;requests\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: import BeautifulSoup\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n\\\\\\\\n# Test: &#39;html_doc&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `html_doc`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to BeautifulSoup() and &#39;soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: &#39;guido_title&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;guido_title\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;soup.title\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `soup.title` to create `guido_title`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to print()\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\n# Test: call to soup.get_text() and &#39;guido_text&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;guido_text\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;soup.get_text\\\\\\\\&quot;)\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(1)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^2V&quot;,&quot;&quot;,&quot;^2W&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Get the title of Guido&#39;s webpage: guido_title\\\\\\\\nguido_title = soup.title\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage to the shell\\\\\\\\nprint(guido_title)\\\\\\\\n\\\\\\\\n# Get Guido&#39;s text: guido_text\\\\\\\\nguido_text = soup.get_text()\\\\\\\\n\\\\\\\\n# Print Guido&#39;s text to the shell\\\\\\\\nprint(guido_text)&quot;,&quot;^2X&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;HTML response object&lt;/em&gt; as an argument to &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;title&lt;/code&gt; attribute of the object &lt;code&gt;soup&lt;/code&gt; by executing &lt;code&gt;soup.title&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The object that contains the title of Guido&#39;s webpage is &lt;code&gt;guido_title&lt;/code&gt;; pass this as an argument to &lt;code&gt;print()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;get_text()&lt;/code&gt; on the HTML soup &lt;code&gt;soup&lt;/code&gt; by executing &lt;code&gt;soup.get_text()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code to print the text from Guido&#39;s webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^2Y&quot;,null,&quot;xp&quot;,100,&quot;^2Z&quot;,[],&quot;^2[&quot;,[],&quot;^30&quot;,&quot;&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null,&quot;^U&quot;,&quot;python&quot;,&quot;^38&quot;,0.6888273169376504,&quot;^39&quot;,42716],[&quot;^ &quot;,&quot;id&quot;,42717,&quot;^B&quot;,&quot;NormalExercise&quot;,&quot;^2S&quot;,&quot;&lt;p&gt;In this exercise, you&#39;ll figure out how to extract the URLs of the hyperlinks from the BDFL&#39;s webpage. In the process, you&#39;ll become close friends with the soup method &lt;code&gt;find_all()&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;^W&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;^2T&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage\\\\\\\\nprint(soup.title)\\\\\\\\n\\\\\\\\n# Find all &#39;a&#39; tags (which define hyperlinks): a_tags\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the URLs to the shell\\\\\\\\nfor ____ in ____:\\\\\\\\n    ____&quot;,&quot;^2U&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;find_all()&lt;/code&gt; to find all hyperlinks in &lt;code&gt;soup&lt;/code&gt;, remembering that hyperlinks are defined by the HTML tag &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt; but passed to &lt;code&gt;find_all()&lt;/code&gt; without angle brackets; store the result in the variable &lt;code&gt;a_tags&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The variable &lt;code&gt;a_tags&lt;/code&gt; is a results set: your job now is to enumerate over it, using a &lt;code&gt;for&lt;/code&gt; loop and to print the actual URLs of the hyperlinks; to do this, for every element &lt;code&gt;link&lt;/code&gt; in &lt;code&gt;a_tags&lt;/code&gt;, you want to &lt;code&gt;print()&lt;/code&gt; &lt;code&gt;link.get(&#39;href&#39;)&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1&lt;&quot;,12,&quot;sct&quot;,&quot;predef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\&quot;requests\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\nEx().check_correct(\\\\\\\\n    check_object(\\\\\\\\&quot;a_tags\\\\\\\\&quot;),\\\\\\\\n    check_function(\\\\\\\\&quot;soup.find_all\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n)\\\\\\\\nEx().check_for_loop().multi(\\\\\\\\n        check_iter().has_equal_value(incorrect_msg = \\\\\\\\&quot;You have to iterate over `a_tags`\\\\\\\\&quot;),\\\\\\\\n        check_body().set_context(&#39;&lt;a href=\\\\\\\\&quot;pics.html\\\\\\\\&quot;&gt;&lt;img border=\\\\\\\\&quot;0\\\\\\\\&quot; src=\\\\\\\\&quot;images/IMG_2192.jpg\\\\\\\\&quot;/&gt;&lt;/a&gt;&#39;).check_function(\\\\\\\\&quot;print\\\\\\\\&quot;).check_args(0).check_function(\\\\\\\\&quot;link.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n    )\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)&quot;,&quot;^2V&quot;,&quot;&quot;,&quot;^2W&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage\\\\\\\\nprint(soup.title)\\\\\\\\n\\\\\\\\n# Find all &#39;a&#39; tags (which define hyperlinks): a_tags\\\\\\\\na_tags = soup.find_all(&#39;a&#39;)\\\\\\\\n\\\\\\\\n# Print the URLs to the shell\\\\\\\\nfor link in a_tags:\\\\\\\\n    print(link.get(&#39;href&#39;))&quot;,&quot;^2X&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;HTML tag&lt;/em&gt; to find (without the angle brackets &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt;) as a string argument to &lt;code&gt;find_all()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Recall that the &lt;code&gt;for&lt;/code&gt; loop recipe is: &lt;code&gt;for&lt;/code&gt; &lt;em&gt;loop variable&lt;/em&gt; &lt;code&gt;in&lt;/code&gt; &lt;em&gt;results set&lt;/em&gt;&lt;code&gt;:&lt;/code&gt;. Don&#39;t forget to pass &lt;code&gt;link.get(&#39;href&#39;)&lt;/code&gt; as an argument to &lt;code&gt;print()&lt;/code&gt; inside the &lt;code&gt;for&lt;/code&gt; loop body.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^2Y&quot;,null,&quot;xp&quot;,100,&quot;^2Z&quot;,[],&quot;^2[&quot;,[],&quot;^30&quot;,&quot;&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;^1H&quot;,null,&quot;^U&quot;,&quot;python&quot;,&quot;^38&quot;,0.22284468403192004,&quot;^39&quot;,42717]]]]],&quot;^H&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;PreFetchedRequestRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;^2&quot;,&quot;NOT_FETCHED&quot;,&quot;^1:&quot;,null]]],&quot;sharedImage&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;PreFetchedRequestRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;^2&quot;,&quot;NOT_FETCHED&quot;,&quot;^1:&quot;,null]]],&quot;^Q&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;PreFetchedRequestRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;^2&quot;,&quot;SUCCESS&quot;,&quot;^1:&quot;,[&quot;^ &quot;,&quot;^R&quot;,&quot;template&quot;,&quot;id&quot;,3046,&quot;^S&quot;,&quot;2022-12-01T15:10:45.124Z&quot;,&quot;^T&quot;,&quot;2022-12-05T06:31:46.338Z&quot;,&quot;key&quot;,&quot;course-dataset-intermediate-importing-data-in-python&quot;,&quot;^U&quot;,&quot;Python&quot;,&quot;^V&quot;,null,&quot;^W&quot;,&quot;Intermediate Importing Data in Python&quot;,&quot;^X&quot;,&quot;Explore the datasets from the course, Intermediate Importing Data in Python.&quot;,&quot;^Y&quot;,false,&quot;^Z&quot;,&quot;d9783ad9a9e677a4f583a5bd9ed5b9d5987a2859&quot;,&quot;^[&quot;,null,&quot;^10&quot;,null,&quot;^11&quot;,null,&quot;^12&quot;,&quot;ade0176d-8e1d-436c-b7c1-44c3f4f1df8f&quot;,&quot;^13&quot;,[&quot;course-dataset&quot;],&quot;^14&quot;,1606,&quot;^15&quot;,[],&quot;^16&quot;,null]]]],&quot;courseImages&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;PreFetchedRequestRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;^2&quot;,&quot;SUCCESS&quot;,&quot;^1:&quot;,[&quot;^ &quot;,&quot;imageTag&quot;,&quot;course-1606-master:cb59605c00ed73a970165be3564ff450-20220811125027179&quot;,&quot;^B&quot;,&quot;singleImage&quot;]]]],&quot;categoryPages&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;PreFetchedRequestRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;^2&quot;,&quot;SUCCESS&quot;,&quot;^1:&quot;,[[&quot;^ &quot;,&quot;^1=&quot;,&quot;power-bi&quot;,&quot;facet&quot;,[&quot;^ &quot;,&quot;technology_array&quot;,[&quot;Power BI&quot;]]],[&quot;^ &quot;,&quot;^1=&quot;,&quot;r&quot;,&quot;^3&gt;&quot;,[&quot;^ &quot;,&quot;^3?&quot;,[&quot;R&quot;]]],[&quot;^ &quot;,&quot;^1=&quot;,&quot;sql&quot;,&quot;^3&gt;&quot;,[&quot;^ &quot;,&quot;^3?&quot;,[&quot;SQL&quot;]]],[&quot;^ &quot;,&quot;^1=&quot;,&quot;tableau&quot;,&quot;^3&gt;&quot;,[&quot;^ &quot;,&quot;^3?&quot;,[&quot;Tableau&quot;]]],[&quot;^ &quot;,&quot;^1=&quot;,&quot;azure&quot;,&quot;^3&gt;&quot;,[&quot;^ &quot;,&quot;^3?&quot;,[&quot;Azure&quot;]]],[&quot;^ &quot;,&quot;^1=&quot;,&quot;probability-and-statistics&quot;,&quot;^3&gt;&quot;,[&quot;^ &quot;,&quot;topic_array&quot;,[&quot;Probability &amp; Statistics&quot;]]],[&quot;^ &quot;,&quot;^1=&quot;,&quot;artificial-intelligence&quot;,&quot;^3&gt;&quot;,[&quot;^ &quot;,&quot;^3@&quot;,[&quot;Artificial Intelligence&quot;]]],[&quot;^ &quot;,&quot;^1=&quot;,&quot;machine-learning&quot;,&quot;^3&gt;&quot;,[&quot;^ &quot;,&quot;^3@&quot;,[&quot;Machine Learning&quot;]]],[&quot;^ &quot;,&quot;^1=&quot;,&quot;data-engineering&quot;,&quot;^3&gt;&quot;,[&quot;^ &quot;,&quot;^3@&quot;,[&quot;Data Engineering&quot;]]],[&quot;^ &quot;,&quot;^1=&quot;,&quot;data-visualization&quot;,&quot;^3&gt;&quot;,[&quot;^ &quot;,&quot;^3@&quot;,[&quot;Data Visualization&quot;]]],[&quot;^ &quot;,&quot;^1=&quot;,&quot;data-analysis&quot;,&quot;^3&gt;&quot;,[&quot;^ &quot;,&quot;^3@&quot;,[&quot;Reporting&quot;,&quot;Data Manipulation&quot;,&quot;Data Preparation&quot;,&quot;Exploratory Data Analysis&quot;,&quot;Data Visualization&quot;]]],[&quot;^ &quot;,&quot;^1=&quot;,&quot;python&quot;,&quot;^3&gt;&quot;,[&quot;^ &quot;,&quot;^3?&quot;,[&quot;Python&quot;]]]]]]]]]],&quot;settings&quot;,[&quot;^?&quot;,[&quot;uiTheme&quot;,&quot;DARK&quot;,&quot;feedbackRatingStatus&quot;,&quot;NONE&quot;,&quot;mobileView&quot;,&quot;CONTEXT&quot;]],&quot;streakInfo&quot;,[&quot;^ &quot;,&quot;^B&quot;,&quot;StreakUnknown&quot;],&quot;streakScreen&quot;,[&quot;^ &quot;,&quot;show&quot;,false],&quot;systemStatus&quot;,[&quot;^?&quot;,[&quot;indicator&quot;,&quot;none&quot;,&quot;description&quot;,&quot;No status has been fetched from the Status Page.&quot;]],&quot;user&quot;,[&quot;^?&quot;,[&quot;status&quot;,&quot;not_initiate&quot;,&quot;settings&quot;,[&quot;^?&quot;,[&quot;aiFlags&quot;,[&quot;^?&quot;,[&quot;aiSolutionExplanationEnabled&quot;,false,&quot;aiErrorExplanationEnabled&quot;,false]]]]]],&quot;images&quot;,[&quot;^ &quot;,&quot;^3&lt;&quot;,&quot;course-1606-master:cb59605c00ed73a970165be3564ff450-20220811125027179&quot;,&quot;^B&quot;,&quot;singleImage&quot;]]]]\";</script><script>window.PRELOADED_LANGUAGE = \"en\";</script><div id=\"root\"><div class=\"theme progress-indicator--visible\"><style data-emotion=\"css 19enzrs\">.css-19enzrs{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:#F7F7FC;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:50px;padding-left:10px;padding-right:10px;position:relative;z-index:15;}</style><header data-cy=\"alpa-navbar\" class=\"css-19enzrs\"><style data-emotion=\"css vpr568\">.css-vpr568{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-pack:start;-ms-flex-pack:start;-webkit-justify-content:flex-start;justify-content:flex-start;}</style><div class=\"css-vpr568\"><style data-emotion=\"css 19lbh5u\">.css-19lbh5u{padding-left:6px;padding-right:6px;}</style><style data-emotion=\"css 12o4242\">.css-12o4242{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-color:transparent;border-radius:4px;border-style:solid;border-width:2px;cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:800;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;line-height:1;margin:0;outline:0;padding:0;position:relative;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:background-color 125ms ease-out;transition:background-color 125ms ease-out;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;vertical-align:middle;background-color:transparent;color:var(--wf-text--link, #0065D1);font-size:var(--wf-button--medium, 14px);height:36px;min-width:36px;width:auto;padding-left:16px;padding-right:16px;padding-left:6px;padding-right:6px;}.css-12o4242::before{border-radius:2px;content:\"\";display:block;height:100%;inset:0;position:absolute;width:100%;z-index:0;}.css-12o4242:active{background-color:transparent;}.css-12o4242:disabled{cursor:default;opacity:0.4;pointer-events:none;}.css-12o4242:hover{border-color:var(--wf-bg--hover, rgba(48, 57, 105, 0.06));}.css-12o4242:hover::before{background-color:var(--wf-bg--hover, rgba(48, 57, 105, 0.06));}.css-12o4242 >*{z-index:1;}</style><a class=\"alpa-navbar-logo css-12o4242\" data-cy=\"header-logo\" data-testid=\"alpa-navbar-logo\" data-trackid=\"alpa-navbar-logo\" href=\"https://www.datacamp.com\" aria-label=\"landing\"><style data-emotion=\"css 61bni1\">.css-61bni1{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:100%;gap:8px;}</style><span class=\"css-61bni1\"><svg viewbox=\"0 0 27 35\" xml:space=\"preserve\" xmlns=\"http://www.w3.org/2000/svg\" height=\"28\" class=\"css-0\"><path clip-rule=\"evenodd\" d=\"M11.699 8.514v8.333L2.858 21.89V3.44l8.841 5.074zm2.861 17.507v-7.51l11.84-6.757-2.88-1.65-8.96 5.112V7.68a1.442 1.442 0 0 0-.718-1.242L3.056.256C3.027.238 2.998.224 2.97.21A2.064 2.064 0 0 0 0 2.07v21.184a2.067 2.067 0 0 0 2.971 1.865l.082-.042 8.64-4.933v6.72c.002.513.277.987.722 1.243L23.502 34.4l2.88-1.651-11.822-6.728z\" fill=\"var(--wf-brand--text, #05192D)\" fill-rule=\"evenodd\"/></svg></span></a><nav aria-label=\"Breadcrumb\" data-testid=\"alpa-navbar-breadcrumbs\"><style data-emotion=\"css 1goqhco\">.css-1goqhco{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding:0;}.css-1goqhco div[aria-hidden=\\'true\\']{color:#5D6A77;display:none;padding:0 4px;}.css-1goqhco div[aria-hidden=\\'true\\']:first-of-type{display:inline-block;}@media screen and (min-width: 820px){.css-1goqhco div[aria-hidden=\\'true\\']{display:inline-block;}}.css-1goqhco li{display:none;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;-webkit-flex-shrink:1;-ms-flex-negative:1;flex-shrink:1;list-style:none;}@media screen and (min-width: 820px){.css-1goqhco li{display:inline-block;}}.css-1goqhco li:first-of-type{display:inline-block;}.css-1goqhco li:last-of-type{display:inline-block;}.css-1goqhco li a{color:#05192D;font-weight:normal;height:30px;line-height:21px;min-height:unset;padding:2px;}.css-1goqhco li a span{display:inline-block;max-width:20vw;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}.css-1goqhco li:last-of-type a{font-weight:bold;}</style><ol itemscope itemtype=\"http://schema.org/BreadcrumbList\" class=\"css-1goqhco\"><li itemprop=\"itemListElement\" itemscope itemtype=\"https://schema.org/ListItem\"><style data-emotion=\"css qwtpyn\">.css-qwtpyn{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-color:transparent;border-radius:4px;border-style:solid;border-width:2px;cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:800;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;line-height:1;margin:0;outline:0;padding:0;position:relative;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:background-color 125ms ease-out;transition:background-color 125ms ease-out;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;vertical-align:middle;background-color:transparent;color:var(--wf-text--link, #0065D1);font-size:var(--wf-button--medium, 14px);height:36px;min-width:36px;width:auto;padding-left:16px;padding-right:16px;}.css-qwtpyn::before{border-radius:2px;content:\"\";display:block;height:100%;inset:0;position:absolute;width:100%;z-index:0;}.css-qwtpyn:active{background-color:transparent;}.css-qwtpyn:disabled{cursor:default;opacity:0.4;pointer-events:none;}.css-qwtpyn:hover{border-color:var(--wf-bg--hover, rgba(48, 57, 105, 0.06));}.css-qwtpyn:hover::before{background-color:var(--wf-bg--hover, rgba(48, 57, 105, 0.06));}.css-qwtpyn >*{z-index:1;}</style><a data-trackid=\"alpa-navbar-breadcrumb-learn\" href=\"https://www.datacamp.com\" itemprop=\"item\" aria-label=\"Learn\" class=\"css-qwtpyn\"><span class=\"css-61bni1\"><span itemprop=\"name\">Learn</span></span></a><meta content=\"0\" itemprop=\"position\"></li><div aria-hidden=\"true\">/</div><li itemprop=\"itemListElement\" itemscope itemtype=\"https://schema.org/ListItem\"><a data-trackid=\"alpa-navbar-breadcrumb-courses\" href=\"https://www.datacamp.com/category/python\" itemprop=\"item\" aria-label=\"Courses\" class=\"css-qwtpyn\"><span class=\"css-61bni1\"><span itemprop=\"name\">Courses</span></span></a><meta content=\"1\" itemprop=\"position\"></li><div aria-hidden=\"true\">/</div><li itemprop=\"itemListElement\" itemscope itemtype=\"https://schema.org/ListItem\"><a data-trackid=\"alpa-navbar-breadcrumb-course\" href=\"https://www.datacamp.com/courses/intermediate-importing-data-in-python\" itemprop=\"item\" aria-label=\"Intermediate Importing Data in Python\" class=\"css-qwtpyn\"><span class=\"css-61bni1\"><span itemprop=\"name\">Intermediate Importing Data in Python</span></span></a><meta content=\"2\" itemprop=\"position\"></li></ol></nav></div><style data-emotion=\"css 1jov1vc\">.css-1jov1vc{-webkit-box-pack:initial;-ms-flex-pack:initial;-webkit-justify-content:initial;justify-content:initial;}</style><div class=\"css-1jov1vc\"></div><style data-emotion=\"css r4fpqc\">.css-r4fpqc{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-pack:end;-ms-flex-pack:end;-webkit-justify-content:flex-end;justify-content:flex-end;}</style><nav class=\"css-r4fpqc\"><style data-emotion=\"css 79elbk\">.css-79elbk{position:relative;}</style><div class=\"css-79elbk\"><style data-emotion=\"css 10ganm4\">.css-10ganm4{border:none;}</style><style data-emotion=\"css 178iibo\">.css-178iibo{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-color:transparent;border-radius:4px;border-style:solid;border-width:2px;cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:800;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;line-height:1;margin:0;outline:0;padding:0;position:relative;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:background-color 125ms ease-out;transition:background-color 125ms ease-out;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;vertical-align:middle;background-color:transparent;color:var(--wf-text--main, #05192D);font-size:var(--wf-button--medium, 14px);height:36px;min-width:36px;width:auto;padding-left:16px;padding-right:16px;border-color:var(--wf-border-color--interactive, rgba(48, 57, 105, 0.6));border:none;}.css-178iibo::before{border-radius:2px;content:\"\";display:block;height:100%;inset:0;position:absolute;width:100%;z-index:0;}.css-178iibo::before{border-radius:2px;margin:0;}.css-178iibo:active{background-color:transparent;}.css-178iibo:disabled{cursor:default;opacity:0.4;pointer-events:none;}.css-178iibo:hover::before{background-color:var(--wf-bg--hover, rgba(48, 57, 105, 0.06));}.css-178iibo >*{z-index:1;}</style><button class=\"css-178iibo\" aria-label=\"course-menu\" type=\"button\"><span class=\"css-61bni1\"><svg aria-hidden=\"true\" height=\"16\" width=\"16\" viewbox=\"0 0 18 18\"><path fill=\"currentColor\" d=\"M2 11a2 2 0 1 1 0-4 2 2 0 0 1 0 4Zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4Zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4Z\"/></svg></span></button><style data-emotion=\"css h7pn2b\">.css-h7pn2b{position:absolute;top:32px;right:0;padding:8px;padding-left:0;padding-right:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;max-width:90dvw;z-index:5000;width:0;height:0;overflow:hidden;}.css-h7pn2b button span{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:start;-ms-flex-pack:start;-webkit-justify-content:flex-start;justify-content:flex-start;font-weight:400;width:100%;}</style><style data-emotion=\"css 1ydqvfl\">.css-1ydqvfl{background-color:var(--wf-bg--contrast, #FFFFFF);border-color:var(--wf-border-color--main, rgba(48, 57, 105, 0.15));border-radius:4px;border-style:solid;border-width:1px;display:block;outline:0;padding:16px;position:relative;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:box-shadow 600ms cubic-bezier(0.1, 0.8, 0.2, 1),-webkit-transform 600ms cubic-bezier(0.1, 0.8, 0.2, 1);transition:box-shadow 600ms cubic-bezier(0.1, 0.8, 0.2, 1),transform 600ms cubic-bezier(0.1, 0.8, 0.2, 1);position:absolute;top:32px;right:0;padding:8px;padding-left:0;padding-right:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;max-width:90dvw;z-index:5000;width:0;height:0;overflow:hidden;}.css-1ydqvfl:where(a, button){cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;}.css-1ydqvfl button span{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:start;-ms-flex-pack:start;-webkit-justify-content:flex-start;justify-content:flex-start;font-weight:400;width:100%;}</style><section aria-hidden=\"true\" class=\"css-1ydqvfl\"><nav><style data-emotion=\"css 15c08fe\">.css-15c08fe{color:#05192D;border:none;width:100%;}@media screen and (min-width: 820px){.css-15c08fe{border-radius:0;}.css-15c08fe:after{border-radius:0;}}</style><style data-emotion=\"css btjij\">.css-btjij{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-color:transparent;border-radius:4px;border-style:solid;border-width:2px;cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:800;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;line-height:1;margin:0;outline:0;padding:0;position:relative;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:background-color 125ms ease-out;transition:background-color 125ms ease-out;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;vertical-align:middle;background-color:transparent;color:var(--wf-text--main, #05192D);font-size:var(--wf-button--medium, 14px);height:36px;min-width:36px;width:auto;padding-left:16px;padding-right:16px;border-color:var(--wf-border-color--interactive, rgba(48, 57, 105, 0.6));color:#05192D;border:none;width:100%;}.css-btjij::before{border-radius:2px;content:\"\";display:block;height:100%;inset:0;position:absolute;width:100%;z-index:0;}.css-btjij::before{border-radius:2px;margin:0;}.css-btjij:active{background-color:transparent;}.css-btjij:disabled{cursor:default;opacity:0.4;pointer-events:none;}.css-btjij:hover::before{background-color:var(--wf-bg--hover, rgba(48, 57, 105, 0.06));}.css-btjij >*{z-index:1;}@media screen and (min-width: 820px){.css-btjij{border-radius:0;}.css-btjij:after{border-radius:0;}}</style><button data-cy=\"header-outline\" class=\"css-btjij\" type=\"button\"><span class=\"css-61bni1\"><svg aria-hidden=\"true\" height=\"16\" width=\"16\" viewbox=\"0 0 18 18\" style=\"flex-shrink:0\"><path fill=\"currentColor\" d=\"M4 6a1 1 0 1 1 0-2h10a1 1 0 0 1 0 2H4Zm0 4a1 1 0 1 1 0-2h10a1 1 0 0 1 0 2H4Zm0 4a1 1 0 0 1 0-2h10a1 1 0 0 1 0 2H4Z\"/></svg>Course Outline</span></button><style data-emotion=\"css ntp9k4\">.css-ntp9k4{height:0;overflow:hidden;position:absolute;top:-9000px;left:-9000px;}</style><div aria-hidden=\"true\" class=\"css-ntp9k4\"><ul data-cy=\"outline-container\"><style data-emotion=\"css p1ihnf\">.css-p1ihnf{border:1px solid rgba(48, 57, 105, 0.15);border-radius:4px;margin-bottom:16px;overflow:hidden;}</style><li data-cy=\"outline-chapter\" class=\"css-p1ihnf\"><style data-emotion=\"css 15hicbc\">.css-15hicbc{padding:16px;padding-bottom:0;}</style><div class=\"css-15hicbc\"><style data-emotion=\"css k008qs\">.css-k008qs{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}</style><header class=\"css-k008qs\"><style data-emotion=\"css mwfol9\">.css-mwfol9{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;gap:8px;}</style><div class=\"css-mwfol9\"><style data-emotion=\"css ty7r4z\">.css-ty7r4z{font-weight:800;}</style><style data-emotion=\"css kdtjtf\">.css-kdtjtf{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-family:Studio-Feixen-Sans,Arial,sans-serif;overflow:hidden;background:var(--wf-navy--main, #05192D);border-radius:24px;color:var(--wf-navy--text-on-color, #FFFFFF);font-size:12px;height:24px;line-height:24px;width:24px;font-weight:800;}</style><div class=\"css-kdtjtf\"><style data-emotion=\"css 3xjkdl\">.css-3xjkdl{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-variant-numeric:tabular-nums;height:100%;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;width:100%;}.css-3xjkdl>*{margin:auto 0;}.css-3xjkdl>:not(svg){min-width:100%;}.css-3xjkdl>*{min-height:14px;}.css-3xjkdl>svg{width:14px;}</style><div class=\"css-3xjkdl\">1</div></div><style data-emotion=\"css 1uk1gs8\">.css-1uk1gs8{margin:0;}</style><style data-emotion=\"css 1g5cl9a\">.css-1g5cl9a{font-weight:800;line-height:1.25;margin-bottom:8px;font-size:16px;margin:0;}</style><style data-emotion=\"css fbt0po\">.css-fbt0po{color:var(--wf-text--main, #05192D);font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:400;line-height:1.5;margin:0;padding:0;font-size:14px;font-weight:800;line-height:1.25;margin-bottom:8px;font-size:16px;margin:0;}</style><h3 class=\"css-fbt0po\">Importing data from the Internet</h3><style data-emotion=\"css 17lebkx\">.css-17lebkx{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:800;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;letter-spacing:0.5px;line-height:1;padding:0 4px;text-transform:uppercase;border-radius:4px;background-color:var(--wf-yellow--main, #FCCE0D);color:var(--wf-yellow--main, #FCCE0D);font-size:12px;height:18px;}.css-17lebkx span{color:var(--wf-yellow--text-on-color, #05192D);}</style><style data-emotion=\"css 1gjxyd4\">.css-1gjxyd4{color:var(--wf-text--main, #05192D);font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:400;line-height:1.5;margin:0;padding:0;font-size:14px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:800;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;letter-spacing:0.5px;line-height:1;padding:0 4px;text-transform:uppercase;border-radius:4px;background-color:var(--wf-yellow--main, #FCCE0D);color:var(--wf-yellow--main, #FCCE0D);font-size:12px;height:18px;}.css-1gjxyd4 span{color:var(--wf-yellow--text-on-color, #05192D);}</style><span class=\"css-1gjxyd4\"><style data-emotion=\"css 19ist84\">.css-19ist84{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:inherit;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;gap:4px;max-width:164px;}</style><span class=\"css-19ist84\"><style data-emotion=\"css 8uhtka\">.css-8uhtka{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}</style><span class=\"css-8uhtka\">Free</span></span></span></div><style data-emotion=\"css 1krpqay\">.css-1krpqay{padding-left:16px;width:200px;}</style><div class=\"css-1krpqay\"><style data-emotion=\"css 18oyfde\">.css-18oyfde{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:var(--wf-text--subtle, #5D6A77);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-family:Studio-Feixen-Sans,Arial,sans-serif;gap:8px;line-height:1;width:100%;font-size:12px;}</style><div data-testid=\"progress-wrapper\" class=\"css-18oyfde\"><style data-emotion=\"css k1hg5o\">.css-k1hg5o{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-flex:1;-ms-flex:1;flex:1;position:relative;width:100%;}</style><div class=\"css-k1hg5o\"><style data-emotion=\"css 11s5m7q\">.css-11s5m7q{-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;background-color:var(--wf-transparent-grey--lighter, rgba(48, 57, 105, 0.15));border:none;width:100%;border-radius:4px;-webkit-clip-path:url(#progress-null-clip);clip-path:url(#progress-null-clip);height:8px;}.css-11s5m7q::-webkit-progress-bar{background-color:inherit;}.css-11s5m7q::-moz-progress-bar{-webkit-transition:width 1s ease-in-out;transition:width 1s ease-in-out;background-color:var(--wf-brand--main, #03EF62);border-radius:4px;}.css-11s5m7q::-webkit-progress-bar{border-radius:4px;}.css-11s5m7q::-webkit-progress-value{-webkit-transition:width 1s ease-in-out;transition:width 1s ease-in-out;background-color:var(--wf-brand--main, #03EF62);border-radius:4px;}</style><progress aria-label=\"Progress\" aria-valuemax=\"100\" aria-valuemin=\"0\" aria-valuenow=\"0\" id=\"progress-null\" max=\"100\" value=\"0\" class=\"css-11s5m7q\"></progress></div><label for=\"progress-null\"><span aria-hidden=\"true\">0%</span></label></div></div></header><style data-emotion=\"css y1tonc\">.css-y1tonc{font-size:14px;font-family:Studio-Feixen-Sans,Arial,sans-serif;line-height:1.5;}</style><div class=\"css-y1tonc\"><p class>The web is a rich source of data from which you can extract various types of insights and findings. In this chapter, you will learn how to get data from the web, whether it is stored in files or in HTML. You&apos;ll also learn the basics of scraping and parsing web data.</p></div></div><style data-emotion=\"css 1lkdjmn\">.css-1lkdjmn{background-color:#F7F7FC;padding:8px;}</style><div class=\"css-1lkdjmn\"><style data-emotion=\"css p9ltrc\">.css-p9ltrc{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-color:transparent;border-radius:4px;border-style:solid;border-width:2px;cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:800;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;line-height:1;margin:0;outline:0;padding:0;position:relative;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:background-color 125ms ease-out;transition:background-color 125ms ease-out;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;vertical-align:middle;background-color:transparent;color:var(--wf-text--link, #0065D1);font-size:12px;height:28px;min-width:28px;width:auto;padding-left:8px;padding-right:8px;}.css-p9ltrc::before{border-radius:2px;content:\"\";display:block;height:100%;inset:0;position:absolute;width:100%;z-index:0;}.css-p9ltrc:active{background-color:transparent;}.css-p9ltrc:disabled{cursor:default;opacity:0.4;pointer-events:none;}.css-p9ltrc:hover{border-color:var(--wf-bg--hover, rgba(48, 57, 105, 0.06));}.css-p9ltrc:hover::before{background-color:var(--wf-bg--hover, rgba(48, 57, 105, 0.06));}.css-p9ltrc >*{z-index:1;}</style><button tabindex=\"-1\" data-cy=\"outline-expand-chapter\" type=\"button\" class=\"css-p9ltrc\"><style data-emotion=\"css xejdhu\">.css-xejdhu{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:100%;gap:4px;}</style><span class=\"css-xejdhu\">View Chapter Details<svg aria-hidden=\"true\" height=\"14\" width=\"14\" viewbox=\"0 0 18 18\" style=\"flex-shrink:0\"><path fill=\"currentColor\" d=\"m8.244 12.155-4.95-4.947a1 1 0 1 1 1.415-1.415l4.294 4.291 4.293-4.279a.998.998 0 0 1 1.413.003c.39.392.388 1.025-.003 1.415l-5.002 4.986a.998.998 0 0 1-1.46-.054Z\"/></svg></span></button></div></li><li data-cy=\"outline-chapter\" class=\"css-p1ihnf\"><div class=\"css-15hicbc\"><header class=\"css-k008qs\"><div class=\"css-mwfol9\"><div class=\"css-kdtjtf\"><div class=\"css-3xjkdl\">2</div></div><h3 class=\"css-fbt0po\">Interacting with APIs to import data from the web</h3></div><div class=\"css-1krpqay\"><div data-testid=\"progress-wrapper\" class=\"css-18oyfde\"><div class=\"css-k1hg5o\"><progress aria-label=\"Progress\" aria-valuemax=\"100\" aria-valuemin=\"0\" aria-valuenow=\"0\" id=\"progress-null\" max=\"100\" value=\"0\" class=\"css-11s5m7q\"></progress></div><label for=\"progress-null\"><span aria-hidden=\"true\">0%</span></label></div></div></header><div class=\"css-y1tonc\"><p class>In this chapter, you will gain a deeper understanding of how to import data from the web. You will learn the basics of extracting data from APIs, gain insight on the importance of APIs, and practice extracting data by diving into the OMDB and Library of Congress APIs.</p></div></div><div class=\"css-1lkdjmn\"><button tabindex=\"-1\" data-cy=\"outline-expand-chapter\" type=\"button\" class=\"css-p9ltrc\"><span class=\"css-xejdhu\">View Chapter Details<svg aria-hidden=\"true\" height=\"14\" width=\"14\" viewbox=\"0 0 18 18\" style=\"flex-shrink:0\"><path fill=\"currentColor\" d=\"m8.244 12.155-4.95-4.947a1 1 0 1 1 1.415-1.415l4.294 4.291 4.293-4.279a.998.998 0 0 1 1.413.003c.39.392.388 1.025-.003 1.415l-5.002 4.986a.998.998 0 0 1-1.46-.054Z\"/></svg></span></button></div></li><li data-cy=\"outline-chapter\" class=\"css-p1ihnf\"><div class=\"css-15hicbc\"><header class=\"css-k008qs\"><div class=\"css-mwfol9\"><div class=\"css-kdtjtf\"><div class=\"css-3xjkdl\">3</div></div><h3 class=\"css-fbt0po\">Diving  deep into the Twitter API</h3></div><div class=\"css-1krpqay\"><div data-testid=\"progress-wrapper\" class=\"css-18oyfde\"><div class=\"css-k1hg5o\"><progress aria-label=\"Progress\" aria-valuemax=\"100\" aria-valuemin=\"0\" aria-valuenow=\"0\" id=\"progress-null\" max=\"100\" value=\"0\" class=\"css-11s5m7q\"></progress></div><label for=\"progress-null\"><span aria-hidden=\"true\">0%</span></label></div></div></header><div class=\"css-y1tonc\"><p class>In this chapter, you will consolidate your knowledge of interacting with APIs in a deep dive into the Twitter streaming API. You&apos;ll learn how to stream real-time Twitter data, and how to analyze and visualize it.</p></div></div><div class=\"css-1lkdjmn\"><button tabindex=\"-1\" data-cy=\"outline-expand-chapter\" type=\"button\" class=\"css-p9ltrc\"><span class=\"css-xejdhu\">View Chapter Details<svg aria-hidden=\"true\" height=\"14\" width=\"14\" viewbox=\"0 0 18 18\" style=\"flex-shrink:0\"><path fill=\"currentColor\" d=\"m8.244 12.155-4.95-4.947a1 1 0 1 1 1.415-1.415l4.294 4.291 4.293-4.279a.998.998 0 0 1 1.413.003c.39.392.388 1.025-.003 1.415l-5.002 4.986a.998.998 0 0 1-1.46-.054Z\"/></svg></span></button></div></li></ul></div></nav><style data-emotion=\"css 16cuyl0\">.css-16cuyl0{color:#05192D;}</style><style data-emotion=\"css r7pabm\">.css-r7pabm{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-color:transparent;border-radius:4px;border-style:solid;border-width:2px;cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:800;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;line-height:1;margin:0;outline:0;padding:0;position:relative;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:background-color 125ms ease-out;transition:background-color 125ms ease-out;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;vertical-align:middle;background-color:transparent;color:var(--wf-text--link, #0065D1);font-size:var(--wf-button--medium, 14px);height:36px;min-width:36px;width:auto;padding-left:16px;padding-right:16px;color:#05192D;}.css-r7pabm::before{border-radius:2px;content:\"\";display:block;height:100%;inset:0;position:absolute;width:100%;z-index:0;}.css-r7pabm:active{background-color:transparent;}.css-r7pabm:disabled{cursor:default;opacity:0.4;pointer-events:none;}.css-r7pabm:hover{border-color:var(--wf-bg--hover, rgba(48, 57, 105, 0.06));}.css-r7pabm:hover::before{background-color:var(--wf-bg--hover, rgba(48, 57, 105, 0.06));}.css-r7pabm >*{z-index:1;}</style><button data-cy=\"header-slides\" class=\"css-r7pabm\" type=\"button\"><span class=\"css-61bni1\"><svg aria-hidden=\"true\" height=\"14\" width=\"14\" viewbox=\"0 0 18 18\"><path fill=\"currentColor\" d=\"M14 9.004H9.996a2 2 0 0 1-2-2V2H4v14h10V9.004Zm1.828-2.815A1.938 1.938 0 0 1 16 7v9a2 2 0 0 1-2 2H4a2 2 0 0 1-2-2V2a2 2 0 0 1 2-2h5.003a2 2 0 0 1 1.415.586l4.997 5a2 2 0 0 1 .413.603Zm-1.832.815-4-4v4h4Z\"/></svg>Show Slides</span></button><button data-cy=\"header-video\" class=\"css-r7pabm\" aria-label=\"Show video\" type=\"button\"><span class=\"css-61bni1\"><svg aria-hidden=\"true\" height=\"14\" width=\"14\" viewbox=\"0 0 18 18\"><path fill=\"currentColor\" d=\"m13 6.3 3.331-2.998A1 1 0 0 1 18 4.045v9.91a1 1 0 0 1-1.669.743L13 11.7V14c0 .552-.485 1-1.083 1H1.083C.485 15 0 14.552 0 14V4c0-.552.485-1 1.083-1h10.834C12.515 3 13 3.448 13 4v2.3Zm0 2.69v.02l3 2.7V6.29l-3 2.7ZM2 5v8h9V5H2Z\"/></svg>Show Video</span></button><button data-cy=\"header-notes\" class=\"css-r7pabm\" type=\"button\"><span class=\"css-61bni1\"><svg aria-hidden=\"true\" height=\"14\" width=\"14\" viewbox=\"0 0 18 18\"><path fill=\"currentColor\" fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M12.528.293a.999.999 0 0 1 1.414 0l3.765 3.765a.999.999 0 0 1 0 1.414L5.472 17.707a1 1 0 0 1-.707.293H1a1 1 0 0 1-1-1v-3.765c0-.265.105-.52.293-.707L12.528.293zM2 13.65V16h2.35l8.412-8.412-2.35-2.35L2 13.65zm9.826-9.826 2.351 2.351 1.409-1.409-2.351-2.35-1.409 1.408zM16.529 18h-8a1 1 0 0 1 0-2h8a1 1 0 0 1 0 2z\"/></svg>Take Notes</span></button><button data-cy=\"header-mobile\" class=\"css-r7pabm\" type=\"button\"><span class=\"css-61bni1\"><svg aria-hidden=\"true\" height=\"14\" width=\"14\" viewbox=\"0 0 18 18\"><path fill=\"currentColor\" d=\"M5.5 2v14h7V2h-7Zm-1-2h9a1 1 0 0 1 1 1v16a1 1 0 0 1-1 1h-9a1 1 0 0 1-1-1V1a1 1 0 0 1 1-1Zm4 13h1a1 1 0 0 1 0 2h-1a1 1 0 0 1 0-2Z\"/></svg>Continue Learning on Mobile</span></button><button data-cy=\"header-issue\" data-test-id=\"header-report-issue-button\" class=\"css-r7pabm\" type=\"button\"><span class=\"css-61bni1\"><svg aria-hidden=\"true\" height=\"14\" width=\"14\" viewbox=\"0 0 18 18\"><path fill=\"currentColor\" fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M17.744 14.31 10.869 1.647a2.119 2.119 0 0 0-3.72 0L.268 14.31a2.116 2.116 0 0 0 1.862 3.148h13.75A2.122 2.122 0 0 0 18 15.383a2.12 2.12 0 0 0-.256-1.052v-.021zm-2.054.419L9.448 3.24a.5.5 0 0 0-.879 0L2.322 14.73a.5.5 0 0 0 .439.739H15.25a.502.502 0 0 0 .44-.74zM8.02 7.017a.994.994 0 1 1 1.99 0v2.57a.994.994 0 1 1-1.99 0v-2.57zm1.021 6.961a1.144 1.144 0 0 1-1.054-.704 1.143 1.143 0 0 1 .247-1.243 1.14 1.14 0 0 1 1.947.807 1.14 1.14 0 0 1-1.14 1.14z\"/></svg>Provide Feedback</span></button><style data-emotion=\"css xqh66l\">.css-xqh66l{margin:8px;color:#05192D;opacity:0.15;border-bottom:none;}</style><hr class=\"css-xqh66l\"><style data-emotion=\"css yv011k\">.css-yv011k{padding:8px;padding-left:16px;padding-right:16px;}</style><div class=\"css-yv011k\"><div data-cy=\"header-session\" css=\"[object Object]\"><style data-emotion=\"css 8zmdb0\">.css-8zmdb0{color:#03EF62;}</style><svg aria-hidden=\"true\" height=\"16\" width=\"16\" viewbox=\"0 0 18 18\" aria-label=\"Session Ready\" class=\"css-8zmdb0\"><path fill=\"currentColor\" d=\"M9 18A9 9 0 1 1 9 0a9 9 0 0 1 0 18Z\"/></svg><style data-emotion=\"css 1isemmb\">.css-1isemmb{margin-left:8px;}</style><span class=\"css-1isemmb\">Initializing</span></div></div></section></div></nav></header><style data-emotion=\"css iqa0tj\">.css-iqa0tj{position:absolute;top:100px;bottom:32px;right:12px;left:12px;overflow:hidden;}</style><main class=\"css-iqa0tj\"><div data-cy=\"server-side-loader-placeholder\"><aside class=\"exercise--sidebar\" style=\"width:40%\"><div class=\"exercise--sidebar-content\"><div class=\"listview__outer\"><div class=\"listview__inner\"><div class=\"listview__section\"><div><div role=\"button\" class=\"listview__header\"><style data-emotion=\"css r7m65a\">.css-r7m65a{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:100%;padding-right:16px;}</style><div class=\"css-r7m65a\"><style data-emotion=\"css 171fln0\">.css-171fln0{font-size:16px;margin:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:8px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-left:16px;}</style><style data-emotion=\"css 10nfsoz\">.css-10nfsoz{font-weight:800;line-height:1.25;margin-bottom:8px;font-size:18px;font-size:16px;margin:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:8px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-left:16px;}</style><style data-emotion=\"css 1tah88q\">.css-1tah88q{color:var(--wf-text--main, #05192D);font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:400;line-height:1.5;margin:0;padding:0;font-size:14px;font-weight:800;line-height:1.25;margin-bottom:8px;font-size:18px;font-size:16px;margin:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:8px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-left:16px;}</style><h2 class=\"css-1tah88q\"><svg aria-hidden=\"true\" height=\"14\" width=\"14\" viewbox=\"0 0 18 18\"><path fill=\"currentColor\" d=\"M4 2a1 1 0 0 0-1 1v12a1 1 0 0 0 1 1h10a1 1 0 0 0 1-1V3a1 1 0 0 0-1-1H4Zm0-2h10a3 3 0 0 1 3 3v12a3 3 0 0 1-3 3H4a3 3 0 0 1-3-3V3a3 3 0 0 1 3-3Zm2 6h6a1 1 0 0 1 0 2H6a1 1 0 1 1 0-2Zm0 4h6a1 1 0 0 1 0 2H6a1 1 0 0 1 0-2Z\"/></svg>Exercise</h2></div></div></div><div class=\"listview__content\"><style data-emotion=\"css ikv0qb\">.css-ikv0qb{position:relative;padding:16px;}</style><div class=\"css-ikv0qb\"><style data-emotion=\"css 1c1rk5o\">.css-1c1rk5o{font-weight:800;line-height:1.25;margin-bottom:8px;font-size:20px;}</style><style data-emotion=\"css fsa3o0\">.css-fsa3o0{color:var(--wf-text--main, #05192D);font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:400;line-height:1.5;margin:0;padding:0;font-size:14px;font-weight:800;line-height:1.25;margin-bottom:8px;font-size:20px;}</style><h1 class=\"css-fsa3o0\">Importing flat files from the web: your turn!</h1><style data-emotion=\"css 8czf7d\">.css-8czf7d{line-height:1.5;}.css-8czf7d code{font-family:JetBrainsMonoNL,Menlo,Monaco,\\'Courier New\\',monospace;margin:0 2px;padding:2px 4px;line-height:1.25;background-color:#EFEFF5;border-radius:4px;font-size:86%;mix-blend-mode:multiply;}.css-8czf7d pre{background-color:#EFEFF5;padding:8px;margin:0;border-radius:4px;tab-size:4;white-space:pre;line-height:1.25;mix-blend-mode:multiply;}.css-8czf7d pre>code{margin:0;padding:0;background-color:transparent;}.css-8czf7d ul,.css-8czf7d ol{padding-left:16px;}.css-8czf7d ul:first-of-type,.css-8czf7d ol:first-of-type{margin-top:0;}.css-8czf7d p:first-of-type{margin-top:0;}.css-8czf7d li{margin-bottom:8px;}.css-8czf7d a{color:#0065D1;-webkit-text-decoration:none;text-decoration:none;font-weight:800;border-radius:4px;outline:0;}.css-8czf7d a:hover{color:#0065D1;-webkit-text-decoration:underline;text-decoration:underline;}.css-8czf7d a:focus-visible{box-shadow:0 0 0 2px #257DFE;}.css-8czf7d a code{color:#0065D1;}.css-8czf7d hr{background-color:rgba(48, 57, 105, 0.15);border:0;height:1px;margin:16px 0;}</style><style data-emotion=\"css alxior\">.css-alxior{color:var(--wf-text--main, #05192D);font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:400;line-height:1.5;margin:0;padding:0;font-size:14px;line-height:1.5;}.css-alxior code{font-family:JetBrainsMonoNL,Menlo,Monaco,\\'Courier New\\',monospace;margin:0 2px;padding:2px 4px;line-height:1.25;background-color:#EFEFF5;border-radius:4px;font-size:86%;mix-blend-mode:multiply;}.css-alxior pre{background-color:#EFEFF5;padding:8px;margin:0;border-radius:4px;tab-size:4;white-space:pre;line-height:1.25;mix-blend-mode:multiply;}.css-alxior pre>code{margin:0;padding:0;background-color:transparent;}.css-alxior ul,.css-alxior ol{padding-left:16px;}.css-alxior ul:first-of-type,.css-alxior ol:first-of-type{margin-top:0;}.css-alxior p:first-of-type{margin-top:0;}.css-alxior li{margin-bottom:8px;}.css-alxior a{color:#0065D1;-webkit-text-decoration:none;text-decoration:none;font-weight:800;border-radius:4px;outline:0;}.css-alxior a:hover{color:#0065D1;-webkit-text-decoration:underline;text-decoration:underline;}.css-alxior a:focus-visible{box-shadow:0 0 0 2px #257DFE;}.css-alxior a code{color:#0065D1;}.css-alxior hr{background-color:rgba(48, 57, 105, 0.15);border:0;height:1px;margin:16px 0;}</style><div class=\"css-alxior\"><div class><p>You are about to import your first file from the web! The flat file you will import will be <code>&apos;winequality-red.csv&apos;</code> from the University of California, Irvine&apos;s <a href=\"https://archive.ics.uci.edu/ml/index.php\">Machine Learning repository</a>. The flat file contains tabular data of physiochemical properties of red wine, such as pH, alcohol content and citric acid content, along with wine quality rating.</p>\\n<p>The URL of the file is</p>\\n<pre><code>&apos;https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&apos;\\n</code></pre>\\n<p>After you import it, you&apos;ll check your working directory to confirm that it is there and then you&apos;ll load it into a <code>pandas</code> DataFrame.</p></div></div></div></div></div><div class=\"listview__section\" style=\"min-height:calc(50% - 33px)\"><div><div role=\"button\" class=\"listview__header\"><div class=\"css-r7m65a\"><style data-emotion=\"css 1ubtfgv\">.css-1ubtfgv{font-size:16px;margin:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:8px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-left:16px;width:100%;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}</style><style data-emotion=\"css ycumlt\">.css-ycumlt{font-weight:800;line-height:1.25;margin-bottom:8px;font-size:18px;font-size:16px;margin:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:8px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-left:16px;width:100%;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}</style><style data-emotion=\"css 1kphf8n\">.css-1kphf8n{color:var(--wf-text--main, #05192D);font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:400;line-height:1.5;margin:0;padding:0;font-size:14px;font-weight:800;line-height:1.25;margin-bottom:8px;font-size:18px;font-size:16px;margin:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:8px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-left:16px;width:100%;-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;}</style><h2 class=\"css-1kphf8n\"><svg aria-hidden=\"true\" height=\"14\" width=\"14\" viewbox=\"0 0 18 18\"><path fill=\"currentColor\" d=\"M9 16A7 7 0 1 0 9 2a7 7 0 0 0 0 14Zm0 2A9 9 0 1 1 9 0a9 9 0 0 1 0 18Zm2.326-11.96a1 1 0 0 1 1.555 1.258L8.773 12.37a1 1 0 0 1-1.534.024l-2.124-2.46a1 1 0 0 1 1.514-1.307l1.342 1.556 3.355-4.144Z\"/></svg>Instructions</h2><style data-emotion=\"css 17lebkx\">.css-17lebkx{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:800;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;letter-spacing:0.5px;line-height:1;padding:0 4px;text-transform:uppercase;border-radius:4px;background-color:var(--wf-yellow--main, #FCCE0D);color:var(--wf-yellow--main, #FCCE0D);font-size:12px;height:18px;}.css-17lebkx span{color:var(--wf-yellow--text-on-color, #05192D);}</style><style data-emotion=\"css 1gjxyd4\">.css-1gjxyd4{color:var(--wf-text--main, #05192D);font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:400;line-height:1.5;margin:0;padding:0;font-size:14px;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-weight:800;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;letter-spacing:0.5px;line-height:1;padding:0 4px;text-transform:uppercase;border-radius:4px;background-color:var(--wf-yellow--main, #FCCE0D);color:var(--wf-yellow--main, #FCCE0D);font-size:12px;height:18px;}.css-1gjxyd4 span{color:var(--wf-yellow--text-on-color, #05192D);}</style><span class=\"css-1gjxyd4\"><style data-emotion=\"css 19ist84\">.css-19ist84{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:inherit;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;gap:4px;max-width:164px;}</style><span class=\"css-19ist84\"><style data-emotion=\"css 8uhtka\">.css-8uhtka{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}</style><span class=\"css-8uhtka\">100 XP</span></span></span></div></div></div><div class=\"listview__content\"><div><style data-emotion=\"css 186pnwx\">.css-186pnwx{-webkit-flex:1;-ms-flex:1;flex:1;position:relative;padding:16px;}</style><div class=\"css-186pnwx\"><div class=\"css-alxior\"><div class=\"exercise--instructions__content\"><ul>\\n<li>Import the function <code>urlretrieve</code> from the subpackage <code>urllib.request</code>.</li>\\n<li>Assign the URL of the file to the variable <code>url</code>.</li>\\n<li>Use the function <code>urlretrieve()</code> to save the file locally as <code>&apos;winequality-red.csv&apos;</code>.</li>\\n<li>Execute the remaining code to load <code>&apos;winequality-red.csv&apos;</code> in a pandas DataFrame and to print its head to the shell.</li>\\n</ul></div></div><style data-emotion=\"css kbabwt\">.css-kbabwt{margin:16px -16px 0;}</style><div class=\"css-kbabwt\"><section class=\"dc-sct-feedback\" tabindex=\"-1\"><div></div><nav class=\"dc-sct-feedback__nav\"><style data-emotion=\"css n085mf\">.css-n085mf{padding-left:16px;}</style><div class=\"css-n085mf\"><style data-emotion=\"css 12j1yck\">.css-12j1yck{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;border-radius:4px;border-style:solid;border-width:2px;cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;-webkit-text-decoration:none;text-decoration:none;text-transform:capitalize;-webkit-transition:0.15s;transition:0.15s;vertical-align:baseline;white-space:nowrap;background-color:transparent;border-color:#05192D;color:#05192D;padding:0 15px;}.css-12j1yck:active{-webkit-transform:perspective(1px) scale(0.975);-moz-transform:perspective(1px) scale(0.975);-ms-transform:perspective(1px) scale(0.975);transform:perspective(1px) scale(0.975);}.css-12j1yck:disabled,.css-12j1yck:hover:disabled,.css-12j1yck:active:disabled{-webkit-transform:none;-moz-transform:none;-ms-transform:none;transform:none;}.css-12j1yck:focus{outline:0;}.css-12j1yck:hover{background-color:rgba(5, 25, 45, 0.15);border-color:#05192D;color:#05192D;}</style><button class=\"dc-sct-feedback__nav--hint-solution css-12j1yck\" type=\"button\" data-cy=\"exercise-show-hint\"><svg viewbox=\"0 0 18 18\" aria-hidden=\"true\" height=\"16\" role=\"img\" width=\"16\"><path fill=\"currentColor\" d=\"M9 0a7 7 0 014.95 11.95l-.001-.001c-.794.795-.949 1.1-.949 2.051a1 1 0 01-2 0c0-1.548.396-2.325 1.535-3.467l.04-.037a5 5 0 10-7.11.037C6.605 11.675 7 12.453 7 14a1 1 0 01-2 0c0-.951-.155-1.256-.949-2.051A7 7 0 019 0zm0 7a1 1 0 011 1v6a1 1 0 01-2 0V8a1 1 0 011-1zm0 11c-1.657 0-3-.895-3-2h6c0 1.105-1.343 2-3 2z\" fill-rule=\"evenodd\"/></svg><style data-emotion=\"css aib9ji\">.css-aib9ji{font-size:14px;line-height:32px;color:#05192D;font-weight:bold;margin-left:8px;}</style><style data-emotion=\"css vvk465\">.css-vvk465{-webkit-font-smoothing:antialiased;color:#05192D;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-style:normal;font-size:14px;font-weight:400;font-size:14px;line-height:32px;color:#05192D;font-weight:bold;margin-left:8px;}</style><span class=\"css-vvk465\">Take Hint (-30 XP)</span></button></div></nav></section></div></div></div></div></div></div></div></div></aside><section class=\"exercise--content\" style=\"width:60%\"><div class=\"exercise-waiting\"><style data-emotion=\"css 1gnr744\">.css-1gnr744{position:absolute;top:50%;left:50%;-webkit-transform:translate(-50%, -50%);-moz-transform:translate(-50%, -50%);-ms-transform:translate(-50%, -50%);transform:translate(-50%, -50%);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;}</style><div class=\"css-1gnr744\"><style data-emotion=\"css 1rm9ybb animation-1pv1bkr\">.css-1rm9ybb{-webkit-animation:animation-1pv1bkr cubic-bezier(0.65, 0, 0.55, 1) 2s infinite alternate;animation:animation-1pv1bkr cubic-bezier(0.65, 0, 0.55, 1) 2s infinite alternate;will-change:clip-path;--wf-loader--color:var(--wf-text--main, #05192D);width:50;}@-webkit-keyframes animation-1pv1bkr{0%,6%{-webkit-clip-path:polygon(0% -12%, 0% -12%, 169% 63%, 169% 63%);clip-path:polygon(0% -12%, 0% -12%, 169% 63%, 169% 63%);}100%{-webkit-clip-path:polygon(0% -12%, 0% 308%, 169% 383%, 169% 63%);clip-path:polygon(0% -12%, 0% 308%, 169% 383%, 169% 63%);}}@keyframes animation-1pv1bkr{0%,6%{-webkit-clip-path:polygon(0% -12%, 0% -12%, 169% 63%, 169% 63%);clip-path:polygon(0% -12%, 0% -12%, 169% 63%, 169% 63%);}100%{-webkit-clip-path:polygon(0% -12%, 0% 308%, 169% 383%, 169% 63%);clip-path:polygon(0% -12%, 0% 308%, 169% 383%, 169% 63%);}}</style><div aria-label=\"Loading\" data-testid=\"loader-wrapper\" role=\"alert\" class=\"css-1rm9ybb\"><style data-emotion=\"css 1j8nxo animation-1h2cwi2\">.css-1j8nxo{-webkit-animation:animation-1h2cwi2 cubic-bezier(0, 0, 0.85, 1) 2s infinite alternate;animation:animation-1h2cwi2 cubic-bezier(0, 0, 0.85, 1) 2s infinite alternate;will-change:clip-path;}@-webkit-keyframes animation-1h2cwi2{0%,71%{-webkit-clip-path:polygon(0% 0%, 0% 78.5%, 100% 34.5%, 100% -44%);clip-path:polygon(0% 0%, 0% 78.5%, 100% 34.5%, 100% -44%);}96%,100%{-webkit-clip-path:polygon(0% 0%, 0% 140%, 100% 96%, 100% -44%);clip-path:polygon(0% 0%, 0% 140%, 100% 96%, 100% -44%);}}@keyframes animation-1h2cwi2{0%,71%{-webkit-clip-path:polygon(0% 0%, 0% 78.5%, 100% 34.5%, 100% -44%);clip-path:polygon(0% 0%, 0% 78.5%, 100% 34.5%, 100% -44%);}96%,100%{-webkit-clip-path:polygon(0% 0%, 0% 140%, 100% 96%, 100% -44%);clip-path:polygon(0% 0%, 0% 140%, 100% 96%, 100% -44%);}}</style><div class=\"css-1j8nxo\"><style data-emotion=\"css zsjzbc\">.css-zsjzbc{-webkit-clip-path:polygon(-0.1% -10%, 169% 65%, -0.1% 139%);clip-path:polygon(-0.1% -10%, 169% 65%, -0.1% 139%);}</style><div class=\"css-zsjzbc\"><style data-emotion=\"css cihpzr\">.css-cihpzr{display:block;overflow:visible;stroke:var(--wf-loader--color, var(--wf-text--main, #05192D));}</style><svg viewbox=\"0 0 2640 3444\" width=\"50\" class=\"css-cihpzr\"><style data-emotion=\"css jy99qt animation-co7x2c\">.css-jy99qt{-webkit-animation:animation-co7x2c cubic-bezier(0.65, 0, 0.55, 1) 2s infinite alternate;animation:animation-co7x2c cubic-bezier(0.65, 0, 0.55, 1) 2s infinite alternate;stroke-dasharray:9800;stroke-dashoffset:9800;will-change:stroke-dashoffset;}@-webkit-keyframes animation-co7x2c{100%{stroke-dashoffset:0;}}@keyframes animation-co7x2c{100%{stroke-dashoffset:0;}}</style><path d=\"M0 0 M2569 1056L143 2447V149l1175 673v1867l1248 715\" fill=\"none\" stroke-linejoin=\"round\" stroke-width=\"300\" class=\"css-jy99qt\"/></svg></div></div></div></div><noscript></noscript></div></section></div></main><div class=\"exercise-footer\"><style data-emotion=\"css 8uttuf\">.css-8uttuf{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;width:100%;max-width:600px;list-style:none;margin:0;padding:0;gap:8px;}</style><ul data-cy=\"progress-container\" class=\"css-8uttuf\"><style data-emotion=\"css 149stfi\">.css-149stfi{-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;max-width:100px;}</style><li class=\"css-149stfi\"><style data-emotion=\"css 46tute\">.css-46tute{display:block;height:8px;border-radius:4px;background-color:rgba(48, 57, 105, 0.15);border-bottom:0;outline:0;}.css-46tute:focus-visible{box-shadow:0 0 0 2px #257DFE;}</style><a href=\"javascript:void(0)\" data-testid=\"progress-indicator-item\" class=\"css-46tute\"><style data-emotion=\"css 1pw8nbl\">.css-1pw8nbl{-webkit-transition:width 250ms linear;transition:width 250ms linear;height:8px;border-radius:4px;background-color:#5EB1FF;}</style><div style=\"width:0%\" class=\"css-1pw8nbl\"></div></a></li><li class=\"css-149stfi\"><a href=\"javascript:void(0)\" data-testid=\"progress-indicator-item\" class=\"css-46tute\"><div style=\"width:0%\" class=\"css-1pw8nbl\"></div></a></li><li class=\"css-149stfi\"><a href=\"javascript:void(0)\" data-testid=\"progress-indicator-item\" class=\"css-46tute\"><div style=\"width:0%\" class=\"css-1pw8nbl\"></div></a></li></ul></div></div></div><script>window.MathJax={options:{ignoreHtmlClass:\"tex2jax_ignore\",processHtmlClass:\"tex2jax_process\"},tex:{autoload:{color:[],colorV2:[\"color\"]},packages:{\"[+]\":[\"noerrors\"]}},loader:{load:[\"[tex]/noerrors\"]}}</script><script src=\"/campus/mathjax@3/es5/tex-chtml.js\" id=\"MathJax-script\" async></script><script>!function(e){function o(o){for(var d,n,f=o[0],s=o[1],m=o[2],b=0,v=[];b<f.length;b++)n=f[b],Object.prototype.hasOwnProperty.call(a,n)&&a[n]&&v.push(a[n][0]),a[n]=0;for(d in s)Object.prototype.hasOwnProperty.call(s,d)&&(e[d]=s[d]);for(t&&t(o);v.length;)v.shift()();return r.push.apply(r,m||[]),c()}function c(){for(var e,o=0;o<r.length;o++){for(var c=r[o],d=!0,n=1;n<c.length;n++){var s=c[n];0!==a[s]&&(d=!1)}d&&(r.splice(o--,1),e=f(f.s=c[0]))}return e}var d={},n={88:0},a={88:0},r=[];function f(o){if(d[o])return d[o].exports;var c=d[o]={i:o,l:!1,exports:{}};return e[o].call(c.exports,c,c.exports,f),c.l=!0,c.exports}f.e=function(e){var o=[];n[e]?o.push(n[e]):0!==n[e]&&{0:1,6:1,8:1,9:1,13:1,14:1,15:1,16:1,20:1,24:1,26:1,29:1,30:1,33:1,40:1,44:1,45:1,46:1,47:1,50:1,51:1,52:1,53:1,57:1,59:1,62:1,64:1,67:1,68:1,69:1,84:1,127:1,130:1,135:1,138:1,139:1,140:1}[e]&&o.push(n[e]=new Promise((function(o,c){for(var d=\"static/css/\"+({0:\"vendors~ce~dnde~ee~ge~idee~pmce~rde~se~ve~253ae210\",1:\"vendors~ce~dnde~ee~ge~idee~pmce~rde~se~ve~d7693ced\",2:\"vendors~ce~dnde~ee~ge~idee~pmce~rde~splitio~ve~253ae210\",3:\"vendors~ce~dnde~ee~ge~idee~pmce~rde~splitio~ve~c7ce39a9\",4:\"vendors~ce~dnde~ee~ge~idee~pmce~rde~splitio~ve~d49b3b41\",5:\"vendors~ce~dnde~ee~ge~idee~pmce~rde~ve~253ae210\",6:\"vendors~ce~dnde~ee~ge~idee~pmce~253ae210\",7:\"vendors~console-monaco~monaco~253ae210\",8:\"vendors~console-monaco~monaco~36912834\",9:\"vendors~console-monaco~monaco~3ec7091e\",10:\"vendors~console-monaco~monaco~49d8ad03\",11:\"vendors~console-monaco~monaco~4d911f2e\",12:\"vendors~console-monaco~monaco~5584ff6c\",13:\"vendors~console-monaco~monaco~57c220dc\",14:\"vendors~console-monaco~monaco~5e1bc0de\",15:\"vendors~console-monaco~monaco~6ddf31e8\",16:\"vendors~console-monaco~monaco~7c919857\",17:\"vendors~console-monaco~monaco~80993005\",18:\"vendors~console-monaco~monaco~85a20469\",19:\"vendors~console-monaco~monaco~86ffeb1a\",20:\"vendors~console-monaco~monaco~900aca88\",21:\"vendors~console-monaco~monaco~9c5b28f6\",22:\"vendors~console-monaco~monaco~ba2d52b5\",23:\"vendors~console-monaco~monaco~c52b823e\",24:\"vendors~console-monaco~monaco~c682183c\",25:\"vendors~console-monaco~monaco~c73e0090\",26:\"vendors~console-monaco~monaco~d7ac9e7b\",27:\"vendors~console-monaco~monaco~dac6e39f\",28:\"vendors~console-monaco~monaco~e66ff6d3\",29:\"vendors~console-monaco~monaco~f6e880b5\",30:\"vendors~console-monaco~monaco~f7d19227\",32:\"ce~06694820\",33:\"ce~2a42e354\",34:\"ce~40b10d04\",35:\"ce~62c53bf1\",36:\"ce~748942c6\",37:\"ce~a4f7b5e1\",38:\"ce~b56a696f\",39:\"console-monaco~31ecd969\",40:\"dnde~253ae210\",41:\"dnde~690b702c\",42:\"dnde~d0058ac6\",43:\"ee~253ae210\",44:\"ee~a4f7b5e1\",45:\"ge~06694820\",46:\"ge~36912834\",47:\"ge~3ec7091e\",48:\"ge~4d911f2e\",49:\"ge~5584ff6c\",50:\"ge~57c220dc\",51:\"ge~5e1bc0de\",52:\"ge~6ddf31e8\",53:\"ge~7f323393\",54:\"ge~80993005\",55:\"ge~85a20469\",56:\"ge~86ffeb1a\",57:\"ge~900aca88\",58:\"ge~9c5b28f6\",59:\"ge~b6351802\",60:\"ge~ba2d52b5\",61:\"ge~c52b823e\",62:\"ge~c682183c\",63:\"ge~c73e0090\",64:\"ge~d7ac9e7b\",65:\"ge~dac6e39f\",66:\"ge~e66ff6d3\",67:\"ge~f6e880b5\",68:\"ge~f7d19227\",69:\"idee~0f485567\",70:\"idee~253ae210\",76:\"markdown-renderer~daf2eb16\",77:\"modal-views~31ecd969\",78:\"monaco~31ecd969\",79:\"pmce~062b2d40\",80:\"pmce~0df6afb5\",81:\"pmce~1f20a385\",82:\"pmce~253ae210\",83:\"pmce~9c5b28f6\",84:\"pmce~a4f7b5e1\",85:\"pmce~d80adb5f\",86:\"pmce~ece0910c\",87:\"rde~3333b9b6\",124:\"vendors~markdown-renderer~253ae210\",125:\"vendors~markdown-renderer~3f764be9\",126:\"vendors~markdown-renderer~678f84af\",127:\"vendors~rde~0f485567\",128:\"vendors~rde~1f059a71\",129:\"vendors~rde~253ae210\",130:\"vendors~rde~3dd5b2e0\",131:\"vendors~rde~5f1bbfc7\",132:\"vendors~rde~7274e1de\",133:\"vendors~rde~b56a696f\",134:\"vendors~rde~c0fbb756\",135:\"vendors~rde~f9ca8911\",136:\"vendors~se~253ae210\",137:\"vendors~se~973c11d3\",138:\"vendors~se~a4f7b5e1\",139:\"vendors~ve~cfba5e8d\",140:\"vendors~xterm~0d30e071\",141:\"xterm~41ecf737\"}[e]||e)+\".\"+{0:\"556b226c\",1:\"31d6cfe0\",2:\"31d6cfe0\",3:\"31d6cfe0\",4:\"31d6cfe0\",5:\"31d6cfe0\",6:\"a8d4fad6\",7:\"31d6cfe0\",8:\"1ad26591\",9:\"98d862d2\",10:\"31d6cfe0\",11:\"31d6cfe0\",12:\"31d6cfe0\",13:\"ea109276\",14:\"9bf089b3\",15:\"d2c4604b\",16:\"8f5e018b\",17:\"31d6cfe0\",18:\"31d6cfe0\",19:\"31d6cfe0\",20:\"7a6ad70e\",21:\"31d6cfe0\",22:\"31d6cfe0\",23:\"31d6cfe0\",24:\"03248981\",25:\"31d6cfe0\",26:\"9f101993\",27:\"31d6cfe0\",28:\"31d6cfe0\",29:\"59e05191\",30:\"27b31da8\",31:\"31d6cfe0\",32:\"31d6cfe0\",33:\"f9065a4f\",34:\"31d6cfe0\",35:\"31d6cfe0\",36:\"31d6cfe0\",37:\"31d6cfe0\",38:\"31d6cfe0\",39:\"31d6cfe0\",40:\"f63aa94e\",41:\"31d6cfe0\",42:\"31d6cfe0\",43:\"31d6cfe0\",44:\"318451f9\",45:\"d498bfb3\",46:\"1ad26591\",47:\"98d862d2\",48:\"31d6cfe0\",49:\"31d6cfe0\",50:\"ea109276\",51:\"9bf089b3\",52:\"d2c4604b\",53:\"30273257\",54:\"31d6cfe0\",55:\"31d6cfe0\",56:\"31d6cfe0\",57:\"7a6ad70e\",58:\"31d6cfe0\",59:\"28103d70\",60:\"31d6cfe0\",61:\"31d6cfe0\",62:\"03248981\",63:\"31d6cfe0\",64:\"9f101993\",65:\"31d6cfe0\",66:\"31d6cfe0\",67:\"59e05191\",68:\"27b31da8\",69:\"c86e1775\",70:\"31d6cfe0\",76:\"31d6cfe0\",77:\"31d6cfe0\",78:\"31d6cfe0\",79:\"31d6cfe0\",80:\"31d6cfe0\",81:\"31d6cfe0\",82:\"31d6cfe0\",83:\"31d6cfe0\",84:\"5f802060\",85:\"31d6cfe0\",86:\"31d6cfe0\",87:\"31d6cfe0\",124:\"31d6cfe0\",125:\"31d6cfe0\",126:\"31d6cfe0\",127:\"095c82a4\",128:\"31d6cfe0\",129:\"31d6cfe0\",130:\"05896465\",131:\"31d6cfe0\",132:\"31d6cfe0\",133:\"31d6cfe0\",134:\"31d6cfe0\",135:\"0cc5eb29\",136:\"31d6cfe0\",137:\"31d6cfe0\",138:\"74da6240\",139:\"aa1da81f\",140:\"9e71d144\",141:\"31d6cfe0\",142:\"31d6cfe0\",143:\"31d6cfe0\",144:\"31d6cfe0\",145:\"31d6cfe0\",146:\"31d6cfe0\",147:\"31d6cfe0\",148:\"31d6cfe0\",149:\"31d6cfe0\",150:\"31d6cfe0\",151:\"31d6cfe0\"}[e]+\".chunk.css\",a=f.p+d,r=document.getElementsByTagName(\"link\"),s=0;s<r.length;s++){var m=(t=r[s]).getAttribute(\"data-href\")||t.getAttribute(\"href\");if(\"stylesheet\"===t.rel&&(m===d||m===a))return o()}var b=document.getElementsByTagName(\"style\");for(s=0;s<b.length;s++){var t;if((m=(t=b[s]).getAttribute(\"data-href\"))===d||m===a)return o()}var v=document.createElement(\"link\");v.rel=\"stylesheet\",v.type=\"text/css\",v.onload=o,v.onerror=function(o){var d=o&&o.target&&o.target.src||a,r=new Error(\"Loading CSS chunk \"+e+\" failed.\\\\n(\"+d+\")\");r.code=\"CSS_CHUNK_LOAD_FAILED\",r.request=d,delete n[e],v.parentNode.removeChild(v),c(r)},v.href=a,document.getElementsByTagName(\"head\")[0].appendChild(v)})).then((function(){n[e]=0})));var c=a[e];if(0!==c)if(c)o.push(c[2]);else{var d=new Promise((function(o,d){c=a[e]=[o,d]}));o.push(c[2]=d);var r,s=document.createElement(\"script\");s.charset=\"utf-8\",s.timeout=120,f.nc&&s.setAttribute(\"nonce\",f.nc),s.src=function(e){return f.p+\"static/js/\"+({0:\"vendors~ce~dnde~ee~ge~idee~pmce~rde~se~ve~253ae210\",1:\"vendors~ce~dnde~ee~ge~idee~pmce~rde~se~ve~d7693ced\",2:\"vendors~ce~dnde~ee~ge~idee~pmce~rde~splitio~ve~253ae210\",3:\"vendors~ce~dnde~ee~ge~idee~pmce~rde~splitio~ve~c7ce39a9\",4:\"vendors~ce~dnde~ee~ge~idee~pmce~rde~splitio~ve~d49b3b41\",5:\"vendors~ce~dnde~ee~ge~idee~pmce~rde~ve~253ae210\",6:\"vendors~ce~dnde~ee~ge~idee~pmce~253ae210\",7:\"vendors~console-monaco~monaco~253ae210\",8:\"vendors~console-monaco~monaco~36912834\",9:\"vendors~console-monaco~monaco~3ec7091e\",10:\"vendors~console-monaco~monaco~49d8ad03\",11:\"vendors~console-monaco~monaco~4d911f2e\",12:\"vendors~console-monaco~monaco~5584ff6c\",13:\"vendors~console-monaco~monaco~57c220dc\",14:\"vendors~console-monaco~monaco~5e1bc0de\",15:\"vendors~console-monaco~monaco~6ddf31e8\",16:\"vendors~console-monaco~monaco~7c919857\",17:\"vendors~console-monaco~monaco~80993005\",18:\"vendors~console-monaco~monaco~85a20469\",19:\"vendors~console-monaco~monaco~86ffeb1a\",20:\"vendors~console-monaco~monaco~900aca88\",21:\"vendors~console-monaco~monaco~9c5b28f6\",22:\"vendors~console-monaco~monaco~ba2d52b5\",23:\"vendors~console-monaco~monaco~c52b823e\",24:\"vendors~console-monaco~monaco~c682183c\",25:\"vendors~console-monaco~monaco~c73e0090\",26:\"vendors~console-monaco~monaco~d7ac9e7b\",27:\"vendors~console-monaco~monaco~dac6e39f\",28:\"vendors~console-monaco~monaco~e66ff6d3\",29:\"vendors~console-monaco~monaco~f6e880b5\",30:\"vendors~console-monaco~monaco~f7d19227\",32:\"ce~06694820\",33:\"ce~2a42e354\",34:\"ce~40b10d04\",35:\"ce~62c53bf1\",36:\"ce~748942c6\",37:\"ce~a4f7b5e1\",38:\"ce~b56a696f\",39:\"console-monaco~31ecd969\",40:\"dnde~253ae210\",41:\"dnde~690b702c\",42:\"dnde~d0058ac6\",43:\"ee~253ae210\",44:\"ee~a4f7b5e1\",45:\"ge~06694820\",46:\"ge~36912834\",47:\"ge~3ec7091e\",48:\"ge~4d911f2e\",49:\"ge~5584ff6c\",50:\"ge~57c220dc\",51:\"ge~5e1bc0de\",52:\"ge~6ddf31e8\",53:\"ge~7f323393\",54:\"ge~80993005\",55:\"ge~85a20469\",56:\"ge~86ffeb1a\",57:\"ge~900aca88\",58:\"ge~9c5b28f6\",59:\"ge~b6351802\",60:\"ge~ba2d52b5\",61:\"ge~c52b823e\",62:\"ge~c682183c\",63:\"ge~c73e0090\",64:\"ge~d7ac9e7b\",65:\"ge~dac6e39f\",66:\"ge~e66ff6d3\",67:\"ge~f6e880b5\",68:\"ge~f7d19227\",69:\"idee~0f485567\",70:\"idee~253ae210\",76:\"markdown-renderer~daf2eb16\",77:\"modal-views~31ecd969\",78:\"monaco~31ecd969\",79:\"pmce~062b2d40\",80:\"pmce~0df6afb5\",81:\"pmce~1f20a385\",82:\"pmce~253ae210\",83:\"pmce~9c5b28f6\",84:\"pmce~a4f7b5e1\",85:\"pmce~d80adb5f\",86:\"pmce~ece0910c\",87:\"rde~3333b9b6\",124:\"vendors~markdown-renderer~253ae210\",125:\"vendors~markdown-renderer~3f764be9\",126:\"vendors~markdown-renderer~678f84af\",127:\"vendors~rde~0f485567\",128:\"vendors~rde~1f059a71\",129:\"vendors~rde~253ae210\",130:\"vendors~rde~3dd5b2e0\",131:\"vendors~rde~5f1bbfc7\",132:\"vendors~rde~7274e1de\",133:\"vendors~rde~b56a696f\",134:\"vendors~rde~c0fbb756\",135:\"vendors~rde~f9ca8911\",136:\"vendors~se~253ae210\",137:\"vendors~se~973c11d3\",138:\"vendors~se~a4f7b5e1\",139:\"vendors~ve~cfba5e8d\",140:\"vendors~xterm~0d30e071\",141:\"xterm~41ecf737\"}[e]||e)+\".\"+{0:\"b8d6ab5d\",1:\"7b950f27\",2:\"0beef2ce\",3:\"6e18cb6c\",4:\"6efdcb51\",5:\"f2d9ce8f\",6:\"265a2e70\",7:\"b149ac17\",8:\"1bcf1773\",9:\"2e72a356\",10:\"704a983b\",11:\"0acec525\",12:\"73fa2739\",13:\"9ec1a795\",14:\"0857c4d3\",15:\"963c30d4\",16:\"4b0d275c\",17:\"56efd9d0\",18:\"4cfa73f7\",19:\"5838ebfe\",20:\"c6c44db0\",21:\"72c79f64\",22:\"31fccadf\",23:\"e4e1ef62\",24:\"f6c9fddf\",25:\"ad5b6555\",26:\"f37c75a6\",27:\"697c2da7\",28:\"1ff755ba\",29:\"70a5a232\",30:\"acffe547\",31:\"41210f62\",32:\"07c0190b\",33:\"2b74490d\",34:\"98098ce2\",35:\"72c60440\",36:\"89281fb8\",37:\"eb42e088\",38:\"a94c883f\",39:\"92725a01\",40:\"ffe6f7a9\",41:\"d57f02b2\",42:\"b50e867b\",43:\"66e318f0\",44:\"6f6533b8\",45:\"9c64896a\",46:\"a91ad370\",47:\"68066c11\",48:\"f6232721\",49:\"2d0cdd31\",50:\"bb803b40\",51:\"ec147c99\",52:\"825bd1a6\",53:\"faa57abc\",54:\"7f1e4178\",55:\"d15fd4a4\",56:\"0cf25b21\",57:\"4b636434\",58:\"e8da7962\",59:\"162a17d2\",60:\"62019af4\",61:\"39ca5f40\",62:\"afa8ed0e\",63:\"2214525a\",64:\"313081b6\",65:\"400c1760\",66:\"58a8fc00\",67:\"094644df\",68:\"4591526b\",69:\"c1d81f66\",70:\"444c4cb2\",76:\"bb5c2252\",77:\"6b750e74\",78:\"1d6955fc\",79:\"ecf71e15\",80:\"bf035ba3\",81:\"e064ef68\",82:\"472625e0\",83:\"accc8fb2\",84:\"7da0d852\",85:\"c28b4fbf\",86:\"9f59aa65\",87:\"c2d60f9a\",124:\"3dafbc5d\",125:\"d0f6a111\",126:\"d5083619\",127:\"2e143bbe\",128:\"ff2c8bc6\",129:\"947968b7\",130:\"d8ab5bcc\",131:\"9230768e\",132:\"346decd2\",133:\"994ba6f7\",134:\"0994c7d7\",135:\"d984b58c\",136:\"a044685e\",137:\"0880a864\",138:\"e663374d\",139:\"abacba51\",140:\"bc08341e\",141:\"2e01d7d8\",142:\"a742b960\",143:\"060edcae\",144:\"0574253e\",145:\"57e3719f\",146:\"35ab7c35\",147:\"d97ab3f1\",148:\"73d24146\",149:\"6aac95fc\",150:\"3d8bba9d\",151:\"6d13b504\"}[e]+\".chunk.js\"}(e);var m=new Error;r=function(o){s.onerror=s.onload=null,clearTimeout(b);var c=a[e];if(0!==c){if(c){var d=o&&(\"load\"===o.type?\"missing\":o.type),n=o&&o.target&&o.target.src;m.message=\"Loading chunk \"+e+\" failed.\\\\n(\"+d+\": \"+n+\")\",m.name=\"ChunkLoadError\",m.type=d,m.request=n,c[1](m)}a[e]=void 0}};var b=setTimeout((function(){r({type:\"timeout\",target:s})}),12e4);s.onerror=s.onload=r,document.head.appendChild(s)}return Promise.all(o)},f.m=e,f.c=d,f.d=function(e,o,c){f.o(e,o)||Object.defineProperty(e,o,{enumerable:!0,get:c})},f.r=function(e){\"undefined\"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:\"Module\"}),Object.defineProperty(e,\"__esModule\",{value:!0})},f.t=function(e,o){if(1&o&&(e=f(e)),8&o)return e;if(4&o&&\"object\"==typeof e&&e&&e.__esModule)return e;var c=Object.create(null);if(f.r(c),Object.defineProperty(c,\"default\",{enumerable:!0,value:e}),2&o&&\"string\"!=typeof e)for(var d in e)f.d(c,d,function(o){return e[o]}.bind(null,d));return c},f.n=function(e){var o=e&&e.__esModule?function(){return e.default}:function(){return e};return f.d(o,\"a\",o),o},f.o=function(e,o){return Object.prototype.hasOwnProperty.call(e,o)},f.p=\"/campus/\",f.oe=function(e){throw console.error(e),e};var s=this[\"webpackJsonpcampus-app-v2\"]=this[\"webpackJsonpcampus-app-v2\"]||[],m=s.push.bind(s);s.push=o,s=s.slice();for(var b=0;b<s.length;b++)o(s[b]);var t=m;c()}([])</script><script src=\"/campus/static/js/vendors~main~253ae210.763a37dc.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~0f485567.740d36e1.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~7091b17b.a68c786f.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~c09866d3.42d41e55.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~ea2ee6ce.0e6a0aca.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~7af1ae76.c564aef1.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~1cba4f0f.087baf32.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~d4581883.368c6333.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~ce5883b0.d1ebd159.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~975751f9.df7e3ac0.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~f6d28abc.9c01ca49.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~89b1000f.4e48751e.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~a122113c.3478848b.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~60b88c48.bab8be16.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~e4d02240.9dc1ddc9.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~2a42e354.a2dad791.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~7274e1de.fe21ef2c.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~d939e436.bb7ae9c9.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~203e0718.e8fe498c.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~1f20a385.3de96eb4.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~7d359b94.67556e7d.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~6bf781bd.eb9fc7eb.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~b3f3b6b4.094566e5.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~690b702c.530e2f3e.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~b9cf3951.8ad032d3.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~678f84af.3b7972c2.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~3f764be9.344cfe68.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~3c941b24.afe76b9c.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~62bec46e.98ec50ba.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~f53fef7e.bf9912e0.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~41b570db.86f60c7b.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~0b1f4feb.64641ad0.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~52a294a5.67cc0e76.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~ec8c427e.39a40a5a.chunk.js\"></script><script src=\"/campus/static/js/vendors~main~cfbf0a2e.8b278ab2.chunk.js\"></script><script src=\"/campus/static/js/main~748942c6.2ff93595.chunk.js\"></script><script src=\"/campus/static/js/main~21833f8f.872f10e6.chunk.js\"></script><script src=\"/campus/static/js/main~970f9218.572f7736.chunk.js\"></script><script src=\"/campus/static/js/main~c714bc7b.7bac3e29.chunk.js\"></script><script src=\"/campus/static/js/main~05fe4184.8908d0b3.chunk.js\"></script><script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement(\\'script\\');d.innerHTML=\"window.__CF$cv$params={r:\\'8b2012b69a68b984\\',t:\\'MTcyMzQ2MTU5NC4wMDAwMDA=\\'};var a=document.createElement(\\'script\\');a.nonce=\\'\\';a.src=\\'/cdn-cgi/challenge-platform/scripts/jsd/main.js\\';document.getElementsByTagName(\\'head\\')[0].appendChild(a);\";b.getElementsByTagName(\\'head\\')[0].appendChild(d)}}if(document.body){var a=document.createElement(\\'iframe\\');a.height=1;a.width=1;a.style.position=\\'absolute\\';a.style.top=0;a.style.left=0;a.style.border=\\'none\\';a.style.visibility=\\'hidden\\';document.body.appendChild(a);if(\\'loading\\'!==document.readyState)c();else if(window.addEventListener)document.addEventListener(\\'DOMContentLoaded\\',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);\\'loading\\'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script></body></html>'\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# This packages the request: request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Extract the response using the read() method\n",
    "html = response.read()\n",
    "\n",
    "# Print the string html\n",
    "print(html)\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Printing HTTP request results in Python using urllib\n",
    "You have just packaged and sent a GET request to \"https://campus.datacamp.com/courses/1606/4135?ex=2\" and then caught the response. You saw that such a response is a http.client.HTTPResponse object. The question remains: what can you do with this response?\n",
    "\n",
    "Well, as it came from an HTML page, you could read it to extract the HTML and, in fact, such a http.client.HTTPResponse object has an associated read() method. In this exercise, you'll build on your previous great work to extract the response and print the HTML.\n",
    "ChatGPT\n",
    "To extract and print the HTML content from an HTTP response using urllib in Python, you'll follow these steps:\n",
    "\n",
    "Send the HTTP GET request to the specified URL.\n",
    "Capture the server's response.\n",
    "Read the content of the response.\n",
    "Decode the content from bytes to a string (HTML).\n",
    "Print the HTML content.\n",
    "Here’s how you can do it:\n",
    "\n",
    "Step-by-Step Code Explanation\n",
    "python\n",
    "Copy code\n",
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "Explanation:\n",
    "from urllib.request import urlopen, Request: Imports the urlopen function to open URLs and the Request class to package HTTP requests.\n",
    "python\n",
    "Copy code\n",
    "# Specify the URL\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "Explanation:\n",
    "url: Assigns the URL of the DataCamp course page to the variable url.\n",
    "python\n",
    "Copy code\n",
    "# This packages the request: request\n",
    "request = Request(url)\n",
    "Explanation:\n",
    "request: Creates a Request object for the URL. This object contains the details needed to send the GET request to the server.\n",
    "python\n",
    "Copy code\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "Explanation:\n",
    "response: Sends the HTTP GET request using the urlopen function and captures the server's response. The response is an http.client.HTTPResponse object.\n",
    "python\n",
    "Copy code\n",
    "# Extract the HTML content from the response\n",
    "html = response.read()\n",
    "Explanation:\n",
    "html = response.read(): Reads the entire content of the response. The read() method returns the HTML content of the page as a byte object.\n",
    "python\n",
    "Copy code\n",
    "# Decode the byte content to a string\n",
    "html = html.decode('utf-8')\n",
    "Explanation:\n",
    "html = html.decode('utf-8'): Decodes the byte object into a string using UTF-8 encoding, which is a standard encoding for web pages. The result is a readable string containing the HTML of the page.\n",
    "python\n",
    "Copy code\n",
    "# Print the HTML content\n",
    "print(html)\n",
    "Explanation:\n",
    "print(html): Prints the decoded HTML content of the page to the console. This is the raw HTML that makes up the web page at the specified URL.\n",
    "python\n",
    "Copy code\n",
    "# Be polite and close the response!\n",
    "response.close()\n",
    "Explanation:\n",
    "response.close(): Closes the connection to the server. This is important to free up system resources and maintain good network etiquette.\n",
    "Complete Example Code\n",
    "python\n",
    "Copy code\n",
    "# Import necessary functions\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the URL of the web page\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# Package the request\n",
    "request = Request(url)\n",
    "\n",
    "# Send the request and get the response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Read the HTML content from the response\n",
    "html = response.read()\n",
    "\n",
    "# Decode the content to a readable string\n",
    "html = html.decode('utf-8')\n",
    "\n",
    "# Print the HTML content\n",
    "print(html)\n",
    "\n",
    "# Close the response to free resources\n",
    "response.close()\n",
    "What This Code Does:\n",
    "Sends a GET request to the DataCamp server to retrieve the HTML content of the specified page.\n",
    "Reads and decodes the response to make the content human-readable.\n",
    "Prints the HTML so you can see the raw structure of the web page.\n",
    "Closes the response to ensure that resources are freed and the connection is properly terminated.\n",
    "This process is fundamental in web scraping and interacting with web APIs in Python. By understanding how to retrieve and handle HTTP responses, you can start to extract and manipulate web data programmatically.\n",
    "\n",
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# This packages the request: request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Extract the response using the read() method\n",
    "html = response.read()\n",
    "\n",
    "# Print the string html\n",
    "print(html)\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()\n",
    "ChatGPT\n",
    "Let’s break down each line of this code and explain what it does:\n",
    "\n",
    "1. Importing Necessary Packages\n",
    "python\n",
    "Copy code\n",
    "from urllib.request import urlopen, Request\n",
    "Explanation:\n",
    "urlopen: A function that opens a URL and returns an object containing the response data from the server.\n",
    "Request: A class that helps you package your HTTP request, including details like the URL and any headers.\n",
    "2. Specifying the URL\n",
    "python\n",
    "Copy code\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "Explanation:\n",
    "url: This variable holds the URL of the webpage you want to access. In this case, it's a specific page on DataCamp.\n",
    "3. Packaging the Request\n",
    "python\n",
    "Copy code\n",
    "request = Request(url)\n",
    "Explanation:\n",
    "Request(url): Creates a request object using the URL. This object packages the details needed to send the request to the server.\n",
    "4. Sending the Request and Catching the Response\n",
    "python\n",
    "Copy code\n",
    "response = urlopen(request)\n",
    "Explanation:\n",
    "urlopen(request): Sends the HTTP GET request to the server using the request object you created. The server processes the request and sends back a response, which is stored in the response variable.\n",
    "The response variable now contains the server’s response data, which is in the form of an http.client.HTTPResponse object.\n",
    "5. Extracting the Response Content\n",
    "python\n",
    "Copy code\n",
    "html = response.read()\n",
    "Explanation:\n",
    "response.read(): Reads the content of the response. Since the response is usually in bytes, this method extracts the raw data from the response.\n",
    "html: This variable now holds the content of the webpage, but in a raw byte format.\n",
    "6. Printing the HTML Content\n",
    "python\n",
    "Copy code\n",
    "print(html)\n",
    "Explanation:\n",
    "print(html): Prints the raw HTML content of the webpage to the console. Since the html variable contains bytes, you might see a mix of HTML tags and other content in its raw byte form.\n",
    "7. Closing the Response\n",
    "python\n",
    "Copy code\n",
    "response.close()\n",
    "Explanation:\n",
    "response.close(): Closes the connection to the server, freeing up system resources. This is a good practice to ensure that the connection is properly terminated after you’re done with the response.\n",
    "Summary of What the Code Does:\n",
    "Imports the necessary modules to send HTTP requests.\n",
    "Specifies the URL of the webpage you want to access.\n",
    "Packages the request into a format suitable for sending to the server.\n",
    "Sends the request to the server and captures the response.\n",
    "Reads the content of the response (in bytes) and stores it in a variable.\n",
    "Prints the raw content of the webpage.\n",
    "Closes the response to clean up and free resources.\n",
    "This code is a basic example of how to fetch and display the content of a webpage using Python's urllib module. Since the printed html content is in bytes, you would typically decode it to a string for easier reading and processing, but in this example, it’s left in its raw byte form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html><html lang=\"en-US\"><head><title>Just a moment...</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\"><meta name=\"robots\" content=\"noindex,nofollow\"><meta name=\"viewport\" content=\"width=device-width,initial-scale=1\"><style>*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131}button,html{font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}@media (prefers-color-scheme:dark){body{background-color:#222;color:#d9d9d9}body a{color:#fff}body a:hover{color:#ee730a;text-decoration:underline}body .lds-ring div{border-color:#999 transparent transparent}body .font-red{color:#b20f03}body .pow-button{background-color:#4693ff;color:#1d1d1d}body #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}}body{display:flex;flex-direction:column;min-height:100vh}body.no-js .loading-spinner{visibility:hidden}body.no-js .challenge-running{display:none}body.dark{background-color:#222;color:#d9d9d9}body.dark a{color:#fff}body.dark a:hover{color:#ee730a;text-decoration:underline}body.dark .lds-ring div{border-color:#999 transparent transparent}body.dark .font-red{color:#b20f03}body.dark .pow-button{background-color:#4693ff;color:#1d1d1d}body.dark #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.dark #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}body.light{background-color:transparent;color:#313131}body.light a{color:#0051c3}body.light a:hover{color:#ee730a;text-decoration:underline}body.light .lds-ring div{border-color:#595959 transparent transparent}body.light .font-red{color:#fc574a}body.light .pow-button{background-color:#003681;border-color:#003681;color:#fff}body.light #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.light #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI2ZjNTc0YSIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjZmM1NzRhIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}a{background-color:transparent;color:#0051c3;text-decoration:none;transition:color .15s ease}a:hover{color:#ee730a;text-decoration:underline}.main-content{margin:8rem auto;max-width:60rem;width:100%}.heading-favicon{height:2rem;margin-right:.5rem;width:2rem}@media (width <= 720px){.main-content{margin-top:4rem}.heading-favicon{height:1.5rem;width:1.5rem}}.footer,.main-content{padding-left:1.5rem;padding-right:1.5rem}.main-wrapper{align-items:center;display:flex;flex:1;flex-direction:column}.font-red{color:#b20f03}.spacer{margin:2rem 0}.h1{font-size:2.5rem;font-weight:500;line-height:3.75rem}.h2{font-weight:500}.core-msg,.h2{font-size:1.5rem;line-height:2.25rem}.body-text,.core-msg{font-weight:400}.body-text{font-size:1rem;line-height:1.25rem}@media (width <= 720px){.h1{font-size:1.5rem;line-height:1.75rem}.h2{font-size:1.25rem}.core-msg,.h2{line-height:1.5rem}.core-msg{font-size:1rem}}#challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI2ZjNTc0YSIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjZmM1NzRhIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+);padding-left:34px}#challenge-error-text,#challenge-success-text{background-repeat:no-repeat;background-size:contain}#challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=);padding-left:42px}.text-center{text-align:center}.pow-button{background-color:#0051c3;border:.063rem solid #0051c3;border-radius:.313rem;color:#fff;font-size:.875rem;line-height:1.313rem;margin:2rem 0;padding:.375rem 1rem;transition-duration:.2s;transition-property:background-color,border-color,color;transition-timing-function:ease}.pow-button:hover{background-color:#003681;border-color:#003681;color:#fff;cursor:pointer}.footer{font-size:.75rem;line-height:1.125rem;margin:0 auto;max-width:60rem;width:100%}.footer-inner{border-top:1px solid #d9d9d9;padding-bottom:1rem;padding-top:1rem}.clearfix:after{clear:both;content:\"\";display:table}.clearfix .column{float:left;padding-right:1.5rem;width:50%}.diagnostic-wrapper{margin-bottom:.5rem}.footer .ray-id{text-align:center}.footer .ray-id code{font-family:monaco,courier,monospace}.core-msg,.zone-name-title{overflow-wrap:break-word}@media (width <= 720px){.diagnostic-wrapper{display:flex;flex-wrap:wrap;justify-content:center}.clearfix:after{clear:none;content:none;display:initial;text-align:center}.column{padding-bottom:2rem}.clearfix .column{float:none;padding:0;width:auto;word-break:keep-all}.zone-name-title{margin-bottom:1rem}}.loading-spinner{height:76.391px}.lds-ring{display:inline-block;position:relative}.lds-ring,.lds-ring div{height:1.875rem;width:1.875rem}.lds-ring div{animation:lds-ring 1.2s cubic-bezier(.5,0,.5,1) infinite;border:.3rem solid transparent;border-radius:50%;border-top-color:#313131;box-sizing:border-box;display:block;position:absolute}.lds-ring div:first-child{animation-delay:-.45s}.lds-ring div:nth-child(2){animation-delay:-.3s}.lds-ring div:nth-child(3){animation-delay:-.15s}@keyframes lds-ring{0%{transform:rotate(0)}to{transform:rotate(1turn)}}@media screen and (-ms-high-contrast:active),screen and (-ms-high-contrast:none){.main-wrapper,body{display:block}}.rtl .heading-favicon{margin-left:.5rem;margin-right:0}.rtl #challenge-success-text{background-position:100%;padding-left:0;padding-right:42px}.rtl #challenge-error-text{background-position:100%;padding-left:0;padding-right:34px}</style><meta http-equiv=\"refresh\" content=\"120\"></head><body class=\"no-js\"><div class=\"main-wrapper\" role=\"main\"><div class=\"main-content\"><noscript><div id=\"challenge-error-title\"><div class=\"h2\"><span id=\"challenge-error-text\">Enable JavaScript and cookies to continue</span></div></div></noscript></div></div><script>(function(){window._cf_chl_opt={cvId: '3',cZone: \"www.datacamp.com\",cType: 'non-interactive',cNounce: '59964',cRay: '8b201a7f2db80a47',cHash: 'f0519f64e360685',cUPMDTk: \"\\/teach\\/documentation?__cf_chl_tk=aTTIQ.SPEqnpHvvcCoLqKLh84yzWpPTFTC.99oqHfbU-1723461913-0.0.1.1-3647\",cFPWv: 'g',cTTimeMs: '1000',cMTimeMs: '120000',cTplV: 5,cTplB: 'cf',cK: \"\",fa: \"\\/teach\\/documentation?__cf_chl_f_tk=aTTIQ.SPEqnpHvvcCoLqKLh84yzWpPTFTC.99oqHfbU-1723461913-0.0.1.1-3647\",md: \"QoCP7tT9rG0Ij.5WKwzF792FwcP40W2EMBRlKXckdRw-1723461913-1.1.1.1-GObT4r_iETbsuFJH8yk5SWUt6dI0BcKVVk0Ll_o1jUgsokCervqG.6ZYGItj4vbD8pO1b6Kxya3dX_uTp5gmk63RxcuX3wDA1oh0BDF0RvhhF7hwvHqy6yUAgnUnDyY4wUUgHxEI8eSMeVjOUIiwb80eSTj1PtRHQ7pJdWUwwHz7obxS6vYZxG_fmatiBe8b6zEUNtZ.rsdXS1rBfy4SogzrlH1sFo14BOmjuRG2sDtG2VDW8Ecb_m0Y9niW.3qc8e_rRpyzkFXaAuI0vJ7tiSn0dqhg.tznezo2Zv0D5njiZdqqAErxed8AhTnNoucrG0mi8i7T8fbfh8mDFDxZhwKRDWRR5kDZXb89BUk5yItW4q77Do_AbC7MgYXOsuvycMIfxo_Gu1GjcEcBAcPGRXlKTKhCWiKLBuGlxf9L85pNNVLkd_dNPKKloP.gkb1GTrXdFBvU0.W6aS.lxHT94_SSC3t072Y49rPKskVjeW.Qdc5SookUzOfVEZG0JpId2xpRNIB4hoNeCycuNMg_auATYrEvOqWodPE3Z9i0krCznhrd2lNeQe_sERbZToZT.HCduV57vMgH1O1teRIyF4Zx_6u9hiMQkqmbLK8qlQkP0qdOosiZUZJ34OEEAlrOiO4EmWcd9eGcdyyqCQY83V7tslqPmzG0TILD8D9fFIpeTCGUqvOX2abP9Y2EX.W.xinUTJhqnPbJoaXsxUjkqrIZn1jhpyaYpSk9TlO3dtjAwknd.L99RT9LCc8VjhasGF42UpShhXr06R3XdSqQ9AdkP3I.W975KdhUEjKWA287nPzZl2JEXH5SqZTF0q_P5VVaJ965TCa5AnivLt_GMRs9fmL7aBgis_SpXi4F5tJGF_Wi7gY7q.V2LdAjzVGC9WnR31Ykkq_LRw6isC6JqVACP_sVv46XMb_Fq6KPKrvdWepigOInUktVLykr39B1wspBkYJSFP0Y7pMYcBJJeGV97SNSVbb5MW8NnkNJBx.VEqasQI.EU5DNXW9sAWR_zXARWYcj4TCo8jjcz_ZoUOeq8eNLrdFxYvYPoFz_3b90OR5EZV8P1X8QB5rLV99euAaNrsrlmSkStgSEHmbp14V4SZIZW2PxS5MoRGLGzTS0Z7SgxJ_q0z6uCcTq1Frt8ulDlds4mbMK5uwdjFZgLoysQHWWCl9vVpu_MbsDIEQ\",mdrd: \"CPXBdyrgmJPmtc7UXvJ1UuaO91YWCWDDKJ6I8t6lvFo-1723461913-1.1.1.1-HgDx.ZS8wwGKw1TnD_b5AFFbCZTGAZrVimQoJ2IM4r3KmcaaKCu.6H4VdqEy09sTkw43kY0om7fldFogt6JeFC10NXYKctCCmqwFF.v4nvZI.SHdC_0QCsrIcdUF5ozanGNrAt0LLLeMhOb.h2urLROwCVLpx2RqZDXRa4zR1SufASZULqYE6Wq0KBm5o5WkZALII1._efN.fKrxmuYjL36.p.HIk_FFatHQsFSrASE3t8UCewu0eh1pGGGLDM1k_JBGKZxb_ONLehfBIiKgDcq_mMr1k4ura1m_eS.dVuuFfVEX1wTRsDCNZvgevQOB6OF2Tu8bs3zWj4IlhOFxdJovE3IvsRjaLs1oDVcyNzfZIUZz8mdkEEeGNQcowphDkMoafByvS6B8A3Ar8tlA78_EgKOmdkUiHsbFSGzzwixYzk7ODet2HDnl2gApmC1aAHHF2InS_GCn7UFtIYDsdcs2mNXE_QsffowM0pXeN8VAwvQVowC3RmQo2eW18XtEpP0sip0P7N8v6MJ6vK26t4MGUZ7z3kYUK0oXGVTtY0lw_2At7ICeZ8xupovmb00WueHcxABCRIUsKzFfLefyI.tvxa.ihxMFB.EGPaNJSnxSDMXjISiwzzeUbIpW1Sq0NR.Rhllch_37IEYrytEbMMkI2hOVf9Kbp7K3URqNZWHKd.eNfm4.D2dOV3s7qHWrhCIRC_c4RELVW.8w6gopw3qeYSu8IIbt3Jkf.uzH4mVAfb5T9Hr1XCK0wQ4bpLc7Q5bROhIDYDhBApN5960o9SO7bJSY.LbFZq8CAYpRc4_HU13njPZ9mOs4mGtaIvvaIt9IWKXieyp5zfPIXqsIP5s8VItE7x7iJN4ezqHXmZPqSAplqetRTgp1RdxN0UgrN_t0wFZw0Fq0vQI0GfUTSbwmdPTusJ3ZC7AxkMbG5YpwRdaCXSO4MDhvi0L0.qX_rGwV.y3Beq6hJOHbodG7YnJdkSTE_x_V5xlnGmAr8oStR2qhpwVEiNjse6MpSXY_hYD087_q.dn2wNYWSpP.HRZPZtN2wXI7KWwf7K.5faZkYoP9dYeaqpDsTDePGG5jgXqA1YA8YnC.O5j241nFuG0F9mR2N2R6MSes4GaUHd74M1FxHDQ10.e8q8Mnac10Ij9169PBAMEsotzkWuY8hnZXhsIyPGeaGOwkuPs2haThWShi28xOltj4sN0n0_wQ7xDeoVA7iN6iuR382nUodBnY4b.xizYgTpFSvTRYQHhJwyoSXATLMnse3RBQ7dYx5l7i.Xy.3_idm_R8oEh99Bk2b1_lSgzYkAuUNxiVLnHgQ2bQVyV1HK8Lo7paMuqU6bbQxQs_z6xdmIOFi1QbyvIihr1IQ0nJ6r5rcQgy3doLUzYzxNcKT5CWFaA7f.0ktO86EfFPqFcPMUTu2BACHXgHZDDY0rwOR5GA1xC.eWUlG_6pV5yuq_QqRQ4FPAVQ4U4NHyhAxsmHl2Nvual4PUVFMDy4i91RvmrR1PvewUIutUoNzPKLhTnu59zIRpSVsRz67oImVOgNTt5Pou9UkNp9KE0ofCiFfeCklBc__iWFsn_fRKU_SnpNM3I9kBmd1V9TnOg325y8TmgATYpiOFOJpQIe9Y.JvqcHYeVM2Bqj5mFWOkoqWX99_t2dW2rIoGGc0a7UVV0EXRf5WuNefjkhWQjxlxgP2WhMbACVHkHXH_cwuGyYNJ6BiM3DFtq1Zdp7MrGiphEtR0iI8r1Jju_RmTuDbu8WssOSPfWAMQIvhR1tIvHd.U4CRkRz0.t3CLCMvvLEoP32beQeFQzxBy162f7yh6xcEaqZPciMYew7ZI8k7z0L7kEXQKd_.LQ47ufpMMAAlX6IH26Z2CRtwT7xjmU.1R.GBDhn8s9qT2g2CczJBHONc8v4aR.IKqOLmeM4JlvCcEhHqg2KOR3Y8RQ0kp40uOVzrLGIaqNps7Sigj93sD2h87wgSrYShOLWdFm5jefJiX15JUEh8exDKBOOmUGceFwL2OCkbdAYuHkI8U_U2x5Sl2HS5xQjPCrHR8GgLJiznNLQp4pKqtjH5oN9L22gFHq7k_2k791UJ63XGTJBduX_EpCcrLcxveASpmk9FfPqtk4WgMc0uW2XRcgnVkO7pW0sCiGOsComAEiJyrPgH8WDLJ74zoE4cqeV7N77OljJg3OO4IlRPPXLWL6xPgs5GLMH79z5xSpq19EFejLOYd.mIEb3WCPcJycX2uS8BhkMeLztlTGLz_JQbUNj7Bd7gkqkFBx1KLxilCORB9YlUsyNSVKu9bCxM8l1vvDN..E2_mPZtg16wKb6uw\",cRq: {ru: 'aHR0cHM6Ly93d3cuZGF0YWNhbXAuY29tL3RlYWNoL2RvY3VtZW50YXRpb24=',ra: 'cHl0aG9uLXJlcXVlc3RzLzIuMzIuMw==',d: 'wURLsQQ5jSueNbyPUjcfILHRazscnntW8lzSMrbyUIaIn4sFiNcc11E9uX/gosHeMdCd0cmu1RLXpxUqnDslv92ygB4nZDWaBBHN7b9uWJzO+OktUqg70Wh8djBeClCf70YmB56kcEOQoqIWYuLcDXhvV7ZH3yYFzdhamd24ZdSXWVH27iNxMFNrmvtsJDZG/F12mMtG2j7l4VjKg+3mURp/AgwhztGzdU0DlTfcUs6cuA01ZbPUnnkQPZrMmOdCOsYOsrxFjlejeF636EhTGJeAkPrbALMHYDwngr8rXJ8pe84Xjop9GEBWlD8P1m938M2Oeydj18PZOoN5hyLLLN1gTeYV/MlZY1R0nRZQX4stK/Q4T7ugXZMkfqmDzqciMa+mAvFH9ImmSlTXsLpjTwDqCneB5nvwOozocOMkKQ46HNup8vyIkU1IxZNhbM6J0hcWVh0t5cmUFX6GHW+IwA7cA9CdW3L1NDQA6OgNI41sDk9my8GPYsEC1NS5nq9swX7KgMlsrVjBLBtVLRo6bg==',t: 'MTcyMzQ2MTkxMy4wMDAwMDA=',cT: Math.floor(Date.now() / 1000),m: 'r4a6wuGOcWoCSSZYQuSjqhUSwjD4E6uI+txhqdH01io=',i1: 'Czd2Rsqk+bKy/YBNntUk3g==',i2: 'odPDsbf5xnFmcJlOBIix8A==',zh: 'hzfiqo9hugT9sHeHQ1zy81NCL/S0295H0+GuRnkSV9o=',uh: 'CiATXs0R4tTCmAZrIKJMI0mbciEXZbJB6weC9NLv1LY=',hh: 'rAZnIHiyrNuZ60h9aAZNML8izDilqmOSNuCtac1WqPs=',}};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/g/orchestrate/chl_page/v1?ray=8b201a7f2db80a47';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/teach\\/documentation?__cf_chl_rt_tk=aTTIQ.SPEqnpHvvcCoLqKLh84yzWpPTFTC.99oqHfbU-1723461913-0.0.1.1-3647\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());</script></body></html>\n"
     ]
    }
   ],
   "source": [
    "# Import the requests package\n",
    "import requests\n",
    "\n",
    "# Specify the URL\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "\n",
    "# Package the request, send the request, and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response: text\n",
    "text = r.text\n",
    "\n",
    "# Print the HTML\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing HTTP requests in Python using requests\n",
    "Now that you've got your head and hands around making HTTP requests using the urllib package, you're going to figure out how to do the same using the higher-level requests library. You'll once again be pinging DataCamp servers for their \"http://www.datacamp.com/teach/documentation\" page\n",
    "ChatGPT\n",
    "To perform HTTP requests in Python using the requests library, you'll follow these steps:\n",
    "\n",
    "Install the requests library (if you haven't already):\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "pip install requests\n",
    "Make a GET request to the URL you want to access. Here's an example of how to do this:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "\n",
    "# Define the URL\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Print the status code of the response\n",
    "print(\"Status Code:\", response.status_code)\n",
    "\n",
    "# Print the content of the response\n",
    "print(\"Content:\", response.text)\n",
    "requests.get(url) sends a GET request to the specified URL.\n",
    "response.status_code gives you the HTTP status code of the response (e.g., 200 for success).\n",
    "response.text contains the content of the response, typically the HTML of the page.\n",
    "Handle the response:\n",
    "\n",
    "You can check if the request was successful by inspecting the status code. If it’s 200, the request was successful.\n",
    "You can then process the content of the response (e.g., parse HTML, extract data, etc.).\n",
    "Here’s a more detailed example where you handle possible errors:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "    print(\"Success! Here's the content:\")\n",
    "    print(response.text)\n",
    "except requests.exceptions.HTTPError as err:\n",
    "    print(f\"HTTP error occurred: {err}\")\n",
    "except Exception as err:\n",
    "    print(f\"An error occurred: {err}\")\n",
    "response.raise_for_status() raises an HTTPError if the request returned an unsuccessful status code.\n",
    "requests.exceptions provides various exceptions to handle different types of errors, such as connection errors, timeouts, and more.\n",
    "This method is more robust, as it allows you to handle potential errors gracefully.\n",
    "\n",
    "\n",
    "3/3\n",
    "\n",
    "# Import the requests package\n",
    "import requests\n",
    "\n",
    "# Specify the URL\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "\n",
    "# Package the request, send the request, and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response: text\n",
    "text = r.text\n",
    "\n",
    "# Print the HTML\n",
    "print(text)\n",
    "\n",
    "ChatGPT\n",
    "Your code successfully demonstrates how to use the requests library to make an HTTP GET request and retrieve the HTML content of a webpage. Here's a brief explanation of each part of your code:\n",
    "\n",
    "Importing the requests package:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "This imports the requests library, which you’ll use to make HTTP requests.\n",
    "Specifying the URL:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "You define the URL of the webpage you want to request.\n",
    "Sending the request and capturing the response:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "r = requests.get(url)\n",
    "The requests.get(url) function sends a GET request to the specified URL and returns a response object, which is stored in r.\n",
    "Extracting the response content:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "text = r.text\n",
    "The .text attribute of the response object r contains the HTML content of the webpage as a string.\n",
    "Printing the HTML content:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(text)\n",
    "Finally, the HTML content of the webpage is printed to the console.\n",
    "This code will output the HTML source code of the \"http://www.datacamp.com/teach/documentation\" page. If you need to parse or analyze the HTML content further, you might consider using libraries like BeautifulSoup for more detailed HTML parsing.\n",
    "\n",
    "ModuleNotFoundError                       Traceback (most recent call last)\n",
    "Cell In[9], line 2\n",
    "      1 # Import the requests package\n",
    "----> 2 import requests\n",
    "      4 # Specify the URL\n",
    "      5 url = \"http://www.datacamp.com/teach/documentation\"\n",
    "\n",
    "ModuleNotFoundError: No module named 'requests'\n",
    "ChatGPT\n",
    "The ModuleNotFoundError indicates that the requests library is not installed in your Python environment. You can install it using pip. Here's how you can do that:\n",
    "\n",
    "Install requests using pip\n",
    "Open your terminal or command prompt.\n",
    "\n",
    "Run the following command to install the requests package:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "pip install requests\n",
    "If you're using a Jupyter notebook, you can run this command directly in a cell by prefixing it with !:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "!pip install requests\n",
    "After installation, try importing requests again in your script:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "\n",
    "# Now, you can proceed with your code\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "r = requests.get(url)\n",
    "text = r.text\n",
    "print(text)\n",
    "If you still encounter issues after installing, make sure that you're working in the correct Python environment where requests is installed.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate the distribution of weights of a sample population of 50 individuals, assuming the weights are normally distributed with a mean of 150 pounds and a standard deviation of 10 pounds, you can use the numpy library in Python. Specifically, the numpy.random.normal() function is ideal for this task.\n",
    "\n",
    "Function: numpy.random.normal()\n",
    "Parameters:\n",
    "loc: Mean of the distribution (in this case, 150 pounds).\n",
    "scale: Standard deviation of the distribution (in this case, 10 pounds).\n",
    "size: Number of samples to generate (in this case, 50 individuals).\n",
    "Example Code:\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "\n",
    "# Mean and standard deviation\n",
    "mean_weight = 150  # mean in pounds\n",
    "std_dev_weight = 10  # standard deviation in pounds\n",
    "\n",
    "# Number of individuals in the sample\n",
    "sample_size = 50\n",
    "\n",
    "# Simulate the weights\n",
    "weights = np.random.normal(loc=mean_weight, scale=std_dev_weight, size=sample_size)\n",
    "\n",
    "# Print the simulated weights\n",
    "print(weights)\n",
    "Explanation:\n",
    "np.random.normal(): This function generates random numbers from a normal (Gaussian) distribution.\n",
    "loc=150: Specifies the mean of the distribution as 150 pounds.\n",
    "scale=10: Specifies the standard deviation as 10 pounds.\n",
    "size=50: Specifies that you want to generate 50 random values (one for each individual).\n",
    "The output weights will be an array containing 50 simulated weights of individuals, based on the specified normal distribution.\n",
    "\n",
    "from scipy.stats import binom\n",
    "samples = binom.\n",
    "?\n",
    "(5, 0.4, size=10)\n",
    "ChatGPT\n",
    "It looks like you're trying to generate random samples from a binomial distribution using the binom object from the scipy.stats module. To generate these samples, you should use the rvs method, which stands for \"random variates.\"\n",
    "\n",
    "Here’s how you can complete the code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from scipy.stats import binom\n",
    "\n",
    "# Generate random samples from a binomial distribution\n",
    "samples = binom.rvs(n=5, p=0.4, size=10)\n",
    "\n",
    "# Print the generated samples\n",
    "print(samples)\n",
    "Explanation:\n",
    "binom.rvs(n=5, p=0.4, size=10):\n",
    "n=5: The number of trials in each experiment.\n",
    "p=0.4: The probability of success on each trial.\n",
    "size=10: The number of random samples (experiments) you want to generate.\n",
    "This will give you an array of 10 samples, where each sample is the number of successes in 5 trials, with a 40% probability of success on each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Guido's Personal Home Page\n",
      "  </title>\n",
      " </head>\n",
      " <body bgcolor=\"#FFFFFF\" text=\"#000000\">\n",
      "  <!-- Built from main -->\n",
      "  <h1>\n",
      "   <a href=\"pics.html\">\n",
      "    <img border=\"0\" src=\"images/IMG_2192.jpg\"/>\n",
      "   </a>\n",
      "   Guido van Rossum - Personal Home Page\n",
      "   <a href=\"pics.html\">\n",
      "    <img border=\"0\" height=\"216\" src=\"images/guido-headshot-2019.jpg\" width=\"270\"/>\n",
      "   </a>\n",
      "  </h1>\n",
      "  <p>\n",
      "   <a href=\"http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\">\n",
      "    <i>\n",
      "     \"Gawky and proud of it.\"\n",
      "    </i>\n",
      "   </a>\n",
      "   <h3>\n",
      "    <a href=\"images/df20000406.jpg\">\n",
      "     Who I Am\n",
      "    </a>\n",
      "   </h3>\n",
      "   <p>\n",
      "    Read\n",
      "my\n",
      "    <a href=\"http://neopythonic.blogspot.com/2016/04/kings-day-speech.html\">\n",
      "     \"King's\n",
      "Day Speech\"\n",
      "    </a>\n",
      "    for some inspiration.\n",
      "    <p>\n",
      "     I am the author of the\n",
      "     <a href=\"http://www.python.org\">\n",
      "      Python\n",
      "     </a>\n",
      "     programming language.  See also my\n",
      "     <a href=\"Resume.html\">\n",
      "      resume\n",
      "     </a>\n",
      "     and my\n",
      "     <a href=\"Publications.html\">\n",
      "      publications list\n",
      "     </a>\n",
      "     , a\n",
      "     <a href=\"bio.html\">\n",
      "      brief bio\n",
      "     </a>\n",
      "     , assorted\n",
      "     <a href=\"http://legacy.python.org/doc/essays/\">\n",
      "      writings\n",
      "     </a>\n",
      "     ,\n",
      "     <a href=\"http://legacy.python.org/doc/essays/ppt/\">\n",
      "      presentations\n",
      "     </a>\n",
      "     and\n",
      "     <a href=\"interviews.html\">\n",
      "      interviews\n",
      "     </a>\n",
      "     (all about Python), some\n",
      "     <a href=\"pics.html\">\n",
      "      pictures of me\n",
      "     </a>\n",
      "     ,\n",
      "     <a href=\"http://neopythonic.blogspot.com\">\n",
      "      my new blog\n",
      "     </a>\n",
      "     , and\n",
      "my\n",
      "     <a href=\"http://www.artima.com/weblogs/index.jsp?blogger=12088\">\n",
      "      old\n",
      "blog\n",
      "     </a>\n",
      "     on Artima.com.  I am\n",
      "     <a href=\"https://twitter.com/gvanrossum\">\n",
      "      @gvanrossum\n",
      "     </a>\n",
      "     on Twitter.\n",
      "     <p>\n",
      "      I am currently a Distinguished Engineer at Microsoft.\n",
      "I have worked for Dropbox, Google, Elemental Security, Zope\n",
      "Corporation, BeOpen.com, CNRI, CWI, and SARA.  (See\n",
      "my\n",
      "      <a href=\"Resume.html\">\n",
      "       resume\n",
      "      </a>\n",
      "      .)  I created Python while at CWI.\n",
      "      <h3>\n",
      "       How to Reach Me\n",
      "      </h3>\n",
      "      <p>\n",
      "       You can send email for me to guido (at) python.org.\n",
      "I read everything sent there, but I receive too much email to respond\n",
      "to everything.\n",
      "       <p>\n",
      "        Please understand that I do not give talks or keynotes any more,\n",
      "nor do I participate in podcasts or give interviews, etc.\n",
      "I am sorry, but I just receive too many such requests to decline\n",
      "them individually.\n",
      "Offering payment or trips to exotic locales just makes things worse, sorry.\n",
      "        <p>\n",
      "         I also prefer not to receive questions about how to use Python\n",
      "(please read the\n",
      "         <a href=\"https://docs.python.org\">\n",
      "          documentation\n",
      "         </a>\n",
      "         or search the internet for Python answers forums),\n",
      "bug reports (use the\n",
      "         <a href=\"https://github.com/python/cpython/issues\">\n",
      "          GitHub issue tracker\n",
      "         </a>\n",
      "         ),\n",
      "proposals for changes to the language (use\n",
      "         <a href=\"https://discuss.python.org\">\n",
      "          Discourse\n",
      "         </a>\n",
      "         ),\n",
      "job offers (I'm happy where I am),\n",
      "or requests to join you in some far-fetched scheme to save humanity (though sometimes I like me a good rant :-).\n",
      "         <h3>\n",
      "          My Name\n",
      "         </h3>\n",
      "         <p>\n",
      "          My name often poses difficulties for Americans.\n",
      "          <p>\n",
      "           <b>\n",
      "            Pronunciation:\n",
      "           </b>\n",
      "           in Dutch, the \"G\" in Guido is a hard G,\n",
      "pronounced roughly like the \"ch\" in Scottish \"loch\".  (Listen to the\n",
      "           <a href=\"guido.au\">\n",
      "            sound clip\n",
      "           </a>\n",
      "           .)  However, if you're\n",
      "American, you may also pronounce it as the Italian \"Guido\".  I'm not\n",
      "too worried about the associations with mob assassins that some people\n",
      "have. :-)\n",
      "           <p>\n",
      "            <b>\n",
      "             Spelling:\n",
      "            </b>\n",
      "            my last name is two words, and I'd like to keep it\n",
      "that way, the spelling on some of my credit cards notwithstanding.\n",
      "Dutch spelling rules dictate that when used in combination with my\n",
      "first name, \"van\" is not capitalized: \"Guido van Rossum\".  But when my\n",
      "last name is used alone to refer to me, it is capitalized, for\n",
      "example: \"As usual, Van Rossum was right.\"\n",
      "            <p>\n",
      "             <b>\n",
      "              Alphabetization:\n",
      "             </b>\n",
      "             in America, I show up in the alphabet under\n",
      "\"V\".  But in Europe, I show up under \"R\".  And some of my friends put\n",
      "me under \"G\" in their address book...\n",
      "             <h3>\n",
      "              More Hyperlinks\n",
      "             </h3>\n",
      "             <ul>\n",
      "              <li>\n",
      "               Here's a collection of\n",
      "               <a href=\"http://legacy.python.org/doc/essays/\">\n",
      "                essays\n",
      "               </a>\n",
      "               relating to Python\n",
      "that I've written, including the foreword I wrote for Mark Lutz' book\n",
      "\"Programming Python\".\n",
      "               <p>\n",
      "                <li>\n",
      "                 I own the official\n",
      "                 <a href=\"images/license.jpg\">\n",
      "                  <img align=\"center\" border=\"0\" height=\"75\" src=\"images/license_thumb.jpg\" width=\"100\"/>\n",
      "                  Python license.\n",
      "                 </a>\n",
      "                 <p>\n",
      "                 </p>\n",
      "                </li>\n",
      "               </p>\n",
      "              </li>\n",
      "             </ul>\n",
      "             <h3>\n",
      "              The Audio File Formats FAQ\n",
      "             </h3>\n",
      "             <p>\n",
      "              I was the original creator and maintainer of the Audio File Formats\n",
      "FAQ.  It was later maintained by Chris Bagwell, but seems to have been\n",
      "lost.  You can still find it on\n",
      "              <a href=\"https://web.archive.org/\">\n",
      "               archive.org\n",
      "              </a>\n",
      "              by searching for\n",
      "              <a href=\"https://web.archive.org/web/20230627131911/http://www.cnpbagwell.com/audio-faq\">\n",
      "               http://www.cnpbagwell.com/audio-faq\n",
      "              </a>\n",
      "              .  And here is a link to\n",
      "              <a href=\"http://sox.sourceforge.net/\">\n",
      "               SOX\n",
      "              </a>\n",
      "              , to which I contributed\n",
      "some early code.\n",
      "              <hr/>\n",
      "              <a href=\"images/internetdog.gif\">\n",
      "               \"On the Internet, nobody knows you're\n",
      "a dog.\"\n",
      "              </a>\n",
      "              <hr/>\n",
      "             </p>\n",
      "            </p>\n",
      "           </p>\n",
      "          </p>\n",
      "         </p>\n",
      "        </p>\n",
      "       </p>\n",
      "      </p>\n",
      "     </p>\n",
      "    </p>\n",
      "   </p>\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Prettify the BeautifulSoup object: pretty_soup\n",
    "pretty_soup = soup.prettify()\n",
    "\n",
    "# Print the prettified HTML\n",
    "print(pretty_soup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing HTML with BeautifulSoup\n",
    "In this interactive exercise, you'll learn how to use the BeautifulSoup package to parse, prettify and extract information from HTML. You'll scrape the data from the webpage of Guido van Rossum, Python's very own Benevolent Dictator for Life. In the following exercises, you'll prettify the HTML and then extract the text and the hyperlinks.\n",
    "\n",
    "The URL of interest is url = 'https://www.python.org/~guido/'.\n",
    "ChatGPT\n",
    "To parse and extract information from Guido van Rossum's webpage using BeautifulSoup, you'll need to follow a series of steps. Here's how you can do it:\n",
    "\n",
    "Step 1: Import necessary packages\n",
    "You need to import the necessary libraries to fetch and parse the HTML content.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "Step 2: Fetch the HTML content\n",
    "Use the requests library to fetch the content of the webpage.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "url = 'https://www.python.org/~guido/'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    html_content = response.text\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "Step 3: Parse the HTML content\n",
    "Use BeautifulSoup to parse the HTML content.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "Step 4: Prettify the HTML\n",
    "You can use the prettify() method to make the HTML more readable.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "pretty_html = soup.prettify()\n",
    "print(pretty_html[:1000])  # Print the first 1000 characters for brevity\n",
    "Step 5: Extract text\n",
    "To extract all the text from the webpage:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "text = soup.get_text()\n",
    "print(text[:1000])  # Print the first 1000 characters for brevity\n",
    "Step 6: Extract all hyperlinks\n",
    "To extract all the hyperlinks from the webpage:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "links = soup.find_all('a')\n",
    "for link in links:\n",
    "    print(link.get('href'))\n",
    "Full Example\n",
    "Here’s a full example putting all the steps together:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.python.org/~guido/'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    html_content = response.text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Prettify the HTML\n",
    "    pretty_html = soup.prettify()\n",
    "    print(pretty_html[:1000])\n",
    "\n",
    "    # Extract all text\n",
    "    text = soup.get_text()\n",
    "    print(text[:1000])\n",
    "\n",
    "    # Extract all hyperlinks\n",
    "    links = soup.find_all('a')\n",
    "    for link in links:\n",
    "        print(link.get('href'))\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "This will help you scrape the required information from Guido van Rossum's webpage. You can adjust the print statements or further manipulate the soup object depending on what specific information you want to extract.\n",
    "\n",
    "\n",
    "2/2\n",
    "\n",
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Prettify the BeautifulSoup object: pretty_soup\n",
    "pretty_soup = soup.prettify()\n",
    "\n",
    "# Print the prettified HTML\n",
    "print(pretty_soup)\n",
    "ChatGPT\n",
    "Your code looks good for fetching and prettifying the HTML content of Guido van Rossum's webpage. Here’s a quick summary of what each part does:\n",
    "\n",
    "Import Packages: Import the requests and BeautifulSoup classes.\n",
    "Specify URL: Define the URL of the webpage you want to scrape.\n",
    "Send Request and Get Response: Use requests.get() to fetch the HTML content of the webpage.\n",
    "Extract HTML: Retrieve the HTML text from the response object.\n",
    "Create BeautifulSoup Object: Parse the HTML content with BeautifulSoup.\n",
    "Prettify HTML: Use BeautifulSoup’s prettify() method to format the HTML in a more readable way.\n",
    "Print Prettified HTML: Print the prettified HTML to view it.\n",
    "This will output the entire HTML content of the page in a more readable format. For large HTML documents, this might be a lot of output, so consider printing only a portion if needed.\n",
    "\n",
    "If you want to see only a snippet of the prettified HTML or extract specific information, you can adjust the print statement or add more parsing code accordingly. If you have any other questions or need further modifications, let me know!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Guido's Personal Home Page\n",
      "  </title>\n",
      " </head>\n",
      " <body bgcolor=\"#FFFFFF\" text=\"#000000\">\n",
      "  <!-- Built from main -->\n",
      "  <h1>\n",
      "   <a href=\"pics.html\">\n",
      "    <img border=\"0\" src=\"images/IMG_2192.jpg\"/>\n",
      "   </a>\n",
      "   Guido van Rossum - Personal Home Page\n",
      "   <a href=\"pics.html\">\n",
      "    <img border=\"0\" height=\"216\" src=\"images/guido-headshot-2019.jpg\" width=\"270\"/>\n",
      "   </a>\n",
      "  </h1>\n",
      "  <p>\n",
      "   <a href=\"http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\">\n",
      "    <i>\n",
      "     \"Gawky and proud of it.\"\n",
      "    </i>\n",
      "   </a>\n",
      "   <h3>\n",
      "    <a href=\"images/df20000406.jpg\">\n",
      "     Who I Am\n",
      "    </a>\n",
      "   </h3>\n",
      "   <p>\n",
      "    Read\n",
      "my\n",
      "    <a href=\"http://neopythonic.blogspot.com/2016/04/kings-day-speech.html\">\n",
      "     \"King's\n",
      "Day Speech\"\n",
      "    </a>\n",
      "    for some inspiration.\n",
      "    <p>\n",
      "     I am the author of the\n",
      "     <a href=\"http://www.python.org\">\n",
      "      Python\n",
      "     </a>\n",
      "     programming language.  See also my\n",
      "     <a href=\"Resume.html\">\n",
      "      resume\n",
      "     </a>\n",
      "     and my\n",
      "     <a href=\"Publications.html\">\n",
      "      publications list\n",
      "     </a>\n",
      "     , a\n",
      "     <a href=\"bio.html\">\n",
      "      brief bio\n",
      "     </a>\n",
      "     , assorted\n",
      "     <a href=\"http://legacy.python.org/doc/essays/\">\n",
      "      writings\n",
      "     </a>\n",
      "     ,\n",
      "     <a href=\"http://legacy.python.org/doc/essays/ppt/\">\n",
      "      presentations\n",
      "     </a>\n",
      "     and\n",
      "     <a href=\"interviews.html\">\n",
      "      interviews\n",
      "     </a>\n",
      "     (all about Python), some\n",
      "     <a href=\"pics.html\">\n",
      "      pictures of me\n",
      "     </a>\n",
      "     ,\n",
      "     <a href=\"http://neopythonic.blogspot.com\">\n",
      "      my new blog\n",
      "     </a>\n",
      "     , and\n",
      "my\n",
      "     <a href=\"http://www.artima.com/weblogs/index.jsp?blogger=12088\">\n",
      "      old\n",
      "blog\n",
      "     </a>\n",
      "     on Artima.com.  I am\n",
      "     <a href=\"https://twitter.com/gvanrossum\">\n",
      "      @gvanrossum\n",
      "     </a>\n",
      "     on Twitter.\n",
      "     <p>\n",
      "      I am currently a Distinguished Engineer at Microsoft.\n",
      "I have worked for Dropbox, Google, Elemental Security, Zope\n",
      "Corporation, BeOpen.com, CNRI, CWI, and SARA.  (See\n",
      "my\n",
      "      <a href=\"Resume.html\">\n",
      "       resume\n",
      "      </a>\n",
      "      .)  I created Python while at CWI.\n",
      "      <h3>\n",
      "       How to Reach Me\n",
      "      </h3>\n",
      "      <p>\n",
      "       You can send email for me to guido (at) python.org.\n",
      "I read everything sent there, but I receive too much email to respond\n",
      "to everything.\n",
      "       <p>\n",
      "        Please understand that I do not give talks or keynotes any more,\n",
      "nor do I participate in podcasts or give interviews, etc.\n",
      "I am sorry, but I just receive too many such requests to decline\n",
      "them individually.\n",
      "Offering payment or trips to exotic locales just makes things worse, sorry.\n",
      "        <p>\n",
      "         I also prefer not to receive questions about how to use Python\n",
      "(please read the\n",
      "         <a href=\"https://docs.python.org\">\n",
      "          documentation\n",
      "         </a>\n",
      "         or search the internet for Python answers forums),\n",
      "bug reports (use the\n",
      "         <a href=\"https://github.com/python/cpython/issues\">\n",
      "          GitHub issue tracker\n",
      "         </a>\n",
      "         ),\n",
      "proposals for changes to the language (use\n",
      "         <a href=\"https://discuss.python.org\">\n",
      "          Discourse\n",
      "         </a>\n",
      "         ),\n",
      "job offers (I'm happy where I am),\n",
      "or requests to join you in some far-fetched scheme to save humanity (though sometimes I like me a good rant :-).\n",
      "         <h3>\n",
      "          My Name\n",
      "         </h3>\n",
      "         <p>\n",
      "          My name often poses difficulties for Americans.\n",
      "          <p>\n",
      "           <b>\n",
      "            Pronunciation:\n",
      "           </b>\n",
      "           in Dutch, the \"G\" in Guido is a hard G,\n",
      "pronounced roughly like the \"ch\" in Scottish \"loch\".  (Listen to the\n",
      "           <a href=\"guido.au\">\n",
      "            sound clip\n",
      "           </a>\n",
      "           .)  However, if you're\n",
      "American, you may also pronounce it as the Italian \"Guido\".  I'm not\n",
      "too worried about the associations with mob assassins that some people\n",
      "have. :-)\n",
      "           <p>\n",
      "            <b>\n",
      "             Spelling:\n",
      "            </b>\n",
      "            my last name is two words, and I'd like to keep it\n",
      "that way, the spelling on some of my credit cards notwithstanding.\n",
      "Dutch spelling rules dictate that when used in combination with my\n",
      "first name, \"van\" is not capitalized: \"Guido van Rossum\".  But when my\n",
      "last name is used alone to refer to me, it is capitalized, for\n",
      "example: \"As usual, Van Rossum was right.\"\n",
      "            <p>\n",
      "             <b>\n",
      "              Alphabetization:\n",
      "             </b>\n",
      "             in America, I show up in the alphabet under\n",
      "\"V\".  But in Europe, I show up under \"R\".  And some of my friends put\n",
      "me under \"G\" in their address book...\n",
      "             <h3>\n",
      "              More Hyperlinks\n",
      "             </h3>\n",
      "             <ul>\n",
      "              <li>\n",
      "               Here's a collection of\n",
      "               <a href=\"http://legacy.python.org/doc/essays/\">\n",
      "                essays\n",
      "               </a>\n",
      "               relating to Python\n",
      "that I've written, including the foreword I wrote for Mark Lutz' book\n",
      "\"Programming Python\".\n",
      "               <p>\n",
      "                <li>\n",
      "                 I own the official\n",
      "                 <a href=\"images/license.jpg\">\n",
      "                  <img align=\"center\" border=\"0\" height=\"75\" src=\"images/license_thumb.jpg\" width=\"100\"/>\n",
      "                  Python license.\n",
      "                 </a>\n",
      "                 <p>\n",
      "                 </p>\n",
      "                </li>\n",
      "               </p>\n",
      "              </li>\n",
      "             </ul>\n",
      "             <h3>\n",
      "              The Audio File Formats FAQ\n",
      "             </h3>\n",
      "             <p>\n",
      "              I was the original creator and maintainer of the Audio File Formats\n",
      "FAQ.  It was later maintained by Chris Bagwell, but seems to have been\n",
      "lost.  You can still find it on\n",
      "              <a href=\"https://web.archive.org/\">\n",
      "               archive.org\n",
      "              </a>\n",
      "              by searching for\n",
      "              <a href=\"https://web.archive.org/web/20230627131911/http://www.cnpbagwell.com/audio-faq\">\n",
      "               http://www.cnpbagwell.com/audio-faq\n",
      "              </a>\n",
      "              .  And here is a link to\n",
      "              <a href=\"http://sox.sourceforge.net/\">\n",
      "               SOX\n",
      "              </a>\n",
      "              , to which I contributed\n",
      "some early code.\n",
      "              <hr/>\n",
      "              <a href=\"images/internetdog.gif\">\n",
      "               \"On the Internet, nobody knows you're\n",
      "a dog.\"\n",
      "              </a>\n",
      "              <hr/>\n",
      "             </p>\n",
      "            </p>\n",
      "           </p>\n",
      "          </p>\n",
      "         </p>\n",
      "        </p>\n",
      "       </p>\n",
      "      </p>\n",
      "     </p>\n",
      "    </p>\n",
      "   </p>\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Prettify the BeautifulSoup object: pretty_soup\n",
    "pretty_soup = soup.prettify()\n",
    "\n",
    "# Print the prettified HTML\n",
    "print(pretty_soup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain\n",
    "Parsing HTML with BeautifulSoup\n",
    "In this interactive exercise, you'll learn how to use the BeautifulSoup package to parse, prettify and extract information from HTML. You'll scrape the data from the webpage of Guido van Rossum, Python's very own Benevolent Dictator for Life. In the following exercises, you'll prettify the HTML and then extract the text and the hyperlinks.\n",
    "\n",
    "The URL of interest is url = 'https://www.python.org/~guido/'.\n",
    "ChatGPT\n",
    "Parsing HTML with BeautifulSoup involves several steps to extract and manipulate data from web pages. Here’s a guide on how you can do this with the provided URL:\n",
    "\n",
    "1. Install BeautifulSoup and Requests\n",
    "First, make sure you have BeautifulSoup and requests installed. You can install them using pip if you haven’t already:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "pip install beautifulsoup4 requests\n",
    "2. Import Libraries\n",
    "Import the necessary libraries in your Python script:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "3. Fetch the Webpage\n",
    "Use the requests library to fetch the HTML content from the provided URL:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "url = 'https://www.python.org/~guido/'\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "4. Parse the HTML Content\n",
    "Create a BeautifulSoup object and parse the HTML content:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "5. Prettify the HTML\n",
    "To make the HTML more readable, you can use the prettify() method:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "pretty_html = soup.prettify()\n",
    "print(pretty_html)\n",
    "This will print out the HTML in a nicely formatted way.\n",
    "\n",
    "6. Extract Text\n",
    "To extract all text from the HTML, you can use the get_text() method:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "text = soup.get_text()\n",
    "print(text)\n",
    "This will give you all the text from the HTML, without the HTML tags.\n",
    "\n",
    "7. Extract Hyperlinks\n",
    "To extract hyperlinks, you can search for all <a> tags and get their href attributes:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "links = soup.find_all('a')\n",
    "for link in links:\n",
    "    href = link.get('href')\n",
    "    print(href)\n",
    "This will print out all the URLs found in the <a> tags on the page.\n",
    "\n",
    "Summary\n",
    "Here’s a complete script that combines all these steps:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Fetch the webpage\n",
    "url = 'https://www.python.org/~guido/'\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Prettify the HTML\n",
    "pretty_html = soup.prettify()\n",
    "print(\"Prettified HTML:\\n\", pretty_html)\n",
    "\n",
    "# Extract text\n",
    "text = soup.get_text()\n",
    "print(\"\\nExtracted Text:\\n\", text)\n",
    "\n",
    "# Extract hyperlinks\n",
    "links = soup.find_all('a')\n",
    "print(\"\\nExtracted Hyperlinks:\")\n",
    "for link in links:\n",
    "    href = link.get('href')\n",
    "    print(href)\n",
    "By running this script, you will be able to see the prettified HTML, extract all the text, and list all hyperlinks from the provided URL.\n",
    "\n",
    "a detail explanation on each line of code\n",
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Prettify the BeautifulSoup object: pretty_soup\n",
    "pretty_soup = soup.prettify()\n",
    "\n",
    "# Print the prettified HTML\n",
    "print(pretty_soup)\n",
    "ChatGPT\n",
    "Sure! Here’s a detailed explanation of each line in the code snippet:\n",
    "\n",
    "1. Import Packages\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import requests: This imports the requests library, which is used to make HTTP requests. It allows you to send HTTP requests to a URL and get the server's response.\n",
    "from bs4 import BeautifulSoup: This imports the BeautifulSoup class from the bs4 (BeautifulSoup 4) library. BeautifulSoup is a library used for parsing HTML and XML documents and extracting data from them.\n",
    "2. Specify URL\n",
    "python\n",
    "Copy code\n",
    "url = 'https://www.python.org/~guido/'\n",
    "url: This variable stores the URL of the webpage you want to scrape. In this case, it points to Guido van Rossum's Python.org page.\n",
    "3. Package the Request, Send the Request, and Catch the Response\n",
    "python\n",
    "Copy code\n",
    "r = requests.get(url)\n",
    "requests.get(url): This sends a GET request to the specified URL. A GET request is used to retrieve data from the server at the given URL.\n",
    "r: This variable holds the response object returned by the requests.get function. It contains information such as the HTML content of the webpage, status code, headers, etc.\n",
    "4. Extract the Response as HTML\n",
    "python\n",
    "Copy code\n",
    "html_doc = r.text\n",
    "r.text: This retrieves the content of the response as a string. For HTML responses, this will be the HTML code of the webpage.\n",
    "html_doc: This variable stores the HTML content of the webpage, which is now in a string format.\n",
    "5. Create a BeautifulSoup Object from the HTML\n",
    "python\n",
    "Copy code\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "BeautifulSoup(html_doc, 'html.parser'): This creates a BeautifulSoup object from the HTML content. The 'html.parser' argument specifies the parser to be used (Python’s built-in HTML parser). BeautifulSoup will parse the HTML and allow you to navigate and search the document.\n",
    "soup: This variable holds the BeautifulSoup object, which represents the HTML document and provides methods to extract data.\n",
    "6. Prettify the BeautifulSoup Object\n",
    "python\n",
    "Copy code\n",
    "pretty_soup = soup.prettify()\n",
    "soup.prettify(): This method formats the HTML in a more readable way by adding line breaks and indentation. This is useful for viewing the structure of the HTML and debugging.\n",
    "pretty_soup: This variable stores the prettified version of the HTML document.\n",
    "7. Print the Prettified HTML\n",
    "python\n",
    "Copy code\n",
    "print(pretty_soup)\n",
    "print(pretty_soup): This prints the prettified HTML to the console. It allows you to see the formatted HTML structure, which can help with understanding the content and structure of the webpage.\n",
    "Summary\n",
    "This script retrieves the HTML content of the specified URL, parses it with BeautifulSoup, and prints the formatted HTML. It’s a basic workflow for web scraping that allows you to inspect the HTML structure and prepare for more detailed data extraction or manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of Guido's webpage: <title>Guido's Personal Home Page</title>\n",
      "Text from Guido's webpage: \n",
      "\n",
      "Guido's Personal Home Page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Guido van Rossum - Personal Home Page\n",
      "\n",
      "\n",
      "\"Gawky and proud of it.\"\n",
      "Who I Am\n",
      "Read\n",
      "my \"King's\n",
      "Day Speech\" for some inspiration.\n",
      "\n",
      "I am the author of the Python\n",
      "programming language.  See also my resume\n",
      "and my publications list, a brief bio, assorted writings, presentations and interviews (all about Python), some\n",
      "pictures of me,\n",
      "my new blog, and\n",
      "my old\n",
      "blog on Artima.com.  I am\n",
      "@gvanrossum on Twitter.\n",
      "\n",
      "I am currently a Distinguished Engineer at Microsoft.\n",
      "I have worked for Dropbox, Google, Elemental Security, Zope\n",
      "Corporation, BeOpen.com, CNRI, CWI, and SARA.  (See\n",
      "my resume.)  I created Python while at CWI.\n",
      "\n",
      "How to Reach Me\n",
      "You can send email for me to guido (at) python.org.\n",
      "I read everything sent there, but I receive too much email to respond\n",
      "to everything.\n",
      "\n",
      "Please understand that I do not give talks or keynotes any more,\n",
      "nor do I participate in podcasts or give interviews, etc.\n",
      "I am sorry, but I just receive too many such requests to decline\n",
      "them individually.\n",
      "Offering payment or trips to exotic locales just makes things worse, sorry.\n",
      "\n",
      "I also prefer not to receive questions about how to use Python\n",
      "(please read the documentation or search the internet for Python answers forums),\n",
      "bug reports (use the GitHub issue tracker),\n",
      "proposals for changes to the language (use Discourse),\n",
      "job offers (I'm happy where I am),\n",
      "or requests to join you in some far-fetched scheme to save humanity (though sometimes I like me a good rant :-).\n",
      "\n",
      "My Name\n",
      "My name often poses difficulties for Americans.\n",
      "\n",
      "Pronunciation: in Dutch, the \"G\" in Guido is a hard G,\n",
      "pronounced roughly like the \"ch\" in Scottish \"loch\".  (Listen to the\n",
      "sound clip.)  However, if you're\n",
      "American, you may also pronounce it as the Italian \"Guido\".  I'm not\n",
      "too worried about the associations with mob assassins that some people\n",
      "have. :-)\n",
      "\n",
      "Spelling: my last name is two words, and I'd like to keep it\n",
      "that way, the spelling on some of my credit cards notwithstanding.\n",
      "Dutch spelling rules dictate that when used in combination with my\n",
      "first name, \"van\" is not capitalized: \"Guido van Rossum\".  But when my\n",
      "last name is used alone to refer to me, it is capitalized, for\n",
      "example: \"As usual, Van Rossum was right.\"\n",
      "\n",
      "Alphabetization: in America, I show up in the alphabet under\n",
      "\"V\".  But in Europe, I show up under \"R\".  And some of my friends put\n",
      "me under \"G\" in their address book...\n",
      "\n",
      "\n",
      "More Hyperlinks\n",
      "\n",
      "Here's a collection of essays relating to Python\n",
      "that I've written, including the foreword I wrote for Mark Lutz' book\n",
      "\"Programming Python\".\n",
      "I own the official \n",
      "Python license.\n",
      "\n",
      "The Audio File Formats FAQ\n",
      "I was the original creator and maintainer of the Audio File Formats\n",
      "FAQ.  It was later maintained by Chris Bagwell, but seems to have been\n",
      "lost.  You can still find it on\n",
      "archive.org by searching for\n",
      "http://www.cnpbagwell.com/audio-faq.  And here is a link to\n",
      "SOX, to which I contributed\n",
      "some early code.\n",
      "\n",
      "\n",
      "\n",
      "\"On the Internet, nobody knows you're\n",
      "a dog.\"\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Get the title of Guido's webpage: guido_title\n",
    "guido_title = str(soup.title)  # Convert the title object to a string to include HTML tags\n",
    "\n",
    "# Print the title of Guido's webpage to the shell\n",
    "print(\"Title of Guido's webpage:\", guido_title)\n",
    "\n",
    "# Get Guido's text: guido_text\n",
    "guido_text = soup.get_text()\n",
    "\n",
    "# Print Guido's text to the shell\n",
    "print(\"Text from Guido's webpage:\", guido_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain\n",
    "Turning a webpage into data using BeautifulSoup: getting the text\n",
    "As promised, in the following exercises, you'll learn the basics of extracting information from HTML soup. In this exercise, you'll figure out how to extract the text from the BDFL's webpage, along with printing the webpage's title.\n",
    "ChatGPT\n",
    "Sure! When working with BeautifulSoup to extract data from a webpage, you can follow these steps:\n",
    "\n",
    "1. Install BeautifulSoup and Requests\n",
    "First, make sure you have BeautifulSoup and Requests installed. You can install them using pip if you haven't already:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "pip install beautifulsoup4 requests\n",
    "2. Fetch the Webpage Content\n",
    "You'll need to fetch the HTML content of the webpage using the requests library. This will get the raw HTML that you can then parse with BeautifulSoup.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "\n",
    "# URL of the webpage\n",
    "url = 'http://example.com'\n",
    "\n",
    "# Fetch the HTML content\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "3. Parse the HTML with BeautifulSoup\n",
    "Create a BeautifulSoup object to parse the HTML content. BeautifulSoup provides methods to navigate and search the HTML tree.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "4. Extract and Print the Webpage Title\n",
    "The title of the webpage can be extracted using the .title attribute of the BeautifulSoup object.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Extract the title\n",
    "title = soup.title.string\n",
    "\n",
    "# Print the title\n",
    "print(\"Title of the webpage:\", title)\n",
    "5. Extract and Print All Text\n",
    "To get all the text from the webpage, you can use the .get_text() method. This method extracts all the text from the HTML, stripping out the tags.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Extract all text\n",
    "text = soup.get_text()\n",
    "\n",
    "# Print the text\n",
    "print(\"Text from the webpage:\", text)\n",
    "Putting It All Together\n",
    "Here's a complete script combining all the steps:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage\n",
    "url = 'http://example.com'\n",
    "\n",
    "# Fetch the HTML content\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extract and print the title\n",
    "title = soup.title.string\n",
    "print(\"Title of the webpage:\", title)\n",
    "\n",
    "# Extract and print all text\n",
    "text = soup.get_text()\n",
    "print(\"Text from the webpage:\", text)\n",
    "Summary\n",
    "Fetching HTML: Use requests.get(url) to get the HTML content of the webpage.\n",
    "Parsing HTML: Use BeautifulSoup(html_content, 'html.parser') to parse the HTML.\n",
    "Extracting Title: Access the title with soup.title.string.\n",
    "Extracting Text: Use soup.get_text() to get all text content from the HTML.\n",
    "This basic workflow will help you get started with web scraping using BeautifulSoup and Requests!\n",
    "\n",
    "details on each line of codde\n",
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "\n",
    "\n",
    "# Get the title of Guido's webpage: guido_title\n",
    "\n",
    "\n",
    "# Print the title of Guido's webpage to the shell\n",
    "\n",
    "\n",
    "# Get Guido's text: guido_text\n",
    "\n",
    "\n",
    "# Print Guido's text to the shell\n",
    "print(guido_text)\n",
    "ChatGPT\n",
    "Certainly! Let's break down each line of the code in detail:\n",
    "\n",
    "1. Import Packages\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import requests: Imports the requests library, which is used to send HTTP requests and handle responses. It’s a simple way to fetch web pages.\n",
    "from bs4 import BeautifulSoup: Imports the BeautifulSoup class from the bs4 module. BeautifulSoup is used to parse HTML and XML documents.\n",
    "2. Specify URL\n",
    "python\n",
    "Copy code\n",
    "url = 'https://www.python.org/~guido/'\n",
    "url = 'https://www.python.org/~guido/': Defines a variable url that holds the URL of the webpage you want to scrape. This URL points to Guido van Rossum's personal page on the Python website.\n",
    "3. Package the Request, Send the Request, and Catch the Response\n",
    "python\n",
    "Copy code\n",
    "r = requests.get(url)\n",
    "r = requests.get(url): Sends a GET request to the specified url using the requests library. The response from the server is stored in the variable r.\n",
    "4. Extract the Response as HTML\n",
    "python\n",
    "Copy code\n",
    "html_doc = r.text\n",
    "html_doc = r.text: Extracts the content of the response as a string of HTML. The .text attribute of the response object contains the raw HTML text of the webpage.\n",
    "5. Create a BeautifulSoup Object from the HTML\n",
    "python\n",
    "Copy code\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "soup = BeautifulSoup(html_doc, 'html.parser'): Creates a BeautifulSoup object from the html_doc. This object allows you to navigate and search the HTML structure. The 'html.parser' argument specifies the parser to use; the built-in HTML parser is sufficient for most cases.\n",
    "6. Get the Title of Guido's Webpage\n",
    "python\n",
    "Copy code\n",
    "guido_title = soup.title.string\n",
    "guido_title = soup.title.string: Extracts the title of the webpage. soup.title retrieves the <title> tag from the HTML, and .string gets the text inside that tag.\n",
    "7. Print the Title of Guido's Webpage to the Shell\n",
    "python\n",
    "Copy code\n",
    "print(guido_title)\n",
    "print(guido_title): Outputs the title of Guido's webpage to the console. This will display the text inside the <title> tag of the HTML.\n",
    "8. Get Guido's Text\n",
    "python\n",
    "Copy code\n",
    "guido_text = soup.get_text()\n",
    "guido_text = soup.get_text(): Extracts all the text from the HTML document, stripping out the HTML tags. This provides a plain text representation of the webpage's content.\n",
    "9. Print Guido's Text to the Shell\n",
    "python\n",
    "Copy code\n",
    "print(guido_text)\n",
    "print(guido_text): Outputs the plain text extracted from the webpage to the console. This will display all the text content without HTML tags.\n",
    "Summary\n",
    "Importing Libraries: Bring in the necessary libraries for fetching and parsing HTML.\n",
    "Specify URL: Define the webpage you want to scrape.\n",
    "Send Request: Fetch the webpage content.\n",
    "Extract HTML: Retrieve the HTML from the response.\n",
    "Parse HTML: Create a BeautifulSoup object for HTML parsing.\n",
    "Get Title: Extract and print the title of the webpage.\n",
    "Get and Print Text: Extract and print all the text content from the webpage.\n",
    "This code provides a basic example of how to use BeautifulSoup and Requests to scrape and analyze web content.\n",
    "\n",
    "'''In the sample code, the HTML response object html_doc has already been created: your first task is to Soupify it using the function BeautifulSoup() and to assign the resulting soup to the variable soup.\n",
    "Extract the title from the HTML soup soup using the attribute title and assign the result to guido_title.\n",
    "Print the title of Guido's webpage to the shell using the print() function.\n",
    "Extract the text from the HTML soup soup using the method get_text() and assign to guido_text.\n",
    "Hit submit to print the text from Guido's webpage to the shell.'''\n",
    "\n",
    "\n",
    "\n",
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "\n",
    "\n",
    "# Get the title of Guido's webpage: guido_title\n",
    "\n",
    "\n",
    "# Print the title of Guido's webpage to the shell\n",
    "\n",
    "\n",
    "# Get Guido's text: guido_text\n",
    "\n",
    "\n",
    "# Print Guido's text to the shell\n",
    "print(guido_text)\n",
    "ChatGPT\n",
    "Here's how you can complete the provided code to achieve the tasks described:\n",
    "\n",
    "Create a BeautifulSoup object from the HTML: This involves parsing the HTML document using BeautifulSoup.\n",
    "Extract the title: Retrieve the title from the BeautifulSoup object.\n",
    "Print the title: Output the title to the console.\n",
    "Extract the text: Get all the text content from the HTML.\n",
    "Print the text: Output the text content to the console.\n",
    "Here's the complete code with the missing parts filled in:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Get the title of Guido's webpage: guido_title\n",
    "guido_title = soup.title.string\n",
    "\n",
    "# Print the title of Guido's webpage to the shell\n",
    "print(\"Title of Guido's webpage:\", guido_title)\n",
    "\n",
    "# Get Guido's text: guido_text\n",
    "guido_text = soup.get_text()\n",
    "\n",
    "# Print Guido's text to the shell\n",
    "print(\"Text from Guido's webpage:\", guido_text)\n",
    "Explanation\n",
    "Create a BeautifulSoup object from the HTML: soup\n",
    "\n",
    "python\n",
    "Copy code\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "Parses the HTML content using BeautifulSoup and the HTML parser. The resulting soup object allows you to navigate and extract data from the HTML.\n",
    "Get the title of Guido's webpage: guido_title\n",
    "\n",
    "python\n",
    "Copy code\n",
    "guido_title = soup.title.string\n",
    "Accesses the <title> tag from the HTML and retrieves its text content.\n",
    "Print the title of Guido's webpage to the shell\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(\"Title of Guido's webpage:\", guido_title)\n",
    "Outputs the extracted title to the console.\n",
    "Get Guido's text: guido_text\n",
    "\n",
    "python\n",
    "Copy code\n",
    "guido_text = soup.get_text()\n",
    "Extracts all the text from the HTML, ignoring HTML tags.\n",
    "Print Guido's text to the shell\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(\"Text from Guido's webpage:\", guido_text)\n",
    "Outputs the extracted text content to the console.\n",
    "This code will fetch the webpage, parse it, and print both the title and the full text content of Guido van Rossum's page.\n",
    "\n",
    "Incorrect submission\n",
    "Did you correctly define the variable guido_title? Expected '<title>Guido's Personal Home Page</title>', but got 'Guido's Personal Home Page'.\n",
    "ChatGPT\n",
    "It looks like the expected output for guido_title includes the HTML <title> tags, but the actual output is just the text content inside the title tag. If you need the output to include the HTML tags, you should directly access the title attribute of the soup object without extracting its .string property.\n",
    "\n",
    "Here's the corrected code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Get the title of Guido's webpage: guido_title\n",
    "guido_title = str(soup.title)  # Convert the title object to a string to include HTML tags\n",
    "\n",
    "# Print the title of Guido's webpage to the shell\n",
    "print(\"Title of Guido's webpage:\", guido_title)\n",
    "\n",
    "# Get Guido's text: guido_text\n",
    "guido_text = soup.get_text()\n",
    "\n",
    "# Print Guido's text to the shell\n",
    "print(\"Text from Guido's webpage:\", guido_text)\n",
    "Explanation of the Correction\n",
    "Correctly Define guido_title:\n",
    "python\n",
    "Copy code\n",
    "guido_title = str(soup.title)\n",
    "By converting soup.title to a string using str(), you include the HTML tags around the title text, ensuring the output matches the expected format.\n",
    "This way, guido_title will include the <title> tags as part of the string, aligning with the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Guido's Personal Home Page</title>\n",
      "pics.html\n",
      "pics.html\n",
      "http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\n",
      "images/df20000406.jpg\n",
      "http://neopythonic.blogspot.com/2016/04/kings-day-speech.html\n",
      "http://www.python.org\n",
      "Resume.html\n",
      "Publications.html\n",
      "bio.html\n",
      "http://legacy.python.org/doc/essays/\n",
      "http://legacy.python.org/doc/essays/ppt/\n",
      "interviews.html\n",
      "pics.html\n",
      "http://neopythonic.blogspot.com\n",
      "http://www.artima.com/weblogs/index.jsp?blogger=12088\n",
      "https://twitter.com/gvanrossum\n",
      "Resume.html\n",
      "https://docs.python.org\n",
      "https://github.com/python/cpython/issues\n",
      "https://discuss.python.org\n",
      "guido.au\n",
      "http://legacy.python.org/doc/essays/\n",
      "images/license.jpg\n",
      "https://web.archive.org/\n",
      "https://web.archive.org/web/20230627131911/http://www.cnpbagwell.com/audio-faq\n",
      "http://sox.sourceforge.net/\n",
      "images/internetdog.gif\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Print the title of Guido's webpage\n",
    "print(soup.title)\n",
    "\n",
    "# Find all 'a' tags (which define hyperlinks): a_tags\n",
    "a_tags = soup.find_all('a')\n",
    "\n",
    "# Print the URLs to the shell\n",
    "for link in a_tags:\n",
    "    print(link.get('href'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Guido's Personal Home Page</title>\n",
      "pics.html\n",
      "pics.html\n",
      "http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\n",
      "images/df20000406.jpg\n",
      "http://neopythonic.blogspot.com/2016/04/kings-day-speech.html\n",
      "http://www.python.org\n",
      "Resume.html\n",
      "Publications.html\n",
      "bio.html\n",
      "http://legacy.python.org/doc/essays/\n",
      "http://legacy.python.org/doc/essays/ppt/\n",
      "interviews.html\n",
      "pics.html\n",
      "http://neopythonic.blogspot.com\n",
      "http://www.artima.com/weblogs/index.jsp?blogger=12088\n",
      "https://twitter.com/gvanrossum\n",
      "Resume.html\n",
      "https://docs.python.org\n",
      "https://github.com/python/cpython/issues\n",
      "https://discuss.python.org\n",
      "guido.au\n",
      "http://legacy.python.org/doc/essays/\n",
      "images/license.jpg\n",
      "https://web.archive.org/\n",
      "https://web.archive.org/web/20230627131911/http://www.cnpbagwell.com/audio-faq\n",
      "http://sox.sourceforge.net/\n",
      "images/internetdog.gif\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Print the title of Guido's webpage\n",
    "print(soup.title)\n",
    "\n",
    "# Find all 'a' tags (which define hyperlinks): a_tags\n",
    "a_tags = soup.find_all('a')\n",
    "\n",
    "# Print the URLs to the shell\n",
    "for link in a_tags:\n",
    "    print(link.get('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain\n",
    "Turning a webpage into data using BeautifulSoup: getting the hyperlinks\n",
    "In this exercise, you'll figure out how to extract the URLs of the hyperlinks from the BDFL's webpage. In the process, you'll become close friends with the soup method find_all().\n",
    "ChatGPT\n",
    "To extract hyperlinks from a webpage using BeautifulSoup, you can follow these steps:\n",
    "\n",
    "1. Import the Necessary Libraries:\n",
    "First, you need to import requests to fetch the webpage content and BeautifulSoup from bs4 to parse the HTML.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "2. Fetch the Webpage Content:\n",
    "Use requests.get() to fetch the content of the webpage.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "url = 'https://www.python.org/~guido/'  # BDFL's webpage\n",
    "response = requests.get(url)\n",
    "3. Parse the Webpage with BeautifulSoup:\n",
    "Once you have the webpage content, parse it using BeautifulSoup.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "4. Extract Hyperlinks using find_all():\n",
    "To extract all the hyperlinks, use the find_all() method, which allows you to find all occurrences of a specific HTML tag. For hyperlinks, the tag is <a>. The href attribute contains the URL.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "links = soup.find_all('a')  # Find all anchor tags\n",
    "5. Extract the URLs:\n",
    "Now, loop through the list of <a> tags and extract the href attribute, which contains the URL.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "urls = [link.get('href') for link in links if link.get('href') is not None]\n",
    "6. Review the Extracted URLs:\n",
    "You can print or further process the list of URLs.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "for url in urls:\n",
    "    print(url)\n",
    "Example:\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Fetch the webpage content\n",
    "url = 'https://www.python.org/~guido/'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Parse the webpage\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Step 3: Find all hyperlinks\n",
    "links = soup.find_all('a')\n",
    "\n",
    "# Step 4: Extract the URLs\n",
    "urls = [link.get('href') for link in links if link.get('href') is not None]\n",
    "\n",
    "# Step 5: Print the URLs\n",
    "for url in urls:\n",
    "    print(url)\n",
    "Key Points:\n",
    "find_all() is a powerful method to search for all occurrences of a tag in the parsed HTML.\n",
    "get('href') fetches the value of the href attribute, which contains the hyperlink URL.\n",
    "You should always check if href is None before adding it to your list to avoid errors.\n",
    "This process is helpful for web scraping tasks where you need to collect all the links from a webpage.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Print the title of Guido's webpage\n",
    "print(soup.title)\n",
    "\n",
    "# Find all 'a' tags (which define hyperlinks): a_tags\n",
    "a_tags = soup.find_all('a')\n",
    "\n",
    "# Print the URLs to the shell\n",
    "for link in a_tags:\n",
    "    print(link.get('href'))\n",
    "ChatGPT\n",
    "Your code effectively extracts and prints the URLs of all hyperlinks from Guido van Rossum's webpage. Here's a quick breakdown of what each part of your code does:\n",
    "\n",
    "Import Libraries:\n",
    "\n",
    "requests is used to fetch the webpage content.\n",
    "BeautifulSoup from bs4 is used to parse the HTML.\n",
    "Specify URL:\n",
    "\n",
    "The URL of Guido van Rossum's webpage is assigned to the variable url.\n",
    "Send Request and Catch the Response:\n",
    "\n",
    "requests.get(url) sends a GET request to the specified URL and stores the response in the variable r.\n",
    "Extract HTML from Response:\n",
    "\n",
    "r.text extracts the HTML content of the response as a string, stored in html_doc.\n",
    "Create a BeautifulSoup Object:\n",
    "\n",
    "BeautifulSoup(html_doc, 'html.parser') parses the HTML content using the 'html.parser' parser and creates a BeautifulSoup object soup.\n",
    "Print the Title of the Webpage:\n",
    "\n",
    "soup.title extracts the <title> tag from the HTML, and print(soup.title) prints it to the shell. Note that soup.title returns the full tag (e.g., <title>Guido van Rossum's Home Page</title>), not just the text.\n",
    "Find All Hyperlinks:\n",
    "\n",
    "soup.find_all('a') finds all <a> tags (which define hyperlinks) and stores them in the list a_tags.\n",
    "Print the URLs:\n",
    "\n",
    "The for loop iterates over each element in a_tags, and link.get('href') extracts the URL from the href attribute of each <a> tag. The URLs are then printed to the shell.\n",
    "Example Output:\n",
    "When running the code, you'll see the title of Guido's webpage followed by the URLs of all hyperlinks on the page.\n",
    "\n",
    "plaintext\n",
    "Copy code\n",
    "<title>Guido van Rossum's Home Page</title>\n",
    "http://www.python.org\n",
    "http://www.python.org/doc/essays/cp4e.html\n",
    "http://www.cwi.nl/~guido/\n",
    "mailto:guido@python.org\n",
    "Note:\n",
    "Title Tag: If you want just the text inside the title tag (e.g., Guido van Rossum's Home Page), you can use soup.title.string.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(soup.title.string)\n",
    "Handling Relative URLs: If the href attribute contains relative URLs (e.g., /about/), you'll need to join them with the base URL to create a full URL.\n",
    "\n",
    "Your code is well-structured and demonstrates how to extract hyperlinks from a webpage using BeautifulSoup.\n",
    "Pop quiz: Exploring your JSON\n",
    "Load the JSON 'a_movie.json' into a variable, which will be a dictionary. Do so by copying, pasting and executing the following code in the IPython Shell:\n",
    "\n",
    "import json\n",
    "with open(\"a_movie.json\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "Print the values corresponding to the keys 'Title' and 'Year' and answer the following question about the movie that the JSON describes:\n",
    "\n",
    "Which of the following statements is true of the movie in question?\n",
    "ChatGPT\n",
    "To explore the JSON data and answer the question, follow these steps:\n",
    "\n",
    "Load the JSON Data:\n",
    "Execute the following code in your IPython Shell or Python environment:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import json\n",
    "with open(\"a_movie.json\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "This will load the JSON data from the file a_movie.json into the variable json_data.\n",
    "\n",
    "Print the Values:\n",
    "To see the values for the keys 'Title' and 'Year', run:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(\"Title:\", json_data.get('Title'))\n",
    "print(\"Year:\", json_data.get('Year'))\n",
    "Analyze the Statements:\n",
    "After printing the values, compare them with the statements provided. You'll need to evaluate which statement is true based on the Title and Year values extracted from the JSON data.\n",
    "\n",
    "If you provide the statements or additional context, I can help you analyze them further!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load JSON: json_data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma_movie.json\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m----> 3\u001b[0m   json_data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mload(json_file)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Print each key-value pair in json_data\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m json_data\u001b[38;5;241m.\u001b[39mkeys():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load JSON: json_data\n",
    "with open(\"a_movie.json\") as json_file:\n",
    "  json_data = json.load(json_file)\n",
    "\n",
    "# Print each key-value pair in json_data\n",
    "for k in json_data.keys():\n",
    "  print(k + ': ', json_data[k])  # Access value using dictionary syntax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  The Social Network\n",
      "Year:  2010\n",
      "Rated:  PG-13\n",
      "Released:  01 Oct 2010\n",
      "Runtime:  120 min\n",
      "Genre:  Biography, Drama\n",
      "Director:  David Fincher\n",
      "Writer:  Aaron Sorkin, Ben Mezrich\n",
      "Actors:  Jesse Eisenberg, Andrew Garfield, Justin Timberlake\n",
      "Plot:  As Harvard student Mark Zuckerberg creates the social networking site that would become known as Facebook, he is sued by the twins who claimed he stole their idea and by the co-founder who was later squeezed out of the business.\n",
      "Language:  English, French\n",
      "Country:  United States\n",
      "Awards:  Won 3 Oscars. 173 wins & 187 nominations total\n",
      "Poster:  https://m.media-amazon.com/images/M/MV5BOGUyZDUxZjEtMmIzMC00MzlmLTg4MGItZWJmMzBhZjE0Mjc1XkEyXkFqcGdeQXVyMTMxODk2OTU@._V1_SX300.jpg\n",
      "Ratings:  [{'Source': 'Internet Movie Database', 'Value': '7.8/10'}, {'Source': 'Rotten Tomatoes', 'Value': '96%'}, {'Source': 'Metacritic', 'Value': '95/100'}]\n",
      "Metascore:  95\n",
      "imdbRating:  7.8\n",
      "imdbVotes:  765,414\n",
      "imdbID:  tt1285016\n",
      "Type:  movie\n",
      "DVD:  N/A\n",
      "BoxOffice:  $96,962,694\n",
      "Production:  N/A\n",
      "Website:  N/A\n",
      "Response:  True\n"
     ]
    }
   ],
   "source": [
    "import json  # Import the json module\n",
    "\n",
    "# Load JSON: json_data\n",
    "with open(\"a_movie.json\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# Print each key-value pair in json_data\n",
    "for k in json_data.keys():\n",
    "    print(k + ': ', json_data[k])  # Access value using dictionary syntax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': 'The Social Network', 'Year': '2010', 'Rated': 'PG-13', 'Released': '01 Oct 2010', 'Runtime': '120 min', 'Genre': 'Biography, Drama', 'Director': 'David Fincher', 'Writer': 'Aaron Sorkin, Ben Mezrich', 'Actors': 'Jesse Eisenberg, Andrew Garfield, Justin Timberlake', 'Plot': 'As Harvard student Mark Zuckerberg creates the social networking site that would become known as Facebook, he is sued by the twins who claimed he stole their idea and by the co-founder who was later squeezed out of the business.', 'Language': 'English, French', 'Country': 'United States', 'Awards': 'Won 3 Oscars. 173 wins & 187 nominations total', 'Poster': 'https://m.media-amazon.com/images/M/MV5BOGUyZDUxZjEtMmIzMC00MzlmLTg4MGItZWJmMzBhZjE0Mjc1XkEyXkFqcGdeQXVyMTMxODk2OTU@._V1_SX300.jpg', 'Ratings': [{'Source': 'Internet Movie Database', 'Value': '7.8/10'}, {'Source': 'Rotten Tomatoes', 'Value': '96%'}, {'Source': 'Metacritic', 'Value': '95/100'}], 'Metascore': '95', 'imdbRating': '7.8', 'imdbVotes': '765,414', 'imdbID': 'tt1285016', 'Type': 'movie', 'DVD': 'N/A', 'BoxOffice': '$96,962,694', 'Production': 'N/A', 'Website': 'N/A', 'Response': 'True'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(\"a_movie.json\", \"r\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# Print the entire content of the JSON file\n",
    "print(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  The Social Network\n",
      "Year:  2010\n",
      "Rated:  PG-13\n",
      "Released:  01 Oct 2010\n",
      "Runtime:  120 min\n",
      "Genre:  Biography, Drama\n",
      "Director:  David Fincher\n",
      "Writer:  Aaron Sorkin, Ben Mezrich\n",
      "Actors:  Jesse Eisenberg, Andrew Garfield, Justin Timberlake\n",
      "Plot:  As Harvard student Mark Zuckerberg creates the social networking site that would become known as Facebook, he is sued by the twins who claimed he stole their idea and by the co-founder who was later squeezed out of the business.\n",
      "Language:  English, French\n",
      "Country:  United States\n",
      "Awards:  Won 3 Oscars. 173 wins & 187 nominations total\n",
      "Poster:  https://m.media-amazon.com/images/M/MV5BOGUyZDUxZjEtMmIzMC00MzlmLTg4MGItZWJmMzBhZjE0Mjc1XkEyXkFqcGdeQXVyMTMxODk2OTU@._V1_SX300.jpg\n",
      "Ratings:  [{'Source': 'Internet Movie Database', 'Value': '7.8/10'}, {'Source': 'Rotten Tomatoes', 'Value': '96%'}, {'Source': 'Metacritic', 'Value': '95/100'}]\n",
      "Metascore:  95\n",
      "imdbRating:  7.8\n",
      "imdbVotes:  765,414\n",
      "imdbID:  tt1285016\n",
      "Type:  movie\n",
      "DVD:  N/A\n",
      "BoxOffice:  $96,962,694\n",
      "Production:  N/A\n",
      "Website:  N/A\n",
      "Response:  True\n"
     ]
    }
   ],
   "source": [
    "# Load JSON: json_data\n",
    "with open(\"a_movie.json\") as json_file:\n",
    "  json_data = json.load(json_file)\n",
    "\n",
    "# Print each key-value pair in json_data\n",
    "for k in json_data.keys():\n",
    "  print(k + ': ', json_data[k])  # Access value using dictionary syntax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load JSON: json_data\u001b[39;00m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma_movie.json\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n",
      "\u001b[1;32m----> 3\u001b[0m   json_data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mload(json_file)\n",
      "\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Print each key-value pair in json_data\u001b[39;00m\n",
      "\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m json_data\u001b[38;5;241m.\u001b[39mkeys():\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load JSON: json_data\n",
    "with open(\"a_movie.json\") as json_file:\n",
    "  json_data = json.load(json_file)\n",
    "\n",
    "# Print each key-value pair in json_data\n",
    "for k in json_data.keys():\n",
    "  print(k + ': ', json_data[k])  # Access value using dictionary syntax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"a_movie.json\") as json_file:\n",
    "    json_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Title\":\"The Social Network\",\"Year\":\"2010\",\"Rated\":\"PG-13\",\"Released\":\"01 Oct 2010\",\"Runtime\":\"120 min\",\"Genre\":\"Biography, Drama\",\"Director\":\"David Fincher\",\"Writer\":\"Aaron Sorkin, Ben Mezrich\",\"Actors\":\"Jesse Eisenberg, Andrew Garfield, Justin Timberlake\",\"Plot\":\"As Harvard student Mark Zuckerberg creates the social networking site that would become known as Facebook, he is sued by the twins who claimed he stole their idea and by the co-founder who was later squeezed out of the business.\",\"Language\":\"English, French\",\"Country\":\"United States\",\"Awards\":\"Won 3 Oscars. 173 wins & 187 nominations total\",\"Poster\":\"https://m.media-amazon.com/images/M/MV5BOGUyZDUxZjEtMmIzMC00MzlmLTg4MGItZWJmMzBhZjE0Mjc1XkEyXkFqcGdeQXVyMTMxODk2OTU@._V1_SX300.jpg\",\"Ratings\":[{\"Source\":\"Internet Movie Database\",\"Value\":\"7.8/10\"},{\"Source\":\"Rotten Tomatoes\",\"Value\":\"96%\"},{\"Source\":\"Metacritic\",\"Value\":\"95/100\"}],\"Metascore\":\"95\",\"imdbRating\":\"7.8\",\"imdbVotes\":\"765,414\",\"imdbID\":\"tt1285016\",\"Type\":\"movie\",\"DVD\":\"N/A\",\"BoxOffice\":\"$96,962,694\",\"Production\":\"N/A\",\"Website\":\"N/A\",\"Response\":\"True\"}\n"
     ]
    }
   ],
   "source": [
    "# Import requests package\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'http://www.omdbapi.com/?apikey=72bc447a&t=the+social+network'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Print the text of the response\n",
    "print(r.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API requests\n",
    "Now it's your turn to pull some movie data down from the Open Movie Database (OMDB) using their API. The movie you'll query the API about is The Social Network. Recall that, in the video, to query the API about the movie Hackers, Hugo's query string was 'http://www.omdbapi.com/?t=hackers' and had a single argument t=hackers\n",
    "ChatGPT\n",
    "To query the Open Movie Database (OMDB) API for information about The Social Network, you'll need to construct a URL similar to the example you provided. Here's how you can do it:\n",
    "\n",
    "Base URL: http://www.omdbapi.com/\n",
    "Query Parameter: t for the movie title\n",
    "For The Social Network, the URL would look like this:\n",
    "\n",
    "ruby\n",
    "Copy code\n",
    "http://www.omdbapi.com/?t=The+Social+Network\n",
    "If you need to include an API key (which you should have if you sign up for an API key on the OMDB website), you would append it like this:\n",
    "\n",
    "ruby\n",
    "Copy code\n",
    "http://www.omdbapi.com/?t=The+Social+Network&apikey=YOUR_API_KEY\n",
    "Replace YOUR_API_KEY with your actual API key.\n",
    "\n",
    "To execute the request in Python using the requests library, you could do something like:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "\n",
    "# Replace 'YOUR_API_KEY' with your actual OMDB API key\n",
    "api_key = 'YOUR_API_KEY'\n",
    "url = f\"http://www.omdbapi.com/?t=The+Social+Network&apikey={api_key}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "print(data)\n",
    "This code sends a GET request to the OMDB API, retrieves the data in JSON format, and prints it out.\n",
    "\n",
    "Note: recently, OMDB has changed their API: you now also have to specify an API key. This means you'll have to add another argument to the URL: apikey=72bc447a\n",
    "ChatGPT\n",
    "Memory updated\n",
    "Got it! With the new requirement to include the API key, the URL for querying The Social Network would look like this:\n",
    "\n",
    "ruby\n",
    "Copy code\n",
    "http://www.omdbapi.com/?t=The+Social+Network&apikey=72bc447a\n",
    "Here's the updated Python code to include the API key:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "\n",
    "# API key provided\n",
    "api_key = '72bc447a'\n",
    "url = f\"http://www.omdbapi.com/?t=The+Social+Network&apikey={api_key}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "print(data)\n",
    "This will fetch the data for The Social Network using the OMDB API with the specified API key.\n",
    "\n",
    "explain each line of code\n",
    "# Import requests package\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'http://www.omdbapi.com/?apikey=72bc447a&t=the+social+network'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Print the text of the response\n",
    "print(r.text)\n",
    "ChatGPT\n",
    "Certainly! Here’s a breakdown of each line of code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import requests package\n",
    "import requests\n",
    "Purpose: This line imports the requests library, which is a popular Python package used to send HTTP requests and handle responses. It simplifies the process of making web requests and working with web data.\n",
    "python\n",
    "Copy code\n",
    "# Assign URL to variable: url\n",
    "url = 'http://www.omdbapi.com/?apikey=72bc447a&t=the+social+network'\n",
    "Purpose: This line creates a variable named url and assigns it a string value. The string is the URL used to query the OMDB API for information about The Social Network.\n",
    "Details:\n",
    "http://www.omdbapi.com/: Base URL for the OMDB API.\n",
    "apikey=72bc447a: Parameter to provide the API key.\n",
    "t=the+social+network: Query parameter to specify the movie title. Note that spaces in the title are replaced with +.\n",
    "python\n",
    "Copy code\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "Purpose: This line sends an HTTP GET request to the URL specified in the url variable. The requests.get() function handles the request and returns a response object, which is stored in the variable r.\n",
    "Details:\n",
    "requests.get(url): Sends the GET request to the URL and retrieves the data from the API.\n",
    "r: The response object containing information returned by the API.\n",
    "python\n",
    "Copy code\n",
    "# Print the text of the response\n",
    "print(r.text)\n",
    "Purpose: This line prints the content of the response object to the console. The .text attribute of the response object contains the body of the response as a string.\n",
    "Details:\n",
    "r.text: Retrieves the textual content of the response, which is typically in JSON format when dealing with APIs.\n",
    "print(r.text): Outputs this content to the console, allowing you to see the data returned by the API.\n",
    "In summary, this code snippet queries the OMDB API for information about The Social Network, retrieves the response, and prints the raw text of that response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  The Social Network\n",
      "Year:  2010\n",
      "Rated:  PG-13\n",
      "Released:  01 Oct 2010\n",
      "Runtime:  120 min\n",
      "Genre:  Biography, Drama\n",
      "Director:  David Fincher\n",
      "Writer:  Aaron Sorkin, Ben Mezrich\n",
      "Actors:  Jesse Eisenberg, Andrew Garfield, Justin Timberlake\n",
      "Plot:  As Harvard student Mark Zuckerberg creates the social networking site that would become known as Facebook, he is sued by the twins who claimed he stole their idea and by the co-founder who was later squeezed out of the business.\n",
      "Language:  English, French\n",
      "Country:  United States\n",
      "Awards:  Won 3 Oscars. 173 wins & 187 nominations total\n",
      "Poster:  https://m.media-amazon.com/images/M/MV5BOGUyZDUxZjEtMmIzMC00MzlmLTg4MGItZWJmMzBhZjE0Mjc1XkEyXkFqcGdeQXVyMTMxODk2OTU@._V1_SX300.jpg\n",
      "Ratings:  [{'Source': 'Internet Movie Database', 'Value': '7.8/10'}, {'Source': 'Rotten Tomatoes', 'Value': '96%'}, {'Source': 'Metacritic', 'Value': '95/100'}]\n",
      "Metascore:  95\n",
      "imdbRating:  7.8\n",
      "imdbVotes:  765,414\n",
      "imdbID:  tt1285016\n",
      "Type:  movie\n",
      "DVD:  N/A\n",
      "BoxOffice:  $96,962,694\n",
      "Production:  N/A\n",
      "Website:  N/A\n",
      "Response:  True\n"
     ]
    }
   ],
   "source": [
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'http://www.omdbapi.com/?apikey=72bc447a&t=social+network'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = r.json()  # Access the JSON data as a dictionary\n",
    "\n",
    "# Print each key-value pair in json_data\n",
    "for k in json_data.keys():\n",
    "  print(k + ': ', json_data[k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain this\n",
    "JSON–from the web to Python\n",
    "Wow, congrats! You've just queried your first API programmatically in Python and printed the text of the response to the shell. However, as you know, your response is actually a JSON, so you can do one step better and decode the JSON. You can then print the key-value pairs of the resulting dictionary. That's what you're going to do now!\n",
    "ChatGPT\n",
    "When you query an API programmatically in Python, the response you receive is often in the form of JSON (JavaScript Object Notation). JSON is a lightweight data format that's easy for humans to read and write and easy for machines to parse and generate. It essentially represents data as key-value pairs, much like a Python dictionary.\n",
    "\n",
    "In the context you're describing:\n",
    "\n",
    "Querying the API: You send a request to a web server (an API endpoint), and it returns some data. This data is typically in JSON format, which is a common format for exchanging data between a server and a client (like your Python program).\n",
    "\n",
    "Printing the Text of the Response: Initially, you might just print the raw text of the response, which will look like a long string of data. This is the JSON in its raw format.\n",
    "\n",
    "Decoding the JSON: Since the response is in JSON format, you can use Python's json module to decode it. Decoding the JSON converts it into a Python dictionary. This allows you to work with the data more easily because you can now access the values using their corresponding keys, just like you would with any Python dictionary.\n",
    "\n",
    "Printing the Key-Value Pairs: Once the JSON is decoded into a dictionary, you can iterate through it and print out each key-value pair. This gives you a clear, structured view of the data that was returned by the API.\n",
    "\n",
    "By decoding the JSON and printing the key-value pairs, you go beyond just seeing the raw data; you can now interact with it programmatically, which is the real power of working with APIs in Python.\n",
    "\n",
    "what each line of code does with detail explanation\n",
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'http://www.omdbapi.com/?apikey=72bc447a&t=social+network'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = r.json()  # Access the JSON data as a dictionary\n",
    "\n",
    "# Print each key-value pair in json_data\n",
    "for k in json_data.keys():\n",
    "  print(k + ': ', json_data[k])\n",
    "\n",
    "ChatGPT\n",
    "Here’s a detailed explanation of what each line of code does:\n",
    "\n",
    "1. Import the requests package\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "Purpose: This line imports the requests library, which is a powerful Python package used to send HTTP requests. It's a popular tool for interacting with APIs, downloading web pages, and submitting forms online. The requests package simplifies making HTTP requests and handling responses.\n",
    "2. Assign URL to variable url\n",
    "python\n",
    "Copy code\n",
    "url = 'http://www.omdbapi.com/?apikey=72bc447a&t=social+network'\n",
    "Purpose: Here, the URL for the API request is assigned to a variable named url. This URL points to the OMDB API, a web service used to retrieve movie information. The query parameters in the URL are as follows:\n",
    "apikey=72bc447a: This is the API key required by the OMDB API for access.\n",
    "t=social+network: This specifies the title of the movie you want information about. In this case, it's \"The Social Network,\" with spaces represented by + signs.\n",
    "3. Package the request, send the request, and catch the response\n",
    "python\n",
    "Copy code\n",
    "r = requests.get(url)\n",
    "Purpose: This line sends a GET request to the URL stored in the url variable. The requests.get() method sends the HTTP request and returns a Response object. This object (stored in variable r) contains all the information returned by the server, including the content (which is the JSON data in this case), status code, headers, and more.\n",
    "4. Decode the JSON data into a dictionary\n",
    "python\n",
    "Copy code\n",
    "json_data = r.json()\n",
    "Purpose: The r.json() method parses the JSON content of the response and returns it as a Python dictionary. JSON (JavaScript Object Notation) is a structured format that is easily converted to a dictionary in Python. This allows you to work with the data using standard dictionary operations.\n",
    "5. Print each key-value pair in json_data\n",
    "python\n",
    "Copy code\n",
    "for k in json_data.keys():\n",
    "  print(k + ': ', json_data[k])\n",
    "Purpose: This loop iterates over each key in the dictionary json_data.\n",
    "json_data.keys() returns a view object that displays a list of all the keys in the dictionary.\n",
    "For each key k, the corresponding value is accessed using json_data[k].\n",
    "The code prints each key followed by its associated value, formatted as key: value. This gives you a clear output of the data returned by the API, showing each field and its content.\n",
    "Overall Flow:\n",
    "The requests library is used to send an HTTP GET request to the OMDB API.\n",
    "The server's response (in JSON format) is automatically parsed into a dictionary.\n",
    "Each key-value pair in the dictionary is printed out, providing a human-readable format of the movie data fetched from the API.\n",
    "This sequence of steps demonstrates how to query an API, decode the JSON response, and interact with the data in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1236090951\">\n",
      "<p class=\"mw-empty-elt\">\n",
      "\n",
      "\n",
      "</p>\n",
      "\n",
      "\n",
      "<p><b>Pizza</b> is an Italian dish typically consisting of a flat base of leavened wheat-based dough topped with tomato, cheese, and other ingredients, baked at a high temperature, traditionally in a wood-fired oven.\n",
      "</p><p>The term <i>pizza</i> was first recorded in the year 997 AD, in a Latin manuscript from the southern Italian town of Gaeta, in Lazio, on the border with Campania. Raffaele Esposito is often credited for creating modern pizza in Naples. In 2009, Neapolitan pizza was registered with the European Union as a traditional speciality guaranteed (TSG) dish. In 2017, the art of making Neapolitan pizza was added to UNESCO's list of intangible cultural heritage.\n",
      "</p><p>Pizza and its variants are among the most popular foods in the world. Pizza is sold at a variety of restaurants, including pizzerias (pizza specialty restaurants), Mediterranean restaurants, via delivery, and as street food. In Italy, pizza served in a restaurant is presented unsliced, and is eaten with the use of a knife and fork. In casual settings, however, it is typically cut into slices to be eaten while held in the hand. Pizza is also sold in grocery stores in a variety of forms, including frozen or as kits for self-assembly. They are then cooked using a home oven.\n",
      "</p><p>In 2017, the world pizza market was US$128 billion, and in the U.S. it was $44 billion spread over 76,000 pizzerias. Overall, 13% of the U.S. population aged two years and over consumed pizza on any given day.\n",
      "</p>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = r.json()\n",
    "\n",
    "# Print the Wikipedia page extract\n",
    "pizza_extract = json_data['query']['pages']['24768']['extract']\n",
    "print(pizza_extract)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Checking out the Wikipedia API\n",
    "You're doing so well and having so much fun that we're going to throw one more API at you: the Wikipedia API (documented here). You'll figure out how to find and extract information from the Wikipedia page for Pizza. What gets a bit wild here is that your query will return nested JSONs, that is, JSONs with JSONs, but Python can handle that because it will translate them into dictionaries within dictionaries.\n",
    "\n",
    "The URL that requests the relevant query from the Wikipedia API is\n",
    "\n",
    "https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza\n",
    "ChatGPT\n",
    "To work with the Wikipedia API and extract information from the Pizza page, you'll need to make an HTTP request to the URL provided and then parse the returned JSON to extract the information you want. Here's how you can do this using Python:\n",
    "\n",
    "Send the request to the Wikipedia API: You can use the requests library to send the HTTP request and retrieve the data.\n",
    "\n",
    "Parse the JSON response: The response will be in JSON format, which Python can easily handle using the json module or directly with the requests library's .json() method.\n",
    "\n",
    "Extract the information: The JSON response will be nested, so you'll need to navigate through the dictionaries to find the extract (the introductory text) of the Pizza page.\n",
    "\n",
    "Here's an example of how you can do this:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "\n",
    "# Step 1: Send the request to the Wikipedia API\n",
    "url = \"https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Parse the JSON response\n",
    "data = response.json()\n",
    "\n",
    "# Step 3: Extract the information\n",
    "page = next(iter(data['query']['pages'].values()))  # Get the first page in the 'pages' dictionary\n",
    "extract = page.get('extract', 'No extract found')   # Get the 'extract' field or a default message\n",
    "\n",
    "# Print the extracted information\n",
    "print(extract)\n",
    "Explanation:\n",
    "Step 1: We make an HTTP GET request to the Wikipedia API using the provided URL.\n",
    "Step 2: The response is parsed into a JSON object.\n",
    "Step 3: The data['query']['pages'] contains a dictionary with page IDs as keys. We use next(iter(...)) to get the first (and only) page in this case, then retrieve the extract field, which contains the introductory text of the Wikipedia page.\n",
    "Output:\n",
    "When you run the script, it should print the introduction text of the Wikipedia page for \"Pizza.\"\n",
    "\n",
    "Let me know if you'd like to explore this further!\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = r.json()\n",
    "\n",
    "# Print the Wikipedia page extract\n",
    "pizza_extract = json_data['query']['pages']['24768']['extract']\n",
    "print(pizza_extract)\n",
    "ChatGPT\n",
    "Your code is almost correct! The main issue is that Wikipedia page IDs can change, so hardcoding the page ID ('24768' in this case) might not always work. Instead, it's better to dynamically get the page ID from the response.\n",
    "\n",
    "Here’s a slightly revised version of your code that dynamically extracts the page ID:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = r.json()\n",
    "\n",
    "# Extract the page ID dynamically\n",
    "page_id = next(iter(json_data['query']['pages']))  # Get the page ID\n",
    "\n",
    "# Extract the Wikipedia page extract\n",
    "pizza_extract = json_data['query']['pages'][page_id]['extract']\n",
    "print(pizza_extract)\n",
    "Explanation:\n",
    "Dynamic Page ID Extraction: next(iter(json_data['query']['pages'])) dynamically retrieves the first key from the pages dictionary, which represents the page ID.\n",
    "Extract and Print: Using the dynamic page ID, we access the extract field to get the introductory text.\n",
    "This approach ensures that your code will work even if the page ID changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I plan to use Twitter’s data and API for several purposes in my data science projects:\n",
    "\n",
    "Trend Analysis: I will use the search_tweets endpoint to track and analyze trending hashtags and topics to understand public interest and sentiment over time.\n",
    "\n",
    "Sentiment Analysis: By fetching tweets related to specific brands or events, I aim to perform sentiment analysis to gauge public opinion and detect changes in sentiment.\n",
    "\n",
    "Event Monitoring: Using the streaming API, I will monitor live tweets about ongoing events to analyze real-time public reactions and discussions.\n",
    "\n",
    "User Behavior Analysis: I will study user engagement metrics, such as likes and retweets, to understand how users interact with content related to specific topics or brands.\n",
    "\n",
    "In each use case, I will handle data collection, storage, and analysis with a focus on managing large datasets and addressing any rate limits imposed by the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected parameter: track\n",
      "Stream encountered HTTP error: 401\n",
      "HTTP error response text: {\n",
      "  \"title\": \"Unauthorized\",\n",
      "  \"type\": \"about:blank\",\n",
      "  \"status\": 401,\n",
      "  \"detail\": \"Unauthorized\"\n",
      "}\n",
      "Stream encountered HTTP error: 401\n",
      "HTTP error response text: {\n",
      "  \"title\": \"Unauthorized\",\n",
      "  \"type\": \"about:blank\",\n",
      "  \"status\": 401,\n",
      "  \"detail\": \"Unauthorized\"\n",
      "}\n",
      "Stream encountered HTTP error: 401\n",
      "HTTP error response text: {\n",
      "  \"title\": \"Unauthorized\",\n",
      "  \"type\": \"about:blank\",\n",
      "  \"status\": 401,\n",
      "  \"detail\": \"Unauthorized\"\n",
      "}\n",
      "Stream encountered HTTP error: 401\n",
      "HTTP error response text: {\n",
      "  \"title\": \"Unauthorized\",\n",
      "  \"type\": \"about:blank\",\n",
      "  \"status\": 401,\n",
      "  \"detail\": \"Unauthorized\"\n",
      "}\n",
      "Stream encountered HTTP error: 401\n",
      "HTTP error response text: {\n",
      "  \"title\": \"Unauthorized\",\n",
      "  \"type\": \"about:blank\",\n",
      "  \"status\": 401,\n",
      "  \"detail\": \"Unauthorized\"\n",
      "}\n",
      "Stream encountered HTTP error: 401\n",
      "HTTP error response text: {\n",
      "  \"title\": \"Unauthorized\",\n",
      "  \"type\": \"about:blank\",\n",
      "  \"status\": 401,\n",
      "  \"detail\": \"Unauthorized\"\n",
      "}\n",
      "Stream encountered HTTP error: 401\n",
      "HTTP error response text: {\n",
      "  \"title\": \"Unauthorized\",\n",
      "  \"type\": \"about:blank\",\n",
      "  \"status\": 401,\n",
      "  \"detail\": \"Unauthorized\"\n",
      "}\n",
      "Stream encountered HTTP error: 401\n",
      "HTTP error response text: {\n",
      "  \"title\": \"Unauthorized\",\n",
      "  \"type\": \"about:blank\",\n",
      "  \"status\": 401,\n",
      "  \"detail\": \"Unauthorized\"\n",
      "}\n",
      "Stream encountered HTTP error: 401\n",
      "HTTP error response text: {\n",
      "  \"title\": \"Unauthorized\",\n",
      "  \"type\": \"about:blank\",\n",
      "  \"status\": 401,\n",
      "  \"detail\": \"Unauthorized\"\n",
      "}\n",
      "Stream connection has errored or timed out\n",
      "Connection error: requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Stream encountered HTTP error: 401\n",
      "HTTP error response text: {\n",
      "  \"title\": \"Unauthorized\",\n",
      "  \"type\": \"about:blank\",\n",
      "  \"status\": 401,\n",
      "  \"detail\": \"Unauthorized\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "# Store credentials in relevant variables\n",
    "consumer_key = \"nZ6EA0FxZ293SxGNg8g8aP0HM\"\n",
    "consumer_secret = \"fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i\"\n",
    "access_token = \"1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy\"\n",
    "access_token_secret = \"X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx\"\n",
    "\n",
    "# Authenticate with the Twitter API\n",
    "auth = tweepy.OAuth1UserHandler(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "\n",
    "# Create a StreamListener class to handle incoming tweets\n",
    "class MyStreamListener(tweepy.StreamingClient):\n",
    "    def on_tweet(self, tweet):\n",
    "        # Handle the incoming tweet\n",
    "        print(f\"Tweet from @{tweet.user.screen_name}: {tweet.text}\")\n",
    "    \n",
    "    def on_error(self, status_code):\n",
    "        if status_code == 420:\n",
    "            # Returning False disconnects the stream\n",
    "            return False\n",
    "\n",
    "# Create your Stream object with credentials\n",
    "stream = MyStreamListener(bearer_token=None)\n",
    "stream.auth = auth\n",
    "\n",
    "# Filter your Stream variable for the keywords\n",
    "keywords = [\"clinton\", \"trump\", \"sanders\", \"cruz\"]\n",
    "stream.filter(track=keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "# Store credentials in relevant variables\n",
    "consumer_key = \"nZ6EA0FxZ293SxGNg8g8aP0HM\"\n",
    "consumer_secret = \"fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i\"\n",
    "access_token = \"1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy\"\n",
    "access_token_secret = \"X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx\"\n",
    "\n",
    "# Create your Stream object with credentials\n",
    "stream = tweepy.Stream(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "\n",
    "# Filter your Stream variable\n",
    "stream.filter(track=[\"clinton\", \"trump\", \"sanders\", \"cruz\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Checking out the Wikipedia API\n",
    "You're doing so well and having so much fun that we're going to throw one more API at you: the Wikipedia API (documented here). You'll figure out how to find and extract information from the Wikipedia page for Pizza. What gets a bit wild here is that your query will return nested JSONs, that is, JSONs with JSONs, but Python can handle that because it will translate them into dictionaries within dictionaries.\n",
    "\n",
    "The URL that requests the relevant query from the Wikipedia API is\n",
    "\n",
    "https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza\n",
    "ChatGPT\n",
    "To work with the Wikipedia API and extract information from the Pizza page, you'll need to make an HTTP request to the URL provided and then parse the returned JSON to extract the information you want. Here's how you can do this using Python:\n",
    "\n",
    "Send the request to the Wikipedia API: You can use the requests library to send the HTTP request and retrieve the data.\n",
    "\n",
    "Parse the JSON response: The response will be in JSON format, which Python can easily handle using the json module or directly with the requests library's .json() method.\n",
    "\n",
    "Extract the information: The JSON response will be nested, so you'll need to navigate through the dictionaries to find the extract (the introductory text) of the Pizza page.\n",
    "\n",
    "Here's an example of how you can do this:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "\n",
    "# Step 1: Send the request to the Wikipedia API\n",
    "url = \"https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Parse the JSON response\n",
    "data = response.json()\n",
    "\n",
    "# Step 3: Extract the information\n",
    "page = next(iter(data['query']['pages'].values()))  # Get the first page in the 'pages' dictionary\n",
    "extract = page.get('extract', 'No extract found')   # Get the 'extract' field or a default message\n",
    "\n",
    "# Print the extracted information\n",
    "print(extract)\n",
    "Explanation:\n",
    "Step 1: We make an HTTP GET request to the Wikipedia API using the provided URL.\n",
    "Step 2: The response is parsed into a JSON object.\n",
    "Step 3: The data['query']['pages'] contains a dictionary with page IDs as keys. We use next(iter(...)) to get the first (and only) page in this case, then retrieve the extract field, which contains the introductory text of the Wikipedia page.\n",
    "Output:\n",
    "When you run the script, it should print the introduction text of the Wikipedia page for \"Pizza.\"\n",
    "\n",
    "Let me know if you'd like to explore this further!\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = r.json()\n",
    "\n",
    "# Print the Wikipedia page extract\n",
    "pizza_extract = json_data['query']['pages']['24768']['extract']\n",
    "print(pizza_extract)\n",
    "ChatGPT\n",
    "Your code is almost correct! The main issue is that Wikipedia page IDs can change, so hardcoding the page ID ('24768' in this case) might not always work. Instead, it's better to dynamically get the page ID from the response.\n",
    "\n",
    "Here’s a slightly revised version of your code that dynamically extracts the page ID:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = r.json()\n",
    "\n",
    "# Extract the page ID dynamically\n",
    "page_id = next(iter(json_data['query']['pages']))  # Get the page ID\n",
    "\n",
    "# Extract the Wikipedia page extract\n",
    "pizza_extract = json_data['query']['pages'][page_id]['extract']\n",
    "print(pizza_extract)\n",
    "Explanation:\n",
    "Dynamic Page ID Extraction: next(iter(json_data['query']['pages'])) dynamically retrieves the first key from the pages dictionary, which represents the page ID.\n",
    "Extract and Print: Using the dynamic page ID, we access the extract field to get the introductory text.\n",
    "This approach ensures that your code will work even if the page ID changes.\n",
    "\n",
    "\n",
    "2/2\n",
    "\n",
    "Streaming tweets\n",
    "It's time to stream some tweets! Your task is to create the Streamobject and to filter tweets according to particular keywords. tweepy has been imported for you.\n",
    "ChatGPT\n",
    "To stream tweets and filter them based on particular keywords using tweepy, you'll need to set up a stream listener and define the keywords you want to filter by. Here's a step-by-step guide to doing this:\n",
    "\n",
    "Set up Tweepy Authentication: You'll need API credentials from Twitter to authenticate with Tweepy.\n",
    "\n",
    "Create a Stream Listener: Define a class that inherits from tweepy.StreamingClient and overrides methods to handle incoming tweets.\n",
    "\n",
    "Start Streaming: Use the filter method to specify which keywords you want to track.\n",
    "\n",
    "Here's an example of how you can set this up:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import tweepy\n",
    "\n",
    "# Define your credentials here\n",
    "API_KEY = 'your_api_key'\n",
    "API_SECRET_KEY = 'your_api_secret_key'\n",
    "ACCESS_TOKEN = 'your_access_token'\n",
    "ACCESS_TOKEN_SECRET = 'your_access_token_secret'\n",
    "\n",
    "class MyStreamListener(tweepy.StreamingClient):\n",
    "    def on_tweet(self, tweet):\n",
    "        # Handle the incoming tweet\n",
    "        print(f\"Tweet from @{tweet.author.username}: {tweet.text}\")\n",
    "    \n",
    "    def on_error(self, status_code):\n",
    "        if status_code == 420:\n",
    "            # Returning False disconnects the stream\n",
    "            return False\n",
    "\n",
    "# Authenticate and create a stream object\n",
    "stream_listener = MyStreamListener(bearer_token=API_KEY)\n",
    "\n",
    "# Define the keywords you want to filter\n",
    "keywords = ['Python', 'Tweepy', 'DataScience']\n",
    "\n",
    "# Start streaming and filter tweets by keywords\n",
    "stream_listener.filter(track=keywords)\n",
    "Explanation:\n",
    "Authentication: Replace the placeholder credentials with your actual Twitter API credentials. If you're using Twitter API v2, you'll use a Bearer Token.\n",
    "\n",
    "Stream Listener Class: MyStreamListener inherits from tweepy.StreamingClient. The on_tweet method handles each incoming tweet and prints it. The on_error method handles errors, and in this case, it disconnects the stream if rate-limiting is encountered (status code 420).\n",
    "\n",
    "Start Streaming: The filter method is used to track tweets containing the specified keywords. Adjust the keywords list to include the terms you're interested in.\n",
    "\n",
    "Make sure to install the tweepy library if you haven’t already:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "pip install tweepy\n",
    "This code provides a basic setup for streaming tweets. You can customize the on_tweet method to handle tweets according to your needs, such as saving them to a file or performing further analysis.\n",
    "\n",
    "how do I get my API key\n",
    "ChatGPT\n",
    "To get your API key and other credentials for using the Twitter API, follow these steps:\n",
    "\n",
    "Create a Twitter Developer Account:\n",
    "\n",
    "Go to the Twitter Developer website and log in with your Twitter account.\n",
    "Apply for a developer account by filling out the necessary forms and providing details about your intended use of the Twitter API.\n",
    "Create a Project and App:\n",
    "\n",
    "Once your developer account is approved, go to the Twitter Developer Dashboard.\n",
    "Click on \"Create Project\" and fill in the required details about your project.\n",
    "After creating a project, you'll need to create an app within that project.\n",
    "Generate API Keys and Tokens:\n",
    "\n",
    "Within your app's details page, navigate to the \"Keys and Tokens\" tab.\n",
    "You'll find your API Key (Consumer Key) and API Secret Key (Consumer Secret). These are required for authenticating API requests.\n",
    "You will also need to generate the Access Token and Access Token Secret, which can be found or generated in the same tab.\n",
    "Generate a Bearer Token (for v2 API):\n",
    "\n",
    "If you’re using Twitter API v2, you’ll also need a Bearer Token. This is available under the \"Bearer Token\" section in the \"Keys and Tokens\" tab.\n",
    "Summary of Credentials:\n",
    "API Key (Consumer Key)\n",
    "API Secret Key (Consumer Secret)\n",
    "Access Token\n",
    "Access Token Secret\n",
    "Bearer Token (for API v2)\n",
    "Example:\n",
    "For using tweepy with API v2, your credentials might look like this:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "API_KEY = 'your_api_key'\n",
    "API_SECRET_KEY = 'your_api_secret_key'\n",
    "ACCESS_TOKEN = 'your_access_token'\n",
    "ACCESS_TOKEN_SECRET = 'your_access_token_secret'\n",
    "Ensure you keep your API keys and tokens secure and do not share them publicly.\n",
    "\n",
    "I student learning data science\n",
    "How can I address this question\n",
    "Describe all of your use cases of Twitter’s data and API\n",
    "ChatGPT\n",
    "Memory updated\n",
    "When describing your use cases for Twitter's data and API, you should outline specific scenarios where you plan to use Twitter data and how you intend to interact with the API to achieve your goals. Here’s a structured approach to help you address this question:\n",
    "\n",
    "1. Use Case Overview\n",
    "Objective: Explain why you're interested in Twitter data. For example, you might want to analyze trends, monitor brand sentiment, or study user behavior.\n",
    "2. Specific Use Cases\n",
    "Trend Analysis:\n",
    "\n",
    "Objective: Identify and analyze trending topics and hashtags.\n",
    "API Usage: Use the search_tweets endpoint to fetch tweets containing specific hashtags or keywords. Analyze tweet volume and sentiment over time to understand trends.\n",
    "Sentiment Analysis:\n",
    "\n",
    "Objective: Assess public sentiment towards certain topics, brands, or events.\n",
    "API Usage: Retrieve tweets related to a particular brand or event using the search_tweets endpoint. Perform sentiment analysis on the tweet content to gauge positive, negative, or neutral sentiments.\n",
    "User Behavior Analysis:\n",
    "\n",
    "Objective: Study how users interact with certain topics or brands.\n",
    "API Usage: Collect tweets from specific users or about specific topics using the user_timeline or search_tweets endpoints. Analyze user engagement metrics such as likes, retweets, and replies.\n",
    "Event Monitoring:\n",
    "\n",
    "Objective: Monitor real-time events and public reactions.\n",
    "API Usage: Use the streaming API to track live tweets related to an ongoing event. This can help in analyzing immediate public reaction and trending discussions.\n",
    "Network Analysis:\n",
    "\n",
    "Objective: Explore the network of users discussing certain topics.\n",
    "API Usage: Use search_tweets to gather tweets and then analyze user interactions and connections by examining retweets, mentions, and replies.\n",
    "3. Implementation Details\n",
    "Data Collection: Explain how you'll collect data using specific API endpoints.\n",
    "Data Storage: Describe how you'll store the collected data, e.g., in a database or data warehouse.\n",
    "Data Analysis: Outline the methods and tools you’ll use for analysis, such as natural language processing, machine learning models, or data visualization techniques.\n",
    "4. Potential Challenges\n",
    "Data Volume: Managing large volumes of tweet data.\n",
    "Rate Limits: Handling API rate limits and ensuring compliance.\n",
    "Data Privacy: Ensuring ethical use of data and respecting user privacy.\n",
    "Example Response\n",
    "“I plan to use Twitter’s data and API for several purposes in my data science projects:\n",
    "\n",
    "Trend Analysis: I will use the search_tweets endpoint to track and analyze trending hashtags and topics to understand public interest and sentiment over time.\n",
    "\n",
    "Sentiment Analysis: By fetching tweets related to specific brands or events, I aim to perform sentiment analysis to gauge public opinion and detect changes in sentiment.\n",
    "\n",
    "Event Monitoring: Using the streaming API, I will monitor live tweets about ongoing events to analyze real-time public reactions and discussions.\n",
    "\n",
    "User Behavior Analysis: I will study user engagement metrics, such as likes and retweets, to understand how users interact with content related to specific topics or brands.\n",
    "\n",
    "In each use case, I will handle data collection, storage, and analysis with a focus on managing large datasets and addressing any rate limits imposed by the API.”\n",
    "\n",
    "This structured approach will help you clearly describe how you plan to utilize Twitter's data and API for your projects.\n",
    "\n",
    "'''Create your Stream object with the credentials given.\n",
    "Filter your Stream variable for the keywords \"clinton\", \"trump\", \"sanders\", and \"cruz\".'''\n",
    "\n",
    "# Store credentials in relevant variables\n",
    "consumer_key = \"nZ6EA0FxZ293SxGNg8g8aP0HM\"\n",
    "consumer_secret = \"fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i\"\n",
    "access_token = \"1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy\"\n",
    "access_token_secret = \"X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx\"\n",
    "\n",
    "# Create your Stream object with credentials\n",
    "stream = tweepy.Stream(____, ____, ____, ____)\n",
    "\n",
    "# Filter your Stream variable\n",
    "stream.filter(____)\n",
    "ChatGPT\n",
    "To create a Stream object and filter tweets based on specific keywords using Tweepy, you need to provide your authentication credentials and define a class to handle the incoming tweets. Here's how you can complete your code:\n",
    "\n",
    "Set Up Authentication: Use the provided credentials to authenticate with the Twitter API.\n",
    "Create the Stream Object: Instantiate the Stream object with your credentials and a custom StreamListener class.\n",
    "Filter Tweets: Use the filter method to track tweets containing specific keywords.\n",
    "Here's a complete example:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import tweepy\n",
    "\n",
    "# Store credentials in relevant variables\n",
    "consumer_key = \"nZ6EA0FxZ293SxGNg8g8aP0HM\"\n",
    "consumer_secret = \"fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i\"\n",
    "access_token = \"1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy\"\n",
    "access_token_secret = \"X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx\"\n",
    "\n",
    "# Authenticate with the Twitter API\n",
    "auth = tweepy.OAuth1UserHandler(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "\n",
    "# Create a StreamListener class to handle incoming tweets\n",
    "class MyStreamListener(tweepy.StreamingClient):\n",
    "    def on_tweet(self, tweet):\n",
    "        # Handle the incoming tweet\n",
    "        print(f\"Tweet from @{tweet.user.screen_name}: {tweet.text}\")\n",
    "    \n",
    "    def on_error(self, status_code):\n",
    "        if status_code == 420:\n",
    "            # Returning False disconnects the stream\n",
    "            return False\n",
    "\n",
    "# Create your Stream object with credentials\n",
    "stream = MyStreamListener(bearer_token=None)\n",
    "stream.auth = auth\n",
    "\n",
    "# Filter your Stream variable for the keywords\n",
    "keywords = [\"clinton\", \"trump\", \"sanders\", \"cruz\"]\n",
    "stream.filter(track=keywords)\n",
    "Explanation:\n",
    "Authentication:\n",
    "\n",
    "Use tweepy.OAuth1UserHandler to authenticate with Twitter using your consumer keys and access tokens.\n",
    "StreamListener Class:\n",
    "\n",
    "Define MyStreamListener to handle incoming tweets with the on_tweet method. This method processes each tweet and prints it out. The on_error method handles potential errors, such as rate limits.\n",
    "Stream Object Creation:\n",
    "\n",
    "Create an instance of MyStreamListener and assign the auth object to it.\n",
    "Filter:\n",
    "\n",
    "Use stream.filter(track=keywords) to start streaming tweets that contain any of the specified keywords.\n",
    "Note:\n",
    "If you're using Twitter API v2, you might need to adjust the setup slightly, particularly with how authentication is handled. For API v2, you would use tweepy.StreamingClient with a Bearer Token, and the on_tweet method would need to be adapted accordingly.\n",
    "age:  English, French\n",
    "Country:  United States\n",
    "Awards:  Won 3 Oscars. 173 wins & 187 nominations total\n",
    "Poster:  https://m.media-amazon.com/images/M/MV5BOGUyZDUxZjEtMmIzMC00MzlmLTg4MGItZWJmMzBhZjE0Mjc1XkEyXkFqcGdeQXVyMTMxODk2OTU@._V1_SX300.jpg\n",
    "Ratings:  [{'Source': 'Internet Movie Database', 'Value': '7.8/10'}, {'Source': 'Rotten Tomatoes', 'Value': '96%'}, {'Source': 'Metacritic', 'Value': '95/100'}]\n",
    "Metascore:  95\n",
    "imdbRating:  7.8\n",
    "imdbVotes:  765,414\n",
    "imdbID:  tt1285016\n",
    "Type:  movie\n",
    "DVD:  N/A\n",
    "BoxOffice:  $96,962,694\n",
    "Production:  N/A\n",
    "Website:  N/A\n",
    "Response:  True\n",
    "<link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1236090951\">\n",
    "<p class=\"mw-empty-elt\">\n",
    "\n",
    "\n",
    "</p>\n",
    "\n",
    "\n",
    "<p><b>Pizza</b> is an Italian dish typically consisting of a flat base of leavened wheat-based dough topped with tomato, cheese, and other ingredients, baked at a high temperature, traditionally in a wood-fired oven.\n",
    "</p><p>The term <i>pizza</i> was first recorded in the year 997 AD, in a Latin manuscript from the southern Italian town of Gaeta, in Lazio, on the border with Campania. Raffaele Esposito is often credited for creating modern pizza in Naples. In 2009, Neapolitan pizza was registered with the European Union as a traditional speciality guaranteed (TSG) dish. In 2017, the art of making Neapolitan pizza was added to UNESCO's list of intangible cultural heritage.\n",
    "</p><p>Pizza and its variants are among the most popular foods in the world. Pizza is sold at a variety of restaurants, including pizzerias (pizza specialty restaurants), Mediterranean restaurants, via delivery, and as street food. In Italy, pizza served in a restaurant is presented unsliced, and is eaten with the use of a knife and fork. In casual settings, however, it is typically cut into slices to be eaten while held in the hand. Pizza is also sold in grocery stores in a variety of forms, including frozen or as kits for self-assembly. They are then cooked using a home oven.\n",
    "</p><p>In 2017, the world pizza market was US$128 billion, and in the U.S. it was $44 billion spread over 76,000 pizzerias. Overall, 13% of the U.S. population aged two years and over consumed pizza on any given day.\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Unexpected parameter: track\n",
    "Stream encountered HTTP error: 401\n",
    "HTTP error response text: {\n",
    "  \"title\": \"Unauthorized\",\n",
    "  \"type\": \"about:blank\",\n",
    "  \"status\": 401,\n",
    "  \"detail\": \"Unauthorized\"\n",
    "}\n",
    "Stream encountered HTTP error: 401\n",
    "HTTP error response text: {\n",
    "  \"title\": \"Unauthorized\",\n",
    "  \"type\": \"about:blank\",\n",
    "  \"status\": 401,\n",
    "  \"detail\": \"Unauthorized\"\n",
    "}\n",
    "Stream encountered HTTP error: 401\n",
    "HTTP error response text: {\n",
    "  \"title\": \"Unauthorized\",\n",
    "  \"type\": \"about:blank\",\n",
    "  \"status\": 401,\n",
    "  \"detail\": \"Unauthorized\"\n",
    "}\n",
    "Stream encountered HTTP error: 401\n",
    "HTTP error response text: {\n",
    "  \"title\": \"Unauthorized\",\n",
    "  \"type\": \"about:blank\",\n",
    "  \"status\": 401,\n",
    "  \"detail\": \"Unauthorized\"\n",
    "}\n",
    "Stream encountered HTTP error: 401\n",
    "HTTP error response text: {\n",
    "  \"title\": \"Unauthorized\",\n",
    "  \"type\": \"about:blank\",\n",
    "  \"status\": 401,\n",
    "  \"detail\": \"Unauthorized\"\n",
    "}\n",
    "ChatGPT\n",
    "The \"Unauthorized\" HTTP error (401) typically indicates an issue with authentication. Here’s how to troubleshoot and resolve this issue:\n",
    "\n",
    "1. Check Authentication Credentials\n",
    "Make sure that you are using the correct credentials for the Twitter API and that they are correctly configured. For the Tweepy v2 API, the Bearer Token should be used if you're working with tweepy.StreamingClient.\n",
    "\n",
    "2. Verify API Access Level\n",
    "Ensure that your Twitter developer account has the appropriate access level. Some API endpoints or features may require elevated access or specific app permissions.\n",
    "\n",
    "3. Use the Correct Tweepy Version\n",
    "Ensure you’re using the appropriate version of Tweepy that matches the Twitter API version you are working with. For API v2, you might need to use Tweepy’s v2-specific classes.\n",
    "\n",
    "4. Update Your Code for API v2\n",
    "For Twitter API v2, the track parameter is not supported in tweepy.StreamingClient. Instead, use add_rules to define your filtering rules.\n",
    "\n",
    "Here’s an example of how to properly set up streaming with the Bearer Token for API v2:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import tweepy\n",
    "\n",
    "# Store credentials in relevant variables\n",
    "bearer_token = \"YOUR_BEARER_TOKEN\"\n",
    "\n",
    "class MyStreamListener(tweepy.StreamingClient):\n",
    "    def on_tweet(self, tweet):\n",
    "        # Handle the incoming tweet\n",
    "        print(f\"Tweet from @{tweet.author_id}: {tweet.text}\")\n",
    "    \n",
    "    def on_error(self, status_code):\n",
    "        if status_code == 420:\n",
    "            # Returning False disconnects the stream\n",
    "            return False\n",
    "\n",
    "# Create your Stream object with credentials\n",
    "stream = MyStreamListener(bearer_token=bearer_token)\n",
    "\n",
    "# Define the rules for filtering\n",
    "rules = [\n",
    "    {'value': 'clinton OR trump OR sanders OR cruz'}\n",
    "]\n",
    "\n",
    "# Add the rules to the stream\n",
    "for rule in rules:\n",
    "    stream.add_rules(tweepy.StreamRule(rule['value']))\n",
    "\n",
    "# Start streaming\n",
    "stream.filter()\n",
    "Key Points:\n",
    "Bearer Token: Use the Bearer Token for API v2 authentication. Replace \"YOUR_BEARER_TOKEN\" with your actual Bearer Token.\n",
    "\n",
    "Define Rules: Use add_rules to add filtering rules. You can use logical operators like OR to filter by multiple keywords.\n",
    "\n",
    "Streaming: The filter method starts streaming based on the rules you have defined.\n",
    "\n",
    "Next Steps:\n",
    "Check Documentation: Review the Tweepy documentation and Twitter API v2 documentation for specific details on how to use the API and handle authentication.\n",
    "Ensure Valid Credentials: Double-check that your credentials are correct and that your app has the necessary permissions.\n",
    "By following these steps, you should be able to resolve the 401 Unauthorized error and properly stream tweets based on your specified keywords.\n",
    "\n",
    "Did you call tweepy.Stream()?\n",
    "ChatGPT\n",
    "In the context of Tweepy v2, the tweepy.Stream method is not used. Instead, you work with tweepy.StreamingClient for the Twitter API v2 to manage streaming.\n",
    "\n",
    "Here’s a clear breakdown of the correct approach for streaming tweets with Tweepy v2:\n",
    "\n",
    "For API v1.1: Use tweepy.Stream. This approach requires API keys and tokens for authentication.\n",
    "\n",
    "For API v2: Use tweepy.StreamingClient. This approach requires a Bearer Token for authentication.\n",
    "\n",
    "Example for API v1.1\n",
    "If you're using Twitter API v1.1, here’s how you would use tweepy.Stream:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import tweepy\n",
    "\n",
    "# Store credentials in relevant variables\n",
    "consumer_key = \"your_consumer_key\"\n",
    "consumer_secret = \"your_consumer_secret\"\n",
    "access_token = \"your_access_token\"\n",
    "access_token_secret = \"your_access_token_secret\"\n",
    "\n",
    "# Authenticate with the Twitter API\n",
    "auth = tweepy.OAuth1UserHandler(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "\n",
    "# Create a StreamListener class to handle incoming tweets\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    def on_status(self, status):\n",
    "        # Handle the incoming tweet\n",
    "        print(f\"Tweet from @{status.user.screen_name}: {status.text}\")\n",
    "    \n",
    "    def on_error(self, status_code):\n",
    "        if status_code == 420:\n",
    "            # Returning False disconnects the stream\n",
    "            return False\n",
    "\n",
    "# Create your Stream object with credentials\n",
    "stream = tweepy.Stream(auth=auth, listener=MyStreamListener())\n",
    "\n",
    "# Filter your Stream variable for the keywords\n",
    "keywords = [\"clinton\", \"trump\", \"sanders\", \"cruz\"]\n",
    "stream.filter(track=keywords)\n",
    "Example for API v2\n",
    "If you're using Twitter API v2, use tweepy.StreamingClient as follows:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import tweepy\n",
    "\n",
    "# Store Bearer Token\n",
    "bearer_token = \"your_bearer_token\"\n",
    "\n",
    "class MyStreamListener(tweepy.StreamingClient):\n",
    "    def on_tweet(self, tweet):\n",
    "        # Handle the incoming tweet\n",
    "        print(f\"Tweet from @{tweet.author_id}: {tweet.text}\")\n",
    "    \n",
    "    def on_error(self, status_code):\n",
    "        if status_code == 420:\n",
    "            # Returning False disconnects the stream\n",
    "            return False\n",
    "\n",
    "# Create your Stream object with credentials\n",
    "stream = MyStreamListener(bearer_token=bearer_token)\n",
    "\n",
    "# Define the rules for filtering\n",
    "rules = [\n",
    "    {'value': 'clinton OR trump OR sanders OR cruz'}\n",
    "]\n",
    "\n",
    "# Add the rules to the stream\n",
    "for rule in rules:\n",
    "    stream.add_rules(tweepy.StreamRule(rule['value']))\n",
    "\n",
    "# Start streaming\n",
    "stream.filter()\n",
    "Summary:\n",
    "For API v1.1: Use tweepy.Stream with OAuth1UserHandler for authentication.\n",
    "For API v2: Use tweepy.StreamingClient with Bearer Token authentication.\n",
    "Ensure that you are using the correct API version and class based on the credentials and authentication method you have.\n",
    "\n",
    "Hint\n",
    "The order that credentials should be passed to tweepy.Stream() is: consumer_key, consumer_secret, access_token, then access_token_secret.\n",
    "Assign the list with the keywords to the argument track in stream.filter().\n",
    "ChatGPT\n",
    "Memory updated\n",
    "Got it! If you're using Twitter API v1.1 and tweepy.Stream, you should provide the credentials in the order: consumer_key, consumer_secret, access_token, and access_token_secret. Here’s how you can set up and filter your stream correctly:\n",
    "\n",
    "Example Code for API v1.1\n",
    "python\n",
    "Copy code\n",
    "import tweepy\n",
    "\n",
    "# Store credentials in relevant variables\n",
    "consumer_key = \"nZ6EA0FxZ293SxGNg8g8aP0HM\"\n",
    "consumer_secret = \"fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i\"\n",
    "access_token = \"1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy\"\n",
    "access_token_secret = \"X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx\"\n",
    "\n",
    "# Authenticate with the Twitter API\n",
    "auth = tweepy.OAuth1UserHandler(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "\n",
    "# Create a StreamListener class to handle incoming tweets\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    def on_status(self, status):\n",
    "        # Handle the incoming tweet\n",
    "        print(f\"Tweet from @{status.user.screen_name}: {status.text}\")\n",
    "    \n",
    "    def on_error(self, status_code):\n",
    "        if status_code == 420:\n",
    "            # Returning False disconnects the stream\n",
    "            return False\n",
    "\n",
    "# Create your Stream object with credentials\n",
    "stream = tweepy.Stream(auth=auth, listener=MyStreamListener())\n",
    "\n",
    "# Filter your Stream variable for the keywords\n",
    "keywords = [\"clinton\", \"trump\", \"sanders\", \"cruz\"]\n",
    "stream.filter(track=keywords)\n",
    "Explanation:\n",
    "Authentication: Use tweepy.OAuth1UserHandler with your consumer key, consumer secret, access token, and access token secret.\n",
    "StreamListener Class: Define a custom StreamListener to handle tweets. Override on_status to process incoming tweets.\n",
    "Stream Object: Instantiate tweepy.Stream with the authentication and the StreamListener.\n",
    "Filter: Use stream.filter(track=keywords) to track tweets containing any of the specified keywords.\n",
    "This setup should allow you to stream tweets that match your keywords correctly.\n",
    "\n",
    "here is the solution# Store credentials in relevant variables\n",
    "consumer_key = \"nZ6EA0FxZ293SxGNg8g8aP0HM\"\n",
    "consumer_secret = \"fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i\"\n",
    "access_token = \"1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy\"\n",
    "access_token_secret = \"X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx\"\n",
    "\n",
    "# Create your Stream object with credentials\n",
    "stream = tweepy.Stream(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "\n",
    "# Filter your Stream variable\n",
    "stream.filter(track=[\"clinton\", \"trump\", \"sanders\", \"cruz\"])\n",
    "ChatGPT\n",
    "Great job! Your solution is spot on for using the Tweepy library with Twitter's API v1.1. You've correctly provided the necessary credentials to create the Stream object and filtered the stream for specific keywords.\n",
    "\n",
    "Here’s a summary of what your code does:\n",
    "\n",
    "Credentials: You've stored the necessary Twitter API credentials (consumer_key, consumer_secret, access_token, and access_token_secret) in variables.\n",
    "\n",
    "Stream Object: You created a Stream object by passing the credentials directly to the tweepy.Stream constructor.\n",
    "\n",
    "Filtering Tweets: You used the filter() method on your stream object to track tweets that mention any of the keywords: \"clinton\", \"trump\", \"sanders\", and \"cruz\".\n",
    "\n",
    "This code will continuously stream tweets that match the keywords and handle them according to how the StreamListener is set up. If you want to process these tweets, you can define a custom listener class that extends tweepy.StreamListener and override the on_status method.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tweets.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m tweets_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Open connection to file\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m tweets_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtweets_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Read in tweets and store in list: tweets_data\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m tweets_file:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tweets.txt'"
     ]
    }
   ],
   "source": [
    "# Import package\n",
    "import json\n",
    "\n",
    "# String of path to file: tweets_data_path\n",
    "tweets_data_path = 'tweets.txt'\n",
    "\n",
    "# Initialize empty list to store tweets: tweets_data\n",
    "tweets_data = []\n",
    "\n",
    "# Open connection to file\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "\n",
    "# Read in tweets and store in list: tweets_data\n",
    "for line in tweets_file:\n",
    "    tweet = json.loads(line)\n",
    "    tweets_data.append(tweet)\n",
    "\n",
    "# Close connection to file\n",
    "tweets_file.close()\n",
    "\n",
    "# Print the keys of the first tweet dict\n",
    "print(tweets_data[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and explore your Twitter data\n",
    "Now that you've got your Twitter data sitting locally in a text file, it's time to explore it! This is what you'll do in the next few interactive exercises. In this exercise, you'll read the Twitter data into a list: tweets_data.\n",
    "\n",
    "Be aware that this is real data from Twitter and as such there is always a risk that it may contain profanity or other offensive content (in this exercise, and any following exercises that also use real Twitter data).\n",
    "\n",
    "ChatGPT\n",
    "To load and explore your Twitter data, you can follow these steps:\n",
    "\n",
    "Step 1: Load the Twitter Data\n",
    "First, let's read the Twitter data from your text file into a list called tweets_data. Assuming your data is stored in a JSON format within the text file, each line would represent a tweet.\n",
    "\n",
    "Here's how you can do that in Python:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Open the file and read in the lines\n",
    "tweets_data_path = 'path_to_your_twitter_data_file.txt'\n",
    "\n",
    "# Initialize an empty list to store the tweets\n",
    "tweets_data = []\n",
    "\n",
    "# Open the file and read line by line\n",
    "with open(tweets_data_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Parse each line as JSON and append to the list\n",
    "        try:\n",
    "            tweet = json.loads(line)\n",
    "            tweets_data.append(tweet)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing line: {e}\")\n",
    "\n",
    "# Output the number of tweets\n",
    "print(f\"Number of tweets loaded: {len(tweets_data)}\")\n",
    "Step 2: Explore the Data\n",
    "Now that you have the data loaded, you can start exploring it. Here are some ways to do that:\n",
    "\n",
    "a. Preview the Data\n",
    "You can preview the first few tweets to get an idea of what the data looks like:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Preview the first 5 tweets\n",
    "for tweet in tweets_data[:5]:\n",
    "    print(json.dumps(tweet, indent=2))\n",
    "b. Check the Structure\n",
    "Since tweets are typically stored as dictionaries, you can check the keys available in each tweet:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Check the keys in the first tweet\n",
    "print(tweets_data[0].keys())\n",
    "c. Look at Specific Fields\n",
    "You might be interested in specific fields, such as the tweet text, user information, or timestamp:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Example: Extracting and printing the text of the first 5 tweets\n",
    "for tweet in tweets_data[:5]:\n",
    "    print(tweet.get('text'))\n",
    "Step 3: Analyze the Data\n",
    "Once you have a feel for the data, you can proceed with more detailed analysis, such as:\n",
    "\n",
    "Counting the number of tweets per user\n",
    "Analyzing the content of the tweets\n",
    "Checking the time distribution of tweets\n",
    "And more\n",
    "Would you like to proceed with a specific type of analysis or visualization?\n",
    "\n",
    "I need an explanation on every line of code\n",
    "# Import package\n",
    "import json\n",
    "\n",
    "# String of path to file: tweets_data_path\n",
    "tweets_data_path = 'tweets.txt'\n",
    "\n",
    "# Initialize empty list to store tweets: tweets_data\n",
    "tweets_data = []\n",
    "\n",
    "# Open connection to file\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "\n",
    "# Read in tweets and store in list: tweets_data\n",
    "for line in tweets_file:\n",
    "    tweet = json.loads(line)\n",
    "    tweets_data.append(tweet)\n",
    "\n",
    "# Close connection to file\n",
    "tweets_file.close()\n",
    "\n",
    "# Print the keys of the first tweet dict\n",
    "print(tweets_data[0].keys())\n",
    "ChatGPT\n",
    "Certainly! Below is an explanation of each line of code in the given script.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import package\n",
    "import json\n",
    "Explanation: This line imports the json module, which provides functions to work with JSON data (JavaScript Object Notation). JSON is a common format for exchanging data between a server and a client, especially in web applications.\n",
    "python\n",
    "Copy code\n",
    "# String of path to file: tweets_data_path\n",
    "tweets_data_path = 'tweets.txt'\n",
    "Explanation: This line defines a string variable tweets_data_path that stores the path to the file containing the Twitter data. The file is named 'tweets.txt'.\n",
    "python\n",
    "Copy code\n",
    "# Initialize empty list to store tweets: tweets_data\n",
    "tweets_data = []\n",
    "Explanation: This line initializes an empty list named tweets_data. This list will be used to store each tweet as it is read from the file.\n",
    "python\n",
    "Copy code\n",
    "# Open connection to file\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "Explanation: This line opens the file specified by tweets_data_path in read mode (\"r\"). The open() function returns a file object, which is stored in the variable tweets_file. This file object allows you to read the contents of the file line by line.\n",
    "python\n",
    "Copy code\n",
    "# Read in tweets and store in list: tweets_data\n",
    "for line in tweets_file:\n",
    "    tweet = json.loads(line)\n",
    "    tweets_data.append(tweet)\n",
    "Explanation:\n",
    "for line in tweets_file:: This line starts a loop that iterates over each line in the file tweets_file. Each line in the file is expected to be a JSON string representing a tweet.\n",
    "tweet = json.loads(line): This line uses the json.loads() function to parse the JSON string from the current line into a Python dictionary. The parsed dictionary is stored in the variable tweet.\n",
    "tweets_data.append(tweet): This line adds the parsed tweet dictionary to the tweets_data list. The append() method adds the item to the end of the list.\n",
    "python\n",
    "Copy code\n",
    "# Close connection to file\n",
    "tweets_file.close()\n",
    "Explanation: This line closes the file using the close() method. Closing the file is important because it frees up system resources that were being used to keep the file open.\n",
    "python\n",
    "Copy code\n",
    "# Print the keys of the first tweet dict\n",
    "print(tweets_data[0].keys())\n",
    "Explanation:\n",
    "tweets_data[0]: This accesses the first tweet in the tweets_data list (index 0).\n",
    ".keys(): This method returns a view object that displays a list of all the keys in the dictionary. In the context of a tweet, these keys might include fields like 'text', 'user', 'created_at', etc.\n",
    "print(): This function outputs the list of keys to the console. This helps you see what information is available in each tweet dictionary.\n",
    "Summary\n",
    "This script reads Twitter data from a file line by line, parses each line as a JSON object (representing a tweet), and stores each parsed tweet into a list. After reading all the tweets, it closes the file and prints the keys of the first tweet dictionary to give an overview of the data's structure.\n",
    "\n",
    "how do I check the content of tweet.txt?\n",
    "ChatGPT\n",
    "To check the content of tweet.txt, you can use a few different methods depending on your environment. Below are some options:\n",
    "\n",
    "1. Using a Text Editor\n",
    "You can open tweet.txt in any text editor like Notepad, VSCode, Sublime Text, or any other editor you prefer.\n",
    "\n",
    "Steps:\n",
    "Open the text editor.\n",
    "Navigate to the location of tweet.txt.\n",
    "Open the file to view its contents.\n",
    "2. Using the Command Line (Terminal or Command Prompt)\n",
    "If you're comfortable with the command line, you can use a command to display the content of the file.\n",
    "\n",
    "On Linux/MacOS:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "cat tweet.txt\n",
    "cat will display the entire content of the file.\n",
    "If the file is large and you want to view it page by page:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "less tweet.txt\n",
    "On Windows:\n",
    "\n",
    "cmd\n",
    "Copy code\n",
    "type tweet.txt\n",
    "type will display the content of the file.\n",
    "3. Using Python to Display Content\n",
    "You can use Python to read and print the contents of the file.\n",
    "\n",
    "Here’s a simple script to display the content of tweet.txt:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Define the path to your file\n",
    "tweets_data_path = 'tweet.txt'\n",
    "\n",
    "# Open and read the file\n",
    "with open(tweets_data_path, 'r') as file:\n",
    "    # Read all lines from the file\n",
    "    content = file.read()\n",
    "\n",
    "# Print the content\n",
    "print(content)\n",
    "4. Using Python to Read Line by Line\n",
    "If the file is large, you might want to read it line by line:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Define the path to your file\n",
    "tweets_data_path = 'tweet.txt'\n",
    "\n",
    "# Open the file and read line by line\n",
    "with open(tweets_data_path, 'r') as file:\n",
    "    for line in file:\n",
    "        print(line.strip())  # strip() removes any extra newlines or spaces\n",
    "5. Using Jupyter Notebook (if you are using it)\n",
    "You can also use a Jupyter Notebook cell to read and display the file content:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Read and display the content of tweet.txt\n",
    "with open('tweet.txt', 'r') as file:\n",
    "    print(file.read())\n",
    "These methods will allow you to view the content of tweet.txt and understand its structure and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [text, lang]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Import package\n",
    "import pandas as pd\n",
    "\n",
    "# Build DataFrame of tweet texts and languages\n",
    "df = pd.DataFrame(tweets_data, columns=['text', 'lang'])\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter data to DataFrame\n",
    "Now you have the Twitter data in a list of dictionaries, tweets_data, where each dictionary corresponds to a single tweet. Next, you're going to extract the text and language of each tweet. The text in a tweet, t1, is stored as the value t1['text']; similarly, the language is stored in t1['lang']. Your task is to build a DataFrame in which each row is a tweet and the columns are 'text' and 'lang'.\n",
    "\n",
    "ChatGPT\n",
    "To convert the tweets_data, which is a list of dictionaries, into a pandas DataFrame with 'text' and 'lang' columns, you can use the following approach:\n",
    "\n",
    "Extract the relevant fields: You'll need to iterate over each dictionary in tweets_data to extract the 'text' and 'lang' fields.\n",
    "\n",
    "Build the DataFrame: Use the extracted data to construct a DataFrame.\n",
    "\n",
    "Here’s how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assume tweets_data is your list of dictionaries containing tweets\n",
    "# Example structure of a single tweet dictionary\n",
    "# t1 = {'text': 'Some tweet text', 'lang': 'en'}\n",
    "\n",
    "# Extract 'text' and 'lang' from each tweet\n",
    "tweets_text = [tweet['text'] for tweet in tweets_data]\n",
    "tweets_lang = [tweet['lang'] for tweet in tweets_data]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text': tweets_text,\n",
    "    'lang': tweets_lang\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())\n",
    "Explanation:\n",
    "List comprehensions: [tweet['text'] for tweet in tweets_data] extracts the 'text' for each tweet, and similarly, [tweet['lang'] for tweet in tweets_data] extracts the 'lang'.\n",
    "DataFrame creation: pd.DataFrame constructs the DataFrame from the lists tweets_text and tweets_lang.\n",
    "This will give you a DataFrame where each row corresponds to a tweet, and the columns are 'text' and 'lang'.\n",
    "\n",
    "explain\n",
    "# Import package\n",
    "import pandas as pd\n",
    "\n",
    "# Build DataFrame of tweet texts and languages\n",
    "df = pd.DataFrame(tweets_data, columns=['text', 'lang'])\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n",
    "ChatGPT\n",
    "This code snippet efficiently converts a list of tweet dictionaries (tweets_data) into a pandas DataFrame, with only the relevant columns ('text' and 'lang') included. Here's a breakdown of how it works:\n",
    "\n",
    "Explanation:\n",
    "Importing the pandas package:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "pandas: A powerful library for data manipulation and analysis in Python. Here, it's being imported with the alias pd, which is a common convention.\n",
    "Building the DataFrame:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "df = pd.DataFrame(tweets_data, columns=['text', 'lang'])\n",
    "tweets_data: A list where each element is a dictionary representing a tweet. For example, one dictionary might look like this:\n",
    "python\n",
    "Copy code\n",
    "{'text': 'This is a tweet', 'lang': 'en', ...}\n",
    "pd.DataFrame(tweets_data): This function call converts the list of dictionaries into a DataFrame. By default, all keys in the dictionaries would become columns.\n",
    "columns=['text', 'lang']: This argument specifies that you only want to include the 'text' and 'lang' columns in the DataFrame. It filters out any other data in the dictionaries that you don't need.\n",
    "Printing the first few rows of the DataFrame:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(df.head())\n",
    "df.head(): This method returns the first five rows of the DataFrame, which is useful for quickly inspecting the contents.\n",
    "print(): This outputs the result of df.head() to the console, so you can see the top rows of the DataFrame.\n",
    "Summary:\n",
    "The code takes a list of dictionaries containing tweet data and extracts only the 'text' and 'lang' fields to create a pandas DataFrame.\n",
    "The resulting DataFrame (df) will have two columns: 'text' (containing the tweet content) and 'lang' (indicating the language of the tweet).\n",
    "The print(df.head()) statement allows you to preview the DataFrame to ensure it has been constructed correctly.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def word_in_text(word, text):\n",
    "    word = word.lower()\n",
    "    text = text.lower()\n",
    "    match = re.search(word, text)\n",
    "\n",
    "    if match:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinton: 0\n",
      "Trump: 0\n",
      "Sanders: 0\n",
      "Cruz: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize list to store tweet counts\n",
    "[clinton, trump, sanders, cruz] = [0, 0, 0, 0]\n",
    "\n",
    "# Iterate through df, counting the number of tweets in which\n",
    "# each candidate is mentioned\n",
    "for index, row in df.iterrows():\n",
    "    clinton += word_in_text('clinton', row['text'])\n",
    "    trump += word_in_text('trump', row['text'])\n",
    "    sanders += word_in_text('sanders', row['text'])\n",
    "    cruz += word_in_text('cruz', row['text'])\n",
    "\n",
    "# Print the counts\n",
    "print(f\"Clinton: {clinton}\")\n",
    "print(f\"Trump: {trump}\")\n",
    "print(f\"Sanders: {sanders}\")\n",
    "print(f\"Cruz: {cruz}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how do I display the content of my dataframe\n",
    "\n",
    "ChatGPT\n",
    "To display the content of your DataFrame in pandas, you can simply use the print() function or just type the name of the DataFrame in a Jupyter notebook or interactive Python shell.\n",
    "\n",
    "Here’s how you can do it:\n",
    "\n",
    "1. Using print():\n",
    "python\n",
    "Copy code\n",
    "print(your_dataframe)\n",
    "2. Just typing the DataFrame's name (in Jupyter Notebook):\n",
    "python\n",
    "Copy code\n",
    "your_dataframe\n",
    "3. Viewing only the first few rows (e.g., first 5 rows):\n",
    "python\n",
    "Copy code\n",
    "print(your_dataframe.head())\n",
    "4. Viewing the last few rows (e.g., last 5 rows):\n",
    "python\n",
    "Copy code\n",
    "print(your_dataframe.tail())\n",
    "This will display the contents of your DataFrame in the console or notebook. If your DataFrame is very large, pandas will automatically truncate the output, showing only the first and last few rows.\n",
    "\n",
    "A little bit of Twitter text analysis\n",
    "Now that you have your DataFrame of tweets set up, you're going to do a bit of text analysis to count how many tweets contain the words 'clinton', 'trump', 'sanders' and 'cruz'. In the pre-exercise code, we have defined the following function word_in_text(), which will tell you whether the first argument (a word) occurs within the 2nd argument (a tweet).\n",
    "\n",
    "import re\n",
    "\n",
    "def word_in_text(word, text):\n",
    "    word = word.lower()\n",
    "    text = text.lower()\n",
    "    match = re.search(word, text)\n",
    "\n",
    "    if match:\n",
    "        return True\n",
    "    return False\n",
    "ChatGPT\n",
    "To perform text analysis on your DataFrame of tweets and count how many tweets contain the words 'clinton', 'trump', 'sanders', and 'cruz', you can follow these steps:\n",
    "\n",
    "Create a new column in your DataFrame for each of the words you want to search for (e.g., 'clinton', 'trump', etc.). The column will contain True if the word is found in the tweet, otherwise False.\n",
    "\n",
    "Sum the values in each of these columns to get the total count of tweets that contain each word.\n",
    "\n",
    "Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Assume your DataFrame is called 'tweets_df' and it has a column 'text' containing the tweets.\n",
    "\n",
    "def word_in_text(word, text):\n",
    "    word = word.lower()\n",
    "    text = text.lower()\n",
    "    match = re.search(word, text)\n",
    "    \n",
    "    if match:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Add a column for each word\n",
    "tweets_df['clinton'] = tweets_df['text'].apply(lambda tweet: word_in_text('clinton', tweet))\n",
    "tweets_df['trump'] = tweets_df['text'].apply(lambda tweet: word_in_text('trump', tweet))\n",
    "tweets_df['sanders'] = tweets_df['text'].apply(lambda tweet: word_in_text('sanders', tweet))\n",
    "tweets_df['cruz'] = tweets_df['text'].apply(lambda tweet: word_in_text('cruz', tweet))\n",
    "\n",
    "# Count the number of occurrences for each word\n",
    "clinton_count = tweets_df['clinton'].sum()\n",
    "trump_count = tweets_df['trump'].sum()\n",
    "sanders_count = tweets_df['sanders'].sum()\n",
    "cruz_count = tweets_df['cruz'].sum()\n",
    "\n",
    "print(f\"Clinton: {clinton_count} tweets\")\n",
    "print(f\"Trump: {trump_count} tweets\")\n",
    "print(f\"Sanders: {sanders_count} tweets\")\n",
    "print(f\"Cruz: {cruz_count} tweets\")\n",
    "Explanation:\n",
    "The word_in_text function converts both the word and the tweet text to lowercase to ensure the search is case-insensitive.\n",
    "re.search() is used to search for the word within the text. If a match is found, it returns True; otherwise, it returns False.\n",
    "The .apply() method is used to apply the word_in_text function to each tweet in the 'text' column, creating new Boolean columns ('clinton', 'trump', 'sanders', 'cruz') that indicate whether the respective word is present.\n",
    "Finally, the .sum() method is used on each new column to count how many True values (i.e., how many tweets contain the word).\n",
    "This will give you the counts of tweets that mention each of the specified words.\n",
    "\n",
    "explain\n",
    "# Initialize list to store tweet counts\n",
    "[clinton, trump, sanders, cruz] = [0, 0, 0, 0]\n",
    "\n",
    "# Iterate through df, counting the number of tweets in which\n",
    "# each candidate is mentioned\n",
    "for index, row in df.iterrows():\n",
    "    clinton += word_in_text('clinton', row['text'])\n",
    "    trump += word_in_text('trump', row['text'])\n",
    "    sanders += word_in_text('sanders', row['text'])\n",
    "    cruz += word_in_text('cruz', row['text'])\n",
    "\n",
    "# Print the counts\n",
    "print(f\"Clinton: {clinton}\")\n",
    "print(f\"Trump: {trump}\")\n",
    "print(f\"Sanders: {sanders}\")\n",
    "print(f\"Cruz: {cruz}\")\n",
    "ChatGPT\n",
    "In this code, you're iterating through each tweet in a DataFrame (df) and counting how many times each candidate's name (Clinton, Trump, Sanders, and Cruz) appears in the tweets. Here's a breakdown of the steps:\n",
    "\n",
    "1. Initialize list to store tweet counts:\n",
    "python\n",
    "Copy code\n",
    "[clinton, trump, sanders, cruz] = [0, 0, 0, 0]\n",
    "This line initializes four variables (clinton, trump, sanders, cruz) and sets them all to 0.\n",
    "These variables will keep track of the number of tweets mentioning each candidate.\n",
    "2. Iterate through the DataFrame:\n",
    "python\n",
    "Copy code\n",
    "for index, row in df.iterrows():\n",
    "This loop iterates over each row of the DataFrame df.\n",
    "index gives the index of the current row, and row is a pandas Series object representing the row's data.\n",
    "3. Count mentions of each candidate:\n",
    "python\n",
    "Copy code\n",
    "    clinton += word_in_text('clinton', row['text'])\n",
    "    trump += word_in_text('trump', row['text'])\n",
    "    sanders += word_in_text('sanders', row['text'])\n",
    "    cruz += word_in_text('cruz', row['text'])\n",
    "For each row (tweet) in the DataFrame, the code checks if the candidate's name is mentioned in the tweet using the word_in_text function.\n",
    "word_in_text('clinton', row['text']) returns True (which is equivalent to 1 when added to an integer) if the word 'clinton' is found in the tweet text (row['text']), and False (equivalent to 0) if not.\n",
    "The counts for each candidate are incremented accordingly.\n",
    "4. Print the counts:\n",
    "python\n",
    "Copy code\n",
    "print(f\"Clinton: {clinton}\")\n",
    "print(f\"Trump: {trump}\")\n",
    "print(f\"Sanders: {sanders}\")\n",
    "print(f\"Cruz: {cruz}\")\n",
    "After the loop has processed all the tweets, the total counts for each candidate are printed.\n",
    "Summary:\n",
    "This code goes through each tweet in the DataFrame and checks for the presence of each candidate's name. It then counts how many tweets mention each candidate and prints out the results.\n",
    "\n",
    "This approach manually iterates through each row of the DataFrame and performs the text search using the word_in_text function. It's a straightforward method, but for larger datasets, vectorized operations (like the one I showed in the previous explanation) would be more efficient.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "# Create a list of labels: cd\n",
    "cd = ['clinton', 'trump', 'sanders', 'cruz']\n",
    "\n",
    "# List of the variables to plot\n",
    "counts = [clinton, trump, sanders, cruz]\n",
    "\n",
    "# Plot the bar chart\n",
    "ax = sns.barplot(x=cd, y=counts)\n",
    "ax.set(ylabel=\"count\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Plotting your Twitter data\n",
    "Now that you have the number of tweets that each candidate was mentioned in, you can plot a bar chart of this data. You'll use the statistical data visualization library seaborn, which you may not have seen before, but we'll guide you through. You'll first import seaborn as sns. You'll then construct a barplot of the data using sns.barplot, passing it two arguments:\n",
    "\n",
    "a list of labels and\n",
    "a list containing the variables you wish to plot (clinton, trump and so on.)\n",
    "Hopefully, you'll see that Trump was unreasonably represented! We have already run the previous exercise solutions in your environment.\n",
    "ChatGPT\n",
    "To plot the number of tweets each candidate was mentioned in using seaborn, follow these steps:\n",
    "\n",
    "Import seaborn: Import the seaborn library and alias it as sns.\n",
    "\n",
    "Prepare the data: You'll need a list of labels (candidate names) and a list of the corresponding variables (number of tweets mentioning each candidate).\n",
    "\n",
    "Create the barplot: Use sns.barplot() to create the plot, passing in the labels and the corresponding counts.\n",
    "\n",
    "Here’s a sample code snippet that demonstrates this:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Step 1: Import seaborn and matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Prepare the data\n",
    "labels = ['Clinton', 'Trump', 'Sanders', 'Cruz']  # Replace with actual candidates' names\n",
    "counts = [clinton_count, trump_count, sanders_count, cruz_count]  # Replace with actual counts\n",
    "\n",
    "# Step 3: Create the barplot\n",
    "sns.barplot(x=labels, y=counts)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Candidates')\n",
    "plt.ylabel('Number of Tweets Mentioned')\n",
    "plt.title('Number of Tweets Mentioned per Candidate')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "Explanation:\n",
    "sns.barplot(x=labels, y=counts): This function creates a barplot where x is the list of labels (candidates) and y is the list of counts (number of tweets).\n",
    "plt.show(): This function displays the plot.\n",
    "Make sure to replace clinton_count, trump_count, sanders_count, and cruz_count with the actual variables that store the number of tweets for each candidate.\n",
    "\n",
    "If you have already run the solutions in the environment, you can directly use those variables in the counts list.\n",
    "\n",
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "# Create a list of labels: cd\n",
    "cd = ['clinton', 'trump', 'sanders', 'cruz']\n",
    "\n",
    "# List of the variables to plot\n",
    "counts = [clinton, trump, sanders, cruz]\n",
    "\n",
    "# Plot the bar chart\n",
    "ax = sns.barplot(x=cd, y=counts)\n",
    "ax.set(ylabel=\"count\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "ChatGPT\n",
    "Your code looks great for plotting the number of tweets mentioning each candidate! Just a quick note: if the variable names (clinton, trump, sanders, cruz) are not yet defined in your environment, you'll need to ensure they contain the correct counts for each candidate.\n",
    "\n",
    "Here's a brief overview of your code:\n",
    "\n",
    "Import Packages: You've imported matplotlib.pyplot and seaborn.\n",
    "\n",
    "Set Seaborn Style: sns.set(color_codes=True) sets the color codes for the plot.\n",
    "\n",
    "Create Labels and Counts: You've created lists cd for candidate names and counts for the corresponding tweet counts.\n",
    "\n",
    "Plot the Bar Chart: sns.barplot(x=cd, y=counts) creates the bar chart. ax.set(ylabel=\"count\") sets the label for the y-axis.\n",
    "\n",
    "Show the Plot: plt.show() displays the plot.\n",
    "\n",
    "Just make sure that your variables clinton, trump, sanders, and cruz are defined with the correct tweet counts. If they are not defined yet, you will need to calculate or load them from your data before running this plot code.\n",
    "\n",
    "No module named 'seaborn'\n",
    "ChatGPT\n",
    "It looks like seaborn is not installed in your environment. You can install it using pip. Open your terminal or command prompt and run:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "pip install seaborn\n",
    "If you are using a Jupyter notebook, you can install seaborn directly within the notebook by running:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "!pip install seaborn\n",
    "After installing seaborn, you should be able to import it and use it as you intended in your code. If you encounter any issues or have further questions, let me know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url =  'https://www.python.org/~guido/'\n",
    "r = requests.get(url)\n",
    "d = r.text\n",
    "s = BeautifulSoup(d)\n",
    "print(type(r))\n",
    "print(type(s))\n",
    "print(type(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "url = 'https://www.python.org/~guido/'\n",
    "request = Request(url)\n",
    "response = urlopen(request)\n",
    "html = response.read()\n",
    "response.close()\n",
    "print(type(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pics.html\n",
      "pics.html\n",
      "http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.python.org/~guido/'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Find all anchor tags and print the href attribute of the first 3 links\n",
    "for link in soup.find_all('a')[:3]:\n",
    "    print(link.get('href'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NameError: name 'requests' is not defined error indicates that the requests library has not been imported in your code. You need to import it before using it to make HTTP requests.\n",
    "\n",
    "Here's how you can modify your code to include the necessary import:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.python.org/~guido/'  # Fixed the URL from 'wwww' to 'www'\n",
    "r = requests.get(url)\n",
    "d = r.text\n",
    "s = BeautifulSoup(d, 'html.parser')  # Specify the parser for BeautifulSoup\n",
    "Key Points:\n",
    "Import requests: Add import requests at the top of your script to use the requests.get function.\n",
    "Correct URL: Ensure the URL is correctly formatted. I fixed it from 'https://wwww.python.org/~guido/' to 'https://www.python.org/~guido/'.\n",
    "Specify Pars\n",
    "er: It's good practice to specify the parser when creating a BeautifulSoup object. Here, 'html.parser' is used, but you can use other parsers like 'lxml' if you have them installed.\n",
    "If you don’t have the requests library installed, you can install it using pip:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "pip install requests\n",
    "If you need any more help or have additional questions, feel free to ask!\n",
    "\n",
    "NameError                                 Traceback (most recent call last)\n",
    "Cell In[9], line 5\n",
    "      3 r = requests.get(url)\n",
    "      4 d = r.text\n",
    "----> 5 s = BeautifulSoup(d)\n",
    "      6 print(type(r))\n",
    "\n",
    "NameError: name 'BeautifulSoup' is not defined\n",
    "ChatGPT\n",
    "It looks like BeautifulSoup was not imported correctly in your code. You need to import it from the bs4 library before using it. Here's how you can fix your code:\n",
    "\n",
    "Import BeautifulSoup: Ensure you import BeautifulSoup from bs4.\n",
    "\n",
    "Specify the Parser: When creating a BeautifulSoup object, you should specify the parser to use.\n",
    "\n",
    "Here’s the corrected code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.python.org/~guido/'\n",
    "r = requests.get(url)\n",
    "d = r.text\n",
    "s = BeautifulSoup(d, 'html.parser')  # Specify the parser\n",
    "\n",
    "# Print the type of the response to check if everything is working\n",
    "print(type(r))\n",
    "Key Points:\n",
    "Import BeautifulSoup: Add from bs4 import BeautifulSoup to your imports.\n",
    "Specify Parser: Use 'html.parser' or another parser of your choice when creating the BeautifulSoup object.\n",
    "If you don't have the beautifulsoup4 package installed, you can install it with pip:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "pip install beautifulsoup4\n",
    "After making these adjustments, your code should work without the NameError. If you have any more issues or questions, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like seaborn is not installed in your environment. You can install it using pip. Open your terminal or command prompt and run:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "pip install seaborn\n",
    "If you are using a Jupyter notebook, you can install seaborn directly within the notebook by running:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "!pip install seaborn\n",
    "After installing seaborn, you should be able to import it and use it as you intended in your code. If you encounter any issues or have further questions, let me know!\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "NameError                                 Traceback (most recent call last)\n",
    "Cell In[7], line 2\n",
    "      1 url =  'https://wwww.python.org/~guido/'\n",
    "----> 2 r = requests.get(url)\n",
    "      3 d = r.text\n",
    "      4 s = BeautifulSoup(d)\n",
    "\n",
    "NameError: name 'requests' is not defined\n",
    "ChatGPT\n",
    "The NameError: name 'requests' is not defined error indicates that the requests library has not been imported in your code. You need to import it before using it to make HTTP requests.\n",
    "\n",
    "Here's how you can modify your code to include the necessary import:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.python.org/~guido/'  # Fixed the URL from 'wwww' to 'www'\n",
    "r = requests.get(url)\n",
    "d = r.text\n",
    "s = BeautifulSoup(d, 'html.parser')  # Specify the parser for BeautifulSoup\n",
    "Key Points:\n",
    "Import requests: Add import requests at the top of your script to use the requests.get function.\n",
    "Correct URL: Ensure the URL is correctly formatted. I fixed it from 'https://wwww.python.org/~guido/' to 'https://www.python.org/~guido/'.\n",
    "Specify Parser: It's good practice to specify the parser when creating a BeautifulSoup object. Here, 'html.parser' is used, but you can use other parsers like 'lxml' if you have them installed.\n",
    "If you don’t have the requests library installed, you can install it using pip:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "pip install requests\n",
    "If you need any more help or have additional questions, feel free to ask!\n",
    "\n",
    "NameError                                 Traceback (most recent call last)\n",
    "Cell In[9], line 5\n",
    "      3 r = requests.get(url)\n",
    "      4 d = r.text\n",
    "----> 5 s = BeautifulSoup(d)\n",
    "      6 print(type(r))\n",
    "\n",
    "NameError: name 'BeautifulSoup' is not defined\n",
    "ChatGPT\n",
    "It looks like BeautifulSoup was not imported correctly in your code. You need to import it from the bs4 library before using it. Here's how you can fix your code:\n",
    "\n",
    "Import BeautifulSoup: Ensure you import BeautifulSoup from bs4.\n",
    "\n",
    "Specify the Parser: When creating a BeautifulSoup object, you should specify the parser to use.\n",
    "\n",
    "Here’s the corrected code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.python.org/~guido/'\n",
    "r = requests.get(url)\n",
    "d = r.text\n",
    "s = BeautifulSoup(d, 'html.parser')  # Specify the parser\n",
    "\n",
    "# Print the type of the response to check if everything is working\n",
    "print(type(r))\n",
    "Key Points:\n",
    "Import BeautifulSoup: Add from bs4 import BeautifulSoup to your imports.\n",
    "Specify Parser: Use 'html.parser' or another parser of your choice when creating the BeautifulSoup object.\n",
    "If you don't have the beautifulsoup4 package installed, you can install it with pip:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "pip install beautifulsoup4\n",
    "After making these adjustments, your code should work without the NameError. If you have any more issues or questions, feel free to ask!\n",
    "\n",
    "NameError                                 Traceback (most recent call last)\n",
    "Cell In[11], line 2\n",
    "      1 import requests\n",
    "----> 2 request = Request(url)\n",
    "      3 response = urlopen(request)\n",
    "      4 html = response.read()\n",
    "\n",
    "NameError: name 'Request' is not defined\n",
    "ChatGPT\n",
    "It looks like you’re trying to use Request and urlopen from the urllib module but forgot to import them. If you're using requests, you should use requests.get() directly, as shown in your previous code. However, if you want to use urllib, you should import the appropriate functions.\n",
    "\n",
    "Here’s how you can fix your code using urllib:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "url = 'https://www.python.org/~guido/'\n",
    "request = Request(url)\n",
    "response = urlopen(request)\n",
    "html = response.read().decode('utf-8')  # Decode bytes to string\n",
    "\n",
    "# Print the type of the response to check if everything is working\n",
    "print(type(html))\n",
    "Key Points:\n",
    "Import Request and urlopen: Use from urllib.request import Request, urlopen to import these functions.\n",
    "Decode Bytes: response.read() returns bytes, so you should decode it to a string using .decode('utf-8').\n",
    "If you prefer using requests, the original approach you had with requests.get(url) is simpler and often more convenient. Just ensure you are not mixing requests with urllib functions. If you have requests installed, stick with that:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.python.org/~guido/'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Print the type of the response to check if everything is working\n",
    "print(type(response))\n",
    "If you have any more questions or need further assistance, feel free to ask!\n",
    "\n",
    "Complete \n",
    "the \n",
    "code \n",
    "to \n",
    "return \n",
    "the \n",
    "output \n",
    "for link in s.find_all(\n",
    "?\n",
    ")[:3]:\n",
    "    print(link.get('href'))\n",
    "ChatGPT\n",
    "To complete the code to print the first three hyperlinks found on the webpage, you should use s.find_all('a'). This finds all the anchor (<a>) tags in the HTML, which are typically used for hyperlinks. Here’s how you can complete your code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.python.org/~guido/'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Find all anchor tags and print the href attribute of the first 3 links\n",
    "for link in soup.find_all('a')[:3]:\n",
    "    print(link.get('href'))\n",
    "Explanation:\n",
    "soup.find_all('a'): Finds all <a> tags in the HTML.\n",
    "[:3]: Slices the list to get the first three links.\n",
    "link.get('href'): Retrieves the value of the href attribute, which contains the URL of the link.\n",
    "This code will print the href attributes of the first three anchor tags on the page. If you have any more questions or need further help, just let me know!\n",
    "\n",
    "\n",
    "2/2\n",
    "\n",
    "url = ('http://www.omdbapi.com/\n",
    "?\n",
    "72bc447a&t=\n",
    "?\n",
    "')\n",
    "r = requests.get(url)\n",
    "json_data = r.json()\n",
    "print(json_data['Awards'])\n",
    "ChatGPT\n",
    "It looks like you want to retrieve data from the OMDB API, but there are a few issues with the URL formatting and parameters. Here’s a revised version of your code that properly formats the URL and handles the API request:\n",
    "\n",
    "Fix the URL: Ensure that you have the correct API endpoint and parameters, including your API key and the query parameters for the movie title or ID.\n",
    "\n",
    "Replace ?: The ? should be replaced with & to separate multiple query parameters.\n",
    "\n",
    "Here's an example of how you might structure your code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import requests\n",
    "\n",
    "# Define the API key and movie title (or IMDb ID)\n",
    "api_key = '72bc447a'  # Replace with your actual API key\n",
    "movie_title = 'Inception'  # Replace with the movie title you want to query\n",
    "\n",
    "# Construct the URL\n",
    "url = f'http://www.omdbapi.com/?apikey={api_key}&t={movie_title}'\n",
    "\n",
    "# Make the request\n",
    "r = requests.get(url)\n",
    "\n",
    "# Parse the JSON data\n",
    "json_data = r.json()\n",
    "\n",
    "# Print the 'Awards' field\n",
    "print(json_data.get('Awards', 'Awards field not found'))\n",
    "Key Points:\n",
    "API Key: Replace '72bc447a' with your actual OMDB API key.\n",
    "Movie Title: Replace 'Inception' with the movie title you want to search for. You can also use an IMDb ID if you have one.\n",
    "Error Handling: Use json_data.get('Awards', 'Awards field not found') to avoid errors if the 'Awards' field is not present in the response.\n",
    "This code constructs the URL correctly, makes the request to the OMDB API, and retrieves the 'Awards' field from the JSON response. If you have any more questions or need further adjustments, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete \n",
    "the \n",
    "code \n",
    "to \n",
    "return \n",
    "the \n",
    "output \n",
    "x = [1, 'A', 2, 'B', 3, 'C']\n",
    "strings = [\n",
    "?\n",
    " \n",
    "?\n",
    " \n",
    "?\n",
    " \n",
    "?\n",
    "]\n",
    "print(strings)\n",
    "['A', 'B', 'C']\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "You can use a list comprehension to filter out the string elements from the list x. Here's the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "x = [1, 'A', 2, 'B', 3, 'C']\n",
    "strings = [item for item in x if isinstance(item, str)]\n",
    "print(strings)\n",
    "This will return the output:\n",
    "\n",
    "css\n",
    "Copy code\n",
    "['A', 'B', 'C']\n",
    "You said:\n",
    "Build \n",
    "a \n",
    "generator \n",
    "function \n",
    "that \n",
    "would \n",
    "convert \n",
    "this \n",
    "list \n",
    "into \n",
    "uppercase: \n",
    "superhero_list = ['thor', 'hulk']\n",
    ". \n",
    "Complete \n",
    "the \n",
    "code \n",
    "to \n",
    "return \n",
    "the \n",
    "output \n",
    "write some code\n",
    "    for i in heroes:\n",
    "        \n",
    "write some code\n",
    " \n",
    "write some code\n",
    "print(list(to_upper(superhero_list)))\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "You can build a generator function to convert the elements of superhero_list to uppercase. Here's the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "def to_upper(heroes):\n",
    "    for i in heroes:\n",
    "        yield i.upper()\n",
    "\n",
    "superhero_list = ['thor', 'hulk']\n",
    "print(list(to_upper(superhero_list)))\n",
    "This will return the output:\n",
    "\n",
    "css\n",
    "Copy code\n",
    "['THOR', 'HULK']\n",
    "You said:\n",
    "Complete \n",
    "the \n",
    "code \n",
    "to \n",
    "return \n",
    "the \n",
    "output \n",
    "flash_vil = ['Mick Rory', 'Leonard Snart', 'Gorilla Grodd']\n",
    "for x, y in \n",
    "write some code\n",
    "    print(\n",
    "write some code\n",
    "(x) + ': ' + y)\n",
    "Try Code\n",
    "3 attempts left\n",
    "Required Output\n",
    "0: Mick Rory\n",
    "1: Leonard Snart\n",
    "2: Gorilla Grodd\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "You can use the enumerate() function to achieve the required output. Here's the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "flash_vil = ['Mick Rory', 'Leonard Snart', 'Gorilla Grodd']\n",
    "for x, y in enumerate(flash_vil):\n",
    "    print(str(x) + ': ' + y)\n",
    "This will return the output:\n",
    "\n",
    "makefile\n",
    "Copy code\n",
    "0: Mick Rory\n",
    "1: Leonard Snart\n",
    "2: Gorilla Grodd\n",
    "You said:\n",
    "\n",
    "Following \n",
    "is \n",
    "a \n",
    "preview \n",
    "of \n",
    "the \n",
    "lists \n",
    "code_names\n",
    "and \n",
    "characters_names\n",
    ": \n",
    "Complete \n",
    "the \n",
    "code \n",
    "to \n",
    "return \n",
    "the \n",
    "output \n",
    "team_arrow = \n",
    "?\n",
    "(code_names, characters_names)\n",
    "cnames, names = \n",
    "?\n",
    "(\n",
    "?\n",
    ")\n",
    "print(names)\n",
    "('John Diggle', 'Oliver Queen')\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "You can use the zip() function to pair the elements of code_names and characters_names lists together. Here's how you can complete the code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "code_names = ['Spartan', 'Green Arrow', 'Arsenal']\n",
    "characters_names = ['John Diggle', 'Oliver Queen', 'Roy Harper']\n",
    "\n",
    "team_arrow = zip(code_names, characters_names)\n",
    "cnames, names = zip(*team_arrow)\n",
    "\n",
    "print(names)\n",
    "This will return the output:\n",
    "\n",
    "arduino\n",
    "Copy code\n",
    "('John Diggle', 'Oliver Queen', 'Roy Harper')\n",
    "To get just ('John Diggle', 'Oliver Queen'), you can slice the names tuple:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(names[:2])\n",
    "This will return:\n",
    "\n",
    "arduino\n",
    "Copy code\n",
    "('John Diggle', 'Oliver Queen')\n",
    "You said:\n",
    "what is the output \n",
    "t = [a,b,c,d,e]\n",
    "flash = iter(t)\n",
    "print(next(flash))\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "The code you've provided will attempt to print the first element of the list t using the next() function on the iterator flash.\n",
    "\n",
    "Here's the breakdown:\n",
    "\n",
    "t = [a, b, c, d, e] is a list containing variables a, b, c, d, and e.\n",
    "flash = iter(t) creates an iterator from the list t.\n",
    "print(next(flash)) retrieves the first element of the list (which is a) and prints it.\n",
    "However, since the variables a, b, c, d, and e are not defined in your code, you'll get a NameError unless these variables have been defined elsewhere in your code.\n",
    "\n",
    "If the variables a, b, c, d, and e were defined (e.g., as integers or strings), the output would be the value of a.\n",
    "\n",
    "You said:\n",
    "Following \n",
    "is \n",
    "a \n",
    "preview \n",
    "of \n",
    "the \n",
    "lists \n",
    "characters_names\n",
    "and \n",
    "code_names\n",
    ": \n",
    "Complete \n",
    "the \n",
    "code \n",
    "to \n",
    "return \n",
    "the \n",
    "output \n",
    "team_arrow = \n",
    "?\n",
    "(\n",
    "?\n",
    ", \n",
    "?\n",
    ")\n",
    "for i, j in \n",
    "?\n",
    ":\n",
    "    print(i + ' is The ' + j)\n",
    "Felicity Smoak is The Overwatch\n",
    "Laurel Lance is The Black Canary\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "To achieve the desired output, you can use the zip() function to pair the elements of characters_names and code_names lists. Here's how you can complete the code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "characters_names = ['Felicity Smoak', 'Laurel Lance']\n",
    "code_names = ['Overwatch', 'Black Canary']\n",
    "\n",
    "team_arrow = zip(characters_names, code_names)\n",
    "for i, j in team_arrow:\n",
    "    print(i + ' is The ' + j)\n",
    "This will return the output:\n",
    "\n",
    "csharp\n",
    "Copy code\n",
    "Felicity Smoak is The Overwatch\n",
    "Laurel Lance is The Black Canary\n",
    "You said:\n",
    "\n",
    "Following \n",
    "is \n",
    "a \n",
    "preview \n",
    "of \n",
    "the \n",
    "lists \n",
    "characters\n",
    "and \n",
    "names\n",
    ": \n",
    "Complete \n",
    "the \n",
    "code \n",
    "to \n",
    "return \n",
    "the \n",
    "output \n",
    "ave_1 = \n",
    "?\n",
    "(characters, names)\n",
    "avengers = \n",
    "?\n",
    "(ave_1)\n",
    "print(avengers)\n",
    "[('The Hulk', 'Bruce Banner'), ('Iron Man', 'Tony Stark')]\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "To achieve the desired output, you can use the zip() function to pair the elements of the characters and names lists, and then convert the resulting iterator to a list. Here's how you can complete the code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "characters = ['The Hulk', 'Iron Man']\n",
    "names = ['Bruce Banner', 'Tony Stark']\n",
    "\n",
    "ave_1 = zip(characters, names)\n",
    "avengers = list(ave_1)\n",
    "print(avengers)\n",
    "This will return the output:\n",
    "\n",
    "css\n",
    "Copy code\n",
    "[('The Hulk', 'Bruce Banner'), ('Iron Man', 'Tony Stark')]\n",
    "You said:\n",
    "intersect\n",
    "except\n",
    "union\n",
    "union all\n",
    "in sql context explain\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "In SQL, these terms refer to different set operations that can be performed on the results of two or more SELECT queries.\n",
    "\n",
    "1. INTERSECT\n",
    "Purpose: Returns the common rows from the results of two SELECT queries. Only rows that appear in both result sets will be included in the final output.\n",
    "Example:\n",
    "sql\n",
    "Copy code\n",
    "SELECT column_name FROM table1\n",
    "INTERSECT\n",
    "SELECT column_name FROM table2;\n",
    "This query will return the rows that are present in both table1 and table2.\n",
    "2. EXCEPT\n",
    "Purpose: Returns the rows from the first SELECT query that are not present in the second SELECT query. It essentially performs a subtraction of the second result set from the first.\n",
    "Example:\n",
    "sql\n",
    "Copy code\n",
    "SELECT column_name FROM table1\n",
    "EXCEPT\n",
    "SELECT column_name FROM table2;\n",
    "This query will return the rows that are present in table1 but not in table2.\n",
    "3. UNION\n",
    "Purpose: Combines the results of two or more SELECT queries into a single result set and removes duplicate rows. It merges the result sets and ensures that each row is unique.\n",
    "Example:\n",
    "sql\n",
    "Copy code\n",
    "SELECT column_name FROM table1\n",
    "UNION\n",
    "SELECT column_name FROM table2;\n",
    "This query will return a combined result set of unique rows from table1 and table2.\n",
    "4. UNION ALL\n",
    "Purpose: Similar to UNION, but it does not remove duplicates. It combines the results of two or more SELECT queries into a single result set, including all rows (even duplicates).\n",
    "Example:\n",
    "sql\n",
    "Copy code\n",
    "SELECT column_name FROM table1\n",
    "UNION ALL\n",
    "SELECT column_name FROM table2;\n",
    "This query will return all rows from both table1 and table2, including any duplicates.\n",
    "Summary of Key Differences:\n",
    "INTERSECT: Returns common rows between two queries.\n",
    "EXCEPT: Returns rows from the first query that are not in the second.\n",
    "UNION: Combines results from multiple queries into a unique set.\n",
    "UNION ALL: Combines results from multiple queries without removing duplicates.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.0\n"
     ]
    }
   ],
   "source": [
    "def money_off(discount, coupon):\n",
    "  \"\"\"Returns the final price paid after the discount (float < 0.5) and coupon (int < 40) have been applied.\"\"\"\n",
    "  full_price = 100\n",
    "  discount_diff = 1 - discount\n",
    "  discounted = full_price * discount_diff\n",
    "  final_price = discounted - coupon\n",
    "  print(final_price)\n",
    "  \n",
    "money_off(0.2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tag.prettify of \n",
       "<html>\n",
       "<head>\n",
       "<title>A website just for you at DataCamp</title>\n",
       "</head>\n",
       "<body>\n",
       "<p>This practice exercise is nice, isn't it.</p>\n",
       "</body>\n",
       "</html>\n",
       ">"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<title>A website just for you at DataCamp</title>\n",
    "</head>\n",
    "<body>\n",
    "<p>This practice exercise is nice, isn't it.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# This will return the bound method, not the result of the prettify method\n",
    "soup.prettify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   A website just for you at DataCamp\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p>\n",
      "   This practice exercise is nice, isn't it.\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This will actually call the prettify method and return the formatted HTML\n",
    "print(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Open the JSON file in read mode ('r') and load its content\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[0;32m      5\u001b[0m     json_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_file)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Now json_data contains the parsed JSON data\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open the JSON file in read mode ('r') and load its content\n",
    "with open('data.json', 'r') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# Now json_data contains the parsed JSON data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reviews table\n",
    "id\tfilm_id\tnum_user\tnum_critic\timdb_score\tnum_votes\tfacebook_likes\n",
    "1\t3934\t588\t432\t7.1\t203461\t46000\n",
    "2\t3405\t285\t267\t6.4\t149998\t0\n",
    "3\t478\t65\t29\t3.2\t8465\t491\n",
    "4\t74\t83\t25\t7.6\t7071\t930\n",
    "5\t1254\t1437\t224\t8\t241030\t13000\n",
    "6\t740\t111\t64\t6.4\t64742\t0\n",
    "7\t4841\t1058\t579\t8.1\t479047\t117000\n",
    "8\t2869\t59\t104\t6.8\t18442\t689\n",
    "9\t3252\t117\t160\t7.2\t49855\t10000\n",
    "10\t1181\t163\t99\t7.3\t16995\t0\n",
    "11\t2020\t402\t145\t6.7\t91092\t0\n",
    "12\t4152\t371\t359\t5.9\t108242\t39000\n",
    "13\t3220\t62\t64\t6.2\t15780\t2000\n",
    "14\t2312\t251\t161\t5.8\t63912\t912\n",
    "15\t1820\t113\t41\t7\t8535\t872\n",
    "16\t718\t85\t43\t6.4\t76850\t1000\n",
    "17\t831\t107\t65\t6.6\t40126\t975\n",
    "18\t1231\t226\t113\t6.5\t26034\t0\n",
    "19\t1746\t125\t71\t6.6\t17261\t455\n",
    "20\t3508\t43\t10\t5.3\t8598\t250\n",
    "21\t1621\t181\t93\t6\t44913\t3000\n",
    "22\t3197\t143\t128\t4.8\t33088\t0\n",
    "23\t4464\t5\t10\t4.8\t241\t261\n",
    "24\t3929\t7\t39\t7.1\t774\t590\n",
    "25\t2183\t18\t33\t4.5\t3119\t151\n",
    "26\t3298\t53\t83\t7.4\t6091\t0\n",
    "27\t2392\t156\t111\t5\t51252\t621\n",
    "28\t978\t41\t29\t6.1\t22748\t666\n",
    "29\t3176\t148\t222\t6.6\t55597\t4000\n",
    "30\t272\t60\t25\t6.1\t18140\t855\n",
    "31\t2414\t69\t87\t6.3\t10052\t0\n",
    "32\t2873\t65\t121\t5.9\t8867\t0\n",
    "33\t2744\t88\t117\t7.4\t46076\t11000\n",
    "34\t4013\t8\t13\t5.4\t133\t424\n",
    "35\t4707\t440\t465\t7.4\t232187\t47000\n",
    "36\t4592\t34\t59\t5.3\t5693\t0\n",
    "37\t2379\t90\t34\t6.6\t19336\t0\n",
    "38\t848\t57\t31\t6.7\t8531\t591\n",
    "39\t308\t134\t52\t6.9\t48629\t0\n",
    "40\t4210\t111\t217\t6.9\t31898\t0\n",
    "41\t1492\t377\t131\t6.6\t52244\t0\n",
    "42\t3879\t55\t162\t7.3\t15600\t0\n",
    "43\t4094\t150\t239\t6.8\t47954\t11000\n",
    "44\t3319\t120\t124\t5.4\t31191\t0\n",
    "45\t302\t152\t75\t6.2\t31260\t0\n",
    "46\t3854\t316\t481\t7.6\t148221\t51000\n",
    "47\t1522\t431\t142\t6.4\t200260\t0\n",
    "48\t1491\t119\t47\t3.6\t2855\t663\n",
    "49\t1526\t12\t15\t7.6\t341\t379\n",
    "50\t4501\t37\t129\t6.6\t8307\t5000\n",
    "51\t143\t64\t15\t7\t3454\t548\n",
    "52\t1048\t207\t78\t5.9\t6082\t853\n",
    "53\t1943\t866\t301\t8.2\t692482\t11000\n",
    "54\t877\t98\t43\t6.1\t13949\t453\n",
    "55\t4362\tnull\tnull\t4.3\t28\t17\n",
    "56\t2577\t306\t171\t5.8\t50170\t773\n",
    "57\t3760\t150\t206\t6.7\t46813\t12000\n",
    "58\t2331\t43\t38\t7.3\t6454\t0\n",
    "59\t1762\t10\t8\t5.8\t191\t7\n",
    "60\t3042\t109\t127\t4.9\t31640\t791\n",
    "61\t2226\t129\t32\t1.9\t25371\t0\n",
    "62\t2239\t4\t16\t7.4\t352\t44\n",
    "63\t4769\t110\t224\t5.8\t28429\t0\n",
    "64\t3016\t1\t5\t7.4\t59\t18\n",
    "65\t783\t283\t72\t5.8\t81783\t0\n",
    "66\t2676\t238\t173\t5.5\t32671\t0\n",
    "67\t2471\t214\t112\t7.1\t30210\t0\n",
    "68\t1379\t498\t169\t7.3\t135246\t0\n",
    "69\t4442\t343\t292\t7.9\t221128\t46000\n",
    "70\t2118\t1624\t287\t7.9\t361169\t18000\n",
    "71\t4922\t781\t275\t7.3\t111609\t89000\n",
    "72\t560\t148\t49\t6.6\t62037\t0\n",
    "73\t2949\t76\t121\t6.5\t16224\t830\n",
    "74\t4173\t79\t143\t5.2\t7384\t0\n",
    "75\t2576\t108\t94\t7.1\t28068\t0\n",
    "76\t1015\t28\t12\t5.8\t1686\t161\n",
    "77\t3209\t30\t8\t3.1\t851\t128\n",
    "78\t2094\t69\t50\t5.7\t10104\t265\n",
    "79\t788\t88\t56\t7.5\t11612\t0\n",
    "80\t1622\t123\t81\t6.2\t25572\t2000\n",
    "81\t578\t391\t196\t7.5\t240241\t0\n",
    "82\t368\t152\t84\t7.1\t71842\t0\n",
    "83\t2274\t148\t118\t6.2\t17373\t767\n",
    "84\t4516\t1\t6\t7.4\t70\t45\n",
    "85\t1835\t101\t64\t5.3\t14315\t201\n",
    "86\t1575\t102\t34\t6.2\t42644\t0\n",
    "87\t671\t31\t12\t5.9\t8904\t0\n",
    "88\t4714\t186\t256\t5.8\t39975\t24000\n",
    "89\t3014\t590\t318\t6.4\t343648\t0\n",
    "90\t1354\t433\t151\t7.7\t98354\t0\n",
    "91\t1351\t254\t125\t6\t81888\t0\n",
    "92\t2888\t103\t53\t5.3\t6267\t368\n",
    "93\t1572\t268\t135\t6.9\t30339\t0\n",
    "94\t4724\t4\t5\t5.9\t917\t224\n",
    "95\t269\t55\t24\t8.4\t9852\t0\n",
    "96\t613\t319\t81\t7.6\t181879\t0\n",
    "97\t730\t282\t63\t3.8\t53115\t0\n",
    "98\t1933\t185\t109\t5.8\t34942\t1000\n",
    "99\t703\t39\t25\t6.9\t6011\t905\n",
    "100\t3098\t274\t245\t7.5\t55842\t13000\n",
    "\n",
    "films table\n",
    "id\ttitle\trelease_year\tcountry\tduration\tlanguage\tcertification\tgross\tbudget\n",
    "1\tIntolerance: Love's Struggle Throughout the Ages\t1916\tUSA\t123\tnull\tNot Rated\tnull\t385907\n",
    "2\tOver the Hill to the Poorhouse\t1920\tUSA\t110\tnull\tnull\t3000000\t100000\n",
    "3\tThe Big Parade\t1925\tUSA\t151\tnull\tNot Rated\tnull\t245000\n",
    "4\tMetropolis\t1927\tGermany\t145\tGerman\tNot Rated\t26435\t6000000\n",
    "5\tPandora's Box\t1929\tGermany\t110\tGerman\tNot Rated\t9950\tnull\n",
    "6\tThe Broadway Melody\t1929\tUSA\t100\tEnglish\tPassed\t2808000\t379000\n",
    "7\tHell's Angels\t1930\tUSA\t96\tEnglish\tPassed\tnull\t3950000\n",
    "8\tA Farewell to Arms\t1932\tUSA\t79\tEnglish\tUnrated\tnull\t800000\n",
    "9\t42nd Street\t1933\tUSA\t89\tEnglish\tUnrated\t2300000\t439000\n",
    "10\tShe Done Him Wrong\t1933\tUSA\t66\tEnglish\tApproved\tnull\t200000\n",
    "11\tIt Happened One Night\t1934\tUSA\t65\tEnglish\tUnrated\tnull\t325000\n",
    "12\tTop Hat\t1935\tUSA\t81\tEnglish\tApproved\t3000000\t609000\n",
    "13\tModern Times\t1936\tUSA\t87\tEnglish\tG\t163245\t1500000\n",
    "14\tThe Charge of the Light Brigade\t1936\tUSA\t100\tEnglish\tApproved\tnull\t1200000\n",
    "15\tSnow White and the Seven Dwarfs\t1937\tUSA\t83\tEnglish\tApproved\t184925485\t2000000\n",
    "16\tThe Prisoner of Zenda\t1937\tUSA\t101\tEnglish\tApproved\tnull\tnull\n",
    "17\tAlexander's Ragtime Band\t1938\tUSA\t106\tEnglish\tApproved\tnull\t2000000\n",
    "18\tYou Can't Take It with You\t1938\tUSA\t126\tEnglish\tApproved\tnull\t1644736\n",
    "19\tGone with the Wind\t1939\tUSA\t226\tEnglish\tG\t198655278\t3977000\n",
    "20\tMr. Smith Goes to Washington\t1939\tUSA\t120\tEnglish\tNot Rated\tnull\t1500000\n",
    "21\tThe Wizard of Oz\t1939\tUSA\t102\tEnglish\tPassed\t22202612\t2800000\n",
    "22\tBoom Town\t1940\tUSA\t119\tEnglish\tPassed\tnull\t1614000\n",
    "23\tFantasia\t1940\tUSA\t120\tEnglish\tG\t76400000\t2280000\n",
    "24\tPinocchio\t1940\tUSA\t88\tEnglish\tApproved\t84300000\t2600000\n",
    "25\tRebecca\t1940\tUSA\t130\tEnglish\tNot Rated\tnull\t1288000\n",
    "26\tThe Blue Bird\t1940\tUSA\t83\tEnglish\tApproved\tnull\tnull\n",
    "27\tHow Green Was My Valley\t1941\tUSA\t118\tEnglish\tApproved\tnull\t1250000\n",
    "28\tBambi\t1942\tUSA\t70\tEnglish\tApproved\t102797150\tnull\n",
    "29\tCasablanca\t1942\tUSA\t82\tEnglish\tPG\tnull\t950000\n",
    "30\tA Guy Named Joe\t1943\tUSA\t122\tEnglish\tPassed\tnull\t2627000\n",
    "31\tBathing Beauty\t1944\tUSA\t101\tEnglish\tPassed\tnull\t2361000\n",
    "32\tSpellbound\t1945\tUSA\t95\tEnglish\tUnrated\tnull\t1696377\n",
    "33\tState Fair\t1945\tUSA\t100\tEnglish\tApproved\tnull\tnull\n",
    "34\tThe Lost Weekend\t1945\tUSA\t101\tEnglish\tNot Rated\tnull\t1250000\n",
    "35\tThe Valley of Decision\t1945\tUSA\t119\tEnglish\tPassed\tnull\t2160000\n",
    "36\tDuel in the Sun\t1946\tUSA\t144\tEnglish\tUnrated\t20400000\t8000000\n",
    "37\tIt's a Wonderful Life\t1946\tUSA\t118\tEnglish\tPG\tnull\t3180000\n",
    "38\tThe Best Years of Our Lives\t1946\tUSA\t172\tEnglish\tNot Rated\t23650000\t2100000\n",
    "39\tGentleman's Agreement\t1947\tUSA\t118\tEnglish\tApproved\tnull\t2000000\n",
    "40\tThe Lady from Shanghai\t1947\tUSA\t92\tEnglish\tNot Rated\t7927\t2300000\n",
    "41\tTycoon\t1947\tUSA\t95\tEnglish\tApproved\tnull\t3209000\n",
    "42\tOpen Secret\t1948\tUSA\t68\tEnglish\tApproved\tnull\tnull\n",
    "43\tRed River\t1948\tUSA\t126\tEnglish\tApproved\tnull\t3000000\n",
    "44\tThe Pirate\t1948\tUSA\t102\tEnglish\tApproved\t2956000\t3700000\n",
    "45\tSands of Iwo Jima\t1949\tUSA\t109\tEnglish\tApproved\tnull\t1000000\n",
    "46\tShe Wore a Yellow Ribbon\t1949\tUSA\t103\tEnglish\tUnrated\tnull\t1600000\n",
    "47\tAnnie Get Your Gun\t1950\tUSA\t107\tEnglish\tPassed\t8000000\t3768785\n",
    "48\tA Streetcar Named Desire\t1951\tUSA\t125\tEnglish\tPG\tnull\t1800000\n",
    "49\tQuo Vadis\t1951\tUSA\t171\tEnglish\tPassed\tnull\t7623000\n",
    "50\tShow Boat\t1951\tUSA\t108\tEnglish\tApproved\tnull\t2295429\n",
    "51\tDeadline - U.S.A.\t1952\tUSA\t87\tEnglish\tnull\tnull\tnull\n",
    "52\tHigh Noon\t1952\tUSA\t85\tEnglish\tPG\tnull\t750000\n",
    "53\tSingin' in the Rain\t1952\tUSA\t103\tEnglish\tApproved\tnull\t2540800\n",
    "54\tThe Greatest Show on Earth\t1952\tUSA\t152\tEnglish\tNot Rated\t36000000\t4000000\n",
    "55\tFrom Here to Eternity\t1953\tUSA\t118\tEnglish\tNot Rated\tnull\t1650000\n",
    "56\tNiagara\t1953\tUSA\t92\tEnglish\tApproved\tnull\t1250000\n",
    "57\tThe Beast from 20,000 Fathoms\t1953\tUSA\t80\tEnglish\tApproved\t5000000\t210000\n",
    "58\tThe Robe\t1953\tUSA\t135\tEnglish\tUnrated\t36000000\t5000000\n",
    "59\t20,000 Leagues Under the Sea\t1954\tUSA\t127\tEnglish\tApproved\tnull\t5000000\n",
    "60\t20,000 Leagues Under the Sea\t1954\tUSA\t127\tEnglish\tApproved\tnull\t5000000\n",
    "61\tOn the Waterfront\t1954\tUSA\t108\tEnglish\tNot Rated\t9600000\t910000\n",
    "62\tSeven Samurai\t1954\tJapan\t202\tJapanese\tUnrated\t269061\t2000000\n",
    "63\tThe Egyptian\t1954\tUSA\t139\tEnglish\tApproved\tnull\t5000000\n",
    "64\tOrdet\t1955\tDenmark\t126\tDanish\tNot Rated\tnull\tnull\n",
    "65\tThe Trouble with Harry\t1955\tUSA\t99\tEnglish\tApproved\tnull\t1200000\n",
    "66\tLove Me Tender\t1956\tUSA\t89\tEnglish\tnull\tnull\t1000000\n",
    "67\tMoby Dick\t1956\tUSA\t115\tEnglish\tUnrated\tnull\t4500000\n",
    "68\tTrapeze\t1956\tUSA\t105\tEnglish\tApproved\tnull\t4000000\n",
    "69\t12 Angry Men\t1957\tUSA\t96\tEnglish\tNot Rated\tnull\t350000\n",
    "70\tThe Bridge on the River Kwai\t1957\tUK\t161\tEnglish\tPG\t27200000\t3000000\n",
    "71\tCat on a Hot Tin Roof\t1958\tUSA\t108\tEnglish\tApproved\tnull\t3000000\n",
    "72\tSolomon and Sheba\t1959\tUSA\t141\tEnglish\tnull\tnull\t5000000\n",
    "73\tSome Like It Hot\t1959\tUSA\t120\tEnglish\tNot Rated\t25000000\t2883848\n",
    "74\tThe Nun's Story\t1959\tUSA\t149\tEnglish\tNot Rated\tnull\t3500000\n",
    "75\tElmer Gantry\t1960\tUSA\t146\tEnglish\tApproved\tnull\t3000000\n",
    "76\tPsycho\t1960\tUSA\t108\tEnglish\tR\t32000000\t806947\n",
    "77\tThe Apartment\t1960\tUSA\t125\tEnglish\tApproved\tnull\t3000000\n",
    "78\tJudgment at Nuremberg\t1961\tUSA\t186\tEnglish\tNot Rated\tnull\t3000000\n",
    "79\tPocketful of Miracles\t1961\tUSA\t136\tEnglish\tApproved\tnull\t2900000\n",
    "80\tThe Hustler\t1961\tUSA\t134\tEnglish\tUnrated\tnull\t2000000\n",
    "81\tThe Misfits\t1961\tUSA\t120\tEnglish\tApproved\tnull\t4000000\n",
    "82\tWest Side Story\t1961\tUSA\t152\tEnglish\tUnrated\t43650000\t6000000\n",
    "83\tDr. No\t1962\tUK\t110\tEnglish\tApproved\t16067035\t1100000\n",
    "84\tLawrence of Arabia\t1962\tUK\t227\tEnglish\tPG\t6000000\t15000000\n",
    "85\tLolita\t1962\tUK\t152\tEnglish\tNot Rated\tnull\t2000000\n",
    "86\tLolita\t1962\tUK\t152\tEnglish\tNot Rated\tnull\t2000000\n",
    "87\tThe Brain That Wouldn't Die\t1962\tUSA\t70\tEnglish\tApproved\tnull\t62000\n",
    "88\tThe Longest Day\t1962\tUSA\t178\tEnglish\tG\tnull\t10000000\n",
    "89\tThe Man Who Shot Liberty Valance\t1962\tUSA\t113\tEnglish\tApproved\tnull\t3200000\n",
    "90\tTo Kill a Mockingbird\t1962\tUSA\t129\tEnglish\tNot Rated\tnull\t2000000\n",
    "91\t55 Days at Peking\t1963\tUSA\t154\tEnglish\tUnrated\tnull\t9000000\n",
    "92\tCleopatra\t1963\tUK\t251\tEnglish\tApproved\t57750000\t31115000\n",
    "93\tDonovan's Reef\t1963\tUSA\t109\tEnglish\tNot Rated\tnull\t2686585\n",
    "94\tFrom Russia with Love\t1963\tUK\t115\tEnglish\tApproved\t24800000\t2000000\n",
    "95\tHud\t1963\tUSA\t112\tEnglish\tApproved\tnull\t2500000\n",
    "96\tIt's a Mad, Mad, Mad, Mad World\t1963\tUSA\t197\tEnglish\tApproved\t46300000\t9400000\n",
    "97\tThe Great Escape\t1963\tUSA\t172\tEnglish\tApproved\tnull\t4000000\n",
    "98\tTom Jones\t1963\tUK\t121\tEnglish\tUnrated\tnull\t1000000\n",
    "99\tA Fistful of Dollars\t1964\tItaly\t99\tItalian\tR\t3500000\t200000\n",
    "100\tA Hard Day's Night\t1964\tUK\t87\tEnglish\tApproved\t515005\t560000\n",
    "\n",
    "people\n",
    "id\tname\tbirthdate\tdeathdate\n",
    "1\t50 Cent\t1975-07-06\tnull\n",
    "2\tA. Michael Baldwin\t1963-04-04\tnull\n",
    "3\tA. Raven Cruz\tnull\tnull\n",
    "4\tA.J. Buckley\t1978-02-09\tnull\n",
    "5\tA.J. DeLucia\tnull\tnull\n",
    "6\tA.J. Langer\t1974-05-22\tnull\n",
    "7\tAaliyah\t1979-01-16\t2001-08-25\n",
    "8\tAaron Ashmore\t1979-10-07\tnull\n",
    "9\tAaron Hann\tnull\tnull\n",
    "10\tAaron Hill\t1983-04-23\tnull\n",
    "11\tAaron Hughes\tnull\tnull\n",
    "12\tAaron Kwok\t1965-10-26\tnull\n",
    "13\tAaron Schneider\tnull\tnull\n",
    "14\tAaron Seltzer\t1974-01-12\tnull\n",
    "15\tAaron Stanford\t1976-12-27\tnull\n",
    "16\tAaron Staton\tnull\tnull\n",
    "17\tAaron Yoo\t1979-05-12\tnull\n",
    "18\tAasheekaa Bathija\tnull\tnull\n",
    "19\tAasif Mandvi\t1966-03-05\tnull\n",
    "20\tAbbie Cornish\t1982-08-07\tnull\n",
    "21\tAbby Elliott\t1987-06-16\tnull\n",
    "22\tAbby Mukiibi Nkaaga\tnull\tnull\n",
    "23\tAbel Ferrara\t1951-07-19\tnull\n",
    "24\tAbhishek Bachchan\t1976-02-05\tnull\n",
    "25\tAbigail Evans\tnull\tnull\n",
    "26\tAbigail Spencer\t1981-08-04\tnull\n",
    "27\tAbraham Benrubi\t1969-10-04\tnull\n",
    "28\tAce Marrero\t1982-07-28\tnull\n",
    "29\tAdam Alexi-Malle\t1964-09-24\tnull\n",
    "30\tAdam Arkin\t1956-08-19\tnull\n",
    "31\tAdam Baldwin\t1962-02-27\tnull\n",
    "32\tAdam Boyer\tnull\tnull\n",
    "33\tAdam Brooks\t1956-09-03\tnull\n",
    "34\tAdam Brown\tnull\tnull\n",
    "35\tAdam Butcher\t1988-10-20\tnull\n",
    "36\tAdam Carolla\t1964-05-27\tnull\n",
    "37\tAdam Copeland\t1973-10-30\tnull\n",
    "38\tAdam DiMarco\t1990-04-14\tnull\n",
    "39\tAdam Garcia\t1973-06-01\tnull\n",
    "40\tAdam Goldberg\t1970-10-25\tnull\n",
    "41\tAdam Green\t1975-03-31\tnull\n",
    "42\tAdam Hicks\t1992-11-28\tnull\n",
    "43\tAdam Jay Epstein\tnull\tnull\n",
    "44\tAdam Lamberg\t1984-09-14\tnull\n",
    "45\tAdam LeFevre\t1950-08-11\tnull\n",
    "46\tAdam Marcus\tnull\tnull\n",
    "47\tAdam McKay\t1968-04-17\tnull\n",
    "48\tAdam Rapp\t1968-06-15\tnull\n",
    "49\tAdam Ratcliffe\tnull\tnull\n",
    "50\tAdam Rayner\tnull\tnull\n",
    "51\tAdam Rifkin\t1966-12-31\tnull\n",
    "52\tAdam Sandler\t1966-09-09\tnull\n",
    "53\tAdam Scott\t1973-04-03\tnull\n",
    "54\tAdam Shankman\t1964-11-27\tnull\n",
    "55\tAdam Trese\t1969-01-04\tnull\n",
    "56\tAdel Karam\tnull\tnull\n",
    "57\tAdelaide Kane\t1990-08-09\tnull\n",
    "58\tAden Young\tnull\tnull\n",
    "59\tAdhir Kalyan\t1983-08-04\tnull\n",
    "60\tAditya Roy Kapoor\t1985-11-16\tnull\n",
    "61\tAdolphe Menjou\t1890-02-18\t1963-10-29\n",
    "62\tAdrian Alonso\t1994-04-06\tnull\n",
    "63\tAdrian Gonzalez\t1938-05-08\t1998-10-23\n",
    "64\tAdrian Holmes\t1974-03-31\tnull\n",
    "65\tAdrian Lester\t1968-08-14\tnull\n",
    "66\tAdrian Lyne\t1941-03-04\tnull\n",
    "67\tAdrian Martinez\tnull\tnull\n",
    "68\tAdrian Pasdar\t1965-04-30\tnull\n",
    "69\tAdrian Paul\t1959-05-29\tnull\n",
    "70\tAdriana Barraza\t1956-03-05\tnull\n",
    "71\tAdriana Caselotti\t1916-05-16\t1997-01-19\n",
    "72\tAdriane Lenox\t1956-09-11\tnull\n",
    "73\tAdriano Giannini\t1971-05-10\tnull\n",
    "74\tAdrien Dorval\t1963-03-22\tnull\n",
    "75\tAdrienne Barbeau\t1945-06-11\tnull\n",
    "76\tAdrienne Frantz\t1978-06-07\tnull\n",
    "77\tAdrienne King\t1955-07-21\tnull\n",
    "78\tAdrienne Pickering\t1981-02-22\tnull\n",
    "79\tAdrienne Shelly\t1966-06-24\t2006-11-01\n",
    "80\tAgame Malembo-Emene\tnull\tnull\n",
    "81\tAgata Kulesza\t1971-09-27\tnull\n",
    "82\tAgata Trzebuchowska\tnull\tnull\n",
    "83\tAgnes Bruckner\t1985-08-16\tnull\n",
    "84\tAgnes Moorehead\t1900-12-06\t1974-04-30\n",
    "85\tAgnia Ditkovskite\t1988-05-11\tnull\n",
    "86\tAgnieszka Holland\t1948-11-28\tnull\n",
    "87\tAgnieszka Wojtowicz-Vosloo\tnull\tnull\n",
    "88\tAgustín Díaz Yanes\tnull\tnull\n",
    "89\tAh-sung Ko\t1992-08-10\tnull\n",
    "90\tAhmad Massad\tnull\tnull\n",
    "91\tAhmed Best\t1973-08-19\tnull\n",
    "92\tAhmed Hassan\tnull\tnull\n",
    "93\tAhmet Ertegun\t1923-07-31\t2006-12-14\n",
    "94\tAhna O'Reilly\t1984-09-21\tnull\n",
    "95\tAhney Her\tnull\tnull\n",
    "96\tAida Elkashef\tnull\tnull\n",
    "97\tAida Folch\t1986-11-24\tnull\n",
    "98\tAida Turturro\t1962-09-25\tnull\n",
    "99\tAidan McArdle\tnull\tnull\n",
    "100\tAidan Quinn\t1959-03-08\tnull\n",
    "\n",
    "reviews\n",
    "id\tfilm_id\tnum_user\tnum_critic\timdb_score\tnum_votes\tfacebook_likes\n",
    "1\t3934\t588\t432\t7.1\t203461\t46000\n",
    "2\t3405\t285\t267\t6.4\t149998\t0\n",
    "3\t478\t65\t29\t3.2\t8465\t491\n",
    "4\t74\t83\t25\t7.6\t7071\t930\n",
    "5\t1254\t1437\t224\t8\t241030\t13000\n",
    "6\t740\t111\t64\t6.4\t64742\t0\n",
    "7\t4841\t1058\t579\t8.1\t479047\t117000\n",
    "8\t2869\t59\t104\t6.8\t18442\t689\n",
    "9\t3252\t117\t160\t7.2\t49855\t10000\n",
    "10\t1181\t163\t99\t7.3\t16995\t0\n",
    "11\t2020\t402\t145\t6.7\t91092\t0\n",
    "12\t4152\t371\t359\t5.9\t108242\t39000\n",
    "13\t3220\t62\t64\t6.2\t15780\t2000\n",
    "14\t2312\t251\t161\t5.8\t63912\t912\n",
    "15\t1820\t113\t41\t7\t8535\t872\n",
    "16\t718\t85\t43\t6.4\t76850\t1000\n",
    "17\t831\t107\t65\t6.6\t40126\t975\n",
    "18\t1231\t226\t113\t6.5\t26034\t0\n",
    "19\t1746\t125\t71\t6.6\t17261\t455\n",
    "20\t3508\t43\t10\t5.3\t8598\t250\n",
    "21\t1621\t181\t93\t6\t44913\t3000\n",
    "22\t3197\t143\t128\t4.8\t33088\t0\n",
    "23\t4464\t5\t10\t4.8\t241\t261\n",
    "24\t3929\t7\t39\t7.1\t774\t590\n",
    "25\t2183\t18\t33\t4.5\t3119\t151\n",
    "26\t3298\t53\t83\t7.4\t6091\t0\n",
    "27\t2392\t156\t111\t5\t51252\t621\n",
    "28\t978\t41\t29\t6.1\t22748\t666\n",
    "29\t3176\t148\t222\t6.6\t55597\t4000\n",
    "30\t272\t60\t25\t6.1\t18140\t855\n",
    "31\t2414\t69\t87\t6.3\t10052\t0\n",
    "32\t2873\t65\t121\t5.9\t8867\t0\n",
    "33\t2744\t88\t117\t7.4\t46076\t11000\n",
    "34\t4013\t8\t13\t5.4\t133\t424\n",
    "35\t4707\t440\t465\t7.4\t232187\t47000\n",
    "36\t4592\t34\t59\t5.3\t5693\t0\n",
    "37\t2379\t90\t34\t6.6\t19336\t0\n",
    "38\t848\t57\t31\t6.7\t8531\t591\n",
    "39\t308\t134\t52\t6.9\t48629\t0\n",
    "40\t4210\t111\t217\t6.9\t31898\t0\n",
    "41\t1492\t377\t131\t6.6\t52244\t0\n",
    "42\t3879\t55\t162\t7.3\t15600\t0\n",
    "43\t4094\t150\t239\t6.8\t47954\t11000\n",
    "44\t3319\t120\t124\t5.4\t31191\t0\n",
    "45\t302\t152\t75\t6.2\t31260\t0\n",
    "46\t3854\t316\t481\t7.6\t148221\t51000\n",
    "47\t1522\t431\t142\t6.4\t200260\t0\n",
    "48\t1491\t119\t47\t3.6\t2855\t663\n",
    "49\t1526\t12\t15\t7.6\t341\t379\n",
    "50\t4501\t37\t129\t6.6\t8307\t5000\n",
    "51\t143\t64\t15\t7\t3454\t548\n",
    "52\t1048\t207\t78\t5.9\t6082\t853\n",
    "53\t1943\t866\t301\t8.2\t692482\t11000\n",
    "54\t877\t98\t43\t6.1\t13949\t453\n",
    "55\t4362\tnull\tnull\t4.3\t28\t17\n",
    "56\t2577\t306\t171\t5.8\t50170\t773\n",
    "57\t3760\t150\t206\t6.7\t46813\t12000\n",
    "58\t2331\t43\t38\t7.3\t6454\t0\n",
    "59\t1762\t10\t8\t5.8\t191\t7\n",
    "60\t3042\t109\t127\t4.9\t31640\t791\n",
    "61\t2226\t129\t32\t1.9\t25371\t0\n",
    "62\t2239\t4\t16\t7.4\t352\t44\n",
    "63\t4769\t110\t224\t5.8\t28429\t0\n",
    "64\t3016\t1\t5\t7.4\t59\t18\n",
    "65\t783\t283\t72\t5.8\t81783\t0\n",
    "66\t2676\t238\t173\t5.5\t32671\t0\n",
    "67\t2471\t214\t112\t7.1\t30210\t0\n",
    "68\t1379\t498\t169\t7.3\t135246\t0\n",
    "69\t4442\t343\t292\t7.9\t221128\t46000\n",
    "70\t2118\t1624\t287\t7.9\t361169\t18000\n",
    "71\t4922\t781\t275\t7.3\t111609\t89000\n",
    "72\t560\t148\t49\t6.6\t62037\t0\n",
    "73\t2949\t76\t121\t6.5\t16224\t830\n",
    "74\t4173\t79\t143\t5.2\t7384\t0\n",
    "75\t2576\t108\t94\t7.1\t28068\t0\n",
    "76\t1015\t28\t12\t5.8\t1686\t161\n",
    "77\t3209\t30\t8\t3.1\t851\t128\n",
    "78\t2094\t69\t50\t5.7\t10104\t265\n",
    "79\t788\t88\t56\t7.5\t11612\t0\n",
    "80\t1622\t123\t81\t6.2\t25572\t2000\n",
    "81\t578\t391\t196\t7.5\t240241\t0\n",
    "82\t368\t152\t84\t7.1\t71842\t0\n",
    "83\t2274\t148\t118\t6.2\t17373\t767\n",
    "84\t4516\t1\t6\t7.4\t70\t45\n",
    "85\t1835\t101\t64\t5.3\t14315\t201\n",
    "86\t1575\t102\t34\t6.2\t42644\t0\n",
    "87\t671\t31\t12\t5.9\t8904\t0\n",
    "88\t4714\t186\t256\t5.8\t39975\t24000\n",
    "89\t3014\t590\t318\t6.4\t343648\t0\n",
    "90\t1354\t433\t151\t7.7\t98354\t0\n",
    "91\t1351\t254\t125\t6\t81888\t0\n",
    "92\t2888\t103\t53\t5.3\t6267\t368\n",
    "93\t1572\t268\t135\t6.9\t30339\t0\n",
    "94\t4724\t4\t5\t5.9\t917\t224\n",
    "95\t269\t55\t24\t8.4\t9852\t0\n",
    "96\t613\t319\t81\t7.6\t181879\t0\n",
    "97\t730\t282\t63\t3.8\t53115\t0\n",
    "98\t1933\t185\t109\t5.8\t34942\t1000\n",
    "99\t703\t39\t25\t6.9\t6011\t905\n",
    "100\t3098\t274\t245\t7.5\t55842\t13000\n",
    "\n",
    "\n",
    "id\tfilm_id\tperson_id\trole\n",
    "1\t1\t1630\tdirector\n",
    "2\t1\t4843\tactor\n",
    "3\t1\t5050\tactor\n",
    "4\t1\t8175\tactor\n",
    "5\t2\t3000\tdirector\n",
    "6\t2\t4019\tactor\n",
    "7\t2\t5274\tactor\n",
    "8\t2\t7449\tactor\n",
    "9\t3\t1459\tactor\n",
    "10\t3\t3929\tactor\n",
    "11\t3\t4581\tdirector\n",
    "12\t3\t6580\tactor\n",
    "13\t4\t1008\tactor\n",
    "14\t4\t2649\tdirector\n",
    "15\t4\t2941\tactor\n",
    "16\t4\t6950\tactor\n",
    "17\t5\t2580\tactor\n",
    "18\t5\t2648\tactor\n",
    "19\t5\t2747\tdirector\n",
    "20\t5\t4958\tactor\n",
    "21\t6\t462\tactor\n",
    "22\t6\t753\tactor\n",
    "23\t6\t1234\tactor\n",
    "24\t6\t2996\tdirector\n",
    "25\t7\t3094\tdirector\n",
    "26\t7\t3344\tactor\n",
    "27\t7\t3519\tactor\n",
    "28\t7\t5146\tactor\n",
    "29\t8\t61\tactor\n",
    "30\t8\t2593\tdirector\n",
    "31\t8\t2693\tactor\n",
    "32\t8\t3038\tactor\n",
    "33\t9\t2070\tactor\n",
    "34\t9\t2751\tactor\n",
    "35\t9\t2838\tactor\n",
    "36\t9\t4906\tdirector\n",
    "37\t10\t2824\tactor\n",
    "38\t10\t4955\tactor\n",
    "39\t10\t4962\tdirector\n",
    "40\t10\t5051\tactor\n",
    "41\t11\t140\tactor\n",
    "42\t11\t1477\tactor\n",
    "43\t11\t2594\tdirector\n",
    "44\t11\t8170\tactor\n",
    "45\t12\t2262\tactor\n",
    "46\t12\t2385\tactor\n",
    "47\t12\t2838\tactor\n",
    "48\t12\t5215\tdirector\n",
    "49\t13\t1228\tdirector\n",
    "50\t13\t2626\tactor\n",
    "51\t13\t6244\tactor\n",
    "52\t13\t7415\tactor\n",
    "53\t14\t1901\tactor\n",
    "54\t14\t2451\tactor\n",
    "55\t14\t5512\tdirector\n",
    "56\t14\t7406\tactor\n",
    "57\t15\t71\tactor\n",
    "58\t15\t817\tactor\n",
    "59\t15\t4971\tactor\n",
    "60\t15\t8245\tdirector\n",
    "61\t16\t1901\tactor\n",
    "62\t16\t3902\tdirector\n",
    "63\t16\t5273\tactor\n",
    "64\t16\t6900\tactor\n",
    "65\t17\t2107\tactor\n",
    "66\t17\t3053\tdirector\n",
    "67\t17\t3895\tactor\n",
    "68\t17\t8028\tactor\n",
    "69\t18\t466\tactor\n",
    "70\t18\t2594\tdirector\n",
    "71\t18\t3517\tactor\n",
    "72\t18\t4874\tactor\n",
    "73\t19\t2779\tactor\n",
    "74\t19\t3012\tactor\n",
    "75\t19\t7762\tactor\n",
    "76\t19\t8101\tdirector\n",
    "77\t20\t1475\tactor\n",
    "78\t20\t2594\tdirector\n",
    "79\t20\t3517\tactor\n",
    "80\t20\t7762\tactor\n",
    "81\t21\t804\tactor\n",
    "82\t21\t5126\tactor\n",
    "83\t21\t7714\tactor\n",
    "84\t21\t8101\tdirector\n",
    "85\t22\t1477\tactor\n",
    "86\t22\t3035\tactor\n",
    "87\t22\t3226\tdirector\n",
    "88\t22\t7400\tactor\n",
    "89\t23\t1989\tactor\n",
    "90\t23\t3304\tdirector\n",
    "91\t23\t4797\tactor\n",
    "92\t24\t1495\tactor\n",
    "93\t24\t2073\tactor\n",
    "94\t24\t5429\tactor\n",
    "95\t24\t6004\tdirector\n",
    "96\t25\t246\tdirector\n",
    "97\t25\t2781\tactor\n",
    "98\t25\t3783\tactor\n",
    "99\t25\t4726\tactor\n",
    "100\t26\t2674\tactor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECT COUNT(film_id) AS count_film_id\n",
    "FROM reviews;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning to COUNT()\n",
    "You saw how to use COUNT() in the video. Do you remember what it returns?\n",
    "\n",
    "Here is a query counting film_id. Select the answer below that correctly describes what the query will return.\n",
    "\n",
    "SELECT COUNT(film_id) AS count_film_id\n",
    "FROM reviews;\n",
    "Run the query in the console to test your theory!\n",
    "\n",
    "Instructions\n",
    "50 XP\n",
    "Possible answers\n",
    "\n",
    "\n",
    "The number of unique films in the reviews table.\n",
    "\n",
    "ans: The number of records containing a film_id.\n",
    "\n",
    "The total number of records in the reviews table.\n",
    "\n",
    "The sum of the film_id field.\n",
    "\n",
    "Correct! COUNT(field_name) returns the number of records containing a value in a field. In this example, that field is film_id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practice with COUNT()\n",
    "As you've seen, COUNT(*) tells you how many records are in a table. However, if you want to count the number of non-missing values in a particular field, you can call COUNT() on just that field.\n",
    "\n",
    "Let's get some practice with COUNT()! You can look at the data in the tables throughout these exercises by clicking on the table name in the console.\n",
    "\n",
    "SELECT COUNT(*) AS count_records -- Count the number of records in the people table\n",
    "FROM people;\n",
    "\n",
    "\n",
    "SELECT COUNT(birthdate) AS count_birthdate -- Count the number of birthdates in the people table\n",
    "FROM people;\n",
    "\n",
    "\n",
    "-- Count the records for languages and countries represented in the films table\n",
    "SELECT\n",
    "COUNT(language) AS count_languages,\n",
    "COUNT(country) AS count_countries\n",
    "-- COUNT(DISTINCT birthdate) AS count_countries\n",
    "FROM \n",
    "films;\n",
    "\n",
    "\n",
    "Tres Bien! Looking at the differences between the count of unique values, total values, and all records can provide useful insights into your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECT DISTINCT\n",
    "Often query results will include many duplicate values. You can use the DISTINCT keyword to select the unique values from a field.\n",
    "\n",
    "This might be useful if, for example, you're interested in knowing which languages are represented in the films table. See if you can find out what countries are represented in this table with the following exercises.\n",
    "\n",
    "-- Return the unique countries from the films table\n",
    "SELECT DISTINCT country\n",
    "FROM films\n",
    "\n",
    "-- Count the distinct countries from the films table\n",
    "SELECT COUNT(DISTINCT country) as count_distinct_countries\n",
    "FROM films;\n",
    "\n",
    "Congratulations! Using DISTINCT is a great tool to see the unique values of a dataset. This table has 64 unique countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order of execution\n",
    "SQL code is processed differently than other programming languages in that you need to let the processor know where to pull the data from before making selections.\n",
    "\n",
    "It's essential to know your code's order of execution compared to the order it is written in to understand what results you'll get from your query and how to fix any errors that may come up.\n",
    "\n",
    "Debugging errors\n",
    "Debugging is an essential skill for all coders, and it comes from making many mistakes and learning from them.\n",
    "\n",
    "In this exercise, you'll be given some buggy code that you'll need to fix.\n",
    "\n",
    "-- Debug this code\n",
    "SELECT COUNT(birthdate) AS count_birthdays\n",
    "FROM people;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting\n",
    "Readable code is highly valued in the coding community and professional settings. Without proper formatting, code and results can be difficult to interpret. You'll often be working with other people that need to understand your code or be able to explain your results, so having a solid formatting habit is essential.\n",
    "\n",
    "In this exercise, you'll correct poorly written code to better adhere to SQL style standards.\n",
    "\n",
    "-- Rewrite this query\n",
    "-- select person_id, role from roles limit 10\n",
    "SELECT \n",
    "    person_id, \n",
    "    role\n",
    "FROM roles\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using WHERE with numbers\n",
    "Filtering with WHERE allows you to analyze your data better. You may have a dataset that includes a range of different movies, and you need to do a case study on the most notable films with the biggest budgets. In this case, you'll want to filter your data to a specific budget range.\n",
    "\n",
    "Now it's your turn to use the WHERE clause to filter numeric values!\n",
    "\n",
    "-- Select film_ids and imdb_score with an imdb_score over 7.0\n",
    "SELECT\n",
    "    film_id,\n",
    "    imdb_score\n",
    "FROM reviews\n",
    "WHERE imdb_score > 7.0\n",
    "\n",
    "\n",
    "-- Select film_ids and facebook_likes for ten records with less than 1000 likes \n",
    "\n",
    "SELECT \n",
    "    film_id,\n",
    "    facebook_likes\n",
    "FROM reviews\n",
    "WHERE facebook_likes  < 1000\n",
    "LIMIT 10\n",
    "\n",
    "\n",
    "-- Count the records with at least 100,000 votes\n",
    "SELECT COUNT(*) AS films_over_100K_votes\n",
    "FROM reviews\n",
    "WHERE num_votes >= 100000;\n",
    "\n",
    "\n",
    "Well done! Applying a WHERE filter with SQL is much easier and faster than scrolling through a spreadsheet or using a highlighter!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using WHERE with text\n",
    "WHERE can also filter string values.\n",
    "\n",
    "Imagine you are part of an organization that gives cinematography awards, and you have several international categories. Before you confirm an award for every language listed in your dataset, it may be worth seeing if there are enough films of a specific language to make it a fair competition. If there is only one movie or a significant skew, it may be worth considering a different way of giving international awards.\n",
    "\n",
    "Let's try this out!\n",
    "\n",
    "-- Count the Spanish-language films\n",
    "SELECT COUNT(language) AS count_spanish\n",
    "FROM films\n",
    "WHERE language = 'Spanish';\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using AND\n",
    "The following exercises combine AND and OR with the WHERE clause. Using these operators together strengthens your queries and analyses of data.\n",
    "\n",
    "You will apply these new skills now on the films table.\n",
    "\n",
    "-- Select the title and release_year for all German-language films released before 2000\n",
    "SELECT title, release_year\n",
    "FROM films\n",
    "WHERE language = 'German' AND release_year < 2000;\n",
    "\n",
    "\n",
    "-- Update the query to see all German-language films released after 2000\n",
    "SELECT title, release_year\n",
    "FROM films\n",
    "WHERE release_year > 2000\n",
    "\tAND language = 'German';\n",
    "\n",
    "\n",
    "-- Select all records for German-language films released after 2000 and before 2010\n",
    "SELECT *\n",
    "FROM films\n",
    "WHERE language = 'German' AND release_year > 2000 AND release_year < 2010;\n",
    "\n",
    "Great work! Combining conditions with AND will prove to be very useful when we want our query to return a specific subset of records.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using OR\n",
    "This time you'll write a query to get the title and release_year of films released in 1990 or 1999, which were in English or Spanish and took in more than $2,000,000 gross.\n",
    "\n",
    "It looks like a lot, but you can build the query up one step at a time to get comfortable with the underlying concept in each step. Let's go!\n",
    "\n",
    "-- Find the title and year of films from the 1990 or 1999\n",
    "SELECT title, release_year\n",
    "FROM films\n",
    "WHERE release_year = 1990 OR release_year = 1999\n",
    "\n",
    "SELECT title, release_year\n",
    "FROM films\n",
    "WHERE (release_year = 1990 OR release_year = 1999)\n",
    "  AND (language = 'English' OR language = 'Spanish');\n",
    "\n",
    "SELECT title, release_year\n",
    "FROM films\n",
    "WHERE (release_year = 1990 OR release_year = 1999)\n",
    "  AND (language = 'English' OR language = 'Spanish')\n",
    "  AND gross > 2000000;\n",
    "\n",
    "Amazing work! Some of these queries were pretty complex. Any Die Hard fans out there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using BETWEEN\n",
    "Let's use BETWEEN with AND on the films database to get the title and release_year of all Spanish-language films released between 1990 and 2000 (inclusive) with budgets over $100 million.\n",
    "\n",
    "We have broken the problem into smaller steps so that you can build the query as you go along!\n",
    "\n",
    "-- Select the title and release_year for films released between 1990 and 2000\n",
    "SELECT  title, release_year\n",
    "FROM films\n",
    "WHERE release_year \n",
    "BETWEEN 1990 AND 2000\n",
    "\n",
    "\n",
    "SELECT title, release_year\n",
    "FROM films\n",
    "WHERE release_year BETWEEN 1990 AND 2000\n",
    "-- Narrow down your query to films with budgets > $100 million\n",
    "AND budget > 100000000\t;\n",
    "\n",
    "SELECT title, release_year\n",
    "FROM films\n",
    "WHERE release_year BETWEEN 1990 AND 2000\n",
    "\tAND budget > 100000000\n",
    "-- Restrict the query to only Spanish-language films\n",
    "AND language = 'Spanish';\n",
    "\n",
    "SELECT title, release_year\n",
    "FROM films\n",
    "WHERE release_year BETWEEN 1990 AND 2000\n",
    "\tAND budget > 100000000\n",
    "-- Amend the query to include Spanish or French-language films\n",
    "AND (language = 'Spanish' OR language = 'French');\n",
    "\n",
    "Superb! Using WHERE with a combination of AND, OR, and BETWEEN is an efficient way to query a desired range of values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIKE and NOT LIKE\n",
    "The LIKE and NOT LIKE operators can be used to find records that either match or do not match a specified pattern, respectively. They can be coupled with the wildcards % and _. The % will match zero or many characters, and _ will match a single character.\n",
    "\n",
    "This is useful when you want to filter text, but not to an exact word.\n",
    "\n",
    "Do the following exercises to gain some practice with these keywords\n",
    "\n",
    "-- Select the names that start with B\n",
    "SELECT name\n",
    "FROM people\n",
    "WHERE name LIKE 'B%';\n",
    "\n",
    "\n",
    "SELECT name\n",
    "FROM people\n",
    "-- Select the names that have r as the second letter\n",
    "wHERE name LIKE '_r%'\n",
    "\n",
    "SELECT name\n",
    "FROM people\n",
    "-- Select names that don't start with A\n",
    "WHERE name NOT LIKE 'A%'\n",
    "\n",
    "I LIKE to see the progress we're making! Filtering your data to find specified patterns is vital to your skillset. Our results still had names that started with Á with an accent, showing that we need to be specific with our filtering criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHERE IN\n",
    "You now know you can query multiple conditions using the IN operator and a set of parentheses. It is a valuable piece of code that helps us keep our queries clean and concise.\n",
    "\n",
    "Try using the IN operator yourself!\n",
    "\n",
    "-- Find the title and release_year for all films over two hours in length released in 1990 and 2000\n",
    "\n",
    "SELECT title, release_year\n",
    "FROM films\n",
    "WHERE release_year IN (1990, 2000)\n",
    "\tAND duration > 120;\n",
    "\n",
    "\n",
    "-- Find the title and language of all films in English, Spanish, and French\n",
    "SELECT title, language\n",
    "FROM films\n",
    "WHERE language IN ('English', 'Spanish', 'French');\n",
    "\n",
    "\n",
    "-- Find the title, certification, and language all films certified NC-17 or R that are in English, Italian, or Greek\n",
    "SELECT title, certification, language\n",
    "FROM films\n",
    "WHERE certification IN ('NC-17', 'R')\n",
    "  AND language IN ('English', 'Italian', 'Greek');\n",
    "\n",
    "\n",
    "Your SQL vocabulary is growing by the minute! Interestingly, A Fistful of Dollars starring Clint Eastwood is listed as Italian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id\ttitle\trelease_year\tcountry\tduration\tlanguage\tcertification\tgross\tbudget\n",
    "1\tIntolerance: Love's Struggle Throughout the Ages\t1916\tUSA\t123\tnull\tNot Rated\tnull\t385907\n",
    "2\tOver the Hill to the Poorhouse\t1920\tUSA\t110\tnull\tnull\t3000000\t100000\n",
    "3\tThe Big Parade\t1925\tUSA\t151\tnull\tNot Rated\tnull\t245000\n",
    "4\tMetropolis\t1927\tGermany\t145\tGerman\tNot Rated\t26435\t6000000\n",
    "5\tPandora's Box\t1929\tGermany\t110\tGerman\tNot Rated\t9950\tnull\n",
    "6\tThe Broadway Melody\t1929\tUSA\t100\tEnglish\tPassed\t2808000\t379000\n",
    "7\tHell's Angels\t1930\tUSA\t96\tEnglish\tPassed\tnull\t3950000\n",
    "8\tA Farewell to Arms\t1932\tUSA\t79\tEnglish\tUnrated\tnull\t800000\n",
    "9\t42nd Street\t1933\tUSA\t89\tEnglish\tUnrated\t2300000\t439000\n",
    "10\tShe Done Him Wrong\t1933\tUSA\t66\tEnglish\tApproved\tnull\t200000\n",
    "11\tIt Happened One Night\t1934\tUSA\t65\tEnglish\tUnrated\tnull\t325000\n",
    "12\tTop Hat\t1935\tUSA\t81\tEnglish\tApproved\t3000000\t609000\n",
    "13\tModern Times\t1936\tUSA\t87\tEnglish\tG\t163245\t1500000\n",
    "14\tThe Charge of the Light Brigade\t1936\tUSA\t100\tEnglish\tApproved\tnull\t1200000\n",
    "15\tSnow White and the Seven Dwarfs\t1937\tUSA\t83\tEnglish\tApproved\t184925485\t2000000\n",
    "16\tThe Prisoner of Zenda\t1937\tUSA\t101\tEnglish\tApproved\tnull\tnull\n",
    "17\tAlexander's Ragtime Band\t1938\tUSA\t106\tEnglish\tApproved\tnull\t2000000\n",
    "18\tYou Can't Take It with You\t1938\tUSA\t126\tEnglish\tApproved\tnull\t1644736\n",
    "19\tGone with the Wind\t1939\tUSA\t226\tEnglish\tG\t198655278\t3977000\n",
    "20\tMr. Smith Goes to Washington\t1939\tUSA\t120\tEnglish\tNot Rated\tnull\t1500000\n",
    "21\tThe Wizard of Oz\t1939\tUSA\t102\tEnglish\tPassed\t22202612\t2800000\n",
    "22\tBoom Town\t1940\tUSA\t119\tEnglish\tPassed\tnull\t1614000\n",
    "23\tFantasia\t1940\tUSA\t120\tEnglish\tG\t76400000\t2280000\n",
    "24\tPinocchio\t1940\tUSA\t88\tEnglish\tApproved\t84300000\t2600000\n",
    "25\tRebecca\t1940\tUSA\t130\tEnglish\tNot Rated\tnull\t1288000\n",
    "26\tThe Blue Bird\t1940\tUSA\t83\tEnglish\tApproved\tnull\tnull\n",
    "27\tHow Green Was My Valley\t1941\tUSA\t118\tEnglish\tApproved\tnull\t1250000\n",
    "28\tBambi\t1942\tUSA\t70\tEnglish\tApproved\t102797150\tnull\n",
    "29\tCasablanca\t1942\tUSA\t82\tEnglish\tPG\tnull\t950000\n",
    "30\tA Guy Named Joe\t1943\tUSA\t122\tEnglish\tPassed\tnull\t2627000\n",
    "31\tBathing Beauty\t1944\tUSA\t101\tEnglish\tPassed\tnull\t2361000\n",
    "32\tSpellbound\t1945\tUSA\t95\tEnglish\tUnrated\tnull\t1696377\n",
    "33\tState Fair\t1945\tUSA\t100\tEnglish\tApproved\tnull\tnull\n",
    "34\tThe Lost Weekend\t1945\tUSA\t101\tEnglish\tNot Rated\tnull\t1250000\n",
    "35\tThe Valley of Decision\t1945\tUSA\t119\tEnglish\tPassed\tnull\t2160000\n",
    "36\tDuel in the Sun\t1946\tUSA\t144\tEnglish\tUnrated\t20400000\t8000000\n",
    "37\tIt's a Wonderful Life\t1946\tUSA\t118\tEnglish\tPG\tnull\t3180000\n",
    "38\tThe Best Years of Our Lives\t1946\tUSA\t172\tEnglish\tNot Rated\t23650000\t2100000\n",
    "39\tGentleman's Agreement\t1947\tUSA\t118\tEnglish\tApproved\tnull\t2000000\n",
    "40\tThe Lady from Shanghai\t1947\tUSA\t92\tEnglish\tNot Rated\t7927\t2300000\n",
    "41\tTycoon\t1947\tUSA\t95\tEnglish\tApproved\tnull\t3209000\n",
    "42\tOpen Secret\t1948\tUSA\t68\tEnglish\tApproved\tnull\tnull\n",
    "43\tRed River\t1948\tUSA\t126\tEnglish\tApproved\tnull\t3000000\n",
    "44\tThe Pirate\t1948\tUSA\t102\tEnglish\tApproved\t2956000\t3700000\n",
    "45\tSands of Iwo Jima\t1949\tUSA\t109\tEnglish\tApproved\tnull\t1000000\n",
    "46\tShe Wore a Yellow Ribbon\t1949\tUSA\t103\tEnglish\tUnrated\tnull\t1600000\n",
    "47\tAnnie Get Your Gun\t1950\tUSA\t107\tEnglish\tPassed\t8000000\t3768785\n",
    "48\tA Streetcar Named Desire\t1951\tUSA\t125\tEnglish\tPG\tnull\t1800000\n",
    "49\tQuo Vadis\t1951\tUSA\t171\tEnglish\tPassed\tnull\t7623000\n",
    "50\tShow Boat\t1951\tUSA\t108\tEnglish\tApproved\tnull\t2295429\n",
    "51\tDeadline - U.S.A.\t1952\tUSA\t87\tEnglish\tnull\tnull\tnull\n",
    "52\tHigh Noon\t1952\tUSA\t85\tEnglish\tPG\tnull\t750000\n",
    "53\tSingin' in the Rain\t1952\tUSA\t103\tEnglish\tApproved\tnull\t2540800\n",
    "54\tThe Greatest Show on Earth\t1952\tUSA\t152\tEnglish\tNot Rated\t36000000\t4000000\n",
    "55\tFrom Here to Eternity\t1953\tUSA\t118\tEnglish\tNot Rated\tnull\t1650000\n",
    "56\tNiagara\t1953\tUSA\t92\tEnglish\tApproved\tnull\t1250000\n",
    "57\tThe Beast from 20,000 Fathoms\t1953\tUSA\t80\tEnglish\tApproved\t5000000\t210000\n",
    "58\tThe Robe\t1953\tUSA\t135\tEnglish\tUnrated\t36000000\t5000000\n",
    "59\t20,000 Leagues Under the Sea\t1954\tUSA\t127\tEnglish\tApproved\tnull\t5000000\n",
    "60\t20,000 Leagues Under the Sea\t1954\tUSA\t127\tEnglish\tApproved\tnull\t5000000\n",
    "61\tOn the Waterfront\t1954\tUSA\t108\tEnglish\tNot Rated\t9600000\t910000\n",
    "62\tSeven Samurai\t1954\tJapan\t202\tJapanese\tUnrated\t269061\t2000000\n",
    "63\tThe Egyptian\t1954\tUSA\t139\tEnglish\tApproved\tnull\t5000000\n",
    "64\tOrdet\t1955\tDenmark\t126\tDanish\tNot Rated\tnull\tnull\n",
    "65\tThe Trouble with Harry\t1955\tUSA\t99\tEnglish\tApproved\tnull\t1200000\n",
    "66\tLove Me Tender\t1956\tUSA\t89\tEnglish\tnull\tnull\t1000000\n",
    "67\tMoby Dick\t1956\tUSA\t115\tEnglish\tUnrated\tnull\t4500000\n",
    "68\tTrapeze\t1956\tUSA\t105\tEnglish\tApproved\tnull\t4000000\n",
    "69\t12 Angry Men\t1957\tUSA\t96\tEnglish\tNot Rated\tnull\t350000\n",
    "70\tThe Bridge on the River Kwai\t1957\tUK\t161\tEnglish\tPG\t27200000\t3000000\n",
    "71\tCat on a Hot Tin Roof\t1958\tUSA\t108\tEnglish\tApproved\tnull\t3000000\n",
    "72\tSolomon and Sheba\t1959\tUSA\t141\tEnglish\tnull\tnull\t5000000\n",
    "73\tSome Like It Hot\t1959\tUSA\t120\tEnglish\tNot Rated\t25000000\t2883848\n",
    "74\tThe Nun's Story\t1959\tUSA\t149\tEnglish\tNot Rated\tnull\t3500000\n",
    "75\tElmer Gantry\t1960\tUSA\t146\tEnglish\tApproved\tnull\t3000000\n",
    "76\tPsycho\t1960\tUSA\t108\tEnglish\tR\t32000000\t806947\n",
    "77\tThe Apartment\t1960\tUSA\t125\tEnglish\tApproved\tnull\t3000000\n",
    "78\tJudgment at Nuremberg\t1961\tUSA\t186\tEnglish\tNot Rated\tnull\t3000000\n",
    "79\tPocketful of Miracles\t1961\tUSA\t136\tEnglish\tApproved\tnull\t2900000\n",
    "80\tThe Hustler\t1961\tUSA\t134\tEnglish\tUnrated\tnull\t2000000\n",
    "81\tThe Misfits\t1961\tUSA\t120\tEnglish\tApproved\tnull\t4000000\n",
    "82\tWest Side Story\t1961\tUSA\t152\tEnglish\tUnrated\t43650000\t6000000\n",
    "83\tDr. No\t1962\tUK\t110\tEnglish\tApproved\t16067035\t1100000\n",
    "84\tLawrence of Arabia\t1962\tUK\t227\tEnglish\tPG\t6000000\t15000000\n",
    "85\tLolita\t1962\tUK\t152\tEnglish\tNot Rated\tnull\t2000000\n",
    "86\tLolita\t1962\tUK\t152\tEnglish\tNot Rated\tnull\t2000000\n",
    "87\tThe Brain That Wouldn't Die\t1962\tUSA\t70\tEnglish\tApproved\tnull\t62000\n",
    "88\tThe Longest Day\t1962\tUSA\t178\tEnglish\tG\tnull\t10000000\n",
    "89\tThe Man Who Shot Liberty Valance\t1962\tUSA\t113\tEnglish\tApproved\tnull\t3200000\n",
    "90\tTo Kill a Mockingbird\t1962\tUSA\t129\tEnglish\tNot Rated\tnull\t2000000\n",
    "91\t55 Days at Peking\t1963\tUSA\t154\tEnglish\tUnrated\tnull\t9000000\n",
    "92\tCleopatra\t1963\tUK\t251\tEnglish\tApproved\t57750000\t31115000\n",
    "93\tDonovan's Reef\t1963\tUSA\t109\tEnglish\tNot Rated\tnull\t2686585\n",
    "94\tFrom Russia with Love\t1963\tUK\t115\tEnglish\tApproved\t24800000\t2000000\n",
    "95\tHud\t1963\tUSA\t112\tEnglish\tApproved\tnull\t2500000\n",
    "96\tIt's a Mad, Mad, Mad, Mad World\t1963\tUSA\t197\tEnglish\tApproved\t46300000\t9400000\n",
    "97\tThe Great Escape\t1963\tUSA\t172\tEnglish\tApproved\tnull\t4000000\n",
    "98\tTom Jones\t1963\tUK\t121\tEnglish\tUnrated\tnull\t1000000\n",
    "99\tA Fistful of Dollars\t1964\tItaly\t99\tItalian\tR\t3500000\t200000\n",
    "100\tA Hard Day's Night\t1964\tUK\t87\tEnglish\tApproved\t515005\t560000\n",
    "Showing 100 out of 4968 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining filtering and selecting\n",
    "Time for a little challenge. So far, your SQL vocabulary from this course includes COUNT(), DISTINCT, LIMIT, WHERE, OR, AND, BETWEEN, LIKE, NOT LIKE, and IN. In this exercise, you will try to use some of these together. Writing more complex queries will be standard for you as you become a qualified SQL programmer.\n",
    "\n",
    "As this query will be a little more complicated than what you've seen so far, we've included a bit of code to get you started. You will be using DISTINCT here too because, surprise, there are two movies named 'Hamlet' in this dataset!\n",
    "\n",
    "Follow the instructions to find out what 90's films we have in our dataset that would be suitable for English-speaking teens.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Count the unique titles from the films database and use the alias provided.\n",
    "Filter to include only movies with a release_year from 1990 to 1999, inclusive.\n",
    "Add another filter narrowing your query down to English-language films.\n",
    "Add a final filter to select only films with 'G', 'PG', 'PG-13' certifications\n",
    "\n",
    "\n",
    "\n",
    "-- Count the unique titles\n",
    "SELECT COUNT(DISTINCT title) AS nineties_english_films_for_teens\n",
    "FROM films\n",
    "-- Filter to release_years to between 1990 and 1999\n",
    "WHERE release_year BETWEEN 1990 AND 1999\n",
    "-- Filter to English-language films\n",
    "  AND language = 'English'\n",
    "-- Narrow it down to G, PG, and PG-13 certifications\n",
    "  AND certification IN ('G', 'PG', 'PG-13');\n",
    "\n",
    "You've got a natural flair for filtering! Nice work, this filter tells us we have 310 films that the 90's obsessed teenagers can enjoy.\n",
    "\n",
    "Correct! NULL is used to represent unknown values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practice with NULLs\n",
    "Well done. Now that you know what NULL means and what it's used for, it's time for some more practice!\n",
    "\n",
    "Let's explore the films table again to better understand what data you have.\n",
    "\n",
    "-- List all film titles with missing budgets\n",
    "SELECT title AS no_budget_info\n",
    "FROM films\n",
    "WHERE budget IS NULL;\n",
    "\n",
    "-- Count the number of films we have language data for\n",
    "SELECT COUNT(language) AS count_language_known\n",
    "FROM films\n",
    "WHERE language IS NOT NULL;\n",
    "\n",
    "\n",
    "Alright! That's 4957 films with language data. We've mastered selecting and filtering data which means you're halfway through the course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate functions and data types\n",
    "Aggregate functions are another valuable tool for the SQL programmer. They are used extensively across businesses to calculate important metrics, such as the average cost of making a film.\n",
    "\n",
    "You know five different aggregate functions:\n",
    "\n",
    "-AVG()\n",
    "\n",
    "-SUM()\n",
    "\n",
    "-MIN()\n",
    "\n",
    "-MAX()\n",
    "\n",
    "-COUNT()\n",
    "\n",
    "Test your knowledge of what data types they are compatible with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practice with aggregate functions\n",
    "Now let's try extracting summary information from a table using these new aggregate functions. Summarizing is helpful in real life when extracting top-line details from your dataset. Perhaps you'd like to know how old the oldest film in the films table is, what the most expensive film is, or how many films you have listed.\n",
    "\n",
    "Now it's your turn to get more insights about the films table!\n",
    "\n",
    "-- Query the sum of film durations\n",
    "SELECT SUM(duration) AS total_duration\n",
    "FROM films;\n",
    "\n",
    "\n",
    "-- Calculate the average duration of all films\n",
    "SELECT AVG(duration) AS average_duration\n",
    "FROM films;\n",
    "\n",
    "-- Find the latest release_year\n",
    "SELECT MAX(release_year) AS latest_year\n",
    "FROM films;\n",
    "\n",
    "-- Find the duration of the shortest film\n",
    "SELECT MIN(duration) AS shortest_film\n",
    "FROM films;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining aggregate functions with WHERE\n",
    "When combining aggregate functions with WHERE, you get a powerful tool that allows you to get more granular with your insights, for example, to get the total budget of movies made from the year 2010 onwards.\n",
    "\n",
    "This combination is useful when you only want to summarize a subset of your data. In your film-industry role, as an example, you may like to summarize each certification category to compare how they each perform or if one certification has a higher average budget than another.\n",
    "\n",
    "Let's see what insights you can gain about the financials in the dataset.\n",
    "\n",
    "\n",
    "-- Calculate the sum of gross from the year 2000 or later\n",
    "SELECT SUM(gross) AS total_gross\n",
    "FROM films\n",
    "WHERE release_year >= 2000;\n",
    "\n",
    "-- Calculate the average gross of films that start with A\n",
    "SELECT AVG(gross) AS avg_gross_A\n",
    "FROM films\n",
    "WHERE title LIKE 'A%';\n",
    "\n",
    "-- Calculate the lowest gross film in 1994\n",
    "SELECT MIN(gross) AS lowest_gross\n",
    "FROM films\n",
    "WHERE release_year = 1994;\n",
    "\n",
    "-- Calculate the highest gross film released between 2000-2012\n",
    "SELECT MAX(gross) AS highest_gross\n",
    "FROM films\n",
    "WHERE release_year BETWEEN 2000 AND 2012;\n",
    "\n",
    "\n",
    "Nice. SQL provides us with several building blocks that we can combine in all kinds of ways, hence the name: Structured Query Language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ROUND()\n",
    "Aggregate functions work great with numerical values; however, these results can sometimes get unwieldy when dealing with long decimal values. Luckily, SQL provides you with the ROUND() function to tame these long decimals.\n",
    "\n",
    "If asked to give the average budget of your films, ten decimal places is not necessary. Instead, you can round to two decimal places to create results that make more sense for currency.\n",
    "\n",
    "Now you try!\n",
    "\n",
    "-- Round the average number of facebook_likes to one decimal place\n",
    "SELECT ROUND(AVG(facebook_likes), 1) AS avg_facebook_likes\n",
    "FROM reviews;\n",
    "\n",
    "Well done! The average, rounded facebook_likes is 7802.9. This insight can be used as a benchmark to measure film reviews; any film with over 7802.9 likes can be considered popular.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUND() with a negative parameter\n",
    "A useful thing you can do with ROUND() is have a negative number as the decimal place parameter. This can come in handy if your manager only needs to know the average number of facebook_likes to the hundreds since granularity below one hundred likes won't impact decision making.\n",
    "\n",
    "Social media plays a significant role in determining success. If a movie trailer is posted and barely gets any likes, the movie itself may not be successful. Remember how 2020's \"Sonic the Hedgehog\" movie got a revamp after the public saw the trailer?\n",
    "\n",
    "Let's apply this to other parts of the dataset and see what the benchmark is for movie budgets so, in the future, it's clear whether the film is above or below budget.\n",
    "\n",
    "-- Calculate the average budget rounded to the thousands\n",
    "SELECT ROUND(AVG(budget) / 1000) * 1000 AS avg_budget_thousands\n",
    "FROM films;\n",
    "\n",
    "ROUND() of applause! The ROUND() function is very handy when making financial calculations to get a top-level view or specify to the penny or cent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using arithmetic\n",
    "SQL arithmetic comes in handy when your table is missing a metric you want to review. Suppose you have some data on movie ticket sales, but the table only has fields for ticket price and discount. In that case, you could combine these by subtracting the discount from the ticket price to get the amount the film-goer paid.\n",
    "\n",
    "You have seen that SQL can act strangely when dividing integers. What is the result if you divide a discount of two dollars by the paid_price of ten dollars to get the discount percentage?\n",
    "Aliasing with functions\n",
    "Aliasing can be a lifesaver, especially as we start to do more complex SQL queries with multiple criteria. Aliases help you keep your code clean and readable. For example, if you want to find the MAX() value of several fields without aliasing, you'll end up with the result with several columns called max and no idea which is which. You can fix this with aliasing.\n",
    "\n",
    "Now, it's over to you to clean up the following queries.\n",
    "\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "Select the title and duration in hours for all films and alias as duration_hours; since the current durations are in minutes, you'll need to divide duration by 60.0.\n",
    "Calculate the percentage of people who are no longer alive and alias the result as percentage_dead.\n",
    "Find how many decades (period of ten years) the films table covers by using MIN() and MAX(); alias as number_of_decades.\n",
    "\n",
    "\n",
    "-- Calculate the title and duration_hours from films\n",
    "SELECT \n",
    "    title,\n",
    "    duration / 60.0 AS duration_hours\n",
    "FROM films;\n",
    "\n",
    "\n",
    "-- Calculate the percentage of people who are no longer alive\n",
    "SELECT COUNT(deathdate) * 100.0 / COUNT(*) AS percentage_dead\n",
    "FROM people;\n",
    "\n",
    "-- Find the number of decades in the films table\n",
    "SELECT (MAX(release_year) - MIN(release_year)) / 10.0 AS number_of_decades\n",
    "FROM films;\n",
    "\n",
    "Amazing work mastering arithmetic, aggregate functions, and aliasing! Now you know that our films table covers films released over one hundred years!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rounding results\n",
    "You found some valuable insights in the previous exercise, but many of the results were inconveniently long. We forgot to round! We won't make you redo them all; however, you'll update the worst offender in this exercise.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Update the query by adding ROUND() around the calculation and round to two decimal places.\n",
    "\n",
    "-- -- Round duration_hours to two decimal places\n",
    "-- SELECT title, duration / 60.0 AS duration_hours\n",
    "-- FROM films;\n",
    "\n",
    "\n",
    "SELECT \n",
    "    title, \n",
    "    ROUND(duration / 60.0, 2) AS duration_hours\n",
    "FROM films;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting single fields\n",
    "Now that you understand how ORDER BY works, you'll put it into practice. In this exercise, you'll work on sorting single fields only. This can be helpful to extract quick insights such as the top-grossing or top-scoring film.\n",
    "\n",
    "The following exercises will help you gain further insights into the film database.\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "Select the name of each person in the people table, sorted alphabetically.\n",
    "Select the title and duration for every film, from longest duration to shortest.\n",
    "\n",
    "-- Select name from people and sort alphabetically\n",
    "SELECT name\n",
    "FROM people\n",
    "ORDER BY name ASC;\n",
    "\n",
    "-- Select the title and duration from longest to shortest film\n",
    "SELECT title, duration\n",
    "FROM films\n",
    "ORDER BY duration DESC;\n",
    "\n",
    "\n",
    "Superb sorting! ORDER BY is another simple yet effective way to gain intelligence about your business and data. You now know how to extract your best and worst-performing assets with only a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting multiple fields\n",
    "ORDER BY can also be used to sort on multiple fields. It will sort by the first field specified, then sort by the next, and so on. As an example, you may want to sort the people data by age and keep the names in alphabetical order.\n",
    "\n",
    "Try using ORDER BY to sort multiple columns.\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "Select the release_year, duration, and title of films ordered by their release year and duration, in that order.\n",
    "Select the certification, release_year, and title from films ordered first by certification (alphabetically) and second by release year, starting with the most recent year.\n",
    "\n",
    "\n",
    "-- Select the release year, duration, and title sorted by release year and duration\n",
    "SELECT release_year, duration, title\n",
    "FROM films\n",
    "ORDER BY release_year, duration;\n",
    "\n",
    "\n",
    "-- Select the certification, release year, and title sorted by certification and release year\n",
    "SELECT certification, release_year, title\n",
    "FROM films\n",
    "ORDER BY certification ASC, release_year DESC;\n",
    "\n",
    "\n",
    "Nicely done! The second column you order on only steps in when the first column has been ordered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GROUP BY single fields\n",
    "GROUP BY is a SQL keyword that allows you to group and summarize results with the additional use of aggregate functions. For example, films can be grouped by the certification and language before counting the film titles in each group. This allows you to see how many films had a particular certification and language grouping.\n",
    "\n",
    "In the following steps, you'll summarize other groups of films to learn more about the films in your database.\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "Select the release_year and count of films released in each year aliased as film_count.\n",
    "Select the release_year and average duration aliased as avg_duration of all films, grouped by release_year.\n",
    "\n",
    "-- Find the release_year and film_count of each year\n",
    "SELECT release_year, COUNT(*) AS film_count\n",
    "FROM films\n",
    "GROUP BY release_year;\n",
    "\n",
    "\n",
    "-- Find the release_year and average duration of films for each year\n",
    "SELECT release_year, AVG(duration) AS avg_duration\n",
    "FROM films\n",
    "GROUP BY release_year;\n",
    "\n",
    "Great job grouping! Using GROUP BY with a time or date field such as release_year can help us identify trends such as a period of time where movies were really short!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GROUP BY multiple fields\n",
    "GROUP BY becomes more powerful when used across multiple fields or combined with ORDER BY and LIMIT.\n",
    "\n",
    "Perhaps you're interested in learning about budget changes throughout the years in individual countries. You'll use grouping in this exercise to look at the maximum budget for each country in each year there is data available.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Select the release_year, country, and the maximum budget aliased as max_budget for each year and each country; sort your results by release_year and country.\n",
    "\n",
    "-- Find the release_year, country, and max_budget, then group and order by release_year and country\n",
    "SELECT release_year, country, MAX(budget) AS max_budget\n",
    "FROM films\n",
    "GROUP BY release_year, country\n",
    "ORDER BY release_year, country;\n",
    "\n",
    "Well done! You can see how building on your SQL queries helps you gain more insights and detect trends in the data, such as how film budgets may change throughout the years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter with HAVING\n",
    "Your final keyword is HAVING. It works similarly to WHERE in that it is a filtering clause, with the difference that HAVING filters grouped data.\n",
    "\n",
    "Filtering grouped data can be especially handy when working with a large dataset. When working with thousands or even millions of rows, HAVING will allow you to filter for just the group of data you want, such as films over two hours in length!\n",
    "\n",
    "Practice using HAVING to find out which countries (or country) have the most varied film certifications.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Select country from the films table, and get the distinct count of certification aliased as certification_count.\n",
    "Group the results by country.\n",
    "Filter the unique count of certifications to only results greater than 10.\n",
    "\n",
    "\n",
    "-- Select the country and distinct count of certification as certification_count\n",
    "SELECT country, COUNT(DISTINCT certification) AS certification_count\n",
    "FROM films\n",
    "\n",
    "-- Group by country\n",
    "GROUP BY country\n",
    "\n",
    "\n",
    "-- Filter results to countries with more than 10 different certifications\n",
    "HAVING COUNT(DISTINCT certification) > 10;\n",
    "\n",
    "Great job! The answer is USA with 12 different certifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HAVING and sorting\n",
    "Filtering and sorting go hand in hand and gives you greater interpretability by ordering our results.\n",
    "\n",
    "Let's see this magic at work by writing a query showing what countries have the highest average film budgets.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Select the country and the average budget as average_budget, rounded to two decimal, from films.\n",
    "Group the results by country.\n",
    "Filter the results to countries with an average budget of more than one billion (1000000000).\n",
    "Sort by descending order of the average_budget.\n",
    "\n",
    "-- Select the country and average_budget from films\n",
    "SELECT \n",
    "    country,\n",
    "    ROUND(AVG(budget), 2) AS average_budget\n",
    "FROM films\n",
    "-- Group by country\n",
    "GROUP BY country\n",
    "-- Filter to countries with an average_budget of more than one billion\n",
    "HAVING AVG(budget) > 1000000000\n",
    "-- Order by descending order of the aggregated budget\n",
    "ORDER BY average_budget DESC;\n",
    "\n",
    "You did it! South Korea and Hungary seem to have pricey films... or do they? Actually, these budgets are pretty standard for their local currency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All together now\n",
    "It's time to use much of what you've learned in one query! This is good preparation for using SQL in the real world where you'll often be asked to write more complex queries since some of the basic queries can be answered by playing around in spreadsheet applications.\n",
    "\n",
    "In this exercise, you'll write a query that returns the average budget and gross earnings for films each year after 1990 if the average budget is greater than 60 million.\n",
    "\n",
    "This will be a big query, but you can handle it!\n",
    "\n",
    "Instructions 1/4\n",
    "25 XP\n",
    "Select the release_year for each film in the films table, filter for records released after 1990, and group by release_year.\n",
    "Modify the query to include the average budget aliased as avg_budget and average gross aliased as avg_gross for the results we have so far.\n",
    "Modify the query once more so that only years with an average budget of greater than 60 million are included.\n",
    "Finally, order the results from the highest average gross and limit to one.\n",
    "\n",
    "\n",
    "-- Select the release_year for films released after 1990 grouped by year\n",
    "SELECT release_year\n",
    "FROM films\n",
    "WHERE release_year > 1990\n",
    "GROUP BY release_year;\n",
    "\n",
    "\n",
    "-- Modify the query to also list the average budget and average gross\n",
    "SELECT release_year, AVG(budget) AS avg_budget, AVG(gross) AS avg_gross\n",
    "FROM films\n",
    "WHERE release_year > 1990\n",
    "GROUP BY release_year;\n",
    "\n",
    "\n",
    "-- SELECT release_year, AVG(budget) AS avg_budget, AVG(gross) AS avg_gross\n",
    "-- FROM films\n",
    "-- WHERE release_year > 1990\n",
    "-- GROUP BY release_year\n",
    "-- -- Modify the query to see only years with an avg_budget of more than 60 million\n",
    "\n",
    "SELECT \n",
    "    release_year, \n",
    "    AVG(budget) AS avg_budget, \n",
    "    AVG(gross) AS avg_gross\n",
    "FROM films\n",
    "WHERE release_year > 1990\n",
    "GROUP BY release_year\n",
    "HAVING AVG(budget) > 60000000;  -- Adjusted to 60 million\n",
    "\n",
    "\n",
    "SELECT release_year, AVG(budget) AS avg_budget, AVG(gross) AS avg_gross\n",
    "FROM films\n",
    "WHERE release_year > 1990\n",
    "GROUP BY release_year\n",
    "HAVING AVG(budget) > 60000000\n",
    "-- Order the results from highest to lowest average gross and limit to one\n",
    "ORDER BY avg_gross DESC\n",
    "LIMIT 1;\n",
    "\n",
    "\n",
    "Superb work! SQL queries can get rather long, but breaking them down into individual clauses makes them easier to write.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# joining data in sql\n",
    "\n",
    "Introducing Inner Joins\n",
    "Free\n",
    "\n",
    "100%\n",
    "In this chapter, you’ll be introduced to the concept of joining tables and will explore all the ways you can enrich your queries using joins—beginning with inner joins.\n",
    "\n",
    "The ins and outs of INNER JOIN\n",
    "\n",
    "\n",
    "Your first join\n",
    "Throughout this course, you'll be working with the countries database, which contains information about the most populous world cities in the world, along with country-level economic, population, and geographic data. The database also contains information on languages spoken in each country.\n",
    "\n",
    "You can see the different tables in this database to get a sense of what they contain by clicking on the corresponding tabs. Click through them and familiarize yourself with the fields that seem to be shared across tables before you continue with the course.\n",
    "\n",
    "In this exercise, you'll use the cities and countries tables to build your first inner join. You'll start off by selecting all columns in step 1, performing your join in step 2, and then refining your join to choose specific columns in step 3.\n",
    "\n",
    "-- Select all columns from cities\n",
    "SELECT *\n",
    "FROM cities;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cities\n",
    "name\tcountry_code\tcity_proper_pop\tmetroarea_pop\turbanarea_pop\n",
    "Abidjan\tCIV\t4765000\tnull\t4765000\n",
    "Abu Dhabi\tARE\t1145000\tnull\t1145000\n",
    "Abuja\tNGA\t1235880\t6000000\t1235880\n",
    "Accra\tGHA\t2070463\t4010054\t2070463\n",
    "Addis Ababa\tETH\t3103673\t4567857\t3103673\n",
    "Ahmedabad\tIND\t5570585\tnull\t5570585\n",
    "Alexandria\tEGY\t4616625\tnull\t4616625\n",
    "Algiers\tDZA\t3415811\t5000000\t3415811\n",
    "Almaty\tKAZ\t1703481\tnull\t1703481\n",
    "Ankara\tTUR\t5271000\t4585000\t5271000\n",
    "Auckland\tNZL\t1495000\t1614300\t1495000\n",
    "Baghdad\tIRQ\t7180889\tnull\t7180889\n",
    "Baku\tAZE\t3202300\t4308740\t3202300\n",
    "Bandung\tIDN\t2575478\t6965655\t2575478\n",
    "Bangkok\tTHA\t8280925\t14998000\t8280925\n",
    "Barcelona\tESP\t1604555\t5375774\t1604555\n",
    "Barranquilla\tCOL\t1386865\t2370753\t1386865\n",
    "Basra\tIRQ\t2750000\tnull\t2750000\n",
    "Beijing\tCHN\t21516000\t24900000\t21516000\n",
    "Belo Horizonte\tBRA\t2502557\t5156217\t2502557\n",
    "Bengaluru\tIND\t8425970\t9807000\t8425970\n",
    "Berlin\tDEU\t3517424\t5871022\t3517424\n",
    "Bhopal\tIND\t1798218\t1864389\t1798218\n",
    "Birmingham\tGBR\t1111300\t3683000\t1111300\n",
    "Bogota\tCOL\t7878783\t9800000\t7878783\n",
    "Brasilia\tBRA\t2556149\t3919864\t2556149\n",
    "Brazzaville\tCOG\t1827000\tnull\t1827000\n",
    "Brisbane\tAUS\t1180285\t2349699\t1180285\n",
    "Bucharest\tROM\t1883425\t2272163\t1883425\n",
    "Budapest\tHUN\t1759407\t2927944\t1759407\n",
    "Buenos Aires\tARG\t3054300\t14122000\t3054300\n",
    "Busan\tKOR\t3510833\t8202239\t3510833\n",
    "Cairo\tEGY\t10230350\t18290000\t10230350\n",
    "Calgary\tCAN\t1235171\t1214839\t1235171\n",
    "Cali\tCOL\t2400653\t3400000\t2400653\n",
    "Caloocan\tPHL\t1583978\tnull\t1583978\n",
    "Campinas\tBRA\t1164098\t3094181\t1164098\n",
    "Cape Town\tZAF\t3740026\tnull\t3740026\n",
    "Caracas\tVEN\t1943901\t2923959\t1943901\n",
    "Casablanca\tMAR\t5117832\t6861739\t5117832\n",
    "Changchun\tCHN\t3815270\t7674439\t3815270\n",
    "Changsha\tCHN\t7044118\tnull\t7044118\n",
    "Chaozhou\tCHN\t2669844\tnull\t2669844\n",
    "Chengdu\tCHN\t4741929\t10376000\t4741929\n",
    "Chennai\tIND\t7088000\tnull\t7088000\n",
    "Chicago\tUSA\t2695598\t9156000\t2695598\n",
    "Chittagong\tBGD\t2581643\t4009423\t2581643\n",
    "Chongqing\tCHN\t8189800\t52100100\t8189800\n",
    "Cologne\tDEU\t1057327\t3573500\t1057327\n",
    "Cordoba\tARG\t1330023\t1528000\t1330023\n",
    "Curitiba\tBRA\t1879355\t3400000\t1879355\n",
    "Daegu\tKOR\t2492994\tnull\t2492994\n",
    "Daejeon\tKOR\t1535028\tnull\t1535028\n",
    "Dakar\tSEN\t1146053\t2452656\t1146053\n",
    "Dalian\tCHN\t2146099\t5935638\t2146099\n",
    "Dallas\tUSA\t1317929\t7233323\t1317929\n",
    "Dar es Salaam\tTZA\t4364541\tnull\t4364541\n",
    "Davao City\tPHL\t1632991\t2516216\t1632991\n",
    "Delhi\tIND\t16787940\t24998000\t16787940\n",
    "Dhaka\tBGD\t14543124\tnull\t14543124\n",
    "Dongguan\tCHN\t8220207\tnull\t8220207\n",
    "Douala\tCMR\t2446945\tnull\t2446945\n",
    "Dubai\tARE\t2643410\tnull\t2643410\n",
    "Durban\tZAF\t3442361\tnull\t3442361\n",
    "Ekurhuleni\tZAF\t3178470\tnull\t3178470\n",
    "Faisalabad\tPAK\t6480765\t3675000\t6480765\n",
    "Fez\tMAR\t1112072\tnull\t1112072\n",
    "Fortaleza\tBRA\t2609716\t4019213\t2609716\n",
    "Foshan\tCHN\t6151622\tnull\t6151622\n",
    "Fukuoka\tJPN\t1483052\t5590378\t1483052\n",
    "Fuzhou\tCHN\t7115369\tnull\t7115369\n",
    "Giza\tEGY\t4239988\tnull\t4239988\n",
    "Guadalajara\tMEX\t1495189\t4424252\t1495189\n",
    "Guangzhou\tCHN\t14043500\t44259000\t14043500\n",
    "Guatemala City\tGTM\t2110100\t4500000\t2110100\n",
    "Guayaquil\tECU\t3600000\t5000000\t3600000\n",
    "Gujranwala\tPAK\t2700003\tnull\t2700003\n",
    "Hamburg\tDEU\t1787408\tnull\t1787408\n",
    "Hangzhou\tCHN\t3560391\tnull\t3560391\n",
    "Hanoi\tVNM\t6844100\tnull\t6844100\n",
    "Harare\tZWE\t1606000\tnull\t1606000\n",
    "Harbin\tCHN\t4280701\t10635971\t4280701\n",
    "Havana\tCUB\t2106146\tnull\t2106146\n",
    "Hefei\tCHN\t3352076\tnull\t3352076\n",
    "Hiroshima\tJPN\t1196274\tnull\t1196274\n",
    "Ho Chi Minh City\tVNM\t7681700\tnull\t7681700\n",
    "Hong Kong\tCHN\t7374900\tnull\t7374900\n",
    "Houston\tUSA\t2489558\t6490180\t2489558\n",
    "Hyderabad (India)\tIND\t7859250\tnull\t7859250\n",
    "Hyderabad\tPAK\t3429471\tnull\t3429471\n",
    "Ibadan\tNGA\t1338659\t2837000\t1338659\n",
    "Incheon\tKOR\t2978367\tnull\t2978367\n",
    "Isfahan\tIRN\t2243249\tnull\t2243249\n",
    "Islamabad\tPAK\t1900000\t2200000\t1900000\n",
    "Istanbul\tTUR\t14025000\t13520000\t14025000\n",
    "Izmir\tTUR\t4168000\t3019000\t4168000\n",
    "Jaipur\tIND\t3073350\tnull\t3073350\n",
    "Jakarta\tIDN\t10075310\t30539000\t10075310\n",
    "Jeddah\tSAU\t3456259\tnull\t3456259\n",
    "Jinan\tCHN\t2009273\t5853196\t2009273\n",
    "Showing 100 out of 236 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# languages\n",
    "lang_id\tcode\tname\tpercent\tofficial\n",
    "1\tAFG\tDari\t50\ttrue\n",
    "2\tAFG\tPashto\t35\ttrue\n",
    "3\tAFG\tTurkic\t11\tfalse\n",
    "4\tAFG\tOther\t4\tfalse\n",
    "5\tALB\tAlbanian\t98.8\ttrue\n",
    "6\tALB\tGreek\t0.5\tfalse\n",
    "7\tALB\tOther\t0.6\tfalse\n",
    "8\tALB\tunspecified\t0.1\tfalse\n",
    "9\tDZA\tArabic\tnull\ttrue\n",
    "10\tDZA\tFrench\tnull\tfalse\n",
    "11\tDZA\tBerber or Tamazight\tnull\ttrue\n",
    "12\tDZA\tShawiya\tnull\tfalse\n",
    "13\tDZA\tMzab\tnull\tfalse\n",
    "14\tDZA\tTuareg\tnull\tfalse\n",
    "15\tASM\tSamoan\t88.6\tfalse\n",
    "16\tASM\tEnglish\t3.9\tfalse\n",
    "17\tASM\tTongan\t2.7\tfalse\n",
    "18\tASM\tOther\t3\tfalse\n",
    "19\tASM\tOther\t1.8\tfalse\n",
    "20\tAND\tCatalan\tnull\ttrue\n",
    "21\tAND\tFrench\tnull\tfalse\n",
    "22\tAND\tCastilian\tnull\tfalse\n",
    "23\tAND\tPortuguese\tnull\tfalse\n",
    "24\tAGO\tPortuguese\t71.2\ttrue\n",
    "25\tAGO\tUmbundu\t23\tfalse\n",
    "26\tAGO\tKikongo\t8.2\tfalse\n",
    "27\tAGO\tKimbundu\t7.8\tfalse\n",
    "28\tAGO\tChokwe\t6.5\tfalse\n",
    "29\tAGO\tNhaneca\t3.4\tfalse\n",
    "30\tAGO\tNganguela\t3.1\tfalse\n",
    "31\tAGO\tFiote\t2.4\tfalse\n",
    "32\tAGO\tKwanhama\t2.3\tfalse\n",
    "33\tAGO\tMuhumbi\t2.1\tfalse\n",
    "34\tAGO\tLuvale\t1\tfalse\n",
    "35\tAGO\tOther\t3.6\tfalse\n",
    "36\tAIA\tEnglish\tnull\ttrue\n",
    "37\tATG\tEnglish\tnull\ttrue\n",
    "38\tATG\tAntiguan creole\tnull\tfalse\n",
    "39\tARG\tSpanish\tnull\ttrue\n",
    "40\tARG\tItalian\tnull\tfalse\n",
    "41\tARG\tEnglish\tnull\tfalse\n",
    "42\tARG\tGerman\tnull\tfalse\n",
    "43\tARG\tFrench\tnull\tfalse\n",
    "44\tARG\tindigenous\tnull\tfalse\n",
    "45\tARM\tArmenian\t97.9\ttrue\n",
    "46\tARM\tKurdish\t1\tfalse\n",
    "47\tARM\tOther\t1\tfalse\n",
    "48\tABW\tPapiamento\t69.4\ttrue\n",
    "49\tABW\tSpanish\t13.7\tfalse\n",
    "50\tABW\tEnglish\t7.1\tfalse\n",
    "51\tABW\tDutch\t6.1\ttrue\n",
    "52\tABW\tChinese\t1.5\tfalse\n",
    "53\tABW\tOther\t1.7\tfalse\n",
    "54\tABW\tunspecified\t0.4\tfalse\n",
    "55\tAUS\tEnglish\t76.8\tfalse\n",
    "56\tAUS\tMandarin\t1.6\tfalse\n",
    "57\tAUS\tItalian\t1.4\tfalse\n",
    "58\tAUS\tArabic\t1.3\tfalse\n",
    "59\tAUS\tGreek\t1.2\tfalse\n",
    "60\tAUS\tCantonese\t1.2\tfalse\n",
    "61\tAUS\tVietnamese\t1.1\tfalse\n",
    "62\tAUS\tOther\t10.4\tfalse\n",
    "63\tAUS\tunspecified\t5\tfalse\n",
    "64\tAUT\tGerman\t88.6\ttrue\n",
    "65\tAUT\tTurkish\t2.3\tfalse\n",
    "66\tAUT\tSerbian\t2.2\tfalse\n",
    "67\tAUT\tCroatian\t1.6\ttrue\n",
    "68\tAUT\tOther\t5.3\tfalse\n",
    "69\tAZE\tAzerbaijani\t92.5\ttrue\n",
    "70\tAZE\tRussian\t1.4\tfalse\n",
    "71\tAZE\tArmenian\t1.4\tfalse\n",
    "72\tAZE\tOther\t4.7\tfalse\n",
    "73\tBHS\tEnglish\tnull\ttrue\n",
    "74\tBHS\tCreole\tnull\tfalse\n",
    "75\tBHR\tArabic\tnull\ttrue\n",
    "76\tBHR\tEnglish\tnull\tfalse\n",
    "77\tBHR\tFarsi\tnull\tfalse\n",
    "78\tBHR\tUrdu\tnull\tfalse\n",
    "79\tBGD\tBangla\t98.8\ttrue\n",
    "80\tBGD\tOther\t1.2\tfalse\n",
    "81\tBRB\tEnglish\tnull\ttrue\n",
    "82\tBRB\tBajan\tnull\tfalse\n",
    "83\tBLR\tRussian\t70.2\ttrue\n",
    "84\tBLR\tBelarusian\t23.4\ttrue\n",
    "85\tBLR\tOther\t3.1\tfalse\n",
    "86\tBLR\tunspecified\t3.3\tfalse\n",
    "87\tBEL\tDutch\t60\ttrue\n",
    "88\tBEL\tFrench\t40\ttrue\n",
    "89\tBEL\tGerman\t1\ttrue\n",
    "90\tBLZ\tEnglish\t62.9\ttrue\n",
    "91\tBLZ\tSpanish\t56.6\tfalse\n",
    "92\tBLZ\tCreole\t44.6\tfalse\n",
    "93\tBLZ\tMaya\t10.5\tfalse\n",
    "94\tBLZ\tGerman\t3.2\tfalse\n",
    "95\tBLZ\tGarifuna\t2.9\tfalse\n",
    "96\tBLZ\tOther\t1.8\tfalse\n",
    "97\tBLZ\tunknown\t0.3\tfalse\n",
    "98\tBLZ\tnone\t0.2\tfalse\n",
    "99\tBEN\tFrench\tnull\ttrue\n",
    "100\tBEN\tFon and Yoruba\tnull\tfalse\n",
    "Showing 100 out of 955 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# currencies\n",
    "curr_id\tcode\tbasic_unit\tcurr_code\tfrac_unit\tfrac_perbasic\n",
    "1\tAFG\tAfghan afghani\tAFN\tPul\t100\n",
    "2\tALB\tAlbanian lek\tALL\tQindarke\t100\n",
    "3\tDZA\tAlgerian dinar\tDZD\tSanteem\t100\n",
    "4\tAND\tEuro\tEUR\tCent\t100\n",
    "5\tAGO\tAngolan kwanza\tAOA\tCentimo\t100\n",
    "6\tAIA\tEast Caribbean dollar\tXCD\tCent\t100\n",
    "7\tATG\tEast Caribbean dollar\tXCD\tCent\t100\n",
    "8\tARG\tArgentine peso\tARS\tCentavo\t100\n",
    "9\tARM\tArmenian dram\tAMD\tLuma\t100\n",
    "10\tABW\tAruban florin\tAWG\tCent\t100\n",
    "11\tAUS\tAustralian dollar\tAUD\tCent\t100\n",
    "12\tAUT\tEuro\tEUR\tCent\t100\n",
    "13\tAZE\tAzerbaijani manat\tAZN\tQəpik\t100\n",
    "14\tBHR\tBahraini dinar\tBHD\tFils\t1000\n",
    "15\tBGD\tBangladeshi taka\tBDT\tPaisa\t100\n",
    "16\tBRB\tBarbadian dollar\tBBD\tCent\t100\n",
    "17\tBLR\tNew Belarusian ruble\tBYN\tKapyeyka\t100\n",
    "18\tBLR\tOld Belarusian ruble\tBYR\tKapyeyka\t100\n",
    "19\tBEL\tEuro\tEUR\tCent\t100\n",
    "20\tBLZ\tBelize dollar\tBZD\tCent\t100\n",
    "21\tBEN\tWest African CFA franc\tXOF\tCentime\t100\n",
    "22\tBMU\tBermudian dollar\tBMD\tCent\t100\n",
    "23\tBTN\tBhutanese ngultrum\tBTN\tChetrum\t100\n",
    "24\tBTN\tIndian rupee\tINR\tPaisa\t100\n",
    "25\tBOL\tBolivian boliviano\tBOB\tCentavo\t100\n",
    "26\tBIH\tBosnia and Herzegovina convertible mark\tBAM\tFening\t100\n",
    "27\tBWA\tBotswana pula\tBWP\tThebe\t100\n",
    "28\tBRA\tBrazilian real\tBRL\tCentavo\t100\n",
    "29\tIOT\tUnited States dollar\tUSD\tCent\t100\n",
    "30\tBRN\tBrunei dollar\tBND\tSen\t100\n",
    "31\tBRN\tSingapore dollar\tSGD\tCent\t100\n",
    "32\tBGR\tBulgarian lev\tBGN\tStotinka\t100\n",
    "33\tBFA\tWest African CFA franc\tXOF\tCentime\t100\n",
    "34\tBDI\tBurundian franc\tBIF\tCentime\t100\n",
    "35\tKHM\tCambodian riel\tKHR\tSen\t100\n",
    "36\tKHM\tUnited States dollar\tUSD\tCent\t100\n",
    "37\tCMR\tCentral African CFA franc\tXAF\tCentime\t100\n",
    "38\tCAN\tCanadian dollar\tCAD\tCent\t100\n",
    "39\tCPV\tCape Verdean escudo\tCVE\tCentavo\t100\n",
    "40\tCYM\tCayman Islands dollar\tKYD\tCent\t100\n",
    "41\tCAF\tCentral African CFA franc\tXAF\tCentime\t100\n",
    "42\tTCD\tCentral African CFA franc\tXAF\tCentime\t100\n",
    "43\tCHL\tChilean peso\tCLP\tCentavo\t100\n",
    "44\tCHN\tChinese yuan\tCNY\tFen\t100\n",
    "45\tCCK\tAustralian dollar\tAUD\tCent\t100\n",
    "46\tCOL\tColombian peso\tCOP\tCentavo\t100\n",
    "47\tCOM\tComorian franc\tKMF\tCentime\t100\n",
    "48\tCOD\tCongolese franc\tCDF\tCentime\t100\n",
    "49\tCOG\tCentral African CFA franc\tXAF\tCentime\t100\n",
    "50\tCOK\tNew Zealand dollar\tNZD\tCent\t100\n",
    "51\tCRI\tCosta Rican colon\tCRC\tCentimo\t100\n",
    "52\tCIV\tWest African CFA franc\tXOF\tCentime\t100\n",
    "53\tHRV\tCroatian kuna\tHRK\tLipa\t100\n",
    "54\tCUB\tCuban convertible peso\tCUC\tCentavo\t100\n",
    "55\tCUB\tCuban peso\tCUP\tCentavo\t100\n",
    "56\tCYP\tEuro\tEUR\tCent\t100\n",
    "57\tCZE\tCzech koruna\tCZK\tHaler\t100\n",
    "58\tDNK\tDanish krone\tDKK\tOre\t100\n",
    "59\tDJI\tDjiboutian franc\tDJF\tCentime\t100\n",
    "60\tDMA\tEast Caribbean dollar\tXCD\tCent\t100\n",
    "61\tDOM\tDominican peso\tDOP\tCentavo\t100\n",
    "62\tTMP\tUnited States dollar\tUSD\tCent\t100\n",
    "63\tECU\tUnited States dollar\tUSD\tCent\t100\n",
    "64\tEGY\tEgyptian pound\tEGP\tPiastre\t100\n",
    "65\tSLV\tUnited States dollar\tUSD\tCent\t100\n",
    "66\tGNQ\tCentral African CFA franc\tXAF\tCentime\t100\n",
    "67\tERI\tEritrean nakfa\tERN\tCent\t100\n",
    "68\tEST\tEuro\tEUR\tCent\t100\n",
    "69\tETH\tEthiopian birr\tETB\tSantim\t100\n",
    "70\tFLK\tFalkland Islands pound\tFKP\tPenny\t100\n",
    "71\tFRO\tDanish krone\tDKK\tOre\t100\n",
    "72\tFIN\tEuro\tEUR\tCent\t100\n",
    "73\tFRA\tEuro\tEUR\tCent\t100\n",
    "74\tPYF\tCFP franc\tXPF\tCentime\t100\n",
    "75\tGAB\tCentral African CFA franc\tXAF\tCentime\t100\n",
    "76\tGEO\tGeorgian lari\tGEL\tTetri\t100\n",
    "77\tDEU\tEuro\tEUR\tCent\t100\n",
    "78\tGHA\tGhanaian cedi\tGHS\tPesewa\t100\n",
    "79\tGIB\tGibraltar pound\tGIP\tPenny\t100\n",
    "80\tGRC\tEuro\tEUR\tCent\t100\n",
    "81\tGRD\tEast Caribbean dollar\tXCD\tCent\t100\n",
    "82\tGTM\tGuatemalan quetzal\tGTQ\tCentavo\t100\n",
    "83\tGIN\tGuinean franc\tGNF\tCentime\t100\n",
    "84\tGNB\tWest African CFA franc\tXOF\tCentime\t100\n",
    "85\tGUY\tGuyanese dollar\tGYD\tCent\t100\n",
    "86\tHTI\tHaitian gourde\tHTG\tCentime\t100\n",
    "87\tHND\tHonduran lempira\tHNL\tCentavo\t100\n",
    "88\tHKG\tHong Kong dollar\tHKD\tCent\t100\n",
    "89\tHUN\tHungarian forint\tHUF\tFiller\t100\n",
    "90\tISL\tIcelandic krona\tISK\tEyrir\t100\n",
    "91\tIND\tIndian rupee\tINR\tPaisa\t100\n",
    "92\tIDN\tIndonesian rupiah\tIDR\tSen\t100\n",
    "93\tIRN\tIranian rial\tIRR\tDinar\t100\n",
    "94\tIRQ\tIraqi dinar\tIQD\tFils\t1000\n",
    "95\tIRL\tEuro\tEUR\tCent\t100\n",
    "96\tISR\tIsraeli new shekel\tILS\tAgora\t100\n",
    "97\tITA\tEuro\tEUR\tCent\t100\n",
    "98\tJAM\tJamaican dollar\tJMD\tCent\t100\n",
    "99\tJPN\tJapanese yen\tJPY\tSen\t100\n",
    "100\tJOR\tJordanian dinar\tJOD\tPiastre\t100\n",
    "Showing 100 out of 224 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# countries\n",
    "code\tname\tcontinent\tregion\tsurface_area\tindep_year\tlocal_name\tgov_form\tcapital\tcap_long\tcap_lat\n",
    "AFG\tAfghanistan\tAsia\tSouthern and Central Asia\t652090\t1919\tAfganistan/Afqanestan\tIslamic Emirate\tKabul\t69.1761\t34.5228\n",
    "NLD\tNetherlands\tEurope\tWestern Europe\t41526\t1581\tNederland\tConstitutional Monarchy\tAmsterdam\t4.89095\t52.3738\n",
    "ALB\tAlbania\tEurope\tSouthern Europe\t28748\t1912\tShqiperia\tRepublic\tTirane\t19.8172\t41.3317\n",
    "DZA\tAlgeria\tAfrica\tNorthern Africa\t2381740\t1962\tAl-Jazair/Algerie\tRepublic\tAlgiers\t3.05097\t36.7397\n",
    "ASM\tAmerican Samoa\tOceania\tPolynesia\t199\tnull\tAmerika Samoa\tUS Territory\tPago Pago\t-170.691\t-14.2846\n",
    "AND\tAndorra\tEurope\tSouthern Europe\t468\t1278\tAndorra\tParliamentary Coprincipality\tAndorra la Vella\t1.5218\t42.5075\n",
    "AGO\tAngola\tAfrica\tCentral Africa\t1246700\t1975\tAngola\tRepublic\tLuanda\t13.242\t-8.81155\n",
    "ATG\tAntigua and Barbuda\tNorth America\tCaribbean\t442\t1981\tAntigua and Barbuda\tConstitutional Monarchy\tSaint John's\t-61.8456\t17.1175\n",
    "ARE\tUnited Arab Emirates\tAsia\tMiddle East\t83600\t1971\tAl-Imarat al-´Arabiya al-Muttahida\tEmirate Federation\tAbu Dhabi\t54.3705\t24.4764\n",
    "ARG\tArgentina\tSouth America\tSouth America\t2780400\t1816\tArgentina\tFederal Republic\tBuenos Aires\t-58.4173\t-34.6118\n",
    "ARM\tArmenia\tAsia\tMiddle East\t29800\t1991\tHajastan\tRepublic\tYerevan\t44.509\t40.1596\n",
    "ABW\tAruba\tNorth America\tCaribbean\t193\tnull\tAruba\tNonmetropolitan Territory of The Netherlands\tOranjestad\t-70.0167\t12.5167\n",
    "AUS\tAustralia\tOceania\tAustralia and New Zealand\t7741220\t1901\tAustralia\tConstitutional Monarchy, Federation\tCanberra\t149.129\t-35.282\n",
    "AZE\tAzerbaijan\tAsia\tMiddle East\t86600\t1991\tAzarbaycan\tFederal Republic\tBaku\t49.8932\t40.3834\n",
    "BHS\tBahamas\tNorth America\tCaribbean\t13878\t1973\tThe Bahamas\tConstitutional Monarchy\tNassau\t-77.339\t25.0661\n",
    "BHR\tBahrain\tAsia\tMiddle East\t694\t1971\tAl-Bahrayn\tMonarchy (Emirate)\tManama\t50.5354\t26.1921\n",
    "BGD\tBangladesh\tAsia\tSouthern and Central Asia\t143998\t1971\tBangladesh\tRepublic\tDhaka\t90.4113\t23.7055\n",
    "BRB\tBarbados\tNorth America\tCaribbean\t430\t1966\tBarbados\tConstitutional Monarchy\tBridgetown\t-59.6105\t13.0935\n",
    "BEL\tBelgium\tEurope\tWestern Europe\t30518\t1830\tBelgie/Belgique\tConstitutional Monarchy, Federation\tBrussels\t4.36761\t50.8371\n",
    "BLZ\tBelize\tNorth America\tCentral America\t22696\t1981\tBelize\tConstitutional Monarchy\tBelmopan\t-88.7713\t17.2534\n",
    "BEN\tBenin\tAfrica\tWestern Africa\t112622\t1960\tBenin\tRepublic\tPorto-Novo\t2.6323\t6.4779\n",
    "BMU\tBermuda\tNorth America\tNorth America\t53\tnull\tBermuda\tDependent Territory of the UK\tHamilton\t-64.706\t32.3293\n",
    "BTN\tBhutan\tAsia\tSouthern and Central Asia\t47000\t1910\tDruk-Yul\tMonarchy\tThimphu\t89.6177\t27.5768\n",
    "BOL\tBolivia\tSouth America\tSouth America\t1098580\t1825\tBolivia\tRepublic\tLa Paz\t-66.1936\t-13.9908\n",
    "BIH\tBosnia and Herzegovina\tEurope\tSouthern Europe\t51197\t1992\tBosna i Hercegovina\tFederal Republic\tSarajevo\t18.4214\t43.8607\n",
    "BWA\tBotswana\tAfrica\tSouthern Africa\t581730\t1966\tBotswana\tRepublic\tGaborone\t25.9201\t-24.6544\n",
    "BRA\tBrazil\tSouth America\tSouth America\t8547400\t1822\tBrasil\tFederal Republic\tBrasilia\t-47.9292\t-15.7801\n",
    "GBR\tUnited Kingdom\tEurope\tBritish Islands\t242900\t1066\tUnited Kingdom\tConstitutional Monarchy\tLondon\t-0.126236\t51.5002\n",
    "VGB\tVirgin Islands, British\tNorth America\tCaribbean\t151\tnull\tBritish Virgin Islands\tDependent Territory of the UK\tRoad Town\t-64.623055\t18.431389\n",
    "BRN\tBrunei\tAsia\tSoutheast Asia\t5765\t1984\tBrunei Darussalam\tMonarchy (Sultanate)\tBandar Seri Begawan\t114.946\t4.94199\n",
    "BGR\tBulgaria\tEurope\tEastern Europe\t110994\t1908\tBalgarija\tRepublic\tSofia\t23.3238\t42.7105\n",
    "BFA\tBurkina Faso\tAfrica\tWestern Africa\t274000\t1960\tBurkina Faso\tRepublic\tOuagadougou\t-1.53395\t12.3605\n",
    "BDI\tBurundi\tAfrica\tEastern Africa\t27834\t1962\tBurundi/Uburundi\tRepublic\tBujumbura\t29.3639\t-3.3784\n",
    "CYM\tCayman Islands\tNorth America\tCaribbean\t264\tnull\tCayman Islands\tDependent Territory of the UK\tGeorge Town\t-81.3857\t19.3022\n",
    "CHL\tChile\tSouth America\tSouth America\t756626\t1810\tChile\tRepublic\tSantiago\t-70.6475\t-33.475\n",
    "CRI\tCosta Rica\tNorth America\tCentral America\t51100\t1821\tCosta Rica\tRepublic\tSan Jose\t-84.0089\t9.63701\n",
    "DJI\tDjibouti\tAfrica\tEastern Africa\t23200\t1977\tDjibouti/Jibuti\tRepublic\tDjibouti\t43.1425\t11.5806\n",
    "DMA\tDominica\tNorth America\tCaribbean\t751\t1978\tDominica\tRepublic\tRoseau\t-61.39\t15.2976\n",
    "DOM\tDominican Republic\tNorth America\tCaribbean\t48511\t1844\tRepublica Dominicana\tRepublic\tSanto Domingo\t-69.8908\t18.479\n",
    "ECU\tEcuador\tSouth America\tSouth America\t283561\t1822\tEcuador\tRepublic\tQuito\t-78.5243\t-0.229498\n",
    "EGY\tEgypt\tAfrica\tNorthern Africa\t1001450\t1922\tMisr\tRepublic\tCairo\t31.2461\t30.0982\n",
    "SLV\tEl Salvador\tNorth America\tCentral America\t21041\t1841\tEl Salvador\tRepublic\tSan Salvador\t-89.2073\t13.7034\n",
    "ERI\tEritrea\tAfrica\tEastern Africa\t117600\t1993\tErtra\tRepublic\tAsmara\t38.9183\t15.3315\n",
    "ESP\tSpain\tEurope\tSouthern Europe\t505992\t1492\tEspana\tConstitutional Monarchy\tMadrid\t-3.70327\t40.4167\n",
    "ZAF\tSouth Africa\tAfrica\tSouthern Africa\t1221040\t1910\tSouth Africa\tRepublic\tPretoria\t28.1871\t-25.746\n",
    "ETH\tEthiopia\tAfrica\tEastern Africa\t1104300\t-1000\tYeItyop´iya\tRepublic\tAddis Ababa\t38.7468\t9.02274\n",
    "FJI\tFiji Islands\tOceania\tMelanesia\t18274\t1970\tFiji Islands\tRepublic\tSuva\t178.399\t-18.1149\n",
    "PHL\tPhilippines\tAsia\tSoutheast Asia\t300000\t1946\tPilipinas\tRepublic\tManila\t121.035\t14.5515\n",
    "FRO\tFaroe Islands\tEurope\tNordic Countries\t1399\tnull\tForoyar\tPart of Denmark\tTorshavn\t-6.91181\t61.8926\n",
    "GAB\tGabon\tAfrica\tCentral Africa\t267668\t1960\tLe Gabon\tRepublic\tLibreville\t9.45162\t0.38832\n",
    "GMB\tGambia\tAfrica\tWestern Africa\t11295\t1965\tThe Gambia\tRepublic\tBanjul\t-16.5885\t13.4495\n",
    "GEO\tGeorgia\tAsia\tMiddle East\t69700\t1991\tSakartvelo\tRepublic\tTbilisi\t44.793\t41.71\n",
    "GHA\tGhana\tAfrica\tWestern Africa\t238533\t1957\tGhana\tRepublic\tAccra\t-0.20795\t5.57045\n",
    "GIB\tGibraltar\tEurope\tSouthern Europe\t6\tnull\tGibraltar\tDependent Territory of the UK\tnull\tnull\tnull\n",
    "GRD\tGrenada\tNorth America\tCaribbean\t344\t1974\tGrenada\tConstitutional Monarchy\tSaint George's\t-61.7449\t12.0653\n",
    "GRL\tGreenland\tNorth America\tNorth America\t2166090\tnull\tKalaallit Nunaat/Gronland\tPart of Denmark\tNuuk\t-51.7214\t64.1836\n",
    "GUM\tGuam\tOceania\tMicronesia\t549\tnull\tGuam\tUS Territory\tAgana\t144.794\t13.4443\n",
    "GTM\tGuatemala\tNorth America\tCentral America\t108889\t1821\tGuatemala\tRepublic\tGuatemala City\t-90.5328\t14.6248\n",
    "GIN\tGuinea\tAfrica\tWestern Africa\t245857\t1958\tGuinee\tRepublic\tConakry\t-13.7\t9.51667\n",
    "GNB\tGuinea-Bissau\tAfrica\tWestern Africa\t36125\t1974\tGuine-Bissau\tRepublic\tBissau\t-15.1804\t11.8037\n",
    "GUY\tGuyana\tSouth America\tSouth America\t214969\t1966\tGuyana\tRepublic\tGeorgetown\t-58.1548\t6.80461\n",
    "HTI\tHaiti\tNorth America\tCaribbean\t27750\t1804\tHaiti/Dayti\tRepublic\tPort-au-Prince\t-72.3288\t18.5392\n",
    "HND\tHonduras\tNorth America\tCentral America\t112088\t1838\tHonduras\tRepublic\tTegucigalpa\t-87.4667\t15.1333\n",
    "IDN\tIndonesia\tAsia\tSoutheast Asia\t1904570\t1945\tIndonesia\tRepublic\tJakarta\t106.83\t-6.19752\n",
    "IND\tIndia\tAsia\tSouthern and Central Asia\t3287260\t1947\tBharat/India\tFederal Republic\tNew Delhi\t77.225\t28.6353\n",
    "IRQ\tIraq\tAsia\tMiddle East\t438317\t1932\tAl-´Iraq\tRepublic\tBaghdad\t44.394\t33.3302\n",
    "IRN\tIran\tAsia\tSouthern and Central Asia\t1648200\t1906\tIran\tIslamic Republic\tTehran\t51.4447\t35.6878\n",
    "IRL\tIreland\tEurope\tBritish Islands\t70273\t1921\tIreland/Eire\tRepublic\tDublin\t-6.26749\t53.3441\n",
    "ISL\tIceland\tEurope\tNordic Countries\t103000\t1944\tIsland\tRepublic\tReykjavik\t-21.8952\t64.1353\n",
    "ISR\tIsrael\tAsia\tMiddle East\t21056\t1948\tYisrael/Israil\tRepublic\tnull\t35.2035\t31.7717\n",
    "ITA\tItaly\tEurope\tSouthern Europe\t301316\t1861\tItalia\tRepublic\tRome\t12.4823\t41.8955\n",
    "AUT\tAustria\tEurope\tWestern Europe\t83859\t1918\tOsterreich\tFederal Republic\tVienna\t16.3798\t48.2201\n",
    "JAM\tJamaica\tNorth America\tCaribbean\t10990\t1962\tJamaica\tConstitutional Monarchy\tKingston\t-76.792\t17.9927\n",
    "JPN\tJapan\tAsia\tEastern Asia\t377829\t-660\tNihon/Nippon\tConstitutional Monarchy\tTokyo\t139.77\t35.67\n",
    "YEM\tYemen\tAsia\tMiddle East\t527968\t1918\tAl-Yaman\tRepublic\tSana'a\t44.2075\t15.352\n",
    "JOR\tJordan\tAsia\tMiddle East\t88946\t1946\tAl-Urdunn\tConstitutional Monarchy\tAmman\t35.9263\t31.9497\n",
    "KHM\tCambodia\tAsia\tSoutheast Asia\t181035\t1953\tKampuchea\tConstitutional Monarchy\tPhnom Penh\t104.874\t11.5556\n",
    "CMR\tCameroon\tAfrica\tCentral Africa\t475442\t1960\tCameroun/Cameroon\tRepublic\tYaounde\t11.5174\t3.8721\n",
    "CAN\tCanada\tNorth America\tNorth America\t9970610\t1867\tCanada\tConstitutional Monarchy, Federation\tOttawa\t-75.6919\t45.4215\n",
    "CPV\tCape Verde\tAfrica\tWestern Africa\t4033\t1975\tCabo Verde\tRepublic\tPraia\t-23.5087\t14.9218\n",
    "KAZ\tKazakhstan\tAsia\tSouthern and Central Asia\t2724900\t1991\tQazaqstan\tRepublic\tAstana\t71.4382\t51.1879\n",
    "KEN\tKenya\tAfrica\tEastern Africa\t580367\t1963\tKenya\tRepublic\tNairobi\t36.8126\t-1.27975\n",
    "CAF\tCentral African Republic\tAfrica\tCentral Africa\t622984\t1960\tCentrafrique/Be-Afrika\tRepublic\tBangui\t21.6407\t5.63056\n",
    "CHN\tChina\tAsia\tEastern Asia\t9572900\t-1523\tZhongquo\tPeople'sRepublic\tBeijing\t116.286\t40.0495\n",
    "KGZ\tKyrgyzstan\tAsia\tSouthern and Central Asia\t199900\t1991\tKyrgyzstan\tRepublic\tBishkek\t74.6057\t42.8851\n",
    "KIR\tKiribati\tOceania\tMicronesia\t726\t1979\tKiribati\tRepublic\tTarawa\t172.979\t1.32905\n",
    "COL\tColombia\tSouth America\tSouth America\t1138910\t1810\tColombia\tRepublic\tBogota\t-74.082\t4.60987\n",
    "COM\tComoros\tAfrica\tEastern Africa\t1862\t1975\tKomori/Comores\tRepublic\tMoroni\t43.2418\t-11.6986\n",
    "COG\tCongo\tAfrica\tCentral Africa\t342000\t1960\tCongo\tRepublic\tBrazzaville\t15.2662\t-4.2767\n",
    "COD\tCongo, The Democratic Republic of the\tAfrica\tCentral Africa\t2344860\t1960\tRepublique Democratique du Congo\tRepublic\tKinshasa\t15.3222\t-4.325\n",
    "PRK\tNorth Korea\tAsia\tEastern Asia\t120538\t1948\tChoson Minjujuui In´min Konghwaguk (Bukhan)\tSocialistic Republic\tPyongyang\t125.754\t39.0319\n",
    "KOR\tSouth Korea\tAsia\tEastern Asia\t99434\t1948\tTaehan Minguk (Namhan)\tRepublic\tSeoul\t126.957\t37.5323\n",
    "GRC\tGreece\tEurope\tSouthern Europe\t131626\t1830\tEllada\tRepublic\tAthens\t23.7166\t37.9792\n",
    "HRV\tCroatia\tEurope\tSouthern Europe\t56538\t1991\tHrvatska\tRepublic\tZagreb\t15.9614\t45.8069\n",
    "CUB\tCuba\tNorth America\tCaribbean\t110861\t1902\tCuba\tSocialistic Republic\tHavana\t-82.3667\t23.1333\n",
    "KWT\tKuwait\tAsia\tMiddle East\t17818\t1961\tAl-Kuwayt\tConstitutional Monarchy (Emirate)\tKuwait City\t47.9824\t29.3721\n",
    "CYP\tCyprus\tAsia\tMiddle East\t9251\t1960\tKypros/Kibris\tRepublic\tNicosia\t33.3736\t35.1676\n",
    "LAO\tLaos\tAsia\tSoutheast Asia\t236800\t1953\tLao\tRepublic\tVientiane\t102.177\t18.5826\n",
    "LVA\tLatvia\tEurope\tBaltic Countries\t64589\t1991\tLatvija\tRepublic\tRiga\t24.1048\t56.9465\n",
    "LSO\tLesotho\tAfrica\tSouthern Africa\t30355\t1966\tLesotho\tConstitutional Monarchy\tMaseru\t27.7167\t-29.5208\n",
    "Showing 100 out of 205 rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# economies\n",
    "econ_id\tcode\tyear\tincome_group\tgdp_percapita\tgross_savings\tinflation_rate\ttotal_investment\tunemployment_rate\texports\timports\n",
    "1\tAFG\t2010\tLow income\t539.667\t37.133\t2.179\t30.402\tnull\t46.394\t24.381\n",
    "2\tAFG\t2015\tLow income\t615.091\t21.466\t-1.549\t18.602\tnull\t-49.11\t-7.294\n",
    "3\tAGO\t2010\tUpper middle income\t3599.27\t23.534\t14.48\t14.433\tnull\t-3.266\t-21.076\n",
    "4\tAGO\t2015\tUpper middle income\t3876.2\t-0.425\t10.287\t9.552\tnull\t6.721\t-21.778\n",
    "5\tALB\t2010\tUpper middle income\t4098.13\t20.011\t3.605\t31.305\t14\t10.645\t-8.013\n",
    "6\tALB\t2015\tUpper middle income\t3943.22\t13.84\t1.896\t24.598\t17.1\t1.827\t0.574\n",
    "7\tARE\t2010\tHigh income\t34628.63\t27.073\t0.878\t27.372\tnull\t3.843\t-0.981\n",
    "8\tARE\t2015\tHigh income\t38649.91\t34.106\t4.07\t27.477\tnull\t7.32\t2.17\n",
    "9\tARG\t2010\tUpper middle income\t10412.95\t17.361\t10.461\t17.706\t7.75\t13.931\t39.877\n",
    "10\tARG\t2015\tUpper middle income\t14643.92\t14.111\tnull\t16.89\tnull\t-1.658\t3.105\n",
    "11\tARM\t2010\tLower middle income\t3121.78\t15.797\t7.274\t29.419\t19\t30.183\t4.09\n",
    "12\tARM\t2015\tLower middle income\t3520.95\t18.306\t3.731\t20.956\t18.5\t15.729\t-9.647\n",
    "13\tATG\t2010\tHigh income\t13531.78\t13.398\t3.37\tnull\tnull\t-3.241\t-14.113\n",
    "14\tATG\t2015\tHigh income\t15155.16\t18.754\t0.969\tnull\tnull\t6.026\t-24.307\n",
    "15\tAUS\t2010\tHigh income\t56362.84\t23.584\t2.863\t27.089\t5.208\t5.782\t15.208\n",
    "16\tAUS\t2015\tHigh income\t51363.9\t22.111\t1.461\t26.304\t6.058\t6.022\t1.99\n",
    "17\tAUT\t2010\tHigh income\t46757.13\t25.521\t1.694\t22.654\t4.8\t13.84\t11.989\n",
    "18\tAUT\t2015\tHigh income\t43749.55\t25.353\t0.81\t23.507\t5.75\t3.558\t3.382\n",
    "19\tAZE\t2010\tUpper middle income\t5847.26\t46.567\t5.666\t18.532\t6.048\t-1.792\t-1.459\n",
    "20\tAZE\t2015\tUpper middle income\t5396.41\t26.4\t4.049\t26.783\t6.048\t4.08\t0.186\n",
    "21\tBDI\t2010\tLow income\t242.84\t3.723\t6.496\t15.086\tnull\t33.917\t86.968\n",
    "22\tBDI\t2015\tLow income\t318.611\t-11.014\t5.553\t11\tnull\t-8.678\t-20.535\n",
    "23\tBEL\t2010\tHigh income\t44691.32\t24.456\t2.334\t22.692\t8.317\t8.121\t6.385\n",
    "24\tBEL\t2015\tHigh income\t40520.1\t23.653\t0.62\t23.211\t8.492\t1.154\t-0.093\n",
    "25\tBEN\t2010\tLow income\t734.278\t14.905\t2.179\t23.132\tnull\t-1.008\t9.397\n",
    "26\tBEN\t2015\tLow income\t763.882\t17.567\t0.271\t25.972\tnull\t-8.991\t-19.18\n",
    "27\tBFA\t2010\tLow income\t588.584\t15.788\t-0.608\t18.02\tnull\t175.541\t44.066\n",
    "28\tBFA\t2015\tLow income\t619.861\t5.31\t0.914\t13.34\tnull\t-14.969\t-0.968\n",
    "29\tBGD\t2010\tLower middle income\t807.531\t29.141\t9.365\t26.874\tnull\t18.81\t22.43\n",
    "30\tBGD\t2015\tLower middle income\t1292.93\t29.707\t6.161\t29.149\tnull\t8.787\t10.5\n",
    "31\tBGR\t2010\tUpper middle income\t6743.74\t20.845\t3.039\t22.568\t10.306\t11.046\t-0.932\n",
    "32\tBGR\t2015\tUpper middle income\t7017.11\t21.056\t-1.067\t21.19\t9.235\t5.737\t5.443\n",
    "33\tBHR\t2010\tHigh income\t20823.22\t36.206\t1.97\t27.286\t3.6\t2.083\t5.06\n",
    "34\tBHR\t2015\tHigh income\t24057.58\t21.955\t1.836\t24.372\tnull\t0.397\t-14.177\n",
    "35\tBHS\t2010\tHigh income\t22957.79\t15.141\t1.621\t25.214\t15.082\t2.549\t-4.618\n",
    "36\tBHS\t2015\tHigh income\t24309.57\t11.243\t1.879\t27.222\t13.379\t-9.46\t-5.834\n",
    "37\tBIH\t2010\tUpper middle income\t4404.37\t10.115\t2.123\t15.553\t27.202\t11.05\t-1.953\n",
    "38\tBIH\t2015\tUpper middle income\t4206.69\t10.217\t-1.018\t15.907\t27.7\t7.606\t2.101\n",
    "39\tBLR\t2010\tUpper middle income\t6023.15\t26.186\t7.743\t40.656\t0.827\t7.669\t12.225\n",
    "40\tBLR\t2015\tUpper middle income\t5941.24\t25.419\t13.523\t29.035\t0.912\t1.304\t-12.183\n",
    "41\tBLZ\t2010\tUpper middle income\t4321.29\t10.33\t0.918\t12.792\t13.502\t8.491\t6.964\n",
    "42\tBLZ\t2015\tUpper middle income\t4757.11\t12.155\t-0.862\t22.014\t10.114\t2.025\t6.743\n",
    "43\tBOL\t2010\tLower middle income\t1994.91\t24.969\t2.502\t17.007\t4.375\t28.526\t8.836\n",
    "44\tBOL\t2015\tLower middle income\t3099.22\t13.31\t4.061\t19.237\t4\t-8.427\t-2.965\n",
    "45\tBRA\t2010\tUpper middle income\t11297.84\t18.368\t5.039\t21.801\t8.556\t7.124\t34.532\n",
    "46\tBRA\t2015\tUpper middle income\t8810.5\t15.865\t9.03\t19.133\t8.3\t8.085\t-13.47\n",
    "47\tBRB\t2010\tHigh income\t16079.89\t8.12\t5.824\t13.564\t10.25\t4.769\t-0.489\n",
    "48\tBRB\t2015\tHigh income\t15808.09\t7.473\t-1.061\t13.391\t11.3\t6.952\t12.81\n",
    "49\tBRN\t2010\tHigh income\t35437.22\tnull\t0.216\t41.711\tnull\t11.705\t-3.108\n",
    "50\tBRN\t2015\tHigh income\t30994.98\tnull\t-0.423\t35.247\t6.9\t-13.765\t-24.068\n",
    "51\tBTN\t2010\tLower middle income\t1998.75\t44.674\t5.726\t66.906\t3.3\t-0.579\t25.991\n",
    "52\tBTN\t2015\tLower middle income\t2603.1\t32.024\t6.336\t60.362\t3.2\t1.505\t15.832\n",
    "53\tBWA\t2010\tUpper middle income\t6853.67\t32.162\t6.95\t35.353\tnull\t16.022\t6.481\n",
    "54\tBWA\t2015\tUpper middle income\t6780.97\t40.026\t3.054\t32.11\tnull\t-17.111\t0.37\n",
    "55\tCAF\t2010\tLow income\t456.564\t4.115\t1.491\t14.266\tnull\t10.301\t13.882\n",
    "56\tCAF\t2015\tLow income\t332.366\t4.882\t4.5\t13.923\tnull\t0.465\t12.874\n",
    "57\tCAN\t2010\tHigh income\t47512.68\t19.898\t1.769\t23.508\t8\t6.645\t13.774\n",
    "58\tCAN\t2015\tHigh income\t43349.62\t20.416\t1.132\t23.817\t6.9\t3.393\t0.336\n",
    "59\tCHE\t2010\tHigh income\t74570.66\t38.881\t0.684\t24.027\t3.516\t12.547\t7.882\n",
    "60\tCHE\t2015\tHigh income\t81410.02\t34.52\t-1.14\t22.995\t3.178\t2.168\t4.303\n",
    "61\tCHL\t2010\tHigh income\t12789.76\t24.839\t1.408\t23.144\t8.153\t2.357\t25.554\n",
    "62\tCHL\t2015\tHigh income\t13469.47\t21.306\t4.349\t23.291\t6.214\t-1.872\t-2.802\n",
    "63\tCHN\t2010\tUpper middle income\t4524.06\t51.802\t3.3\t47.881\t4.14\t25.641\t19.925\n",
    "64\tCHN\t2015\tUpper middle income\t8166.76\t47.457\t1.441\t44.748\t4.05\t-2.216\t-0.478\n",
    "65\tCIV\t2010\tLower middle income\t1195.44\t16.765\t1.366\t14.901\tnull\t-7.681\t1.856\n",
    "66\tCIV\t2015\tLower middle income\t1381.81\t16.82\t1.244\t17.804\tnull\t2.774\t19.427\n",
    "67\tCMR\t2010\tLower middle income\t1158.78\t17.524\t1.279\t20.281\tnull\t-4.699\t-1.617\n",
    "68\tCMR\t2015\tLower middle income\t1230.38\t17.088\t2.683\t21.336\tnull\t16.087\t2.879\n",
    "69\tCOD\t2010\tLow income\t292.955\t15.574\t23.464\t13.75\tnull\t54.001\t35.575\n",
    "70\tCOD\t2015\tLow income\t471.306\t16.495\t0.959\t20.403\tnull\t-4.995\t-3.984\n",
    "71\tCOG\t2010\tLower middle income\t3183.68\t27.651\t0.392\t19.848\tnull\t13.299\t21.024\n",
    "72\tCOG\t2015\tLower middle income\t1958.1\t-8.049\t2.742\t34.83\tnull\t-6.289\t2.196\n",
    "73\tCOL\t2010\tUpper middle income\t6305.29\t19.107\t2.272\t22.126\t11.792\t1.264\t10.845\n",
    "74\tCOL\t2015\tUpper middle income\t6047.97\t20.23\t4.99\t26.72\t8.925\t-3.225\t-6.321\n",
    "75\tCOM\t2010\tLow income\t789.466\t15.22\t3.899\t15.403\tnull\t8.524\t2.157\n",
    "76\tCOM\t2015\tLow income\t736.406\t19.221\t2\t18.411\tnull\t7.238\t-3.58\n",
    "77\tCPV\t2010\tLower middle income\t3413.26\t25.257\t2.079\t37.684\t10.7\t0.873\t-0.6\n",
    "78\tCPV\t2015\tLower middle income\t3001.29\t37.094\t0.125\t41.483\t10\t-4.248\t-19.31\n",
    "79\tCRI\t2010\tUpper middle income\t8300.69\t16.489\t5.665\t19.715\t7.345\t9.099\t18.711\n",
    "80\tCRI\t2015\tUpper middle income\t11435.97\t15.725\t0.802\t20.218\t9.249\t0.001\t4.629\n",
    "81\tCYP\t2010\tHigh income\t31262.53\t12.507\t2.557\t23.663\t6.283\t4.452\t7.965\n",
    "82\tCYP\t2015\tHigh income\t23105.4\t11.55\t-1.539\t13.956\t14.892\t0.032\t2.128\n",
    "83\tCZE\t2010\tHigh income\t19787.29\t23.588\t1.491\t27.174\t7.279\t14.847\t14.899\n",
    "84\tCZE\t2015\tHigh income\t17569.89\t28.266\t0.335\t27.357\t5.046\t7.703\t8.22\n",
    "85\tDEU\t2010\tHigh income\t42641.68\t25.241\t1.149\t19.626\t6.942\t14.534\t12.85\n",
    "86\tDEU\t2015\tHigh income\t41197.41\t27.571\t0.134\t19.243\t4.608\t5.161\t5.467\n",
    "87\tDJI\t2010\tLower middle income\t1306.54\t24.089\t3.954\t21.259\tnull\t1.368\t-20.199\n",
    "88\tDJI\t2015\tLower middle income\t1788.36\t19.039\t2.104\t50.82\tnull\t12.843\t31.171\n",
    "89\tDMA\t2010\tUpper middle income\t6975.99\t2.462\t2.81\t18.38\tnull\t11.788\t-6.675\n",
    "90\tDMA\t2015\tUpper middle income\t7311.65\t8.814\t-0.794\t16.787\tnull\t8.783\t5.578\n",
    "91\tDNK\t2010\tHigh income\t58177.16\t24.639\t2.311\t18.076\t7.483\t2.94\t0.541\n",
    "92\tDNK\t2015\tHigh income\t53237.28\t28.91\t0.452\t19.755\t6.192\t1.833\t1.261\n",
    "93\tDOM\t2010\tUpper middle income\t5685.77\t18.745\t6.33\t26.178\t5.001\t12.363\t14.755\n",
    "94\tDOM\t2015\tUpper middle income\t6833.24\t21.447\t0.837\t23.494\t5.94\t7.284\t10.571\n",
    "95\tDZA\t2010\tUpper middle income\t4480.72\t49.869\t3.913\t42.328\t9.961\t-3.218\t-1.781\n",
    "96\tDZA\t2015\tUpper middle income\t4123.3\t34.738\t4.784\t51.299\t11.214\t5.733\t-7.507\n",
    "97\tECU\t2010\tUpper middle income\t4633.25\t25.757\t3.552\t28.037\t5.019\t1.109\t14.283\n",
    "98\tECU\t2015\tUpper middle income\t6153.8\t24.657\t3.97\t26.854\t4.77\t2.195\t-7.794\n",
    "99\tEGY\t2010\tLower middle income\t2921.76\t19.421\t11.69\t21.298\t9.21\t3.628\t11.663\n",
    "100\tEGY\t2015\tLower middle income\t3731.18\t10.62\t10.994\t14.289\t12.859\t12.456\t10.42\n",
    "Showing 100 out of 380 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cities\n",
    "name\tcountry_code\tcity_proper_pop\tmetroarea_pop\turbanarea_pop\n",
    "Abidjan\tCIV\t4765000\tnull\t4765000\n",
    "Abu Dhabi\tARE\t1145000\tnull\t1145000\n",
    "Abuja\tNGA\t1235880\t6000000\t1235880\n",
    "Accra\tGHA\t2070463\t4010054\t2070463\n",
    "Addis Ababa\tETH\t3103673\t4567857\t3103673\n",
    "Ahmedabad\tIND\t5570585\tnull\t5570585\n",
    "Alexandria\tEGY\t4616625\tnull\t4616625\n",
    "Algiers\tDZA\t3415811\t5000000\t3415811\n",
    "Almaty\tKAZ\t1703481\tnull\t1703481\n",
    "Ankara\tTUR\t5271000\t4585000\t5271000\n",
    "Auckland\tNZL\t1495000\t1614300\t1495000\n",
    "Baghdad\tIRQ\t7180889\tnull\t7180889\n",
    "Baku\tAZE\t3202300\t4308740\t3202300\n",
    "Bandung\tIDN\t2575478\t6965655\t2575478\n",
    "Bangkok\tTHA\t8280925\t14998000\t8280925\n",
    "Barcelona\tESP\t1604555\t5375774\t1604555\n",
    "Barranquilla\tCOL\t1386865\t2370753\t1386865\n",
    "Basra\tIRQ\t2750000\tnull\t2750000\n",
    "Beijing\tCHN\t21516000\t24900000\t21516000\n",
    "Belo Horizonte\tBRA\t2502557\t5156217\t2502557\n",
    "Bengaluru\tIND\t8425970\t9807000\t8425970\n",
    "Berlin\tDEU\t3517424\t5871022\t3517424\n",
    "Bhopal\tIND\t1798218\t1864389\t1798218\n",
    "Birmingham\tGBR\t1111300\t3683000\t1111300\n",
    "Bogota\tCOL\t7878783\t9800000\t7878783\n",
    "Brasilia\tBRA\t2556149\t3919864\t2556149\n",
    "Brazzaville\tCOG\t1827000\tnull\t1827000\n",
    "Brisbane\tAUS\t1180285\t2349699\t1180285\n",
    "Bucharest\tROM\t1883425\t2272163\t1883425\n",
    "Budapest\tHUN\t1759407\t2927944\t1759407\n",
    "Buenos Aires\tARG\t3054300\t14122000\t3054300\n",
    "Busan\tKOR\t3510833\t8202239\t3510833\n",
    "Cairo\tEGY\t10230350\t18290000\t10230350\n",
    "Calgary\tCAN\t1235171\t1214839\t1235171\n",
    "Cali\tCOL\t2400653\t3400000\t2400653\n",
    "Caloocan\tPHL\t1583978\tnull\t1583978\n",
    "Campinas\tBRA\t1164098\t3094181\t1164098\n",
    "Cape Town\tZAF\t3740026\tnull\t3740026\n",
    "Caracas\tVEN\t1943901\t2923959\t1943901\n",
    "Casablanca\tMAR\t5117832\t6861739\t5117832\n",
    "Changchun\tCHN\t3815270\t7674439\t3815270\n",
    "Changsha\tCHN\t7044118\tnull\t7044118\n",
    "Chaozhou\tCHN\t2669844\tnull\t2669844\n",
    "Chengdu\tCHN\t4741929\t10376000\t4741929\n",
    "Chennai\tIND\t7088000\tnull\t7088000\n",
    "Chicago\tUSA\t2695598\t9156000\t2695598\n",
    "Chittagong\tBGD\t2581643\t4009423\t2581643\n",
    "Chongqing\tCHN\t8189800\t52100100\t8189800\n",
    "Cologne\tDEU\t1057327\t3573500\t1057327\n",
    "Cordoba\tARG\t1330023\t1528000\t1330023\n",
    "Curitiba\tBRA\t1879355\t3400000\t1879355\n",
    "Daegu\tKOR\t2492994\tnull\t2492994\n",
    "Daejeon\tKOR\t1535028\tnull\t1535028\n",
    "Dakar\tSEN\t1146053\t2452656\t1146053\n",
    "Dalian\tCHN\t2146099\t5935638\t2146099\n",
    "Dallas\tUSA\t1317929\t7233323\t1317929\n",
    "Dar es Salaam\tTZA\t4364541\tnull\t4364541\n",
    "Davao City\tPHL\t1632991\t2516216\t1632991\n",
    "Delhi\tIND\t16787940\t24998000\t16787940\n",
    "Dhaka\tBGD\t14543124\tnull\t14543124\n",
    "Dongguan\tCHN\t8220207\tnull\t8220207\n",
    "Douala\tCMR\t2446945\tnull\t2446945\n",
    "Dubai\tARE\t2643410\tnull\t2643410\n",
    "Durban\tZAF\t3442361\tnull\t3442361\n",
    "Ekurhuleni\tZAF\t3178470\tnull\t3178470\n",
    "Faisalabad\tPAK\t6480765\t3675000\t6480765\n",
    "Fez\tMAR\t1112072\tnull\t1112072\n",
    "Fortaleza\tBRA\t2609716\t4019213\t2609716\n",
    "Foshan\tCHN\t6151622\tnull\t6151622\n",
    "Fukuoka\tJPN\t1483052\t5590378\t1483052\n",
    "Fuzhou\tCHN\t7115369\tnull\t7115369\n",
    "Giza\tEGY\t4239988\tnull\t4239988\n",
    "Guadalajara\tMEX\t1495189\t4424252\t1495189\n",
    "Guangzhou\tCHN\t14043500\t44259000\t14043500\n",
    "Guatemala City\tGTM\t2110100\t4500000\t2110100\n",
    "Guayaquil\tECU\t3600000\t5000000\t3600000\n",
    "Gujranwala\tPAK\t2700003\tnull\t2700003\n",
    "Hamburg\tDEU\t1787408\tnull\t1787408\n",
    "Hangzhou\tCHN\t3560391\tnull\t3560391\n",
    "Hanoi\tVNM\t6844100\tnull\t6844100\n",
    "Harare\tZWE\t1606000\tnull\t1606000\n",
    "Harbin\tCHN\t4280701\t10635971\t4280701\n",
    "Havana\tCUB\t2106146\tnull\t2106146\n",
    "Hefei\tCHN\t3352076\tnull\t3352076\n",
    "Hiroshima\tJPN\t1196274\tnull\t1196274\n",
    "Ho Chi Minh City\tVNM\t7681700\tnull\t7681700\n",
    "Hong Kong\tCHN\t7374900\tnull\t7374900\n",
    "Houston\tUSA\t2489558\t6490180\t2489558\n",
    "Hyderabad (India)\tIND\t7859250\tnull\t7859250\n",
    "Hyderabad\tPAK\t3429471\tnull\t3429471\n",
    "Ibadan\tNGA\t1338659\t2837000\t1338659\n",
    "Incheon\tKOR\t2978367\tnull\t2978367\n",
    "Isfahan\tIRN\t2243249\tnull\t2243249\n",
    "Islamabad\tPAK\t1900000\t2200000\t1900000\n",
    "Istanbul\tTUR\t14025000\t13520000\t14025000\n",
    "Izmir\tTUR\t4168000\t3019000\t4168000\n",
    "Jaipur\tIND\t3073350\tnull\t3073350\n",
    "Jakarta\tIDN\t10075310\t30539000\t10075310\n",
    "Jeddah\tSAU\t3456259\tnull\t3456259\n",
    "Jinan\tCHN\t2009273\t5853196\t2009273\n",
    "Showing 100 out of 236 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first join\n",
    "Throughout this course, you'll be working with the countries database, which contains information about the most populous world cities in the world, along with country-level economic, population, and geographic data. The database also contains information on languages spoken in each country.\n",
    "\n",
    "You can see the different tables in this database to get a sense of what they contain by clicking on the corresponding tabs. Click through them and familiarize yourself with the fields that seem to be shared across tables before you continue with the course.\n",
    "\n",
    "In this exercise, you'll use the cities and countries tables to build your first inner join. You'll start off by selecting all columns in step 1, performing your join in step 2, and then refining your join to choose specific columns in step 3.\n",
    "\n",
    "SELECT * \n",
    "FROM cities\n",
    "-- Inner join to countries\n",
    "INNER JOIN countries\n",
    "\n",
    "-- Match on country codes\n",
    "ON cities.country_code = countries.code;\n",
    "\n",
    "\n",
    "SELECT cities.name AS city, countries.name AS country, countries.region\n",
    "FROM cities\n",
    "INNER JOIN countries\n",
    "ON cities.country_code = countries.code;\n",
    "\n",
    "Great work! Writing out full table names when joining tables creates a lot of extra code. In the next exercise, you'll explore how you can do more aliasing to limit the amount of writing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# economies\n",
    "econ_id\tcode\tyear\tincome_group\tgdp_percapita\tgross_savings\tinflation_rate\ttotal_investment\tunemployment_rate\texports\timports\n",
    "1\tAFG\t2010\tLow income\t539.667\t37.133\t2.179\t30.402\tnull\t46.394\t24.381\n",
    "2\tAFG\t2015\tLow income\t615.091\t21.466\t-1.549\t18.602\tnull\t-49.11\t-7.294\n",
    "3\tAGO\t2010\tUpper middle income\t3599.27\t23.534\t14.48\t14.433\tnull\t-3.266\t-21.076\n",
    "4\tAGO\t2015\tUpper middle income\t3876.2\t-0.425\t10.287\t9.552\tnull\t6.721\t-21.778\n",
    "5\tALB\t2010\tUpper middle income\t4098.13\t20.011\t3.605\t31.305\t14\t10.645\t-8.013\n",
    "6\tALB\t2015\tUpper middle income\t3943.22\t13.84\t1.896\t24.598\t17.1\t1.827\t0.574\n",
    "7\tARE\t2010\tHigh income\t34628.63\t27.073\t0.878\t27.372\tnull\t3.843\t-0.981\n",
    "8\tARE\t2015\tHigh income\t38649.91\t34.106\t4.07\t27.477\tnull\t7.32\t2.17\n",
    "9\tARG\t2010\tUpper middle income\t10412.95\t17.361\t10.461\t17.706\t7.75\t13.931\t39.877\n",
    "10\tARG\t2015\tUpper middle income\t14643.92\t14.111\tnull\t16.89\tnull\t-1.658\t3.105\n",
    "11\tARM\t2010\tLower middle income\t3121.78\t15.797\t7.274\t29.419\t19\t30.183\t4.09\n",
    "12\tARM\t2015\tLower middle income\t3520.95\t18.306\t3.731\t20.956\t18.5\t15.729\t-9.647\n",
    "13\tATG\t2010\tHigh income\t13531.78\t13.398\t3.37\tnull\tnull\t-3.241\t-14.113\n",
    "14\tATG\t2015\tHigh income\t15155.16\t18.754\t0.969\tnull\tnull\t6.026\t-24.307\n",
    "15\tAUS\t2010\tHigh income\t56362.84\t23.584\t2.863\t27.089\t5.208\t5.782\t15.208\n",
    "16\tAUS\t2015\tHigh income\t51363.9\t22.111\t1.461\t26.304\t6.058\t6.022\t1.99\n",
    "17\tAUT\t2010\tHigh income\t46757.13\t25.521\t1.694\t22.654\t4.8\t13.84\t11.989\n",
    "18\tAUT\t2015\tHigh income\t43749.55\t25.353\t0.81\t23.507\t5.75\t3.558\t3.382\n",
    "19\tAZE\t2010\tUpper middle income\t5847.26\t46.567\t5.666\t18.532\t6.048\t-1.792\t-1.459\n",
    "20\tAZE\t2015\tUpper middle income\t5396.41\t26.4\t4.049\t26.783\t6.048\t4.08\t0.186\n",
    "21\tBDI\t2010\tLow income\t242.84\t3.723\t6.496\t15.086\tnull\t33.917\t86.968\n",
    "22\tBDI\t2015\tLow income\t318.611\t-11.014\t5.553\t11\tnull\t-8.678\t-20.535\n",
    "23\tBEL\t2010\tHigh income\t44691.32\t24.456\t2.334\t22.692\t8.317\t8.121\t6.385\n",
    "24\tBEL\t2015\tHigh income\t40520.1\t23.653\t0.62\t23.211\t8.492\t1.154\t-0.093\n",
    "25\tBEN\t2010\tLow income\t734.278\t14.905\t2.179\t23.132\tnull\t-1.008\t9.397\n",
    "26\tBEN\t2015\tLow income\t763.882\t17.567\t0.271\t25.972\tnull\t-8.991\t-19.18\n",
    "27\tBFA\t2010\tLow income\t588.584\t15.788\t-0.608\t18.02\tnull\t175.541\t44.066\n",
    "28\tBFA\t2015\tLow income\t619.861\t5.31\t0.914\t13.34\tnull\t-14.969\t-0.968\n",
    "29\tBGD\t2010\tLower middle income\t807.531\t29.141\t9.365\t26.874\tnull\t18.81\t22.43\n",
    "30\tBGD\t2015\tLower middle income\t1292.93\t29.707\t6.161\t29.149\tnull\t8.787\t10.5\n",
    "31\tBGR\t2010\tUpper middle income\t6743.74\t20.845\t3.039\t22.568\t10.306\t11.046\t-0.932\n",
    "32\tBGR\t2015\tUpper middle income\t7017.11\t21.056\t-1.067\t21.19\t9.235\t5.737\t5.443\n",
    "33\tBHR\t2010\tHigh income\t20823.22\t36.206\t1.97\t27.286\t3.6\t2.083\t5.06\n",
    "34\tBHR\t2015\tHigh income\t24057.58\t21.955\t1.836\t24.372\tnull\t0.397\t-14.177\n",
    "35\tBHS\t2010\tHigh income\t22957.79\t15.141\t1.621\t25.214\t15.082\t2.549\t-4.618\n",
    "36\tBHS\t2015\tHigh income\t24309.57\t11.243\t1.879\t27.222\t13.379\t-9.46\t-5.834\n",
    "37\tBIH\t2010\tUpper middle income\t4404.37\t10.115\t2.123\t15.553\t27.202\t11.05\t-1.953\n",
    "38\tBIH\t2015\tUpper middle income\t4206.69\t10.217\t-1.018\t15.907\t27.7\t7.606\t2.101\n",
    "39\tBLR\t2010\tUpper middle income\t6023.15\t26.186\t7.743\t40.656\t0.827\t7.669\t12.225\n",
    "40\tBLR\t2015\tUpper middle income\t5941.24\t25.419\t13.523\t29.035\t0.912\t1.304\t-12.183\n",
    "41\tBLZ\t2010\tUpper middle income\t4321.29\t10.33\t0.918\t12.792\t13.502\t8.491\t6.964\n",
    "42\tBLZ\t2015\tUpper middle income\t4757.11\t12.155\t-0.862\t22.014\t10.114\t2.025\t6.743\n",
    "43\tBOL\t2010\tLower middle income\t1994.91\t24.969\t2.502\t17.007\t4.375\t28.526\t8.836\n",
    "44\tBOL\t2015\tLower middle income\t3099.22\t13.31\t4.061\t19.237\t4\t-8.427\t-2.965\n",
    "45\tBRA\t2010\tUpper middle income\t11297.84\t18.368\t5.039\t21.801\t8.556\t7.124\t34.532\n",
    "46\tBRA\t2015\tUpper middle income\t8810.5\t15.865\t9.03\t19.133\t8.3\t8.085\t-13.47\n",
    "47\tBRB\t2010\tHigh income\t16079.89\t8.12\t5.824\t13.564\t10.25\t4.769\t-0.489\n",
    "48\tBRB\t2015\tHigh income\t15808.09\t7.473\t-1.061\t13.391\t11.3\t6.952\t12.81\n",
    "49\tBRN\t2010\tHigh income\t35437.22\tnull\t0.216\t41.711\tnull\t11.705\t-3.108\n",
    "50\tBRN\t2015\tHigh income\t30994.98\tnull\t-0.423\t35.247\t6.9\t-13.765\t-24.068\n",
    "51\tBTN\t2010\tLower middle income\t1998.75\t44.674\t5.726\t66.906\t3.3\t-0.579\t25.991\n",
    "52\tBTN\t2015\tLower middle income\t2603.1\t32.024\t6.336\t60.362\t3.2\t1.505\t15.832\n",
    "53\tBWA\t2010\tUpper middle income\t6853.67\t32.162\t6.95\t35.353\tnull\t16.022\t6.481\n",
    "54\tBWA\t2015\tUpper middle income\t6780.97\t40.026\t3.054\t32.11\tnull\t-17.111\t0.37\n",
    "55\tCAF\t2010\tLow income\t456.564\t4.115\t1.491\t14.266\tnull\t10.301\t13.882\n",
    "56\tCAF\t2015\tLow income\t332.366\t4.882\t4.5\t13.923\tnull\t0.465\t12.874\n",
    "57\tCAN\t2010\tHigh income\t47512.68\t19.898\t1.769\t23.508\t8\t6.645\t13.774\n",
    "58\tCAN\t2015\tHigh income\t43349.62\t20.416\t1.132\t23.817\t6.9\t3.393\t0.336\n",
    "59\tCHE\t2010\tHigh income\t74570.66\t38.881\t0.684\t24.027\t3.516\t12.547\t7.882\n",
    "60\tCHE\t2015\tHigh income\t81410.02\t34.52\t-1.14\t22.995\t3.178\t2.168\t4.303\n",
    "61\tCHL\t2010\tHigh income\t12789.76\t24.839\t1.408\t23.144\t8.153\t2.357\t25.554\n",
    "62\tCHL\t2015\tHigh income\t13469.47\t21.306\t4.349\t23.291\t6.214\t-1.872\t-2.802\n",
    "63\tCHN\t2010\tUpper middle income\t4524.06\t51.802\t3.3\t47.881\t4.14\t25.641\t19.925\n",
    "64\tCHN\t2015\tUpper middle income\t8166.76\t47.457\t1.441\t44.748\t4.05\t-2.216\t-0.478\n",
    "65\tCIV\t2010\tLower middle income\t1195.44\t16.765\t1.366\t14.901\tnull\t-7.681\t1.856\n",
    "66\tCIV\t2015\tLower middle income\t1381.81\t16.82\t1.244\t17.804\tnull\t2.774\t19.427\n",
    "67\tCMR\t2010\tLower middle income\t1158.78\t17.524\t1.279\t20.281\tnull\t-4.699\t-1.617\n",
    "68\tCMR\t2015\tLower middle income\t1230.38\t17.088\t2.683\t21.336\tnull\t16.087\t2.879\n",
    "69\tCOD\t2010\tLow income\t292.955\t15.574\t23.464\t13.75\tnull\t54.001\t35.575\n",
    "70\tCOD\t2015\tLow income\t471.306\t16.495\t0.959\t20.403\tnull\t-4.995\t-3.984\n",
    "71\tCOG\t2010\tLower middle income\t3183.68\t27.651\t0.392\t19.848\tnull\t13.299\t21.024\n",
    "72\tCOG\t2015\tLower middle income\t1958.1\t-8.049\t2.742\t34.83\tnull\t-6.289\t2.196\n",
    "73\tCOL\t2010\tUpper middle income\t6305.29\t19.107\t2.272\t22.126\t11.792\t1.264\t10.845\n",
    "74\tCOL\t2015\tUpper middle income\t6047.97\t20.23\t4.99\t26.72\t8.925\t-3.225\t-6.321\n",
    "75\tCOM\t2010\tLow income\t789.466\t15.22\t3.899\t15.403\tnull\t8.524\t2.157\n",
    "76\tCOM\t2015\tLow income\t736.406\t19.221\t2\t18.411\tnull\t7.238\t-3.58\n",
    "77\tCPV\t2010\tLower middle income\t3413.26\t25.257\t2.079\t37.684\t10.7\t0.873\t-0.6\n",
    "78\tCPV\t2015\tLower middle income\t3001.29\t37.094\t0.125\t41.483\t10\t-4.248\t-19.31\n",
    "79\tCRI\t2010\tUpper middle income\t8300.69\t16.489\t5.665\t19.715\t7.345\t9.099\t18.711\n",
    "80\tCRI\t2015\tUpper middle income\t11435.97\t15.725\t0.802\t20.218\t9.249\t0.001\t4.629\n",
    "81\tCYP\t2010\tHigh income\t31262.53\t12.507\t2.557\t23.663\t6.283\t4.452\t7.965\n",
    "82\tCYP\t2015\tHigh income\t23105.4\t11.55\t-1.539\t13.956\t14.892\t0.032\t2.128\n",
    "83\tCZE\t2010\tHigh income\t19787.29\t23.588\t1.491\t27.174\t7.279\t14.847\t14.899\n",
    "84\tCZE\t2015\tHigh income\t17569.89\t28.266\t0.335\t27.357\t5.046\t7.703\t8.22\n",
    "85\tDEU\t2010\tHigh income\t42641.68\t25.241\t1.149\t19.626\t6.942\t14.534\t12.85\n",
    "86\tDEU\t2015\tHigh income\t41197.41\t27.571\t0.134\t19.243\t4.608\t5.161\t5.467\n",
    "87\tDJI\t2010\tLower middle income\t1306.54\t24.089\t3.954\t21.259\tnull\t1.368\t-20.199\n",
    "88\tDJI\t2015\tLower middle income\t1788.36\t19.039\t2.104\t50.82\tnull\t12.843\t31.171\n",
    "89\tDMA\t2010\tUpper middle income\t6975.99\t2.462\t2.81\t18.38\tnull\t11.788\t-6.675\n",
    "90\tDMA\t2015\tUpper middle income\t7311.65\t8.814\t-0.794\t16.787\tnull\t8.783\t5.578\n",
    "91\tDNK\t2010\tHigh income\t58177.16\t24.639\t2.311\t18.076\t7.483\t2.94\t0.541\n",
    "92\tDNK\t2015\tHigh income\t53237.28\t28.91\t0.452\t19.755\t6.192\t1.833\t1.261\n",
    "93\tDOM\t2010\tUpper middle income\t5685.77\t18.745\t6.33\t26.178\t5.001\t12.363\t14.755\n",
    "94\tDOM\t2015\tUpper middle income\t6833.24\t21.447\t0.837\t23.494\t5.94\t7.284\t10.571\n",
    "95\tDZA\t2010\tUpper middle income\t4480.72\t49.869\t3.913\t42.328\t9.961\t-3.218\t-1.781\n",
    "96\tDZA\t2015\tUpper middle income\t4123.3\t34.738\t4.784\t51.299\t11.214\t5.733\t-7.507\n",
    "97\tECU\t2010\tUpper middle income\t4633.25\t25.757\t3.552\t28.037\t5.019\t1.109\t14.283\n",
    "98\tECU\t2015\tUpper middle income\t6153.8\t24.657\t3.97\t26.854\t4.77\t2.195\t-7.794\n",
    "99\tEGY\t2010\tLower middle income\t2921.76\t19.421\t11.69\t21.298\t9.21\t3.628\t11.663\n",
    "100\tEGY\t2015\tLower middle income\t3731.18\t10.62\t10.994\t14.289\t12.859\t12.456\t10.42\n",
    "Showing 100 out of 380 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining with aliased tables\n",
    "Recall from the video that instead of writing full table names in queries, you can use table aliasing as a shortcut. The alias can be used in other parts of your query, such as the SELECT statement!\n",
    "\n",
    "You also learned that when you SELECT fields, a field can be ambiguous. For example, imagine two tables, apples and oranges, both containing a column called color. You need to use the syntax apples.color or oranges.color in your SELECT statement to point SQL to the correct table. Without this, you would get the following error:\n",
    "\n",
    "  column reference \"color\" is ambiguous\n",
    "In this exercise, you'll practice joining with aliased tables. You'll use data from both the countries and economies tables to examine the inflation rate in 2010 and 2015.\n",
    "\n",
    "When writing joins, many SQL users prefer to write the SELECT statement after writing the join code, in case the SELECT statement requires using table aliases.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Start with your inner join in line 5; join the tables countries AS c (left) with economies (right), aliasing economies AS e.\n",
    "Next, use code as your joining field in line 7; do not use the USING command here.\n",
    "Lastly, select the following columns in order in line 2: code from the countries table (aliased as country_code), name, year, and inflation_rate\n",
    "\n",
    "SELECT c.code AS country_code, c.name, e.year, e.inflation_rate\n",
    "FROM countries AS c\n",
    "INNER JOIN economies AS e\n",
    "ON c.code = e.code;\n",
    "/*\n",
    "Check the highlighted code. The checker expected to find c.code = e.code in there.\n",
    "*/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining with aliased tables\n",
    "Recall from the video that instead of writing full table names in queries, you can use table aliasing as a shortcut. The alias can be used in other parts of your query, such as the SELECT statement!\n",
    "\n",
    "You also learned that when you SELECT fields, a field can be ambiguous. For example, imagine two tables, apples and oranges, both containing a column called color. You need to use the syntax apples.color or oranges.color in your SELECT statement to point SQL to the correct table. Without this, you would get the following error:\n",
    "\n",
    "  column reference \"color\" is ambiguous\n",
    "In this exercise, you'll practice joining with aliased tables. You'll use data from both the countries and economies tables to examine the inflation rate in 2010 and 2015.\n",
    "\n",
    "When writing joins, many SQL users prefer to write the SELECT statement after writing the join code, in case the SELECT statement requires using table aliases.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Start with your inner join in line 5; join the tables countries AS c (left) with economies (right), aliasing economies AS e.\n",
    "Next, use code as your joining field in line 7; do not use the USING command here.\n",
    "Lastly, select the following columns in order in line 2: code from the countries table (aliased as country_code), name, year, and inflation_rate.\n",
    "\n",
    "SELECT c.code AS country_code, c.name, e.year, e.inflation_rate\n",
    "FROM countries AS c\n",
    "INNER JOIN economies AS e\n",
    "ON c.code = e.code;\n",
    "/*\n",
    "Check the highlighted code. The checker expected to find c.code = e.code in there.\n",
    "*/\n",
    "\n",
    "Nicely done! Notice that only the code field is ambiguous, so it requires a table name or alias before it. All the other fields (name, year, and inflation_rate) do not occur in more than one table name, so do not require table names or aliasing in the SELECT statement. Using table aliases takes some getting used to, but it will save you a lot of typing, especially when your query involves joining tables!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING in action\n",
    "In the previous exercises, you performed your joins using the ON keyword. Recall that when both the field names being joined on are the same, you can take advantage of the USING clause.\n",
    "\n",
    "You'll now explore the languages table from our database. Which languages are official languages, and which ones are unofficial?\n",
    "\n",
    "You'll employ USING to simplify your query as you explore this question.\n",
    "\n",
    "SELECT c.name AS country, l.name AS language, official\n",
    "FROM countries AS c\n",
    "INNER JOIN languages AS l\n",
    "USING (code);\n",
    "\n",
    "Great work! It looks like Afghanistan has multiple official and unofficial languages! A parting word of caution when using USING: columns can sometimes have the same name but actually contain vastly different data. Always remember to check what you are joining on by displaying and viewing your data first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relationships in our database\n",
    "Now that you know more about the different types of relationships that can exist between tables, it's time to examine a few relationships in the countries database!\n",
    "\n",
    "To answer questions about table relationships, you can explore the tables displayed as tabs in your console.\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "Question\n",
    "What best describes the relationship between code in the countries table and country_code in the cities table?\n",
    "\n",
    "Possible answers\n",
    "\n",
    "\n",
    "This is a many-to-many relationship.\n",
    "\n",
    "This is a one-to-one relationship.\n",
    "\n",
    "ans: This is a one-to-many relationship.\n",
    "\n",
    "Question\n",
    "Which of these options best describes the relationship between the countries table and the languages table?\n",
    "\n",
    "Possible answers\n",
    "\n",
    "\n",
    "ans: This is a one-to-many relationship.\n",
    "\n",
    "This is a many-to-many relationship.\n",
    "\n",
    "This is a one-to-one relationship.\n",
    "\n",
    "\n",
    "That's right! Recall the example from the video: Belgium has three official languages, French, German, and Dutch. Conversely, languages can be official in many countries: Dutch is an official language of both the Netherlands and Belgium, but not Germany. Because of the many types of relationships tables can have, there are many ways to join data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "world database\n",
    "economies table\n",
    "\n",
    "econ_id\tcode\tyear\tincome_group\tgdp_percapita\tgross_savings\tinflation_rate\ttotal_investment\tunemployment_rate\texports\timports\n",
    "1\tAFG\t2010\tLow income\t539.667\t37.133\t2.179\t30.402\tnull\t46.394\t24.381\n",
    "2\tAFG\t2015\tLow income\t615.091\t21.466\t-1.549\t18.602\tnull\t-49.11\t-7.294\n",
    "3\tAGO\t2010\tUpper middle income\t3599.27\t23.534\t14.48\t14.433\tnull\t-3.266\t-21.076\n",
    "4\tAGO\t2015\tUpper middle income\t3876.2\t-0.425\t10.287\t9.552\tnull\t6.721\t-21.778\n",
    "5\tALB\t2010\tUpper middle income\t4098.13\t20.011\t3.605\t31.305\t14\t10.645\t-8.013\n",
    "6\tALB\t2015\tUpper middle income\t3943.22\t13.84\t1.896\t24.598\t17.1\t1.827\t0.574\n",
    "7\tARE\t2010\tHigh income\t34628.63\t27.073\t0.878\t27.372\tnull\t3.843\t-0.981\n",
    "8\tARE\t2015\tHigh income\t38649.91\t34.106\t4.07\t27.477\tnull\t7.32\t2.17\n",
    "9\tARG\t2010\tUpper middle income\t10412.95\t17.361\t10.461\t17.706\t7.75\t13.931\t39.877\n",
    "10\tARG\t2015\tUpper middle income\t14643.92\t14.111\tnull\t16.89\tnull\t-1.658\t3.105\n",
    "11\tARM\t2010\tLower middle income\t3121.78\t15.797\t7.274\t29.419\t19\t30.183\t4.09\n",
    "12\tARM\t2015\tLower middle income\t3520.95\t18.306\t3.731\t20.956\t18.5\t15.729\t-9.647\n",
    "13\tATG\t2010\tHigh income\t13531.78\t13.398\t3.37\tnull\tnull\t-3.241\t-14.113\n",
    "14\tATG\t2015\tHigh income\t15155.16\t18.754\t0.969\tnull\tnull\t6.026\t-24.307\n",
    "15\tAUS\t2010\tHigh income\t56362.84\t23.584\t2.863\t27.089\t5.208\t5.782\t15.208\n",
    "16\tAUS\t2015\tHigh income\t51363.9\t22.111\t1.461\t26.304\t6.058\t6.022\t1.99\n",
    "17\tAUT\t2010\tHigh income\t46757.13\t25.521\t1.694\t22.654\t4.8\t13.84\t11.989\n",
    "18\tAUT\t2015\tHigh income\t43749.55\t25.353\t0.81\t23.507\t5.75\t3.558\t3.382\n",
    "19\tAZE\t2010\tUpper middle income\t5847.26\t46.567\t5.666\t18.532\t6.048\t-1.792\t-1.459\n",
    "20\tAZE\t2015\tUpper middle income\t5396.41\t26.4\t4.049\t26.783\t6.048\t4.08\t0.186\n",
    "21\tBDI\t2010\tLow income\t242.84\t3.723\t6.496\t15.086\tnull\t33.917\t86.968\n",
    "22\tBDI\t2015\tLow income\t318.611\t-11.014\t5.553\t11\tnull\t-8.678\t-20.535\n",
    "23\tBEL\t2010\tHigh income\t44691.32\t24.456\t2.334\t22.692\t8.317\t8.121\t6.385\n",
    "24\tBEL\t2015\tHigh income\t40520.1\t23.653\t0.62\t23.211\t8.492\t1.154\t-0.093\n",
    "25\tBEN\t2010\tLow income\t734.278\t14.905\t2.179\t23.132\tnull\t-1.008\t9.397\n",
    "26\tBEN\t2015\tLow income\t763.882\t17.567\t0.271\t25.972\tnull\t-8.991\t-19.18\n",
    "27\tBFA\t2010\tLow income\t588.584\t15.788\t-0.608\t18.02\tnull\t175.541\t44.066\n",
    "28\tBFA\t2015\tLow income\t619.861\t5.31\t0.914\t13.34\tnull\t-14.969\t-0.968\n",
    "29\tBGD\t2010\tLower middle income\t807.531\t29.141\t9.365\t26.874\tnull\t18.81\t22.43\n",
    "30\tBGD\t2015\tLower middle income\t1292.93\t29.707\t6.161\t29.149\tnull\t8.787\t10.5\n",
    "31\tBGR\t2010\tUpper middle income\t6743.74\t20.845\t3.039\t22.568\t10.306\t11.046\t-0.932\n",
    "32\tBGR\t2015\tUpper middle income\t7017.11\t21.056\t-1.067\t21.19\t9.235\t5.737\t5.443\n",
    "33\tBHR\t2010\tHigh income\t20823.22\t36.206\t1.97\t27.286\t3.6\t2.083\t5.06\n",
    "34\tBHR\t2015\tHigh income\t24057.58\t21.955\t1.836\t24.372\tnull\t0.397\t-14.177\n",
    "35\tBHS\t2010\tHigh income\t22957.79\t15.141\t1.621\t25.214\t15.082\t2.549\t-4.618\n",
    "36\tBHS\t2015\tHigh income\t24309.57\t11.243\t1.879\t27.222\t13.379\t-9.46\t-5.834\n",
    "37\tBIH\t2010\tUpper middle income\t4404.37\t10.115\t2.123\t15.553\t27.202\t11.05\t-1.953\n",
    "38\tBIH\t2015\tUpper middle income\t4206.69\t10.217\t-1.018\t15.907\t27.7\t7.606\t2.101\n",
    "39\tBLR\t2010\tUpper middle income\t6023.15\t26.186\t7.743\t40.656\t0.827\t7.669\t12.225\n",
    "40\tBLR\t2015\tUpper middle income\t5941.24\t25.419\t13.523\t29.035\t0.912\t1.304\t-12.183\n",
    "41\tBLZ\t2010\tUpper middle income\t4321.29\t10.33\t0.918\t12.792\t13.502\t8.491\t6.964\n",
    "42\tBLZ\t2015\tUpper middle income\t4757.11\t12.155\t-0.862\t22.014\t10.114\t2.025\t6.743\n",
    "43\tBOL\t2010\tLower middle income\t1994.91\t24.969\t2.502\t17.007\t4.375\t28.526\t8.836\n",
    "44\tBOL\t2015\tLower middle income\t3099.22\t13.31\t4.061\t19.237\t4\t-8.427\t-2.965\n",
    "45\tBRA\t2010\tUpper middle income\t11297.84\t18.368\t5.039\t21.801\t8.556\t7.124\t34.532\n",
    "46\tBRA\t2015\tUpper middle income\t8810.5\t15.865\t9.03\t19.133\t8.3\t8.085\t-13.47\n",
    "47\tBRB\t2010\tHigh income\t16079.89\t8.12\t5.824\t13.564\t10.25\t4.769\t-0.489\n",
    "48\tBRB\t2015\tHigh income\t15808.09\t7.473\t-1.061\t13.391\t11.3\t6.952\t12.81\n",
    "49\tBRN\t2010\tHigh income\t35437.22\tnull\t0.216\t41.711\tnull\t11.705\t-3.108\n",
    "50\tBRN\t2015\tHigh income\t30994.98\tnull\t-0.423\t35.247\t6.9\t-13.765\t-24.068\n",
    "51\tBTN\t2010\tLower middle income\t1998.75\t44.674\t5.726\t66.906\t3.3\t-0.579\t25.991\n",
    "52\tBTN\t2015\tLower middle income\t2603.1\t32.024\t6.336\t60.362\t3.2\t1.505\t15.832\n",
    "53\tBWA\t2010\tUpper middle income\t6853.67\t32.162\t6.95\t35.353\tnull\t16.022\t6.481\n",
    "54\tBWA\t2015\tUpper middle income\t6780.97\t40.026\t3.054\t32.11\tnull\t-17.111\t0.37\n",
    "55\tCAF\t2010\tLow income\t456.564\t4.115\t1.491\t14.266\tnull\t10.301\t13.882\n",
    "56\tCAF\t2015\tLow income\t332.366\t4.882\t4.5\t13.923\tnull\t0.465\t12.874\n",
    "57\tCAN\t2010\tHigh income\t47512.68\t19.898\t1.769\t23.508\t8\t6.645\t13.774\n",
    "58\tCAN\t2015\tHigh income\t43349.62\t20.416\t1.132\t23.817\t6.9\t3.393\t0.336\n",
    "59\tCHE\t2010\tHigh income\t74570.66\t38.881\t0.684\t24.027\t3.516\t12.547\t7.882\n",
    "60\tCHE\t2015\tHigh income\t81410.02\t34.52\t-1.14\t22.995\t3.178\t2.168\t4.303\n",
    "61\tCHL\t2010\tHigh income\t12789.76\t24.839\t1.408\t23.144\t8.153\t2.357\t25.554\n",
    "62\tCHL\t2015\tHigh income\t13469.47\t21.306\t4.349\t23.291\t6.214\t-1.872\t-2.802\n",
    "63\tCHN\t2010\tUpper middle income\t4524.06\t51.802\t3.3\t47.881\t4.14\t25.641\t19.925\n",
    "64\tCHN\t2015\tUpper middle income\t8166.76\t47.457\t1.441\t44.748\t4.05\t-2.216\t-0.478\n",
    "65\tCIV\t2010\tLower middle income\t1195.44\t16.765\t1.366\t14.901\tnull\t-7.681\t1.856\n",
    "66\tCIV\t2015\tLower middle income\t1381.81\t16.82\t1.244\t17.804\tnull\t2.774\t19.427\n",
    "67\tCMR\t2010\tLower middle income\t1158.78\t17.524\t1.279\t20.281\tnull\t-4.699\t-1.617\n",
    "68\tCMR\t2015\tLower middle income\t1230.38\t17.088\t2.683\t21.336\tnull\t16.087\t2.879\n",
    "69\tCOD\t2010\tLow income\t292.955\t15.574\t23.464\t13.75\tnull\t54.001\t35.575\n",
    "70\tCOD\t2015\tLow income\t471.306\t16.495\t0.959\t20.403\tnull\t-4.995\t-3.984\n",
    "71\tCOG\t2010\tLower middle income\t3183.68\t27.651\t0.392\t19.848\tnull\t13.299\t21.024\n",
    "72\tCOG\t2015\tLower middle income\t1958.1\t-8.049\t2.742\t34.83\tnull\t-6.289\t2.196\n",
    "73\tCOL\t2010\tUpper middle income\t6305.29\t19.107\t2.272\t22.126\t11.792\t1.264\t10.845\n",
    "74\tCOL\t2015\tUpper middle income\t6047.97\t20.23\t4.99\t26.72\t8.925\t-3.225\t-6.321\n",
    "75\tCOM\t2010\tLow income\t789.466\t15.22\t3.899\t15.403\tnull\t8.524\t2.157\n",
    "76\tCOM\t2015\tLow income\t736.406\t19.221\t2\t18.411\tnull\t7.238\t-3.58\n",
    "77\tCPV\t2010\tLower middle income\t3413.26\t25.257\t2.079\t37.684\t10.7\t0.873\t-0.6\n",
    "78\tCPV\t2015\tLower middle income\t3001.29\t37.094\t0.125\t41.483\t10\t-4.248\t-19.31\n",
    "79\tCRI\t2010\tUpper middle income\t8300.69\t16.489\t5.665\t19.715\t7.345\t9.099\t18.711\n",
    "80\tCRI\t2015\tUpper middle income\t11435.97\t15.725\t0.802\t20.218\t9.249\t0.001\t4.629\n",
    "81\tCYP\t2010\tHigh income\t31262.53\t12.507\t2.557\t23.663\t6.283\t4.452\t7.965\n",
    "82\tCYP\t2015\tHigh income\t23105.4\t11.55\t-1.539\t13.956\t14.892\t0.032\t2.128\n",
    "83\tCZE\t2010\tHigh income\t19787.29\t23.588\t1.491\t27.174\t7.279\t14.847\t14.899\n",
    "84\tCZE\t2015\tHigh income\t17569.89\t28.266\t0.335\t27.357\t5.046\t7.703\t8.22\n",
    "85\tDEU\t2010\tHigh income\t42641.68\t25.241\t1.149\t19.626\t6.942\t14.534\t12.85\n",
    "86\tDEU\t2015\tHigh income\t41197.41\t27.571\t0.134\t19.243\t4.608\t5.161\t5.467\n",
    "87\tDJI\t2010\tLower middle income\t1306.54\t24.089\t3.954\t21.259\tnull\t1.368\t-20.199\n",
    "88\tDJI\t2015\tLower middle income\t1788.36\t19.039\t2.104\t50.82\tnull\t12.843\t31.171\n",
    "89\tDMA\t2010\tUpper middle income\t6975.99\t2.462\t2.81\t18.38\tnull\t11.788\t-6.675\n",
    "90\tDMA\t2015\tUpper middle income\t7311.65\t8.814\t-0.794\t16.787\tnull\t8.783\t5.578\n",
    "91\tDNK\t2010\tHigh income\t58177.16\t24.639\t2.311\t18.076\t7.483\t2.94\t0.541\n",
    "92\tDNK\t2015\tHigh income\t53237.28\t28.91\t0.452\t19.755\t6.192\t1.833\t1.261\n",
    "93\tDOM\t2010\tUpper middle income\t5685.77\t18.745\t6.33\t26.178\t5.001\t12.363\t14.755\n",
    "94\tDOM\t2015\tUpper middle income\t6833.24\t21.447\t0.837\t23.494\t5.94\t7.284\t10.571\n",
    "95\tDZA\t2010\tUpper middle income\t4480.72\t49.869\t3.913\t42.328\t9.961\t-3.218\t-1.781\n",
    "96\tDZA\t2015\tUpper middle income\t4123.3\t34.738\t4.784\t51.299\t11.214\t5.733\t-7.507\n",
    "97\tECU\t2010\tUpper middle income\t4633.25\t25.757\t3.552\t28.037\t5.019\t1.109\t14.283\n",
    "98\tECU\t2015\tUpper middle income\t6153.8\t24.657\t3.97\t26.854\t4.77\t2.195\t-7.794\n",
    "99\tEGY\t2010\tLower middle income\t2921.76\t19.421\t11.69\t21.298\t9.21\t3.628\t11.663\n",
    "100\tEGY\t2015\tLower middle income\t3731.18\t10.62\t10.994\t14.289\t12.859\t12.456\t10.42\n",
    "Showing 100 out of 380 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cities\n",
    "name\tcountry_code\tcity_proper_pop\tmetroarea_pop\turbanarea_pop\n",
    "Abidjan\tCIV\t4765000\tnull\t4765000\n",
    "Abu Dhabi\tARE\t1145000\tnull\t1145000\n",
    "Abuja\tNGA\t1235880\t6000000\t1235880\n",
    "Accra\tGHA\t2070463\t4010054\t2070463\n",
    "Addis Ababa\tETH\t3103673\t4567857\t3103673\n",
    "Ahmedabad\tIND\t5570585\tnull\t5570585\n",
    "Alexandria\tEGY\t4616625\tnull\t4616625\n",
    "Algiers\tDZA\t3415811\t5000000\t3415811\n",
    "Almaty\tKAZ\t1703481\tnull\t1703481\n",
    "Ankara\tTUR\t5271000\t4585000\t5271000\n",
    "Auckland\tNZL\t1495000\t1614300\t1495000\n",
    "Baghdad\tIRQ\t7180889\tnull\t7180889\n",
    "Baku\tAZE\t3202300\t4308740\t3202300\n",
    "Bandung\tIDN\t2575478\t6965655\t2575478\n",
    "Bangkok\tTHA\t8280925\t14998000\t8280925\n",
    "Barcelona\tESP\t1604555\t5375774\t1604555\n",
    "Barranquilla\tCOL\t1386865\t2370753\t1386865\n",
    "Basra\tIRQ\t2750000\tnull\t2750000\n",
    "Beijing\tCHN\t21516000\t24900000\t21516000\n",
    "Belo Horizonte\tBRA\t2502557\t5156217\t2502557\n",
    "Bengaluru\tIND\t8425970\t9807000\t8425970\n",
    "Berlin\tDEU\t3517424\t5871022\t3517424\n",
    "Bhopal\tIND\t1798218\t1864389\t1798218\n",
    "Birmingham\tGBR\t1111300\t3683000\t1111300\n",
    "Bogota\tCOL\t7878783\t9800000\t7878783\n",
    "Brasilia\tBRA\t2556149\t3919864\t2556149\n",
    "Brazzaville\tCOG\t1827000\tnull\t1827000\n",
    "Brisbane\tAUS\t1180285\t2349699\t1180285\n",
    "Bucharest\tROM\t1883425\t2272163\t1883425\n",
    "Budapest\tHUN\t1759407\t2927944\t1759407\n",
    "Buenos Aires\tARG\t3054300\t14122000\t3054300\n",
    "Busan\tKOR\t3510833\t8202239\t3510833\n",
    "Cairo\tEGY\t10230350\t18290000\t10230350\n",
    "Calgary\tCAN\t1235171\t1214839\t1235171\n",
    "Cali\tCOL\t2400653\t3400000\t2400653\n",
    "Caloocan\tPHL\t1583978\tnull\t1583978\n",
    "Campinas\tBRA\t1164098\t3094181\t1164098\n",
    "Cape Town\tZAF\t3740026\tnull\t3740026\n",
    "Caracas\tVEN\t1943901\t2923959\t1943901\n",
    "Casablanca\tMAR\t5117832\t6861739\t5117832\n",
    "Changchun\tCHN\t3815270\t7674439\t3815270\n",
    "Changsha\tCHN\t7044118\tnull\t7044118\n",
    "Chaozhou\tCHN\t2669844\tnull\t2669844\n",
    "Chengdu\tCHN\t4741929\t10376000\t4741929\n",
    "Chennai\tIND\t7088000\tnull\t7088000\n",
    "Chicago\tUSA\t2695598\t9156000\t2695598\n",
    "Chittagong\tBGD\t2581643\t4009423\t2581643\n",
    "Chongqing\tCHN\t8189800\t52100100\t8189800\n",
    "Cologne\tDEU\t1057327\t3573500\t1057327\n",
    "Cordoba\tARG\t1330023\t1528000\t1330023\n",
    "Curitiba\tBRA\t1879355\t3400000\t1879355\n",
    "Daegu\tKOR\t2492994\tnull\t2492994\n",
    "Daejeon\tKOR\t1535028\tnull\t1535028\n",
    "Dakar\tSEN\t1146053\t2452656\t1146053\n",
    "Dalian\tCHN\t2146099\t5935638\t2146099\n",
    "Dallas\tUSA\t1317929\t7233323\t1317929\n",
    "Dar es Salaam\tTZA\t4364541\tnull\t4364541\n",
    "Davao City\tPHL\t1632991\t2516216\t1632991\n",
    "Delhi\tIND\t16787940\t24998000\t16787940\n",
    "Dhaka\tBGD\t14543124\tnull\t14543124\n",
    "Dongguan\tCHN\t8220207\tnull\t8220207\n",
    "Douala\tCMR\t2446945\tnull\t2446945\n",
    "Dubai\tARE\t2643410\tnull\t2643410\n",
    "Durban\tZAF\t3442361\tnull\t3442361\n",
    "Ekurhuleni\tZAF\t3178470\tnull\t3178470\n",
    "Faisalabad\tPAK\t6480765\t3675000\t6480765\n",
    "Fez\tMAR\t1112072\tnull\t1112072\n",
    "Fortaleza\tBRA\t2609716\t4019213\t2609716\n",
    "Foshan\tCHN\t6151622\tnull\t6151622\n",
    "Fukuoka\tJPN\t1483052\t5590378\t1483052\n",
    "Fuzhou\tCHN\t7115369\tnull\t7115369\n",
    "Giza\tEGY\t4239988\tnull\t4239988\n",
    "Guadalajara\tMEX\t1495189\t4424252\t1495189\n",
    "Guangzhou\tCHN\t14043500\t44259000\t14043500\n",
    "Guatemala City\tGTM\t2110100\t4500000\t2110100\n",
    "Guayaquil\tECU\t3600000\t5000000\t3600000\n",
    "Gujranwala\tPAK\t2700003\tnull\t2700003\n",
    "Hamburg\tDEU\t1787408\tnull\t1787408\n",
    "Hangzhou\tCHN\t3560391\tnull\t3560391\n",
    "Hanoi\tVNM\t6844100\tnull\t6844100\n",
    "Harare\tZWE\t1606000\tnull\t1606000\n",
    "Harbin\tCHN\t4280701\t10635971\t4280701\n",
    "Havana\tCUB\t2106146\tnull\t2106146\n",
    "Hefei\tCHN\t3352076\tnull\t3352076\n",
    "Hiroshima\tJPN\t1196274\tnull\t1196274\n",
    "Ho Chi Minh City\tVNM\t7681700\tnull\t7681700\n",
    "Hong Kong\tCHN\t7374900\tnull\t7374900\n",
    "Houston\tUSA\t2489558\t6490180\t2489558\n",
    "Hyderabad (India)\tIND\t7859250\tnull\t7859250\n",
    "Hyderabad\tPAK\t3429471\tnull\t3429471\n",
    "Ibadan\tNGA\t1338659\t2837000\t1338659\n",
    "Incheon\tKOR\t2978367\tnull\t2978367\n",
    "Isfahan\tIRN\t2243249\tnull\t2243249\n",
    "Islamabad\tPAK\t1900000\t2200000\t1900000\n",
    "Istanbul\tTUR\t14025000\t13520000\t14025000\n",
    "Izmir\tTUR\t4168000\t3019000\t4168000\n",
    "Jaipur\tIND\t3073350\tnull\t3073350\n",
    "Jakarta\tIDN\t10075310\t30539000\t10075310\n",
    "Jeddah\tSAU\t3456259\tnull\t3456259\n",
    "Jinan\tCHN\t2009273\t5853196\t2009273\n",
    "Showing 100 out of 236 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "population\n",
    "pop_id\tcountry_code\tyear\tfertility_rate\tlife_expectancy\tsize\n",
    "20\tABW\t2010\t1.704\t74.95354\t101597\n",
    "19\tABW\t2015\t1.647\t75.573586\t103889\n",
    "2\tAFG\t2010\t5.746\t58.97083\t27962208\n",
    "1\tAFG\t2015\t4.653\t60.71717\t32526562\n",
    "12\tAGO\t2010\t6.416\t50.65417\t21219954\n",
    "11\tAGO\t2015\t5.996\t52.666096\t25021974\n",
    "4\tALB\t2010\t1.663\t77.03695\t2913021\n",
    "3\tALB\t2015\t1.793\t78.014465\t2889167\n",
    "10\tAND\t2010\t1.27\tnull\t84419\n",
    "9\tAND\t2015\tnull\tnull\t70473\n",
    "409\tARE\t2010\t1.868\t76.67525\t8329453\n",
    "408\tARE\t2015\t1.767\t77.541245\t9156963\n",
    "16\tARG\t2010\t2.37\t75.48498\t41222876\n",
    "15\tARG\t2015\t2.308\t76.33422\t43416756\n",
    "18\tARM\t2010\t1.648\t74.22634\t2963496\n",
    "17\tARM\t2015\t1.517\t74.79712\t3017712\n",
    "8\tASM\t2010\tnull\tnull\t55636\n",
    "7\tASM\t2015\tnull\tnull\t55538\n",
    "14\tATG\t2010\t2.13\t75.30878\t87233\n",
    "13\tATG\t2015\t2.063\t76.10022\t91818\n",
    "22\tAUS\t2010\t1.928\t81.69512\t22031750\n",
    "21\tAUS\t2015\t1.833\t82.45122\t23789752\n",
    "24\tAUT\t2010\t1.44\t80.58049\t8363404\n",
    "23\tAUT\t2015\t1.47\t81.8439\t8638366\n",
    "26\tAZE\t2010\t1.92\t70.45132\t9054332\n",
    "25\tAZE\t2015\t1.97\t70.84878\t9649341\n",
    "64\tBDI\t2010\t6.302\t54.829197\t9461117\n",
    "63\tBDI\t2015\t5.863\t57.107048\t11178921\n",
    "38\tBEL\t2010\t1.86\t80.18293\t10895586\n",
    "37\tBEL\t2015\t1.74\t81.2878\t11249420\n",
    "42\tBEN\t2010\t5.094\t58.729706\t9509798\n",
    "41\tBEN\t2015\t4.688\t59.720707\t10879829\n",
    "62\tBFA\t2010\t5.867\t57.05139\t15632066\n",
    "61\tBFA\t2015\t5.437\t58.931293\t18105570\n",
    "32\tBGD\t2010\t2.332\t70.08029\t151616780\n",
    "31\tBGD\t2015\t2.144\t72.0012\t160995650\n",
    "60\tBGR\t2010\t1.57\t73.51219\t7395599\n",
    "59\tBGR\t2015\t1.53\t74.46585\t7177991\n",
    "30\tBHR\t2010\t2.142\t76.12756\t1261319\n",
    "29\tBHR\t2015\t2.035\t76.8191\t1377237\n",
    "28\tBHS\t2010\t1.904\t74.59239\t360830\n",
    "27\tBHS\t2015\t1.861\t75.39683\t388019\n",
    "50\tBIH\t2010\t1.288\t75.807686\t3835258\n",
    "49\tBIH\t2015\t1.253\t76.588585\t3810416\n",
    "36\tBLR\t2010\t1.494\t70.40488\t9490583\n",
    "35\tBLR\t2015\t1.724\t73.62439\t9489616\n",
    "40\tBLZ\t2010\t2.714\t69.78205\t321609\n",
    "39\tBLZ\t2015\t2.546\t70.19237\t359287\n",
    "44\tBMU\t2010\t1.76\t79.288536\t65124\n",
    "43\tBMU\t2015\t1.62\t81.01219\t65235\n",
    "48\tBOL\t2010\t3.2\t66.40775\t9918245\n",
    "47\tBOL\t2015\t2.923\t68.73961\t10724705\n",
    "54\tBRA\t2010\t1.838\t73.264145\t198614210\n",
    "53\tBRA\t2015\t1.778\t74.67588\t207847520\n",
    "34\tBRB\t2010\t1.781\t74.84529\t279566\n",
    "33\tBRB\t2015\t1.796\t75.65844\t284215\n",
    "58\tBRN\t2010\t1.953\t77.59827\t393302\n",
    "57\tBRN\t2015\t1.856\t79.04083\t423188\n",
    "46\tBTN\t2010\t2.331\t67.89242\t720246\n",
    "45\tBTN\t2015\t1.984\t69.832855\t774830\n",
    "52\tBWA\t2010\t2.893\t63.402294\t2047831\n",
    "51\tBWA\t2015\t2.799\t64.48741\t2262485\n",
    "76\tCAF\t2010\t4.624\t47.625317\t4444973\n",
    "75\tCAF\t2015\t4.206\t51.41912\t4900274\n",
    "72\tCAN\t2010\t1.6269\t81.19756\t34005272\n",
    "71\tCAN\t2015\t1.6\t82.137634\t35848608\n",
    "377\tCHE\t2010\t1.52\t82.24634\t7824909\n",
    "376\tCHE\t2015\t1.54\t83.19756\t8281430\n",
    "80\tCHI\t2010\t1.436\t80.01037\t159583\n",
    "79\tCHI\t2015\t1.473\t80.751686\t163692\n",
    "82\tCHL\t2010\t1.824\t80.2758\t17015048\n",
    "81\tCHL\t2015\t1.749\t81.78756\t17948140\n",
    "84\tCHN\t2010\t1.539\t75.007416\t1337705000\n",
    "83\tCHN\t2015\t1.569\t75.98634\t1371220000\n",
    "96\tCIV\t2010\t5.231\t50.151463\t20131708\n",
    "95\tCIV\t2015\t4.937\t51.919758\t22701556\n",
    "70\tCMR\t2010\t5.017\t53.694828\t20590666\n",
    "69\tCMR\t2015\t4.63\t55.93439\t23344180\n",
    "90\tCOD\t2010\t6.386\t56.896145\t65938712\n",
    "89\tCOD\t2015\t5.908\t59.023903\t77266816\n",
    "92\tCOG\t2010\t5.02\t59.143124\t4066078\n",
    "91\tCOG\t2015\t4.811\t62.867657\t4620330\n",
    "86\tCOL\t2010\t2.01\t73.277855\t45918100\n",
    "85\tCOL\t2015\t1.875\t74.18202\t48228704\n",
    "88\tCOM\t2010\t4.755\t61.830513\t698695\n",
    "87\tCOM\t2015\t4.418\t63.554024\t788474\n",
    "66\tCPV\t2010\t2.464\t72.5979\t490379\n",
    "65\tCPV\t2015\t2.268\t73.35564\t520502\n",
    "94\tCRI\t2010\t1.922\t78.736046\t4545273\n",
    "93\tCRI\t2015\t1.8\t79.586464\t4807850\n",
    "100\tCUB\t2010\t1.635\t78.93563\t11308133\n",
    "99\tCUB\t2015\t1.606\t79.54642\t11389562\n",
    "102\tCUW\t2010\t2.2\tnull\t148703\n",
    "101\tCUW\t2015\t1.9\tnull\t157979\n",
    "74\tCYM\t2010\tnull\tnull\t55509\n",
    "73\tCYM\t2015\tnull\tnull\t59967\n",
    "104\tCYP\t2010\t1.481\t79.38195\t1103685\n",
    "103\tCYP\t2015\t1.438\t80.3071\t1165300\n",
    "106\tCZE\t2010\t1.51\t77.42439\t10474410\n",
    "105\tCZE\t2015\t1.53\t79.47317\t10546059\n",
    "Showing 100 out of 434 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lang_id\tcode\tname\tpercent\tofficial\n",
    "1\tAFG\tDari\t50\ttrue\n",
    "2\tAFG\tPashto\t35\ttrue\n",
    "3\tAFG\tTurkic\t11\tfalse\n",
    "4\tAFG\tOther\t4\tfalse\n",
    "5\tALB\tAlbanian\t98.8\ttrue\n",
    "6\tALB\tGreek\t0.5\tfalse\n",
    "7\tALB\tOther\t0.6\tfalse\n",
    "8\tALB\tunspecified\t0.1\tfalse\n",
    "9\tDZA\tArabic\tnull\ttrue\n",
    "10\tDZA\tFrench\tnull\tfalse\n",
    "11\tDZA\tBerber or Tamazight\tnull\ttrue\n",
    "12\tDZA\tShawiya\tnull\tfalse\n",
    "13\tDZA\tMzab\tnull\tfalse\n",
    "14\tDZA\tTuareg\tnull\tfalse\n",
    "15\tASM\tSamoan\t88.6\tfalse\n",
    "16\tASM\tEnglish\t3.9\tfalse\n",
    "17\tASM\tTongan\t2.7\tfalse\n",
    "18\tASM\tOther\t3\tfalse\n",
    "19\tASM\tOther\t1.8\tfalse\n",
    "20\tAND\tCatalan\tnull\ttrue\n",
    "21\tAND\tFrench\tnull\tfalse\n",
    "22\tAND\tCastilian\tnull\tfalse\n",
    "23\tAND\tPortuguese\tnull\tfalse\n",
    "24\tAGO\tPortuguese\t71.2\ttrue\n",
    "25\tAGO\tUmbundu\t23\tfalse\n",
    "26\tAGO\tKikongo\t8.2\tfalse\n",
    "27\tAGO\tKimbundu\t7.8\tfalse\n",
    "28\tAGO\tChokwe\t6.5\tfalse\n",
    "29\tAGO\tNhaneca\t3.4\tfalse\n",
    "30\tAGO\tNganguela\t3.1\tfalse\n",
    "31\tAGO\tFiote\t2.4\tfalse\n",
    "32\tAGO\tKwanhama\t2.3\tfalse\n",
    "33\tAGO\tMuhumbi\t2.1\tfalse\n",
    "34\tAGO\tLuvale\t1\tfalse\n",
    "35\tAGO\tOther\t3.6\tfalse\n",
    "36\tAIA\tEnglish\tnull\ttrue\n",
    "37\tATG\tEnglish\tnull\ttrue\n",
    "38\tATG\tAntiguan creole\tnull\tfalse\n",
    "39\tARG\tSpanish\tnull\ttrue\n",
    "40\tARG\tItalian\tnull\tfalse\n",
    "41\tARG\tEnglish\tnull\tfalse\n",
    "42\tARG\tGerman\tnull\tfalse\n",
    "43\tARG\tFrench\tnull\tfalse\n",
    "44\tARG\tindigenous\tnull\tfalse\n",
    "45\tARM\tArmenian\t97.9\ttrue\n",
    "46\tARM\tKurdish\t1\tfalse\n",
    "47\tARM\tOther\t1\tfalse\n",
    "48\tABW\tPapiamento\t69.4\ttrue\n",
    "49\tABW\tSpanish\t13.7\tfalse\n",
    "50\tABW\tEnglish\t7.1\tfalse\n",
    "51\tABW\tDutch\t6.1\ttrue\n",
    "52\tABW\tChinese\t1.5\tfalse\n",
    "53\tABW\tOther\t1.7\tfalse\n",
    "54\tABW\tunspecified\t0.4\tfalse\n",
    "55\tAUS\tEnglish\t76.8\tfalse\n",
    "56\tAUS\tMandarin\t1.6\tfalse\n",
    "57\tAUS\tItalian\t1.4\tfalse\n",
    "58\tAUS\tArabic\t1.3\tfalse\n",
    "59\tAUS\tGreek\t1.2\tfalse\n",
    "60\tAUS\tCantonese\t1.2\tfalse\n",
    "61\tAUS\tVietnamese\t1.1\tfalse\n",
    "62\tAUS\tOther\t10.4\tfalse\n",
    "63\tAUS\tunspecified\t5\tfalse\n",
    "64\tAUT\tGerman\t88.6\ttrue\n",
    "65\tAUT\tTurkish\t2.3\tfalse\n",
    "66\tAUT\tSerbian\t2.2\tfalse\n",
    "67\tAUT\tCroatian\t1.6\ttrue\n",
    "68\tAUT\tOther\t5.3\tfalse\n",
    "69\tAZE\tAzerbaijani\t92.5\ttrue\n",
    "70\tAZE\tRussian\t1.4\tfalse\n",
    "71\tAZE\tArmenian\t1.4\tfalse\n",
    "72\tAZE\tOther\t4.7\tfalse\n",
    "73\tBHS\tEnglish\tnull\ttrue\n",
    "74\tBHS\tCreole\tnull\tfalse\n",
    "75\tBHR\tArabic\tnull\ttrue\n",
    "76\tBHR\tEnglish\tnull\tfalse\n",
    "77\tBHR\tFarsi\tnull\tfalse\n",
    "78\tBHR\tUrdu\tnull\tfalse\n",
    "79\tBGD\tBangla\t98.8\ttrue\n",
    "80\tBGD\tOther\t1.2\tfalse\n",
    "81\tBRB\tEnglish\tnull\ttrue\n",
    "82\tBRB\tBajan\tnull\tfalse\n",
    "83\tBLR\tRussian\t70.2\ttrue\n",
    "84\tBLR\tBelarusian\t23.4\ttrue\n",
    "85\tBLR\tOther\t3.1\tfalse\n",
    "86\tBLR\tunspecified\t3.3\tfalse\n",
    "87\tBEL\tDutch\t60\ttrue\n",
    "88\tBEL\tFrench\t40\ttrue\n",
    "89\tBEL\tGerman\t1\ttrue\n",
    "90\tBLZ\tEnglish\t62.9\ttrue\n",
    "91\tBLZ\tSpanish\t56.6\tfalse\n",
    "92\tBLZ\tCreole\t44.6\tfalse\n",
    "93\tBLZ\tMaya\t10.5\tfalse\n",
    "94\tBLZ\tGerman\t3.2\tfalse\n",
    "95\tBLZ\tGarifuna\t2.9\tfalse\n",
    "96\tBLZ\tOther\t1.8\tfalse\n",
    "97\tBLZ\tunknown\t0.3\tfalse\n",
    "98\tBLZ\tnone\t0.2\tfalse\n",
    "99\tBEN\tFrench\tnull\ttrue\n",
    "100\tBEN\tFon and Yoruba\tnull\tfalse\n",
    "Showing 100 out of 955 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "currencies\n",
    "curr_id\tcode\tbasic_unit\tcurr_code\tfrac_unit\tfrac_perbasic\n",
    "1\tAFG\tAfghan afghani\tAFN\tPul\t100\n",
    "2\tALB\tAlbanian lek\tALL\tQindarke\t100\n",
    "3\tDZA\tAlgerian dinar\tDZD\tSanteem\t100\n",
    "4\tAND\tEuro\tEUR\tCent\t100\n",
    "5\tAGO\tAngolan kwanza\tAOA\tCentimo\t100\n",
    "6\tAIA\tEast Caribbean dollar\tXCD\tCent\t100\n",
    "7\tATG\tEast Caribbean dollar\tXCD\tCent\t100\n",
    "8\tARG\tArgentine peso\tARS\tCentavo\t100\n",
    "9\tARM\tArmenian dram\tAMD\tLuma\t100\n",
    "10\tABW\tAruban florin\tAWG\tCent\t100\n",
    "11\tAUS\tAustralian dollar\tAUD\tCent\t100\n",
    "12\tAUT\tEuro\tEUR\tCent\t100\n",
    "13\tAZE\tAzerbaijani manat\tAZN\tQəpik\t100\n",
    "14\tBHR\tBahraini dinar\tBHD\tFils\t1000\n",
    "15\tBGD\tBangladeshi taka\tBDT\tPaisa\t100\n",
    "16\tBRB\tBarbadian dollar\tBBD\tCent\t100\n",
    "17\tBLR\tNew Belarusian ruble\tBYN\tKapyeyka\t100\n",
    "18\tBLR\tOld Belarusian ruble\tBYR\tKapyeyka\t100\n",
    "19\tBEL\tEuro\tEUR\tCent\t100\n",
    "20\tBLZ\tBelize dollar\tBZD\tCent\t100\n",
    "21\tBEN\tWest African CFA franc\tXOF\tCentime\t100\n",
    "22\tBMU\tBermudian dollar\tBMD\tCent\t100\n",
    "23\tBTN\tBhutanese ngultrum\tBTN\tChetrum\t100\n",
    "24\tBTN\tIndian rupee\tINR\tPaisa\t100\n",
    "25\tBOL\tBolivian boliviano\tBOB\tCentavo\t100\n",
    "26\tBIH\tBosnia and Herzegovina convertible mark\tBAM\tFening\t100\n",
    "27\tBWA\tBotswana pula\tBWP\tThebe\t100\n",
    "28\tBRA\tBrazilian real\tBRL\tCentavo\t100\n",
    "29\tIOT\tUnited States dollar\tUSD\tCent\t100\n",
    "30\tBRN\tBrunei dollar\tBND\tSen\t100\n",
    "31\tBRN\tSingapore dollar\tSGD\tCent\t100\n",
    "32\tBGR\tBulgarian lev\tBGN\tStotinka\t100\n",
    "33\tBFA\tWest African CFA franc\tXOF\tCentime\t100\n",
    "34\tBDI\tBurundian franc\tBIF\tCentime\t100\n",
    "35\tKHM\tCambodian riel\tKHR\tSen\t100\n",
    "36\tKHM\tUnited States dollar\tUSD\tCent\t100\n",
    "37\tCMR\tCentral African CFA franc\tXAF\tCentime\t100\n",
    "38\tCAN\tCanadian dollar\tCAD\tCent\t100\n",
    "39\tCPV\tCape Verdean escudo\tCVE\tCentavo\t100\n",
    "40\tCYM\tCayman Islands dollar\tKYD\tCent\t100\n",
    "41\tCAF\tCentral African CFA franc\tXAF\tCentime\t100\n",
    "42\tTCD\tCentral African CFA franc\tXAF\tCentime\t100\n",
    "43\tCHL\tChilean peso\tCLP\tCentavo\t100\n",
    "44\tCHN\tChinese yuan\tCNY\tFen\t100\n",
    "45\tCCK\tAustralian dollar\tAUD\tCent\t100\n",
    "46\tCOL\tColombian peso\tCOP\tCentavo\t100\n",
    "47\tCOM\tComorian franc\tKMF\tCentime\t100\n",
    "48\tCOD\tCongolese franc\tCDF\tCentime\t100\n",
    "49\tCOG\tCentral African CFA franc\tXAF\tCentime\t100\n",
    "50\tCOK\tNew Zealand dollar\tNZD\tCent\t100\n",
    "51\tCRI\tCosta Rican colon\tCRC\tCentimo\t100\n",
    "52\tCIV\tWest African CFA franc\tXOF\tCentime\t100\n",
    "53\tHRV\tCroatian kuna\tHRK\tLipa\t100\n",
    "54\tCUB\tCuban convertible peso\tCUC\tCentavo\t100\n",
    "55\tCUB\tCuban peso\tCUP\tCentavo\t100\n",
    "56\tCYP\tEuro\tEUR\tCent\t100\n",
    "57\tCZE\tCzech koruna\tCZK\tHaler\t100\n",
    "58\tDNK\tDanish krone\tDKK\tOre\t100\n",
    "59\tDJI\tDjiboutian franc\tDJF\tCentime\t100\n",
    "60\tDMA\tEast Caribbean dollar\tXCD\tCent\t100\n",
    "61\tDOM\tDominican peso\tDOP\tCentavo\t100\n",
    "62\tTMP\tUnited States dollar\tUSD\tCent\t100\n",
    "63\tECU\tUnited States dollar\tUSD\tCent\t100\n",
    "64\tEGY\tEgyptian pound\tEGP\tPiastre\t100\n",
    "65\tSLV\tUnited States dollar\tUSD\tCent\t100\n",
    "66\tGNQ\tCentral African CFA franc\tXAF\tCentime\t100\n",
    "67\tERI\tEritrean nakfa\tERN\tCent\t100\n",
    "68\tEST\tEuro\tEUR\tCent\t100\n",
    "69\tETH\tEthiopian birr\tETB\tSantim\t100\n",
    "70\tFLK\tFalkland Islands pound\tFKP\tPenny\t100\n",
    "71\tFRO\tDanish krone\tDKK\tOre\t100\n",
    "72\tFIN\tEuro\tEUR\tCent\t100\n",
    "73\tFRA\tEuro\tEUR\tCent\t100\n",
    "74\tPYF\tCFP franc\tXPF\tCentime\t100\n",
    "75\tGAB\tCentral African CFA franc\tXAF\tCentime\t100\n",
    "76\tGEO\tGeorgian lari\tGEL\tTetri\t100\n",
    "77\tDEU\tEuro\tEUR\tCent\t100\n",
    "78\tGHA\tGhanaian cedi\tGHS\tPesewa\t100\n",
    "79\tGIB\tGibraltar pound\tGIP\tPenny\t100\n",
    "80\tGRC\tEuro\tEUR\tCent\t100\n",
    "81\tGRD\tEast Caribbean dollar\tXCD\tCent\t100\n",
    "82\tGTM\tGuatemalan quetzal\tGTQ\tCentavo\t100\n",
    "83\tGIN\tGuinean franc\tGNF\tCentime\t100\n",
    "84\tGNB\tWest African CFA franc\tXOF\tCentime\t100\n",
    "85\tGUY\tGuyanese dollar\tGYD\tCent\t100\n",
    "86\tHTI\tHaitian gourde\tHTG\tCentime\t100\n",
    "87\tHND\tHonduran lempira\tHNL\tCentavo\t100\n",
    "88\tHKG\tHong Kong dollar\tHKD\tCent\t100\n",
    "89\tHUN\tHungarian forint\tHUF\tFiller\t100\n",
    "90\tISL\tIcelandic krona\tISK\tEyrir\t100\n",
    "91\tIND\tIndian rupee\tINR\tPaisa\t100\n",
    "92\tIDN\tIndonesian rupiah\tIDR\tSen\t100\n",
    "93\tIRN\tIranian rial\tIRR\tDinar\t100\n",
    "94\tIRQ\tIraqi dinar\tIQD\tFils\t1000\n",
    "95\tIRL\tEuro\tEUR\tCent\t100\n",
    "96\tISR\tIsraeli new shekel\tILS\tAgora\t100\n",
    "97\tITA\tEuro\tEUR\tCent\t100\n",
    "98\tJAM\tJamaican dollar\tJMD\tCent\t100\n",
    "99\tJPN\tJapanese yen\tJPY\tSen\t100\n",
    "100\tJOR\tJordanian dinar\tJOD\tPiastre\t100\n",
    "Showing 100 out of 224 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "countries\n",
    "code\tname\tcontinent\tregion\tsurface_area\tindep_year\tlocal_name\tgov_form\tcapital\tcap_long\tcap_lat\n",
    "AFG\tAfghanistan\tAsia\tSouthern and Central Asia\t652090\t1919\tAfganistan/Afqanestan\tIslamic Emirate\tKabul\t69.1761\t34.5228\n",
    "NLD\tNetherlands\tEurope\tWestern Europe\t41526\t1581\tNederland\tConstitutional Monarchy\tAmsterdam\t4.89095\t52.3738\n",
    "ALB\tAlbania\tEurope\tSouthern Europe\t28748\t1912\tShqiperia\tRepublic\tTirane\t19.8172\t41.3317\n",
    "DZA\tAlgeria\tAfrica\tNorthern Africa\t2381740\t1962\tAl-Jazair/Algerie\tRepublic\tAlgiers\t3.05097\t36.7397\n",
    "ASM\tAmerican Samoa\tOceania\tPolynesia\t199\tnull\tAmerika Samoa\tUS Territory\tPago Pago\t-170.691\t-14.2846\n",
    "AND\tAndorra\tEurope\tSouthern Europe\t468\t1278\tAndorra\tParliamentary Coprincipality\tAndorra la Vella\t1.5218\t42.5075\n",
    "AGO\tAngola\tAfrica\tCentral Africa\t1246700\t1975\tAngola\tRepublic\tLuanda\t13.242\t-8.81155\n",
    "ATG\tAntigua and Barbuda\tNorth America\tCaribbean\t442\t1981\tAntigua and Barbuda\tConstitutional Monarchy\tSaint John's\t-61.8456\t17.1175\n",
    "ARE\tUnited Arab Emirates\tAsia\tMiddle East\t83600\t1971\tAl-Imarat al-´Arabiya al-Muttahida\tEmirate Federation\tAbu Dhabi\t54.3705\t24.4764\n",
    "ARG\tArgentina\tSouth America\tSouth America\t2780400\t1816\tArgentina\tFederal Republic\tBuenos Aires\t-58.4173\t-34.6118\n",
    "ARM\tArmenia\tAsia\tMiddle East\t29800\t1991\tHajastan\tRepublic\tYerevan\t44.509\t40.1596\n",
    "ABW\tAruba\tNorth America\tCaribbean\t193\tnull\tAruba\tNonmetropolitan Territory of The Netherlands\tOranjestad\t-70.0167\t12.5167\n",
    "AUS\tAustralia\tOceania\tAustralia and New Zealand\t7741220\t1901\tAustralia\tConstitutional Monarchy, Federation\tCanberra\t149.129\t-35.282\n",
    "AZE\tAzerbaijan\tAsia\tMiddle East\t86600\t1991\tAzarbaycan\tFederal Republic\tBaku\t49.8932\t40.3834\n",
    "BHS\tBahamas\tNorth America\tCaribbean\t13878\t1973\tThe Bahamas\tConstitutional Monarchy\tNassau\t-77.339\t25.0661\n",
    "BHR\tBahrain\tAsia\tMiddle East\t694\t1971\tAl-Bahrayn\tMonarchy (Emirate)\tManama\t50.5354\t26.1921\n",
    "BGD\tBangladesh\tAsia\tSouthern and Central Asia\t143998\t1971\tBangladesh\tRepublic\tDhaka\t90.4113\t23.7055\n",
    "BRB\tBarbados\tNorth America\tCaribbean\t430\t1966\tBarbados\tConstitutional Monarchy\tBridgetown\t-59.6105\t13.0935\n",
    "BEL\tBelgium\tEurope\tWestern Europe\t30518\t1830\tBelgie/Belgique\tConstitutional Monarchy, Federation\tBrussels\t4.36761\t50.8371\n",
    "BLZ\tBelize\tNorth America\tCentral America\t22696\t1981\tBelize\tConstitutional Monarchy\tBelmopan\t-88.7713\t17.2534\n",
    "BEN\tBenin\tAfrica\tWestern Africa\t112622\t1960\tBenin\tRepublic\tPorto-Novo\t2.6323\t6.4779\n",
    "BMU\tBermuda\tNorth America\tNorth America\t53\tnull\tBermuda\tDependent Territory of the UK\tHamilton\t-64.706\t32.3293\n",
    "BTN\tBhutan\tAsia\tSouthern and Central Asia\t47000\t1910\tDruk-Yul\tMonarchy\tThimphu\t89.6177\t27.5768\n",
    "BOL\tBolivia\tSouth America\tSouth America\t1098580\t1825\tBolivia\tRepublic\tLa Paz\t-66.1936\t-13.9908\n",
    "BIH\tBosnia and Herzegovina\tEurope\tSouthern Europe\t51197\t1992\tBosna i Hercegovina\tFederal Republic\tSarajevo\t18.4214\t43.8607\n",
    "BWA\tBotswana\tAfrica\tSouthern Africa\t581730\t1966\tBotswana\tRepublic\tGaborone\t25.9201\t-24.6544\n",
    "BRA\tBrazil\tSouth America\tSouth America\t8547400\t1822\tBrasil\tFederal Republic\tBrasilia\t-47.9292\t-15.7801\n",
    "GBR\tUnited Kingdom\tEurope\tBritish Islands\t242900\t1066\tUnited Kingdom\tConstitutional Monarchy\tLondon\t-0.126236\t51.5002\n",
    "VGB\tVirgin Islands, British\tNorth America\tCaribbean\t151\tnull\tBritish Virgin Islands\tDependent Territory of the UK\tRoad Town\t-64.623055\t18.431389\n",
    "BRN\tBrunei\tAsia\tSoutheast Asia\t5765\t1984\tBrunei Darussalam\tMonarchy (Sultanate)\tBandar Seri Begawan\t114.946\t4.94199\n",
    "BGR\tBulgaria\tEurope\tEastern Europe\t110994\t1908\tBalgarija\tRepublic\tSofia\t23.3238\t42.7105\n",
    "BFA\tBurkina Faso\tAfrica\tWestern Africa\t274000\t1960\tBurkina Faso\tRepublic\tOuagadougou\t-1.53395\t12.3605\n",
    "BDI\tBurundi\tAfrica\tEastern Africa\t27834\t1962\tBurundi/Uburundi\tRepublic\tBujumbura\t29.3639\t-3.3784\n",
    "CYM\tCayman Islands\tNorth America\tCaribbean\t264\tnull\tCayman Islands\tDependent Territory of the UK\tGeorge Town\t-81.3857\t19.3022\n",
    "CHL\tChile\tSouth America\tSouth America\t756626\t1810\tChile\tRepublic\tSantiago\t-70.6475\t-33.475\n",
    "CRI\tCosta Rica\tNorth America\tCentral America\t51100\t1821\tCosta Rica\tRepublic\tSan Jose\t-84.0089\t9.63701\n",
    "DJI\tDjibouti\tAfrica\tEastern Africa\t23200\t1977\tDjibouti/Jibuti\tRepublic\tDjibouti\t43.1425\t11.5806\n",
    "DMA\tDominica\tNorth America\tCaribbean\t751\t1978\tDominica\tRepublic\tRoseau\t-61.39\t15.2976\n",
    "DOM\tDominican Republic\tNorth America\tCaribbean\t48511\t1844\tRepublica Dominicana\tRepublic\tSanto Domingo\t-69.8908\t18.479\n",
    "ECU\tEcuador\tSouth America\tSouth America\t283561\t1822\tEcuador\tRepublic\tQuito\t-78.5243\t-0.229498\n",
    "EGY\tEgypt\tAfrica\tNorthern Africa\t1001450\t1922\tMisr\tRepublic\tCairo\t31.2461\t30.0982\n",
    "SLV\tEl Salvador\tNorth America\tCentral America\t21041\t1841\tEl Salvador\tRepublic\tSan Salvador\t-89.2073\t13.7034\n",
    "ERI\tEritrea\tAfrica\tEastern Africa\t117600\t1993\tErtra\tRepublic\tAsmara\t38.9183\t15.3315\n",
    "ESP\tSpain\tEurope\tSouthern Europe\t505992\t1492\tEspana\tConstitutional Monarchy\tMadrid\t-3.70327\t40.4167\n",
    "ZAF\tSouth Africa\tAfrica\tSouthern Africa\t1221040\t1910\tSouth Africa\tRepublic\tPretoria\t28.1871\t-25.746\n",
    "ETH\tEthiopia\tAfrica\tEastern Africa\t1104300\t-1000\tYeItyop´iya\tRepublic\tAddis Ababa\t38.7468\t9.02274\n",
    "FJI\tFiji Islands\tOceania\tMelanesia\t18274\t1970\tFiji Islands\tRepublic\tSuva\t178.399\t-18.1149\n",
    "PHL\tPhilippines\tAsia\tSoutheast Asia\t300000\t1946\tPilipinas\tRepublic\tManila\t121.035\t14.5515\n",
    "FRO\tFaroe Islands\tEurope\tNordic Countries\t1399\tnull\tForoyar\tPart of Denmark\tTorshavn\t-6.91181\t61.8926\n",
    "GAB\tGabon\tAfrica\tCentral Africa\t267668\t1960\tLe Gabon\tRepublic\tLibreville\t9.45162\t0.38832\n",
    "GMB\tGambia\tAfrica\tWestern Africa\t11295\t1965\tThe Gambia\tRepublic\tBanjul\t-16.5885\t13.4495\n",
    "GEO\tGeorgia\tAsia\tMiddle East\t69700\t1991\tSakartvelo\tRepublic\tTbilisi\t44.793\t41.71\n",
    "GHA\tGhana\tAfrica\tWestern Africa\t238533\t1957\tGhana\tRepublic\tAccra\t-0.20795\t5.57045\n",
    "GIB\tGibraltar\tEurope\tSouthern Europe\t6\tnull\tGibraltar\tDependent Territory of the UK\tnull\tnull\tnull\n",
    "GRD\tGrenada\tNorth America\tCaribbean\t344\t1974\tGrenada\tConstitutional Monarchy\tSaint George's\t-61.7449\t12.0653\n",
    "GRL\tGreenland\tNorth America\tNorth America\t2166090\tnull\tKalaallit Nunaat/Gronland\tPart of Denmark\tNuuk\t-51.7214\t64.1836\n",
    "GUM\tGuam\tOceania\tMicronesia\t549\tnull\tGuam\tUS Territory\tAgana\t144.794\t13.4443\n",
    "GTM\tGuatemala\tNorth America\tCentral America\t108889\t1821\tGuatemala\tRepublic\tGuatemala City\t-90.5328\t14.6248\n",
    "GIN\tGuinea\tAfrica\tWestern Africa\t245857\t1958\tGuinee\tRepublic\tConakry\t-13.7\t9.51667\n",
    "GNB\tGuinea-Bissau\tAfrica\tWestern Africa\t36125\t1974\tGuine-Bissau\tRepublic\tBissau\t-15.1804\t11.8037\n",
    "GUY\tGuyana\tSouth America\tSouth America\t214969\t1966\tGuyana\tRepublic\tGeorgetown\t-58.1548\t6.80461\n",
    "HTI\tHaiti\tNorth America\tCaribbean\t27750\t1804\tHaiti/Dayti\tRepublic\tPort-au-Prince\t-72.3288\t18.5392\n",
    "HND\tHonduras\tNorth America\tCentral America\t112088\t1838\tHonduras\tRepublic\tTegucigalpa\t-87.4667\t15.1333\n",
    "IDN\tIndonesia\tAsia\tSoutheast Asia\t1904570\t1945\tIndonesia\tRepublic\tJakarta\t106.83\t-6.19752\n",
    "IND\tIndia\tAsia\tSouthern and Central Asia\t3287260\t1947\tBharat/India\tFederal Republic\tNew Delhi\t77.225\t28.6353\n",
    "IRQ\tIraq\tAsia\tMiddle East\t438317\t1932\tAl-´Iraq\tRepublic\tBaghdad\t44.394\t33.3302\n",
    "IRN\tIran\tAsia\tSouthern and Central Asia\t1648200\t1906\tIran\tIslamic Republic\tTehran\t51.4447\t35.6878\n",
    "IRL\tIreland\tEurope\tBritish Islands\t70273\t1921\tIreland/Eire\tRepublic\tDublin\t-6.26749\t53.3441\n",
    "ISL\tIceland\tEurope\tNordic Countries\t103000\t1944\tIsland\tRepublic\tReykjavik\t-21.8952\t64.1353\n",
    "ISR\tIsrael\tAsia\tMiddle East\t21056\t1948\tYisrael/Israil\tRepublic\tnull\t35.2035\t31.7717\n",
    "ITA\tItaly\tEurope\tSouthern Europe\t301316\t1861\tItalia\tRepublic\tRome\t12.4823\t41.8955\n",
    "AUT\tAustria\tEurope\tWestern Europe\t83859\t1918\tOsterreich\tFederal Republic\tVienna\t16.3798\t48.2201\n",
    "JAM\tJamaica\tNorth America\tCaribbean\t10990\t1962\tJamaica\tConstitutional Monarchy\tKingston\t-76.792\t17.9927\n",
    "JPN\tJapan\tAsia\tEastern Asia\t377829\t-660\tNihon/Nippon\tConstitutional Monarchy\tTokyo\t139.77\t35.67\n",
    "YEM\tYemen\tAsia\tMiddle East\t527968\t1918\tAl-Yaman\tRepublic\tSana'a\t44.2075\t15.352\n",
    "JOR\tJordan\tAsia\tMiddle East\t88946\t1946\tAl-Urdunn\tConstitutional Monarchy\tAmman\t35.9263\t31.9497\n",
    "KHM\tCambodia\tAsia\tSoutheast Asia\t181035\t1953\tKampuchea\tConstitutional Monarchy\tPhnom Penh\t104.874\t11.5556\n",
    "CMR\tCameroon\tAfrica\tCentral Africa\t475442\t1960\tCameroun/Cameroon\tRepublic\tYaounde\t11.5174\t3.8721\n",
    "CAN\tCanada\tNorth America\tNorth America\t9970610\t1867\tCanada\tConstitutional Monarchy, Federation\tOttawa\t-75.6919\t45.4215\n",
    "CPV\tCape Verde\tAfrica\tWestern Africa\t4033\t1975\tCabo Verde\tRepublic\tPraia\t-23.5087\t14.9218\n",
    "KAZ\tKazakhstan\tAsia\tSouthern and Central Asia\t2724900\t1991\tQazaqstan\tRepublic\tAstana\t71.4382\t51.1879\n",
    "KEN\tKenya\tAfrica\tEastern Africa\t580367\t1963\tKenya\tRepublic\tNairobi\t36.8126\t-1.27975\n",
    "CAF\tCentral African Republic\tAfrica\tCentral Africa\t622984\t1960\tCentrafrique/Be-Afrika\tRepublic\tBangui\t21.6407\t5.63056\n",
    "CHN\tChina\tAsia\tEastern Asia\t9572900\t-1523\tZhongquo\tPeople'sRepublic\tBeijing\t116.286\t40.0495\n",
    "KGZ\tKyrgyzstan\tAsia\tSouthern and Central Asia\t199900\t1991\tKyrgyzstan\tRepublic\tBishkek\t74.6057\t42.8851\n",
    "KIR\tKiribati\tOceania\tMicronesia\t726\t1979\tKiribati\tRepublic\tTarawa\t172.979\t1.32905\n",
    "COL\tColombia\tSouth America\tSouth America\t1138910\t1810\tColombia\tRepublic\tBogota\t-74.082\t4.60987\n",
    "COM\tComoros\tAfrica\tEastern Africa\t1862\t1975\tKomori/Comores\tRepublic\tMoroni\t43.2418\t-11.6986\n",
    "COG\tCongo\tAfrica\tCentral Africa\t342000\t1960\tCongo\tRepublic\tBrazzaville\t15.2662\t-4.2767\n",
    "COD\tCongo, The Democratic Republic of the\tAfrica\tCentral Africa\t2344860\t1960\tRepublique Democratique du Congo\tRepublic\tKinshasa\t15.3222\t-4.325\n",
    "PRK\tNorth Korea\tAsia\tEastern Asia\t120538\t1948\tChoson Minjujuui In´min Konghwaguk (Bukhan)\tSocialistic Republic\tPyongyang\t125.754\t39.0319\n",
    "KOR\tSouth Korea\tAsia\tEastern Asia\t99434\t1948\tTaehan Minguk (Namhan)\tRepublic\tSeoul\t126.957\t37.5323\n",
    "GRC\tGreece\tEurope\tSouthern Europe\t131626\t1830\tEllada\tRepublic\tAthens\t23.7166\t37.9792\n",
    "HRV\tCroatia\tEurope\tSouthern Europe\t56538\t1991\tHrvatska\tRepublic\tZagreb\t15.9614\t45.8069\n",
    "CUB\tCuba\tNorth America\tCaribbean\t110861\t1902\tCuba\tSocialistic Republic\tHavana\t-82.3667\t23.1333\n",
    "KWT\tKuwait\tAsia\tMiddle East\t17818\t1961\tAl-Kuwayt\tConstitutional Monarchy (Emirate)\tKuwait City\t47.9824\t29.3721\n",
    "CYP\tCyprus\tAsia\tMiddle East\t9251\t1960\tKypros/Kibris\tRepublic\tNicosia\t33.3736\t35.1676\n",
    "LAO\tLaos\tAsia\tSoutheast Asia\t236800\t1953\tLao\tRepublic\tVientiane\t102.177\t18.5826\n",
    "LVA\tLatvia\tEurope\tBaltic Countries\t64589\t1991\tLatvija\tRepublic\tRiga\t24.1048\t56.9465\n",
    "LSO\tLesotho\tAfrica\tSouthern Africa\t30355\t1966\tLesotho\tConstitutional Monarchy\tMaseru\t27.7167\t-29.5208\n",
    "Showing 100 out of 205 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting a relationship\n",
    "You've just identified that the countries table has a many-to-many relationship with the languages table. That is, many languages can be spoken in a country, and a language can be spoken in many countries.\n",
    "\n",
    "This exercise looks at each of these in turn. First, what is the best way to query all the different languages spoken in a country? And second, how is this different from the best way to query all the countries that speak each language?\n",
    "\n",
    "Recall that when writing joins, many users prefer to write SQL code out of order by writing the join first (along with any table aliases), and writing the SELECT statement at the end.\n",
    "\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "Start with the join statement in line 6; perform an inner join with the countries table as c on the left with the languages table as l on the right.\n",
    "Make use of the USING keyword to join on code in line 8.\n",
    "Lastly, in line 2, select the country name, aliased as country, and the language name, aliased as language.\n",
    "\n",
    "-- Select country and language names, aliased\n",
    "SELECT c.name AS country, l.name AS language\n",
    "-- From countries (aliased)\n",
    "FROM countries AS c\n",
    "-- Join to languages (aliased)\n",
    "INNER JOIN languages AS l\n",
    "-- Use code as the joining field with the USING keyword\n",
    "USING (code);\n",
    "\n",
    "\n",
    "-- Rearrange SELECT statement, keeping aliases\n",
    "-- SELECT c.name AS country, l.name AS language\n",
    "SELECT l.name AS language, c.name AS country\n",
    "FROM countries AS c\n",
    "INNER JOIN languages AS l\n",
    "USING(code)\n",
    "-- Order the results by language\n",
    "ORDER BY language;\n",
    "\n",
    "Question\n",
    "Select the incorrect answer from the following options.\n",
    "\n",
    "The query you generated in step 1 is provided below. Run this query (or the amendment you made in step 2) in the console to find the answer to the question.\n",
    "\n",
    "SELECT c.name AS country, l.name AS language\n",
    "FROM countries AS c\n",
    "INNER JOIN languages AS l\n",
    "USING(code)\n",
    "ORDER BY country;\n",
    "Possible answers\n",
    "\n",
    "\n",
    "There are at least three languages spoken in Armenia.\n",
    "\n",
    "Alsatian is spoken in more than one country.\n",
    "\n",
    "Bhojpuri is spoken in two countries.\n",
    "\n",
    "\n",
    "Correct! Alsatian is only spoken in France. Well done getting through this exercise. When we read SQL results, we expect the most important column to be on the far left, and it's helpful if results are ordered by relevance to the question at hand. By default, results are ordered by the column from the left table, but you can change this using ORDER BY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining multiple tables\n",
    "You've seen that the ability to combine multiple joins using a single query is a powerful feature of SQL.\n",
    "\n",
    "Suppose you are interested in the relationship between fertility and unemployment rates. Your task in this exercise is to join tables to return the country name, year, fertility rate, and unemployment rate in a single result from the countries, populations and economies tables.\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "Perform an inner join of countries AS c (left) with populations AS p (right), on code.\n",
    "Select name, year and fertility_rate.\n",
    "Chain another inner join to your query with the economies table AS e, using code.\n",
    "Select name, and using table aliases, select year and unemployment_rate from the economies table.\n",
    "\n",
    "\n",
    "-- Select relevant fields\n",
    "SELECT name, year, fertility_rate\n",
    "FROM countries AS c\n",
    "\n",
    "-- Inner join countries and populations, aliased, on code\n",
    "INNER JOIN populations AS p ON c.code = p.country_code;\n",
    "\n",
    "\n",
    "-- Select relevant fields\n",
    "SELECT \n",
    "    name, \n",
    "    e.year, \n",
    "    fertility_rate,\n",
    "    unemployment_rate\n",
    "-- Inner join countries and populations, aliased, on code\n",
    "FROM \n",
    "    countries AS c\n",
    "INNER JOIN \n",
    "    populations AS p \n",
    "ON \n",
    "    c.code = p.country_code\n",
    "-- Chain another inner join with the economies table, aliased, on code and year\n",
    "INNER JOIN \n",
    "    economies AS e \n",
    "ON \n",
    "    c.code = e.code\n",
    "\n",
    "Good work! You may have noticed an issue with these results, though. Have a look at the results for Albania. The 2010 value for fertility_rate is also paired with the 2015 value for unemployment_rate, whereas we only want data from one year per record. You'll fix this in the next exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking multi-table joins\n",
    "Have a look at the results for Albania from the previous query below. You can see that the 2015 fertility_rate has been paired with 2010 unemployment_rate, and vice versa.\n",
    "\n",
    "name\tyear\tfertility_rate\tunemployment_rate\n",
    "Albania\t2015\t1.663\t17.1\n",
    "Albania\t2010\t1.663\t14\n",
    "Albania\t2015\t1.793\t17.1\n",
    "Albania\t2010\t1.793\t14\n",
    "Instead of four records, the query should return two: one for each year. The last join was performed on c.code = e.code, without also joining on year. Your task in this exercise is to fix your query by explicitly stating that both the country code and year should match!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Modify your query so that you are joining to economies on year as well as code.\n",
    "\n",
    "-- SELECT name, e.year, fertility_rate, unemployment_rate\n",
    "-- FROM countries AS c\n",
    "-- INNER JOIN populations AS p\n",
    "-- ON c.code = p.country_code\n",
    "-- INNER JOIN economies AS e\n",
    "-- ON c.code = e.code\n",
    "-- Add an additional joining condition such that you are also joining on year modifying the code.\n",
    "SELECT c.name, e.year, p.fertility_rate, e.unemployment_rate\n",
    "FROM countries AS c\n",
    "INNER JOIN populations AS p ON c.code = p.country_code\n",
    "INNER JOIN economies AS e ON c.code = e.code AND p.year = e.year;\n",
    "\n",
    "\n",
    "Good work! You can check that this fixed the issue by looking at the results for Albania. There are only two lines of Albania results now: one for 2010 and one for 2015.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a LEFT JOIN, right?\n",
    "Nice work getting to grips with the structure of joins! In this exercise, you'll explore the differences between INNER JOIN and LEFT JOIN. This will help you decide which type of join to use.\n",
    "\n",
    "As before, you will be using the cities and countries tables.\n",
    "\n",
    "You'll begin with an INNER JOIN with the cities table (left) and countries table (right). This helps if you are interested only in records where a country is present in both tables.\n",
    "\n",
    "You'll then change to a LEFT JOIN. This helps if you're interested in returning all countries in the cities table, whether or not they have a match in the countries table.\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "Perform an inner join with cities AS c1 on the left and countries as c2 on the right.\n",
    "Use code as the field to merge your tables on.\n",
    "Change the code to perform a LEFT JOIN instead of an INNER JOIN.\n",
    "After executing this query, have a look at how many records the query result contains.\n",
    "\n",
    "-- Select columns from both tables with clear aliases\n",
    "SELECT \n",
    "  c1.name AS city,  -- City name from 'cities' table (aliased as c1) renamed as 'city'\n",
    "  code,               -- Country code (likely shared by both tables)\n",
    "  c2.name AS country,  -- Country name from 'countries' table (aliased as c2) renamed as 'country'\n",
    "  region,              -- Region from 'countries' table\n",
    "  city_proper_pop     -- City population from 'cities' table\n",
    "FROM cities AS c1  -- Select data from 'cities' table and assign alias 'c1'\n",
    "\n",
    "-- Inner join 'countries' table on matching country code\n",
    "INNER JOIN countries AS c2 ON c1.country_code = c2.code\n",
    "\n",
    "-- Order results by country code (descending)\n",
    "ORDER BY code DESC;\n",
    "\n",
    "\n",
    "SELECT c1.name AS city, code, c2.name AS country, region, city_proper_pop\n",
    "FROM cities AS c1\n",
    "LEFT JOIN countries AS c2 ON c1.country_code = c2.code\n",
    "ORDER BY code DESC;\n",
    "\n",
    "\n",
    "Perfect! Notice that the INNER JOIN resulted in 230 records, whereas the LEFT JOIN returned 236 records. Remember that the LEFT JOIN is a type of outer join: its result is not limited to only those records that have matches for both tables on the joining field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECT name, region, gdp_percapita\n",
    "FROM countries AS c\n",
    "LEFT JOIN economies AS e ON c.code = e.code  -- Match on code fields\n",
    "WHERE year = 2010;  -- Filter for the year 2010\n",
    "\n",
    "\n",
    "Building on your LEFT JOIN\n",
    "You'll now revisit the use of the AVG() function introduced in a previous course.\n",
    "\n",
    "Being able to build more than one SQL function into your query will enable you to write compact, supercharged queries.\n",
    "\n",
    "You will use AVG() in combination with a LEFT JOIN to determine the average gross domestic product (GDP) per capita by region in 2010.\n",
    "\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "Complete the LEFT JOIN with the countries table on the left and the economies table on the right on the code field.\n",
    "Filter the records from the year 2010.\n",
    "\n",
    "-- Select region, and average gdp_percapita as avg_gdp\n",
    "SELECT region, AVG(gdp_percapita) AS avg_gdp\n",
    "FROM countries AS c\n",
    "LEFT JOIN economies AS e\n",
    "USING(code)\n",
    "WHERE year = 2010\n",
    "-- Group by region\n",
    "GROUP BY region;\n",
    "\n",
    "\n",
    "SELECT region, AVG(gdp_percapita) AS avg_gdp\n",
    "FROM countries AS c\n",
    "LEFT JOIN economies AS e\n",
    "USING(code)\n",
    "WHERE year = 2010\n",
    "GROUP BY region\n",
    "-- Order by descending avg_gdp\n",
    "ORDER BY avg_gdp DESC\n",
    "-- Return only first 10 records\n",
    "LIMIT 10;\n",
    "\n",
    "Nice work! You successfully executed a LEFT JOIN and applied a GROUP BY to the result of your JOIN. Building up your SQL vocabulary in this way will enable you to answer questions of ever-increasing complexity!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this RIGHT?\n",
    "You learned that right joins are not used as commonly as left joins. A key reason for this is that right joins can always be re-written as left joins, and because joins are typically typed from left to right, joining from the left feels more intuitive when constructing queries.\n",
    "\n",
    "It can be tricky to wrap one's head around when left and right joins return equivalent results. You'll explore this in this exercise!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Write a new query using RIGHT JOIN that produces an identical result to the LEFT JOIN provided.\n",
    "\n",
    "\n",
    "\n",
    "SELECT countries.name AS country, languages.name AS language, percent\n",
    "FROM languages\n",
    "RIGHT JOIN countries USING(code)\n",
    "ORDER BY language;\n",
    "\n",
    "Correct: when converting a LEFT JOIN to a RIGHT JOIN, change both the type of join and the order of the tables to get equivalent results. You would get different results if you only changed the table order. The order of fields you are joining ON still does not matter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing joins\n",
    "In this exercise, you'll examine how results can differ when performing a full join compared to a left join and inner join by joining the countries and currencies tables. You'll be focusing on the North American region and records where the name of the country is missing.\n",
    "\n",
    "You'll begin with a full join with countries on the left and currencies on the right. Recall the workings of a full join with the diagram below!\n",
    "\n",
    "\n",
    "\n",
    "You'll then complete a similar left join and conclude with an inner join, observing the results you see along the way.\n",
    "\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "Perform a full join with countries (left) and currencies (right).\n",
    "Filter for the North America region or NULL country names.\n",
    "Repeat the same query as before, turning your full join into a left join with the currencies table.\n",
    "Have a look at what has changed in the output by comparing it to the full join result.\n",
    "Repeat the same query again, this time performing an inner join of countries with currencies.\n",
    "Have a look at what has changed in the output by comparing it to the full join and left join results!\n",
    "\n",
    "SELECT name AS country, code, region, basic_unit\n",
    "FROM countries\n",
    "FULL JOIN currencies USING (code)  -- Full join on code field\n",
    "WHERE region = 'North America' OR name IS NULL\n",
    "ORDER BY region;\n",
    "\n",
    "\n",
    "SELECT name AS country, code, region, basic_unit\n",
    "FROM countries\n",
    "LEFT JOIN currencies USING (code)  -- Left join on code field\n",
    "WHERE region = 'North America' OR name IS NULL\n",
    "ORDER BY region;\n",
    "\n",
    "SELECT name AS country, code, region, basic_unit\n",
    "FROM countries\n",
    "INNER JOIN currencies USING (code)  -- Inner join on code field\n",
    "WHERE region = 'North America' OR name IS NULL\n",
    "ORDER BY region;\n",
    "\n",
    "\n",
    "Have you kept an eye out for the different numbers of records these queries returned? The FULL JOIN query returned 18 records, the LEFT JOIN returned four records, and the INNER JOIN only returned three records. Does this add more color to the diagrams you have seen for these three types of join?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECT c1.name AS country, region, l.name AS language, basic_unit, frac_unit\n",
    "FROM countries AS c1\n",
    "FULL JOIN languages AS l ON c1.code = l.code  -- Full join with languages\n",
    "FULL JOIN currencies AS c2 ON c1.code = c2.code  -- Chained full join with currencies\n",
    "WHERE region LIKE 'M%esia';  -- Filter for Melanesia and Micronesia\n",
    "\n",
    "Chaining FULL JOINs\n",
    "As you have seen in the previous chapter on INNER JOIN, it is possible to chain joins in SQL, such as when looking to connect data from more than two tables.\n",
    "\n",
    "Suppose you are doing some research on Melanesia and Micronesia, and are interested in pulling information about languages and currencies into the data we see for these regions in the countries table. Since languages and currencies exist in separate tables, this will require two consecutive full joins involving the countries, languages and currencies tables.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Complete the FULL JOIN with countries as c1 on the left and languages as l on the right, using code to perform this join.\n",
    "Next, chain this join with another FULL JOIN, placing currencies on the right, joining on code again.\n",
    "\n",
    "Take Hint (-30 XP)\n",
    "\n",
    "Well done! The first FULL JOIN in the query pulled countries and languages, and the second FULL JOIN added in currency data for each record in the result of the first FULL JOIN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECT c.name AS country, l.name AS language\n",
    "FROM countries AS c\n",
    "INNER JOIN languages AS l ON c.code = l.code\n",
    "WHERE c.name IN ('Pakistan', 'India');\n",
    "\n",
    "\n",
    "SELECT c.name AS country, l.name AS language\n",
    "FROM countries AS c\n",
    "CROSS JOIN languages AS l  -- Changed to CROSS JOIN\n",
    "WHERE c.code IN ('PAK', 'IND')\n",
    "AND l.code IN ('PAK', 'IND');  -- Maintained for clarity (has no effect with CROSS JOIN)\n",
    "\n",
    "\n",
    "Histories and languages\n",
    "Well done getting to know all about CROSS JOIN! As you have learned, CROSS JOIN can be incredibly helpful when asking questions that involve looking at all possible combinations or pairings between two sets of data.\n",
    "\n",
    "Imagine you are a researcher interested in the languages spoken in two countries: Pakistan and India. You are interested in asking:\n",
    "\n",
    "What are the languages presently spoken in the two countries?\n",
    "Given the shared history between the two countries, what languages could potentially have been spoken in either country over the course of their history?\n",
    "In this exercise, we will explore how INNER JOIN and CROSS JOIN can help us answer these two questions, respectively.\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "Complete the code to perform an INNER JOIN of countries AS c with languages AS l using the code field to obtain the languages currently spoken in the two countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECT c.name AS country, region, life_expectancy AS life_exp\n",
    "FROM countries AS c\n",
    "INNER JOIN populations AS p ON c.code = p.country_code  -- Inner join on code\n",
    "WHERE p.year = 2010  -- Filter for year 2010\n",
    "ORDER BY life_expectancy ASC  -- Sort by life expectancy (ascending)\n",
    "LIMIT 5;  -- Limit to top 5 records\n",
    "Choosing your join\n",
    "Now that you're fully equipped to use joins, try a challenge problem to test your knowledge!\n",
    "\n",
    "You will determine the names of the five countries and their respective regions with the lowest life expectancy for the year 2010. Use your knowledge about joins, filtering, sorting and limiting to create this list!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Complete the join of countries AS c with populations as p.\n",
    "Filter on the year 2010.\n",
    "Sort your results by life expectancy in ascending order.\n",
    "Limit the result to five countries.\n",
    "\n",
    "Take Hint (-30 XP)\n",
    "Nice work! Did you notice that more than one type of join can be used to return the five records in our result? All four types of joins we have learned will return the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECT p1.country_code, p1.size AS size2010, p2.size AS size2015\n",
    "FROM populations AS p1\n",
    "INNER JOIN populations AS p2 ON p1.country_code = p2.country_code;\n",
    "\n",
    "\n",
    "SELECT p1.country_code, p1.size AS size2010, p2.size AS size2015\n",
    "FROM populations AS p1\n",
    "INNER JOIN populations AS p2 ON p1.country_code = p2.country_code\n",
    "WHERE p1.year = 2010  -- Filter p1 for year 2010\n",
    "AND p2.year = p1.year + 5;  -- Filter p2 for year 5 years after p1.year (2015)\n",
    "\n",
    "\n",
    "Comparing a country to itself\n",
    "Self joins are very useful for comparing data from one part of a table with another part of the same table. Suppose you are interested in finding out how much the populations for each country changed from 2010 to 2015. You can visualize this change by performing a self join.\n",
    "\n",
    "In this exercise, you'll work to answer this question by joining the populations table with itself. Recall that, with self joins, tables must be aliased. Use this as an opportunity to practice your aliasing!\n",
    "\n",
    "Since you'll be joining the populations table to itself, you can alias populations first as p1 and again as p2. This is good practice whenever you are aliasing tables with the same first letter.\n",
    "\n",
    "Instructions 2/2\n",
    "50 XP\n",
    "Since you want to compare records from 2010 and 2015, eliminate unwanted records by extending the WHERE statement to include only records where the p1.year matches p2.year - 5.\n",
    "\n",
    "Nice one! See how it's possible to eliminate unwanted records using a calculated field, such as the one you've subtracted five from? That's a great trick to know.\n",
    "\n",
    "All joins on deck\n",
    "Excellent work! You've made it to the end of the chapter. In this exercise, you will test your knowledge on all the joins you've learned so far.\n",
    "\n",
    "For each of the problems presented, think carefully about what types of tables are involved and how each of the joins you have learned relates to NULL values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\n",
    "Set Theory for SQL Joins\n",
    "\n",
    "100%\n",
    "In this chapter, you will learn about using set theory operations in SQL, with an introduction to UNION, UNION ALL, INTERSECT, and EXCEPT clauses. You’ll explore the predominant ways in which set theory operations differ from join operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECT *\n",
    "FROM economies2015\n",
    "\n",
    "UNION\n",
    "\n",
    "SELECT *\n",
    "FROM economies2019\n",
    "\n",
    "ORDER BY code, year;\n",
    "\n",
    "\n",
    "Comparing global economies\n",
    "Are you ready to perform your first set operation?\n",
    "\n",
    "In this exercise, you have two tables, economies2015 and economies2019, available to you under the tabs in the console. You'll perform a set operation to stack all records in these two tables on top of each other, excluding duplicates.\n",
    "\n",
    "When drafting queries containing set operations, it is often helpful to write the queries on either side of the operation first, and then call the set operator. The instructions are ordered accordingly.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Begin your query by selecting all fields from economies2015.\n",
    "Create a second query that selects all fields from economies2019.\n",
    "Perform a set operation to combine the two queries you just created, ensuring you do not return duplicates.\n",
    "\n",
    "Your first UNION! UNION can be helpful for consolidating data from multiple tables into one result, which as you have seen, can then be ordered in meaningful ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECT code, year\n",
    "FROM economies\n",
    "\n",
    "UNION\n",
    "\n",
    "SELECT country_code, year\n",
    "FROM populations\n",
    "\n",
    "ORDER BY code, year;\n",
    "\n",
    "SELECT code, year\n",
    "FROM economies\n",
    "UNION ALL\n",
    "SELECT country_code, year\n",
    "FROM populations\n",
    "ORDER BY code, year;\n",
    "\n",
    "Comparing two set operations\n",
    "You learned in the video exercise that UNION ALL returns duplicates, whereas UNION does not. In this exercise, you will dive deeper into this, looking at cases for when UNION is appropriate compared to UNION ALL.\n",
    "\n",
    "You will be looking at combinations of country code and year from the economies and populations tables.\n",
    "\n",
    "Instructions 2/2\n",
    "50 XP\n",
    "Perform an appropriate set operation that determines all pairs of country code and year (in that order) from economies and populations, excluding duplicates.\n",
    "Order by country code and year.\n",
    "Amend the query to return all combinations (including duplicates) of country code and year in the economies or the populations tables.\n",
    "\n",
    "Nicely done! UNION returned 434 records, whereas UNION ALL returned 814. Are you able to spot the duplicates in the UNION ALL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Return all cities with the same name as a country\n",
    "SELECT name\n",
    "FROM cities\n",
    "INTERSECT\n",
    "SELECT name\n",
    "FROM countries;\n",
    "\n",
    "\n",
    "INTERSECT\n",
    "Well done getting through the material on INTERSECT!\n",
    "\n",
    "Let's say you are interested in those countries that share names with cities. Use this task as an opportunity to show off your knowledge of set theory in SQL!\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Return all city names that are also country names.\n",
    "\n",
    "Nice one! It looks as though Singapore is the only country in our database that has a city with the same name!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've got it, EXCEPT...\n",
    "Just as you were able to leverage INTERSECT to find the names of cities with the same names as countries, you can also do the reverse, using EXCEPT.\n",
    "\n",
    "In this exercise, you will find the names of cities that do not have the same names as their countries.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Return all cities that do not have the same name as a country.\n",
    "\n",
    "-- Return all cities that do not have the same name as a country\n",
    "SELECT name\n",
    "FROM cities\n",
    "EXCEPT\n",
    "SELECT name\n",
    "FROM countries\n",
    "ORDER BY name;\n",
    "\n",
    "\n",
    "\n",
    "EXCEPTional! Note that if countries had been on the left and cities on the right, you would have returned the opposite: all countries that do not have the same name as a city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Select country code for countries in the Middle East\n",
    "SELECT code\n",
    "FROM countries\n",
    "WHERE region = 'Middle East';\n",
    "\n",
    "-- Select unique language names\n",
    "-- Select distinct language names from the languages table\n",
    "SELECT DISTINCT \n",
    "    name \n",
    "FROM \n",
    "    languages\n",
    "-- Order by the name of the language\n",
    "-- Order by name in ascending order\n",
    "ORDER BY \n",
    "    name ASC;\n",
    "\n",
    "\n",
    "-- Select distinct language names from the languages table\n",
    "SELECT DISTINCT name\n",
    "FROM languages\n",
    "-- Use a subquery to filter by country codes in the Middle East region\n",
    "WHERE code IN (\n",
    "    SELECT code\n",
    "    FROM countries\n",
    "    WHERE region = 'Middle East'\n",
    ")\n",
    "-- Order by name in ascending order\n",
    "ORDER BY name;\n",
    "\n",
    "\n",
    "Semi join\n",
    "Great job getting acquainted with semi joins and anti joins! You are now going to practice using semi joins.\n",
    "\n",
    "Let's say you are interested in identifying languages spoken in the Middle East. The languages table contains information about languages and countries, but it does not tell you what region the countries belong to. You can build up a semi join by filtering the countries table by a particular region, and then using this to further filter the languages table.\n",
    "\n",
    "You'll build up your semi join as you did in the video exercise, block by block, starting with a selection of countries from the countries table, and then leveraging a WHERE clause to filter the languages table by this selection.\n",
    "\n",
    "Instructions 3/3\n",
    "30 XP\n",
    "Create a semi join out of the two queries you've written, which filters unique languages returned in the first query for only those languages spoken in the 'Middle East'.\n",
    "\n",
    "Well done writing your first subquery in the form of a semi join. Think of all the opportunities that open up when queries become building blocks of larger queries!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Select code and name of countries from Oceania\n",
    "\n",
    "SELECT code, name\n",
    "FROM countries\n",
    "WHERE continent = 'Oceania';\n",
    "\n",
    "\n",
    "SELECT code, name\n",
    "FROM countries\n",
    "WHERE continent = 'Oceania'\n",
    "-- Filter for countries not included in the bracketed subquery\n",
    "AND code NOT IN (\n",
    "  SELECT code\n",
    "  FROM currencies\n",
    ");\n",
    "\n",
    "\n",
    "Diagnosing problems using anti join\n",
    "Nice work on semi joins! The anti join is a related and powerful joining tool. It can be particularly useful for identifying whether an incorrect number of records appears in a join.\n",
    "\n",
    "Say you are interested in identifying currencies of Oceanian countries. You have written the following INNER JOIN, which returns 15 records. Now, you want to ensure that all Oceanian countries from the countries table are included in this result. You'll do this in the first step.\n",
    "\n",
    "SELECT c1.code, name, basic_unit AS currency\n",
    "FROM countries AS c1\n",
    "INNER JOIN currencies AS c2\n",
    "ON c1.code = c2.code\n",
    "WHERE c1.continent = 'Oceania';\n",
    "If there are any Oceanian countries excluded in this INNER JOIN, you want to return the names of these countries. You'll write an anti join to this in the second step!\n",
    "\n",
    "Instructions 2/2\n",
    "50 XP\n",
    "Now, build on your query to complete your anti join, by adding an additional filter to return every country code that is not included in the currencies table.\n",
    "\n",
    "Nice! Your anti join determined which five out of 19 countries that were not included in the INNER JOIN provided. Did you notice that Tuvalu has two currencies, and therefore shows up twice in the INNER JOIN? This is why the INNER JOIN returned 15 rather than 14 results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Select average life_expectancy from the populations table\n",
    "SELECT AVG(life_expectancy) \n",
    "FROM populations\n",
    "-- Filter for the year 2015\n",
    "WHERE year = 2015;\n",
    "\n",
    "SELECT *\n",
    "FROM populations\n",
    "WHERE life_expectancy > 1.15 * (\n",
    "  SELECT AVG(life_expectancy) AS avg_life_expectancy\n",
    "  FROM populations\n",
    "  WHERE year = 2015\n",
    ")\n",
    "AND year = 2015;\n",
    "\n",
    "\n",
    "Subquery inside WHERE\n",
    "The video pointed out that subqueries inside WHERE can either be from the same table or a different table. In this exercise, you will nest a subquery from the populations table inside another query from the same table, populations. Your goal is to figure out which countries had high average life expectancies in 2015.\n",
    "\n",
    "You can use SQL to do calculations for you. Suppose you only want records from 2015 with life_expectancy above 1.15 * avg_life_expectancy. You could use the following SQL query.\n",
    "\n",
    "SELECT *\n",
    "FROM populations\n",
    "WHERE life_expectancy > 1.15 * avg_life_expectancy\n",
    "  AND year = 2015;\n",
    "In the first step, you'll write a query to calculate a value for avg_life_expectancy. In the second step, you will nest this calculation into another query.\n",
    "\n",
    "Instructions 2/2\n",
    "50 XP\n",
    "The answer from your query has now been nested into another query; use this calculation to filter populations for all records where life_expectancy is 1.15 times higher than average.\n",
    "\n",
    "Nice work! You may recognize many of these country codes as being relatively wealthy countries, which makes sense as we might expect life expectancy to be higher in wealthier nations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHERE do people live?\n",
    "In this exercise, you will strengthen your knowledge of subquerying by identifying capital cities in order of largest to smallest population.\n",
    "\n",
    "Follow the instructions below to get the urban area population for capital cities only. You'll use the countries and cities tables displayed in the console to help identify columns of interest as you build your query.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Return the name, country_code and urbanarea_pop for all capital cities (not aliased).\n",
    "\n",
    "SELECT name, country_code, urbanarea_pop\n",
    "FROM cities\n",
    "WHERE name IN (\n",
    "    SELECT capital\n",
    "    FROM countries\n",
    ")\n",
    "ORDER BY urbanarea_pop DESC;\n",
    "\n",
    "\n",
    "Alright! You've got plenty of practice on subqueries inside WHERE. Let's move on to subqueries inside the SELECT statement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subquery inside SELECT\n",
    "As explored in the video, there are often multiple ways to produce the same result in SQL. You saw that subqueries can provide an alternative to joins to obtain the same result.\n",
    "\n",
    "In this exercise, you'll go further in exploring how some queries can be written using either a join or a subquery.\n",
    "\n",
    "In Step 1, you'll begin with a LEFT JOIN combined with a GROUP BY to select the nine countries with the most cities appearing in the cities table, along with the counts of these cities. In Step 2, you'll write a query that returns the same result as the join, but leveraging a nested query instead.\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "Write a LEFT JOIN with countries on the left and the cities on the right, joining on country code.\n",
    "In the SELECT statement of your join, include country names as country, and count the cities in each country, aliased as cities_num.\n",
    "Sort by cities_num (descending), and country (ascending), limiting to the first nine records.\n",
    "\n",
    "-- Find top nine countries with the most cities\n",
    "SELECT \n",
    "    countries.name AS country, \n",
    "    COUNT(cities.name) AS cities_num\n",
    "FROM \n",
    "    countries\n",
    "LEFT JOIN \n",
    "    cities ON countries.code = cities.country_code\n",
    "GROUP BY \n",
    "    countries.name\n",
    "ORDER BY \n",
    "    cities_num DESC, \n",
    "    country ASC\n",
    "LIMIT 9;\n",
    "\n",
    "\n",
    "SELECT \n",
    "    countries.name AS country,\n",
    "    -- Subquery that provides the count of cities\n",
    "    (SELECT COUNT(*)\n",
    "     FROM cities\n",
    "     WHERE cities.country_code = countries.code) AS cities_num\n",
    "FROM \n",
    "    countries\n",
    "ORDER BY \n",
    "    cities_num DESC, \n",
    "    country\n",
    "LIMIT 9;\n",
    "\n",
    "Excellent job! Notice how the subquery involves only one additional step in your SELECT statement, whereas the JOIN and GROUP BY are a two-step process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECT code, COUNT(*) AS lang_num\n",
    "FROM languages\n",
    "GROUP BY code\n",
    "ORDER BY code ASC;\n",
    "\n",
    "\n",
    "Subquery inside FROM\n",
    "Subqueries inside FROM can help select columns from multiple tables in a single query.\n",
    "\n",
    "Say you are interested in determining the number of languages spoken for each country. You want to present this information alongside each country's local_name, which is a field only present in the countries table and not in the languages table. You'll use a subquery inside FROM to bring information from these two tables together!\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "Begin with a query that groups by each country code from languages, and counts the languages spoken in each country as lang_num.\n",
    "In your SELECT statement, return code and lang_num (in that order).\n",
    "\n",
    "SELECT c.local_name, sub.lang_num\n",
    "FROM countries AS c\n",
    "INNER JOIN (\n",
    "  SELECT code, COUNT(*) AS lang_num\n",
    "  FROM languages\n",
    "  GROUP BY code\n",
    ") AS sub ON c.code = sub.code\n",
    "ORDER BY sub.lang_num DESC;\n",
    "\n",
    "This one brought several joining concepts together! Did you notice that the top three countries in our result are located in Africa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subquery challenge\n",
    "You're near the finish line! Test your understanding of subquerying with a challenge problem.\n",
    "\n",
    "Suppose you're interested in analyzing inflation and unemployment rate for certain countries in 2015. You are not interested in countries with \"Republic\" or \"Monarchy\" as their form of government, but are interested in all other forms of government, such as emirate federations, socialist states, and commonwealths.\n",
    "\n",
    "You will use the field gov_form to filter for these two conditions, which represents a country's form of government. You can review the different entries for gov_form in the countries table.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Select country code, inflation_rate, and unemployment_rate from economies.\n",
    "Filter code for the set of countries which do not contain the words \"Republic\" or \"Monarchy\" in their gov_form.\n",
    "\n",
    "SELECT code, inflation_rate, unemployment_rate\n",
    "FROM economies\n",
    "WHERE year = 2015\n",
    "AND code NOT IN\n",
    "  (SELECT code\n",
    "   FROM countries\n",
    "   WHERE gov_form LIKE '%Republic%' OR gov_form LIKE '%Monarchy%')\n",
    "ORDER BY inflation_rate;\n",
    "\n",
    "\n",
    "Superb work writing the majority of the query yourself. You found that in 2015, South Sudan (with country code SSD) had inflation above 50%! Did you also notice that this query is an anti join?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final challenge\n",
    "You've made it to the final challenge problem! Get ready to tackle this step-by-step.\n",
    "\n",
    "Your task is to determine the top 10 capital cities in Europe and the Americas by city_perc, a metric you'll calculate. city_perc is a percentage that calculates the \"proper\" population in a city as a percentage of the total population in the wider metro area, as follows:\n",
    "\n",
    "city_proper_pop / metroarea_pop * 100\n",
    "\n",
    "Do not use table aliasing in this exercise.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "From cities, select the city name, country code, proper population, and metro area population, as well as the field city_perc, which calculates the proper population as a percentage of metro area population for each city (using the formula provided).\n",
    "Filter city name with a subquery that selects capital cities from countries in 'Europe' or continents with 'America' at the end of their name.\n",
    "Exclude NULL values in metroarea_pop.\n",
    "Order by city_perc (descending) and return only the first 10 rows.\n",
    "\n",
    "\n",
    "-- Select fields from cities\n",
    "SELECT \n",
    "    name, \n",
    "    country_code, \n",
    "    city_proper_pop, \n",
    "    metroarea_pop,\n",
    "    (city_proper_pop / metroarea_pop * 100) AS city_perc\n",
    "FROM \n",
    "    cities\n",
    "-- Use subquery to filter city name\n",
    "WHERE \n",
    "    name IN (\n",
    "        SELECT \n",
    "            capital\n",
    "        FROM \n",
    "            countries\n",
    "        WHERE \n",
    "            continent = 'Europe' \n",
    "            OR continent LIKE '%America'\n",
    "    )\n",
    "-- Add filter condition such that metroarea_pop does not have null values\n",
    "AND \n",
    "    metroarea_pop IS NOT NULL\n",
    "-- Sort and limit the result\n",
    "ORDER BY \n",
    "    city_perc DESC\n",
    "LIMIT 10;\n",
    "\n",
    "You've identified that Lima has the highest percentage of people living in the city 'proper', relative to the wider metropolitan population! Nicely done getting to the top of the summit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# introduction to git\n",
    "\n",
    "Description\n",
    "This course introduces learners to version control using Git. You will discover the importance of version control when working on data science projects and explore how you can use Git to track files, compare differences, modify and save files, undo changes, and allow collaborative development through the use of branches. You will gain an introduction to the structure of a repository, how to create new repositories and clone existing ones, and show how Git stores data. By working through typical data science tasks, you will gain the skills to handle conflicting files.\n",
    "\n",
    "\n",
    "1\n",
    "Introduction to Git\n",
    "\n",
    "100%\n",
    "In the first chapter, you’ll learn what version control is and why it is essential for data projects. Then, you’ll discover what Git is and how to use it for a version control workflow.\n",
    "2\n",
    "Making changes\n",
    "\n",
    "100%\n",
    "Next, you’ll examine how Git stores data, learn essential commands to compare files and repositories at different times, and understand the process for restoring earlier versions of files in your data projects.\n",
    "3\n",
    "Git workflows\n",
    "\n",
    "100%\n",
    "In this chapter, you'll learn tips and tricks for configuring Git to make you more efficient! You'll also discover branches, identify how to create and switch to different branches, compare versions of files between branches, merge branches together, and deal with conflicting files across branches.\n",
    "4\n",
    "Collaborating with Git\n",
    "\n",
    "100%\n",
    "This final chapter is all about collaboration! You'll gain an introduction to remote repositories and learn how to work with them to synchronize content between the cloud and your local computer. You'll also see how to create new repositories and clone existing ones, along with discovering a workflow to minimize the risk of conflicts between local and remote repositories.\n",
    "\n",
    "\n",
    "Great work! The --version flag allows you to check what version of Git is installed locally, which is useful if you upgrade, or need to compare versions among colleagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DataCamp', 'Python', 'Practice']\n"
     ]
    }
   ],
   "source": [
    "x = [[1,2,3,4,5],[\"DataCamp\",\"Python\",\"Practice\",\"Exercises\"],[True, False, 0, 1]]\n",
    "print(x[1][:3])\n",
    "# ['DataCamp', 'Python', 'Practice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 6, 12, 13, 14, 16]\n"
     ]
    }
   ],
   "source": [
    "p = [1, 16, 13, 6, 12, 14]\n",
    "print(sorted(p, reverse=False))\n",
    "# [1, 6, 12, 13, 14, 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# developing python packages\n",
    "From script to package\n",
    "One common way to begin writing a package is to start with code you have already written as a script. At the time you first write this code, you may not realize how useful it might be in other places.\n",
    "\n",
    "If you did the prerequisite course, in one exercise you wrote a script to count the number of times cats were mentioned in the book Alice in Wonderland.\n",
    "\n",
    "In this exercise, you'll copy from that script to make a generalized function you can use on any text file for any words. This will be the first function in a new library.\n",
    "\n",
    "Ide Exercise Instruction\n",
    "100XP\n",
    "Create a new directory called textanalysis for your package. Click File > New Folder in the IDE.\n",
    "Create __init__.py and textanalysis.py modules inside textanalysis. Click the new textanalysis folder, then click File > New File in the IDE to create new files inside it.\n",
    "Copy the code from myscript.py into textanalysis.py.\n",
    "Modify textanalysis.py to create the function count_words(filepath, words_list) which opens the text file filepath, and returns the number of times the words in words_list appear.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lewis Carroll uses the word \"cat\" 27 times\n"
     ]
    }
   ],
   "source": [
    "# Open the text file\n",
    "with open('alice.txt') as file:\n",
    "    text = file.read()\n",
    "\n",
    "n = 0\n",
    "for word in text.split():\n",
    "    # Count the number of times the words in the list appear\n",
    "    if word.lower() in ['cat', 'cats']:\n",
    "        n += 1\n",
    "\n",
    "print('Lewis Carroll uses the word \"cat\" {} times'.format(n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# textanalysis.py\n",
    "alice = 'os.path.dirname(alice.txt)'\n",
    "# alice = 'home\\repl\\alice.txt'\n",
    "def count_words(alice, words_list):\n",
    "    # Open the text file\n",
    "    with open(alice, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    n = 0\n",
    "    for word in text.split():\n",
    "        # Count the number of times the words in the list appear\n",
    "        if word.lower() in words_list:\n",
    "            n += 1\n",
    "\n",
    "    return n\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    words_to_count = ['cat', 'cats']\n",
    "    alice = 'os.path.dirname(alice.txt)'\n",
    "    count = count_words(alice, words_to_count)\n",
    "    print('Lewis Carroll uses the words {} {} times'.format(words_to_count, count))\n",
    "\n",
    "\n",
    "# Well done. Generalizing functions that you already have is a common way of starting a library. This means you can use the same code in other projects without having to copy and paste it across."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting your package to work\n",
    "Now you have wrapped your word-counting function into a package, you can reuse it easily in other projects.\n",
    "\n",
    "In the initial script, you were analyzing the book Alice in Wonderland. In this new project, you will use the same function to analyze hotel reviews from TripAdvisor.\n",
    "\n",
    "The count_words() function has been imported for you at the top of this script. We'll talk more about importing from your packages in a later lesson.\n",
    "\n",
    "Ide Exercise Instruction\n",
    "100XP\n",
    "Use your new package to count the number of times the positive words 'good' or 'great' appear in the file 'hotel-reviews.txt'.\n",
    "Use the package to count the number of times the negative words 'bad' or 'awful' appear.\n",
    "\n",
    "from textanalysis.textanalysis import count_words\n",
    "\n",
    "# Count the number of positive words\n",
    "nb_positive_words = ____(____, ____)\n",
    "\n",
    "# Count the number of negative words\n",
    "nb_negative_words = ____\n",
    "\n",
    "print(\"{} positive words.\".format(nb_positive_words))\n",
    "print(\"{} negative words.\".format(nb_negative_words))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# documentation\n",
    "Writing function documentation with pyment\n",
    "Using documentation templates helps you stick to one of the standard styles.\n",
    "\n",
    "In this exercise, you'll use pyment, which was mentioned in the previous video, to create NumPy style documentation for a function.\n",
    "\n",
    "Ide Exercise Instruction\n",
    "100XP\n",
    "In the terminal at the bottom of the screen, write a command using pyment to create NumPy style documentation for the file impyrial/length/core.py.\n",
    "Run the command in the terminal before Submit Answer, please note that you do not need to fill in the details in the generated documentation after you run the command.\n",
    "\n",
    "imperial/__init__.py\n",
    "imperial/lenght/core.py\n",
    "INCHES_PER_FOOT = 12.0  # 12 inches in a foot\n",
    "INCHES_PER_YARD = INCHES_PER_FOOT * 3.0  # 3 feet in a yard\n",
    "\n",
    "UNITS = (\"in\", \"ft\", \"yd\")\n",
    "\n",
    "\n",
    "def inches_to_feet(x, reverse=False):\n",
    "    if reverse:\n",
    "        return x * INCHES_PER_FOOT\n",
    "    else:\n",
    "        return x / INCHES_PER_FOOT\n",
    "\n",
    "\n",
    "lenght/__init__.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing function documentation with pyment II\n",
    "Documentation helps your users learn how to use your functions, and can even help remind you how to use them.\n",
    "\n",
    "In this exercise, you'll fill out your NumPy style documentation template to make some beautiful documentation.\n",
    "\n",
    "Please refer to the previous video and this document about writing documentation in NumPy style.\n",
    "\n",
    "Ide Exercise Instruction\n",
    "100XP\n",
    "In the text editor, complete the documentation for the inches_to_feet() function. The short description for this function should read \"Convert lengths between inches and feet.\"\n",
    "Complete the x parameter documentation with type numpy.ndarray and description \"Lengths in feet.\"\n",
    "Complete the reverse parameter documentation with type bool, optional and the description \"If true this function converts from feet to inches instead of the default behavior of inches to feet. (Default value = False)\".\n",
    "Set the return type to numpy.ndarray.\n",
    "\n",
    "imperial/__init__.py\n",
    "# __init__ code here\n",
    "\n",
    "imperial/lenght/core.py\n",
    "core.py code here\n",
    "INCHES_PER_FOOT = 12.0  # 12 inches in a foot\n",
    "INCHES_PER_YARD = INCHES_PER_FOOT * 3.0  # 3 feet in a yard\n",
    "\n",
    "UNITS = (\"in\", \"ft\", \"yd\")\n",
    "\n",
    "\n",
    "def inches_to_feet(x, reverse=False):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x :\n",
    "\n",
    "    reverse :\n",
    "         (Default value = False)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    if reverse:\n",
    "        return x * INCHES_PER_FOOT\n",
    "    else:\n",
    "        return x / INCHES_PER_FOOT\n",
    "\n",
    "\n",
    "imperial/lenght/__init__.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package and module documentation\n",
    "Package and module level documentation helps your users navigate your package.\n",
    "\n",
    "In this exercise, you will write documentation for the impyrial package. Pay attention to this documentation, you're going to be working on this package throughout this course, and it's worth knowing what its different parts do.\n",
    "\n",
    "Ide Exercise Instruction\n",
    "100XP\n",
    "Add the following package level documentation to impyrial:\n",
    "impyrial\n",
    "========\n",
    "A package for converting between imperial \n",
    "measurements of length and weight.\n",
    "Add the following subpackage level documentation to impyrial.length:\n",
    "impyrial.length\n",
    "===============\n",
    "Length conversion between imperial units.\n",
    "Add the following module documentation to impyrial.length.core:\n",
    "Conversions between inches and \n",
    "larger imperial length units\n",
    "\n",
    "\n",
    "imperial/lenght/core.py\n",
    "INCHES_PER_FOOT = 12.0  # 12 inches in a foot\n",
    "INCHES_PER_YARD = INCHES_PER_FOOT * 3.0  # 3 feet in a yard\n",
    "\n",
    "UNITS = (\"in\", \"ft\", \"yd\")\n",
    "\n",
    "\n",
    "def inches_to_feet(x, reverse=False):\n",
    "    \"\"\"Convert lengths between inches and feet.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy.ndarray\n",
    "        Lengths in feet.\n",
    "    reverse : bool, optional\n",
    "        If true this function converts from feet to inches \n",
    "        instead of the default behavior of inches to feet. \n",
    "        (Default value = False)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "    \"\"\"\n",
    "    if reverse:\n",
    "        return x * INCHES_PER_FOOT\n",
    "    else:\n",
    "        return x / INCHES_PER_FOOT\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simplify 3^2x+1 = 243\n",
    "\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "To simplify the equation \n",
    "3\n",
    "2\n",
    "𝑥\n",
    "+\n",
    "1\n",
    "=\n",
    "243\n",
    "3 \n",
    "2x+1\n",
    " =243, follow these steps:\n",
    "\n",
    "Express 243 as a power of 3:\n",
    "\n",
    "243\n",
    "=\n",
    "3\n",
    "5\n",
    "243=3 \n",
    "5\n",
    " \n",
    "So the equation becomes:\n",
    "\n",
    "3\n",
    "2\n",
    "𝑥\n",
    "+\n",
    "1\n",
    "=\n",
    "3\n",
    "5\n",
    "3 \n",
    "2x+1\n",
    " =3 \n",
    "5\n",
    " \n",
    "Since the bases are the same, set the exponents equal to each other:\n",
    "\n",
    "2\n",
    "𝑥\n",
    "+\n",
    "1\n",
    "=\n",
    "5\n",
    "2x+1=5\n",
    "Solve for \n",
    "𝑥\n",
    "x:\n",
    "\n",
    "2\n",
    "𝑥\n",
    "=\n",
    "5\n",
    "−\n",
    "1\n",
    "=\n",
    "4\n",
    "2x=5−1=4\n",
    "\n",
    "𝑥\n",
    "=\n",
    "4\n",
    "2\n",
    "=\n",
    "2\n",
    "x= \n",
    "2\n",
    "4\n",
    "​\n",
    " =2\n",
    "So, the simplified solution is \n",
    "𝑥\n",
    "=\n",
    "2\n",
    "x=2.\n",
    "\n",
    "You said:\n",
    "x = [[1,2,3,4,5],[\"DataCamp\",\"Python\",\"Practice\",\"Exercises\"],[True, False, 0, 1]]\n",
    "print(x[\n",
    "?\n",
    "][:3])\n",
    "['DataCamp', 'Python', 'Practice']\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "To print ['DataCamp', 'Python', 'Practice'] from the list x, you need to access the second sublist within x. In Python, list indices start at 0, so the index of the second sublist is 1.\n",
    "\n",
    "The correct code would be:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(x[1][:3])\n",
    "This will output:\n",
    "\n",
    "css\n",
    "Copy code\n",
    "['DataCamp', 'Python', 'Practice']\n",
    "You said:\n",
    "\n",
    "Complete \n",
    "the \n",
    "code \n",
    "to \n",
    "return \n",
    "the \n",
    "output \n",
    "p = [1, 16, 13, 6, 12, 14]\n",
    "print(\n",
    "?\n",
    "(p, reverse=False))\n",
    "[1, 6, 12, 13, 14, 16]\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "To return the output [1, 6, 12, 13, 14, 16] from the list p = [1, 16, 13, 6, 12, 14], you need to sort the list in ascending order using the sorted() function.\n",
    "\n",
    "The correct code would be:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "p = [1, 16, 13, 6, 12, 14]\n",
    "print(sorted(p, reverse=False))\n",
    "This will output:\n",
    "\n",
    "csharp\n",
    "Copy code\n",
    "[1, 6, 12, 13, 14, 16]\n",
    "Here, reverse=False ensures that the list is sorted in ascending order.\n",
    "\n",
    "You said:\n",
    "in sql what is the role of intersec\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "In SQL, the INTERSECT operator is used to return the common records (or rows) from two or more SELECT statements. Essentially, it combines the results of multiple SELECT queries and returns only the rows that appear in all the result sets. The number and order of columns in the SELECT statements must be the same, and the data types of corresponding columns must be compatible.\n",
    "\n",
    "Example:\n",
    "Suppose you have two tables, employees and managers, and you want to find the employees who are also managers.\n",
    "\n",
    "employees table:\n",
    "emp_id\tname\n",
    "1\tAlice\n",
    "2\tBob\n",
    "3\tCarol\n",
    "managers table:\n",
    "emp_id\tname\n",
    "2\tBob\n",
    "3\tCarol\n",
    "4\tDave\n",
    "SQL Query:\n",
    "sql\n",
    "Copy code\n",
    "SELECT emp_id, name FROM employees\n",
    "INTERSECT\n",
    "SELECT emp_id, name FROM managers;\n",
    "Result:\n",
    "emp_id\tname\n",
    "2\tBob\n",
    "3\tCarol\n",
    "This query returns only the rows that are common between the employees and managers tables.\n",
    "\n",
    "You said:\n",
    "\n",
    "\n",
    "The \n",
    "songs\n",
    ", \n",
    "releases\n",
    ", \n",
    "and \n",
    "listens\n",
    "tables \n",
    "are \n",
    "shown. \n",
    "The \n",
    "releases\n",
    "table \n",
    "contains \n",
    "exactly \n",
    "one \n",
    "release \n",
    "date \n",
    "for \n",
    "each \n",
    "song \n",
    "in \n",
    "the \n",
    "songs\n",
    "table. \n",
    "The \n",
    "listens\n",
    "table \n",
    "contains \n",
    "information \n",
    "about \n",
    "listeners \n",
    "and \n",
    "all \n",
    "the \n",
    "songs \n",
    "they \n",
    "have \n",
    "listened \n",
    "to. \n",
    "Which \n",
    "of \n",
    "the \n",
    "following \n",
    "tables \n",
    "have \n",
    "a \n",
    "one-to-one \n",
    "relationship? \n",
    "\n",
    "\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "Based on the information provided:\n",
    "\n",
    "Songs and Releases Tables:\n",
    "\n",
    "Each song in the songs table has exactly one release date in the releases table.\n",
    "The song_id in the songs table corresponds directly to a song_id in the releases table.\n",
    "Since each song has only one release date and every release date corresponds to one song, the relationship between songs and releases is one-to-one.\n",
    "Songs and Listens Tables:\n",
    "\n",
    "The listens table contains multiple entries for each song, indicating which listeners have listened to the songs.\n",
    "This means one song can appear multiple times in the listens table, corresponding to different listeners.\n",
    "Therefore, the relationship between songs and listens is one-to-many.\n",
    "Conclusion:\n",
    "The tables that have a one-to-one relationship are the **songs** and **releases** tables.\n",
    "\n",
    "You said:\n",
    "\n",
    "The \n",
    "flat \n",
    "file \n",
    "'winequality-red.csv' \n",
    "is \n",
    "available \n",
    "at \n",
    "the \n",
    "following \n",
    "url, \n",
    "as \n",
    "'url'. \n",
    "Complete \n",
    "the \n",
    "code \n",
    "to \n",
    "return \n",
    "the \n",
    "output \n",
    "?\n",
    "(url,'winequality-red.csv')\n",
    "df = pd.read_csv('winequality-red.csv',\n",
    "     sep = ';')\n",
    "print(df[['alcohol','quality']].head())\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "To load the CSV file from the provided URL into a pandas DataFrame, you'll need to replace 'winequality-red.csv' with the variable url that contains the file's URL. Here's how you can complete the code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "url = 'URL_TO_THE_CSV_FILE'  # Replace with the actual URL\n",
    "df = pd.read_csv(url, sep=';')\n",
    "print(df[['alcohol', 'quality']].head())\n",
    "Replace 'URL_TO_THE_CSV_FILE' with the actual URL of the CSV file. This code will load the data from the specified URL and print the first few rows of the alcohol and quality columns.\n",
    "\n",
    "You said:\n",
    "\n",
    "The \n",
    "file, \n",
    "'winequality_white.csv' \n",
    "is \n",
    "available \n",
    "at \n",
    "the \n",
    "link \n",
    "shown \n",
    "below \n",
    "and \n",
    "saved \n",
    "to \n",
    "url\n",
    ". \n",
    "Each \n",
    "data \n",
    "value \n",
    "is \n",
    "separated \n",
    "by \n",
    "a \n",
    ";\n",
    ". \n",
    "Complete \n",
    "the \n",
    "code \n",
    "to \n",
    "return \n",
    "the \n",
    "output \n",
    "urlretrieve(url,'winequal-wh.csv')\n",
    "data = \n",
    "?\n",
    "('winequal-wh.csv',\n",
    "     \n",
    "?\n",
    ")\n",
    "print(data[['alcohol','quality']].head())\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "To complete the code to download the file and load it into a pandas DataFrame, you can use the following code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Define the URL and file name\n",
    "url = 'URL_TO_THE_CSV_FILE'  # Replace with the actual URL\n",
    "urlretrieve(url, 'winequal-wh.csv')\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "data = pd.read_csv('winequal-wh.csv', sep=';')\n",
    "\n",
    "# Print the first few rows of the 'alcohol' and 'quality' columns\n",
    "print(data[['alcohol', 'quality']].head())\n",
    "Replace 'URL_TO_THE_CSV_FILE' with the actual URL of the winequality_white.csv file. This code will download the file, read it into a DataFrame, and then print the first few rows of the alcohol and quality columns.\n",
    "\n",
    "You said:\n",
    "\n",
    "The \n",
    "file \n",
    "'tweets_data' \n",
    "is \n",
    "available \n",
    "locally. \n",
    "It \n",
    "contains \n",
    "sub-parts \n",
    "text\n",
    "and \n",
    "lang\n",
    "(e.g. \n",
    "'en'). \n",
    "Complete \n",
    "the \n",
    "code \n",
    "to \n",
    "return \n",
    "the \n",
    "output \n",
    "df = pd.DataFrame(\n",
    "?\n",
    ",\n",
    "        columns = \n",
    "?\n",
    ")\n",
    "print(df.head(2))\n",
    "  lang                                               text\n",
    "0   en  b\"RT @bpolitics: .@krollbondrating's Christoph...\n",
    "1   en  b'RT @HeidiAlpine: @dmartosko Cruz video found...\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "To create a DataFrame from the tweets_data file and include only the lang and text columns, you can follow this approach:\n",
    "\n",
    "Assuming that tweets_data is already structured as a list of dictionaries or a similar iterable format where each item contains lang and text:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming tweets_data is a list of dictionaries\n",
    "tweets_data = [\n",
    "    {\"lang\": \"en\", \"text\": \"b\\\"RT @bpolitics: .@krollbondrating's Christoph...\"},\n",
    "    {\"lang\": \"en\", \"text\": \"b'RT @HeidiAlpine: @dmartosko Cruz video found...\"}\n",
    "    # Add more tweet data here...\n",
    "]\n",
    "\n",
    "# Create a DataFrame with the specified columns\n",
    "df = pd.DataFrame(tweets_data, columns=['lang', 'text'])\n",
    "\n",
    "# Print the first 2 rows\n",
    "print(df.head(2))\n",
    "If tweets_data is loaded from a file, ensure that it is read correctly into an appropriate Python data structure before creating the DataFrame. The example assumes tweets_data is already in memory as a list of dictionaries.\n",
    "\n",
    "You said:\n",
    "Copy the code from myscript.py into textanalysis.py.\n",
    "Modify textanalysis.py to create the function count_words(filepath, words_list) which opens the text file filepath, and returns the number of times the words in words_list appear.\n",
    "\n",
    "\n",
    "here is the code\n",
    "textanalysis.py\n",
    "# Open the text file\n",
    "with open('alice.txt') as file:\n",
    "    text = file.read()\n",
    "\n",
    "n = 0\n",
    "for word in text.split():\n",
    "    # Count the number of times the words in the list appear\n",
    "    if word.lower() in ['cat', 'cats']:\n",
    "        n += 1\n",
    "\n",
    "print('Lewis Carroll uses the word \"cat\" {} times'.format(n))\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "It seems that my previous response included some syntax errors when I attempted to introduce the explanation. Here's the correct version of the textanalysis.py code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# textanalysis.py\n",
    "\n",
    "def count_words(filepath, words_list):\n",
    "    # Open the text file\n",
    "    with open(filepath, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    n = 0\n",
    "    for word in text.split():\n",
    "        # Count the number of times the words in the list appear\n",
    "        if word.lower() in words_list:\n",
    "            n += 1\n",
    "\n",
    "    return n\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    words_to_count = ['cat', 'cats']\n",
    "    filepath = 'alice.txt'\n",
    "    count = count_words(filepath, words_to_count)\n",
    "    print('Lewis Carroll uses the words {} {} times'.format(words_to_count, count))\n",
    "Explanation:\n",
    "Function Definition:\n",
    "\n",
    "The count_words(filepath, words_list) function takes two parameters:\n",
    "filepath: The path to the text file.\n",
    "words_list: A list of words to count in the text.\n",
    "File Handling:\n",
    "\n",
    "The text file specified by filepath is opened and read.\n",
    "Word Counting:\n",
    "\n",
    "The text is split into words, and for each word, the function checks if it matches any word in words_list. The count is incremented if a match is found.\n",
    "Return Statement:\n",
    "\n",
    "The function returns the total count of the specified words.\n",
    "Example Usage:\n",
    "\n",
    "The example usage demonstrates how to call the function to count occurrences of the words 'cat' and 'cats' in the file 'alice.txt'.\n",
    "You can now save this code in textanalysis.py to create the function as requested. ​​\n",
    "\n",
    "You said:\n",
    "Create a new directory called textanalysis for your package. Click File > New Folder in the IDE.\n",
    "Create __init__.py and textanalysis.py modules inside textanalysis. Click the new textanalysis folder, then click File > New File in the IDE to create new files inside it.\n",
    "Copy the code from myscript.py into textanalysis.py.\n",
    "Modify textanalysis.py to create the function count_words(filepath, words_list) which opens the text file filepath, and returns the number of times the words in words_list appear.\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "Memory updated\n",
    "To create your textanalysis package and modify the textanalysis.py module as described, follow these steps:\n",
    "\n",
    "Step 1: Create a New Directory for the Package\n",
    "In your IDE, click on File > New Folder.\n",
    "Name the folder textanalysis.\n",
    "Step 2: Create __init__.py and textanalysis.py Modules\n",
    "Inside the newly created textanalysis folder, create two new files:\n",
    "__init__.py: This file can be empty or you can add a docstring to describe the package.\n",
    "textanalysis.py: This will contain your function and the code.\n",
    "Step 3: Copy Code from myscript.py to textanalysis.py\n",
    "Open the myscript.py file.\n",
    "Copy the existing code (which opens a file and counts specific words).\n",
    "Paste this code into textanalysis.py.\n",
    "Step 4: Modify textanalysis.py to Create the count_words Function\n",
    "Here’s how to modify the code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# textanalysis.py\n",
    "\n",
    "def count_words(filepath, words_list):\n",
    "    \"\"\"\n",
    "    This function opens the text file at the given filepath\n",
    "    and counts the number of times the words in words_list appear.\n",
    "    \n",
    "    :param filepath: Path to the text file\n",
    "    :param words_list: List of words to count in the text\n",
    "    :return: Integer count of word occurrences\n",
    "    \"\"\"\n",
    "    # Open the text file\n",
    "    with open(filepath, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    n = 0\n",
    "    for word in text.split():\n",
    "        # Count the number of times the words in the list appear\n",
    "        if word.lower() in words_list:\n",
    "            n += 1\n",
    "\n",
    "    return n\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    words_to_count = ['cat', 'cats']\n",
    "    filepath = 'alice.txt'\n",
    "    count = count_words(filepath, words_to_count)\n",
    "    print('Lewis Carroll uses the words {} {} times'.format(words_to_count, count))\n",
    "Step 5: Save Your Changes\n",
    "Save the textanalysis.py file after adding the code above.\n",
    "\n",
    "Step 6: Testing the Function\n",
    "To test the function:\n",
    "\n",
    "Run the textanalysis.py script or import the count_words function in another script and pass a filepath and list of words to count.\n",
    "This will set up your textanalysis package with the desired functionality!\n",
    "\n",
    "You said:\n",
    "the alic.txt is in the main directory while textanalysis.py is in subdirectory\n",
    "how would I add the path\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "Since alice.txt is in the main directory and textanalysis.py is in a subdirectory (textanalysis), you need to specify the correct relative path to the file when opening it in textanalysis.py.\n",
    "\n",
    "Here’s how you can modify the path:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# textanalysis.py\n",
    "\n",
    "import os\n",
    "\n",
    "def count_words(filepath, words_list):\n",
    "    \"\"\"\n",
    "    This function opens the text file at the given filepath\n",
    "    and counts the number of times the words in words_list appear.\n",
    "    \n",
    "    :param filepath: Path to the text file\n",
    "    :param words_list: List of words to count in the text\n",
    "    :return: Integer count of word occurrences\n",
    "    \"\"\"\n",
    "    # Get the absolute path to the file\n",
    "    file_path = os.path.join(os.path.dirname(__file__), '..', filepath)\n",
    "    \n",
    "    # Open the text file\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    n = 0\n",
    "    for word in text.split():\n",
    "        # Count the number of times the words in the list appear\n",
    "        if word.lower() in words_list:\n",
    "            n += 1\n",
    "\n",
    "    return n\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    words_to_count = ['cat', 'cats']\n",
    "    filepath = 'alice.txt'\n",
    "    count = count_words(filepath, words_to_count)\n",
    "    print('Lewis Carroll uses the words {} {} times'.format(words_to_count, count))\n",
    "Explanation:\n",
    "Importing os: The os module is imported to work with file paths in a platform-independent way.\n",
    "\n",
    "Constructing the File Path:\n",
    "\n",
    "os.path.dirname(__file__) gives the directory of the current script (textanalysis.py).\n",
    "os.path.join(os.path.dirname(__file__), '..', filepath) constructs the path to alice.txt, moving one level up from the textanalysis directory to the main directory.\n",
    "Opening the File:\n",
    "\n",
    "The constructed path (file_path) is used to open alice.txt.\n",
    "Testing\n",
    "Run the script, and it should correctly open alice.txt from the main directory while being executed from the subdirectory textanalysis.\n",
    "\n",
    "You said:\n",
    "how can i use sql in vscode\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "Using SQL in Visual Studio Code (VS Code) is straightforward with the right extensions and setup. Here's a step-by-step guide:\n",
    "\n",
    "Step 1: Install SQL-Related Extensions\n",
    "SQL Server (mssql):\n",
    "\n",
    "Go to the Extensions view by clicking on the Extensions icon on the sidebar or pressing Ctrl+Shift+X.\n",
    "Search for \"SQL Server (mssql)\" by Microsoft.\n",
    "Click \"Install.\"\n",
    "SQLTools:\n",
    "\n",
    "Search for \"SQLTools\" in the Extensions view.\n",
    "Click \"Install.\"\n",
    "You may also need to install additional SQLTools drivers for specific databases (e.g., MySQL, PostgreSQL, SQLite).\n",
    "Step 2: Set Up a Connection\n",
    "After installing the necessary extensions, you need to set up a connection to your SQL database.\n",
    "\n",
    "For SQL Server (mssql):\n",
    "\n",
    "Press Ctrl+Shift+P to open the Command Palette.\n",
    "Type MS SQL: Connect and select it.\n",
    "Follow the prompts to enter your server details, including the server name, database name, username, and password.\n",
    "Once connected, you can start running queries.\n",
    "For SQLTools:\n",
    "\n",
    "Click on the SQLTools icon in the sidebar.\n",
    "Click \"New Connection\" or go to the Command Palette (Ctrl+Shift+P) and type SQLTools: New Connection.\n",
    "Select your database type and fill in the connection details.\n",
    "Step 3: Write and Run SQL Queries\n",
    "Create a New SQL File:\n",
    "\n",
    "In VS Code, create a new file and save it with a .sql extension.\n",
    "Write SQL Queries:\n",
    "\n",
    "Type your SQL queries in the file.\n",
    "Execute Queries:\n",
    "\n",
    "Highlight the query you want to execute (or place your cursor on it).\n",
    "Right-click and select \"Run Query\" (for mssql) or press Ctrl+Alt+E.\n",
    "If you're using SQLTools, you can also use the SQLTools query execution button.\n",
    "View Results:\n",
    "\n",
    "The results will appear in the \"Output\" or \"Results\" pane, depending on the extension you're using.\n",
    "Step 4: Additional Features\n",
    "IntelliSense: Both the mssql and SQLTools extensions provide IntelliSense for SQL queries, which includes auto-completion and syntax highlighting.\n",
    "Manage Connections: You can manage and switch between different database connections using the SQLTools sidebar or the mssql extension commands.\n",
    "Export Results: Results from queries can often be exported to CSV or JSON directly from the results pane.\n",
    "Summary\n",
    "By installing the mssql or SQLTools extensions, setting up your database connection, and using the query editor, you can effectively write and run SQL queries directly within VS Code. This setup is highly customizable, allowing you to work with various types of databases right from your coding environment.\n",
    "\n",
    "You said:\n",
    "how do I change string to uppercase in python\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "To change a string to uppercase in Python, you can use the .upper() method. This method converts all the characters in a string to uppercase.\n",
    "\n",
    "Example:\n",
    "python\n",
    "Copy code\n",
    "# Define a string\n",
    "my_string = \"hello world\"\n",
    "\n",
    "# Convert the string to uppercase\n",
    "uppercase_string = my_string.upper()\n",
    "\n",
    "# Print the result\n",
    "print(uppercase_string)\n",
    "Output:\n",
    "Copy code\n",
    "HELLO WORLD\n",
    "In this example, \"hello world\" is converted to \"HELLO WORLD\" using the .upper() method.\n",
    "\n",
    "You said:\n",
    "explain asc and desc in sql concept\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "In SQL, ASC and DESC are keywords used to specify the sort order of the results returned by a SELECT query. These keywords are typically used in conjunction with the ORDER BY clause.\n",
    "\n",
    "ASC (Ascending Order)\n",
    "Default Sort Order: When you use ORDER BY without specifying ASC or DESC, the query will default to ASC.\n",
    "Numerical Order: Numbers will be sorted from the smallest to the largest.\n",
    "Alphabetical Order: Text strings (VARCHAR, CHAR) will be sorted alphabetically from A to Z.\n",
    "Date/Time Order: Dates and times will be sorted from the earliest to the latest.\n",
    "Example of ASC:\n",
    "sql\n",
    "Copy code\n",
    "SELECT * \n",
    "FROM employees \n",
    "ORDER BY salary ASC;\n",
    "Explanation: This query selects all columns from the employees table and orders the results by the salary column in ascending order. Employees with the lowest salaries will appear first.\n",
    "DESC (Descending Order)\n",
    "Reverse Order: When DESC is specified, the sort order is reversed.\n",
    "Numerical Order: Numbers will be sorted from the largest to the smallest.\n",
    "Alphabetical Order: Text strings will be sorted from Z to A.\n",
    "Date/Time Order: Dates and times will be sorted from the latest to the earliest.\n",
    "Example of DESC:\n",
    "sql\n",
    "Copy code\n",
    "SELECT * \n",
    "FROM employees \n",
    "ORDER BY salary DESC;\n",
    "Explanation: This query selects all columns from the employees table and orders the results by the salary column in descending order. Employees with the highest salaries will appear first.\n",
    "Combining ASC and DESC with Multiple Columns\n",
    "You can order by multiple columns, specifying different sort orders for each column.\n",
    "\n",
    "Example:\n",
    "sql\n",
    "Copy code\n",
    "SELECT * \n",
    "FROM employees \n",
    "ORDER BY department_id ASC, salary DESC;\n",
    "Explanation: This query orders the results first by department_id in ascending order, and within each department, employees are ordered by salary in descending order.\n",
    "Summary\n",
    "ASC (Ascending): Sorts from smallest to largest (numerically) or A to Z (alphabetically).\n",
    "DESC (Descending): Sorts from largest to smallest (numerically) or Z to A (alphabetically).\n",
    "Usage: ASC and DESC are used with the ORDER BY clause to control the sort order of query results.\n",
    "You said:\n",
    "Fill \n",
    "in \n",
    "the \n",
    "blank \n",
    "in \n",
    "the \n",
    "for \n",
    "loop \n",
    "below \n",
    "to \n",
    "output \n",
    "the \n",
    "desired \n",
    "number \n",
    "of \n",
    "hyperlinks. \n",
    "Complete \n",
    "the \n",
    "code \n",
    "to \n",
    "return \n",
    "the \n",
    "output \n",
    "for link in s.find_all(\n",
    "?\n",
    ")[:3]:\n",
    "    print(link.get('href'))\n",
    "pics.html\n",
    "http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\n",
    "http://metalab.unc.edu/Dave/Dr-Fun/df200004/df20000406.jpg\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "To extract hyperlinks using BeautifulSoup, you typically use the find_all() method to search for all anchor tags (<a>), as anchor tags are used for hyperlinks.\n",
    "\n",
    "Here's how you can complete the code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Assuming 's' is your BeautifulSoup object\n",
    "for link in s.find_all('a')[:3]:\n",
    "    print(link.get('href'))\n",
    "Explanation:\n",
    "'a': This is the HTML tag for anchor elements, which are used to define hyperlinks.\n",
    "[:3]: This slice notation limits the output to the first three hyperlinks found.\n",
    "link.get('href'): The get() method is used to extract the value of the href attribute from the anchor tag, which contains the URL.\n",
    "Output:\n",
    "plaintext\n",
    "Copy code\n",
    "pics.html\n",
    "http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\n",
    "http://metalab.unc.edu/Dave/Dr-Fun/df200004/df20000406.jpg\n",
    "This will print out the first three hyperlinks from the parsed HTML content.\n",
    "\n",
    "You said:\n",
    "A \n",
    "marketing \n",
    "team \n",
    "wants \n",
    "to \n",
    "understand \n",
    "the \n",
    "relationship \n",
    "between \n",
    "advertising \n",
    "spend \n",
    "and \n",
    "sales \n",
    "revenue. \n",
    "What \n",
    "type \n",
    "of \n",
    "chart \n",
    "would \n",
    "best \n",
    "illustrate \n",
    "this \n",
    "correlation? \n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "A scatter plot would be the best type of chart to illustrate the correlation between advertising spend and sales revenue.\n",
    "\n",
    "Why a Scatter Plot?\n",
    "Correlation Visualization: Scatter plots show the relationship between two continuous variables by displaying data points on a two-dimensional graph. Each point represents a pair of values, with one variable on the x-axis and the other on the y-axis.\n",
    "Trend Identification: By plotting advertising spend on the x-axis and sales revenue on the y-axis, the scatter plot can reveal trends, such as whether an increase in advertising spend is associated with an increase in sales revenue.\n",
    "Outliers Detection: Scatter plots also make it easy to spot outliers or any deviations from the expected trend.\n",
    "Example:\n",
    "X-axis: Advertising Spend\n",
    "Y-axis: Sales Revenue\n",
    "The resulting plot will help the marketing team visually assess the strength and direction of the relationship between the two variables.\n",
    "\n",
    "You said:\n",
    "\n",
    "When \n",
    "analyzing \n",
    "a \n",
    "dataset, \n",
    "what \n",
    "measure \n",
    "of \n",
    "center \n",
    "can \n",
    "you \n",
    "use \n",
    "if \n",
    "the \n",
    "dataset \n",
    "contains \n",
    "outliers? \n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "When analyzing a dataset that contains outliers, the median is often the best measure of center to use.\n",
    "\n",
    "Why Use the Median?\n",
    "Resilience to Outliers: The median is the middle value in a dataset when the data points are arranged in order. Unlike the mean, which can be heavily influenced by extreme values (outliers), the median remains unaffected by outliers. This makes it a more reliable measure of center for skewed distributions or datasets with outliers.\n",
    "Example:\n",
    "Dataset: [1, 2, 3, 4, 100]\n",
    "Mean: 22 (heavily influenced by the outlier 100)\n",
    "Median: 3 (not affected by the outlier)\n",
    "In this example, the median (3) gives a better sense of the central tendency of the dataset compared to the mean (22), which is skewed by the outlier.\n",
    "\n",
    "Summary:\n",
    "Median is the preferred measure of center when dealing with datasets that contain outliers.\n",
    "You said:\n",
    "\n",
    "Suppose \n",
    "you \n",
    "are \n",
    "analyzing \n",
    "the \n",
    "preferred \n",
    "programming \n",
    "languages \n",
    "of \n",
    "employees \n",
    "in \n",
    "a \n",
    "software \n",
    "development \n",
    "company. \n",
    "You \n",
    "are \n",
    "given \n",
    "a \n",
    "dataset \n",
    "df_languages\n",
    "which \n",
    "contains \n",
    "each \n",
    "employee's \n",
    "favorite \n",
    "programming_language\n",
    ". \n",
    "Get \n",
    "the \n",
    "most \n",
    "preferred \n",
    "programming \n",
    "languages \n",
    "among \n",
    "the \n",
    "employees. \n",
    "Complete \n",
    "the \n",
    "code \n",
    "to \n",
    "return \n",
    "the \n",
    "output \n",
    "import statistics\n",
    "# Get the preferred programming languages\n",
    "preferred_language = statistics.\n",
    "?\n",
    "(df_languages['programming_language'])\n",
    "print(\"Preferred programming language:\", preferred_language)\n",
    "Preferred programming language: Python\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "To get the most preferred programming language among the employees using the statistics module, you would use the mode() function, which returns the most common data point in a dataset.\n",
    "\n",
    "Here's how you can complete the code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import statistics\n",
    "\n",
    "# Get the preferred programming languages\n",
    "preferred_language = statistics.mode(df_languages['programming_language'])\n",
    "\n",
    "print(\"Preferred programming language:\", preferred_language)\n",
    "Explanation:\n",
    "statistics.mode(): This function returns the most frequently occurring value (mode) in the provided dataset. In this case, it finds the most common programming language in the programming_language column of the df_languages DataFrame.\n",
    "Example Output:\n",
    "plaintext\n",
    "Copy code\n",
    "Preferred programming language: Python\n",
    "This code will correctly identify and print the most preferred programming language among the employees.\n",
    "\n",
    "You said:\n",
    "Consider \n",
    "a \n",
    "Poisson \n",
    "distribution \n",
    "where \n",
    "the \n",
    "average \n",
    "number \n",
    "of \n",
    "events \n",
    "in \n",
    "a \n",
    "given \n",
    "time \n",
    "interval \n",
    "is \n",
    "5. \n",
    "How \n",
    "should \n",
    "you \n",
    "calculate \n",
    "the \n",
    "probability \n",
    "of \n",
    "observing \n",
    "exactly \n",
    "3 \n",
    "events \n",
    "in \n",
    "this \n",
    "time \n",
    "interval? \n",
    "Complete \n",
    "the \n",
    "code \n",
    "to \n",
    "return \n",
    "the \n",
    "output \n",
    "from scipy.stats import poisson\n",
    "poisson.\n",
    "?\n",
    "(3, 5)\n",
    "0.1403738958142805\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "To calculate the probability of observing exactly 3 events in a Poisson distribution where the average number of events is 5, you would use the pmf (Probability Mass Function) from the poisson distribution in scipy.stats. The pmf function computes the probability of a specific number of events occurring.\n",
    "\n",
    "Here's how you can complete the code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from scipy.stats import poisson\n",
    "\n",
    "# Calculate the probability of observing exactly 3 events\n",
    "probability = poisson.pmf(3, 5)\n",
    "\n",
    "print(probability)\n",
    "Explanation:\n",
    "poisson.pmf(3, 5):\n",
    "3: The exact number of events you are interested in observing.\n",
    "5: The average number of events (λ or lambda) in the Poisson distribution.\n",
    "pmf: The function that returns the probability of observing exactly k events in a Poisson distribution with a given average rate λ.\n",
    "Example Output:\n",
    "plaintext\n",
    "Copy code\n",
    "0.1403738958142805\n",
    "This code calculates the probability of observing exactly 3 events in the given time interval for a Poisson distribution with an average rate of 5 events.\n",
    "\n",
    "You said:\n",
    "\n",
    "You \n",
    "are \n",
    "analyzing \n",
    "data \n",
    "that \n",
    "records \n",
    "the \n",
    "decay \n",
    "rate \n",
    "of \n",
    "a \n",
    "radioactive \n",
    "substance, \n",
    "and \n",
    "you \n",
    "need \n",
    "to \n",
    "model \n",
    "this \n",
    "data \n",
    "using \n",
    "a \n",
    "probability \n",
    "distribution. \n",
    "Which \n",
    "distribution \n",
    "is \n",
    "most \n",
    "suitable \n",
    "for \n",
    "modeling \n",
    "the \n",
    "time \n",
    "until \n",
    "the \n",
    "next \n",
    "decay \n",
    "event? \n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "The exponential distribution is the most suitable probability distribution for modeling the time until the next decay event of a radioactive substance.\n",
    "\n",
    "Why the Exponential Distribution?\n",
    "Memorylessness Property: The exponential distribution is often used to model the time between events in a Poisson process, where events occur continuously and independently at a constant average rate. The memoryless property of the exponential distribution means that the probability of an event occurring in the next time interval is independent of how much time has already elapsed, making it ideal for modeling decay events.\n",
    "Rate Parameter (λ): The distribution is characterized by a rate parameter (λ), which represents the average number of events (e.g., decays) per time unit.\n",
    "Example:\n",
    "If the average decay rate of a substance is λ decays per minute, the exponential distribution can model the probability of the time until the next decay event.\n",
    "Summary:\n",
    "The exponential distribution is the best choice for modeling the time until the next decay event for a radioactive substance, as it effectively captures the nature of such random processes.\n",
    "\n",
    "You said:\n",
    "Define \n",
    "a \n",
    "function \n",
    "print_name\n",
    "that \n",
    "takes \n",
    "one \n",
    "parameter \n",
    "name\n",
    "and \n",
    "prints \n",
    "a \n",
    "single \n",
    "value, \n",
    "then \n",
    "call \n",
    "the \n",
    "function. \n",
    "Complete \n",
    "the \n",
    "code \n",
    "to \n",
    "return \n",
    "the \n",
    "output \n",
    "?\n",
    "  \"\"\"Prints Hello and the name of the person.\"\"\"\n",
    "  statement = \"Hello \" + name + \"!\"\n",
    "  \n",
    "?\n",
    "?\n",
    "Hello Amy!\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "To define a function print_name that takes one parameter name, prints a greeting with that name, and then call the function, you can complete the code as follows:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "def print_name(name):\n",
    "    \"\"\"Prints Hello and the name of the person.\"\"\"\n",
    "    statement = \"Hello \" + name + \"!\"\n",
    "    print(statement)\n",
    "\n",
    "# Call the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "​\n",
    "olymp = pd.read_csv(\"data/athlete_events.csv.gz\")\n",
    "print(olymp.shape)\n",
    "olymp.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Olympics\n",
    "This is a historical dataset on the modern Olympic Games, from Athens 1896 to Rio 2016. Each row consists of an individual athlete competing in an Olympic event and which medal was won (if any).\n",
    "\n",
    "Not sure where to begin? Scroll to the bottom to find challenges!\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "olymp = pd.read_csv(\"data/athlete_events.csv.gz\")\n",
    "print(olymp.shape)\n",
    "olymp.head(100)\n",
    "import pandas as pd\n",
    "​\n",
    "olymp = pd.read_csv(\"data/athlete_events.csv.gz\")\n",
    "print(olymp.shape)\n",
    "olymp.head(100)\n",
    "(271116, 15)\n",
    "id\n",
    "name\n",
    "sex\n",
    "age\n",
    "height\n",
    "weight\n",
    "team\n",
    "noc\n",
    "games\n",
    "year\n",
    "season\n",
    "city\n",
    "sport\n",
    "event\n",
    "medal\n",
    "0\n",
    "1\n",
    "A Dijiang\n",
    "M\n",
    "24\n",
    "180\n",
    "80\n",
    "China\n",
    "CHN\n",
    "1992 Summer\n",
    "1992\n",
    "Summer\n",
    "Barcelona\n",
    "Basketball\n",
    "Basketball Men's Basketball\n",
    "null\n",
    "1\n",
    "2\n",
    "A Lamusi\n",
    "M\n",
    "23\n",
    "170\n",
    "60\n",
    "China\n",
    "CHN\n",
    "2012 Summer\n",
    "2012\n",
    "Summer\n",
    "London\n",
    "Judo\n",
    "Judo Men's Extra-Lightweight\n",
    "null\n",
    "2\n",
    "3\n",
    "Gunnar Nielsen Aaby\n",
    "M\n",
    "24\n",
    "null\n",
    "null\n",
    "Denmark\n",
    "DEN\n",
    "1920 Summer\n",
    "1920\n",
    "Summer\n",
    "Antwerpen\n",
    "Football\n",
    "Football Men's Football\n",
    "null\n",
    "3\n",
    "4\n",
    "Edgar Lindenau Aabye\n",
    "M\n",
    "34\n",
    "null\n",
    "null\n",
    "Denmark/Sweden\n",
    "DEN\n",
    "1900 Summer\n",
    "1900\n",
    "Summer\n",
    "Paris\n",
    "Tug-Of-War\n",
    "Tug-Of-War Men's Tug-Of-War\n",
    "Gold\n",
    "4\n",
    "5\n",
    "Christine Jacoba Aaftink\n",
    "F\n",
    "21\n",
    "185\n",
    "82\n",
    "Netherlands\n",
    "NED\n",
    "1988 Winter\n",
    "1988\n",
    "Winter\n",
    "Calgary\n",
    "Speed Skating\n",
    "Speed Skating Women's 500 metres\n",
    "null\n",
    "5\n",
    "5\n",
    "Christine Jacoba Aaftink\n",
    "F\n",
    "21\n",
    "185\n",
    "82\n",
    "Netherlands\n",
    "NED\n",
    "1988 Winter\n",
    "1988\n",
    "Winter\n",
    "Calgary\n",
    "Speed Skating\n",
    "Speed Skating Women's 1,000 metres\n",
    "null\n",
    "6\n",
    "5\n",
    "Christine Jacoba Aaftink\n",
    "F\n",
    "25\n",
    "185\n",
    "82\n",
    "Netherlands\n",
    "NED\n",
    "1992 Winter\n",
    "1992\n",
    "Winter\n",
    "Albertville\n",
    "Speed Skating\n",
    "Speed Skating Women's 500 metres\n",
    "null\n",
    "7\n",
    "5\n",
    "Christine Jacoba Aaftink\n",
    "F\n",
    "25\n",
    "185\n",
    "82\n",
    "Netherlands\n",
    "NED\n",
    "1992 Winter\n",
    "1992\n",
    "Winter\n",
    "Albertville\n",
    "Speed Skating\n",
    "Speed Skating Women's 1,000 metres\n",
    "null\n",
    "8\n",
    "5\n",
    "Christine Jacoba Aaftink\n",
    "F\n",
    "27\n",
    "185\n",
    "82\n",
    "Netherlands\n",
    "NED\n",
    "1994 Winter\n",
    "1994\n",
    "Winter\n",
    "Lillehammer\n",
    "Speed Skating\n",
    "Speed Skating Women's 500 metres\n",
    "null\n",
    "9\n",
    "5\n",
    "Christine Jacoba Aaftink\n",
    "F\n",
    "27\n",
    "185\n",
    "82\n",
    "Netherlands\n",
    "NED\n",
    "1994 Winter\n",
    "1994\n",
    "Winter\n",
    "Lillehammer\n",
    "Speed Skating\n",
    "Speed Skating Women's 1,000 metres\n",
    "null\n",
    "10\n",
    "6\n",
    "Per Knut Aaland\n",
    "M\n",
    "31\n",
    "188\n",
    "75\n",
    "United States\n",
    "USA\n",
    "1992 Winter\n",
    "1992\n",
    "Winter\n",
    "Albertville\n",
    "Cross Country Skiing\n",
    "Cross Country Skiing Men's 10 kilometres\n",
    "null\n",
    "11\n",
    "6\n",
    "Per Knut Aaland\n",
    "M\n",
    "31\n",
    "188\n",
    "75\n",
    "United States\n",
    "USA\n",
    "1992 Winter\n",
    "1992\n",
    "Winter\n",
    "Albertville\n",
    "Cross Country Skiing\n",
    "Cross Country Skiing Men's 50 kilometres\n",
    "null\n",
    "12\n",
    "6\n",
    "Per Knut Aaland\n",
    "M\n",
    "31\n",
    "188\n",
    "75\n",
    "United States\n",
    "USA\n",
    "1992 Winter\n",
    "1992\n",
    "Winter\n",
    "Albertville\n",
    "Cross Country Skiing\n",
    "Cross Country Skiing Men's 10/15 kilometres Pursuit\n",
    "null\n",
    "13\n",
    "6\n",
    "Per Knut Aaland\n",
    "M\n",
    "31\n",
    "188\n",
    "75\n",
    "United States\n",
    "USA\n",
    "1992 Winter\n",
    "1992\n",
    "Winter\n",
    "Albertville\n",
    "Cross Country Skiing\n",
    "Cross Country Skiing Men's 4 x 10 kilometres Relay\n",
    "null\n",
    "14\n",
    "6\n",
    "Per Knut Aaland\n",
    "M\n",
    "33\n",
    "188\n",
    "75\n",
    "United States\n",
    "USA\n",
    "1994 Winter\n",
    "1994\n",
    "Winter\n",
    "Lillehammer\n",
    "Cross Country Skiing\n",
    "Cross Country Skiing Men's 10 kilometres\n",
    "null\n",
    "15\n",
    "6\n",
    "Per Knut Aaland\n",
    "M\n",
    "33\n",
    "188\n",
    "75\n",
    "United States\n",
    "USA\n",
    "1994 Winter\n",
    "1994\n",
    "Winter\n",
    "Lillehammer\n",
    "Cross Country Skiing\n",
    "Cross Country Skiing Men's 30 kilometres\n",
    "null\n",
    "16\n",
    "6\n",
    "Per Knut Aaland\n",
    "M\n",
    "33\n",
    "188\n",
    "75\n",
    "United States\n",
    "USA\n",
    "1994 Winter\n",
    "1994\n",
    "Winter\n",
    "Lillehammer\n",
    "Cross Country Skiing\n",
    "Cross Country Skiing Men's 10/15 kilometres Pursuit\n",
    "null\n",
    "17\n",
    "6\n",
    "Per Knut Aaland\n",
    "M\n",
    "33\n",
    "188\n",
    "75\n",
    "United States\n",
    "USA\n",
    "1994 Winter\n",
    "1994\n",
    "Winter\n",
    "Lillehammer\n",
    "Cross Country Skiing\n",
    "Cross Country Skiing Men's 4 x 10 kilometres Relay\n",
    "null\n",
    "18\n",
    "7\n",
    "John Aalberg\n",
    "M\n",
    "31\n",
    "183\n",
    "72\n",
    "United States\n",
    "USA\n",
    "1992 Winter\n",
    "1992\n",
    "Winter\n",
    "Albertville\n",
    "Cross Country Skiing\n",
    "Cross Country Skiing Men's 10 kilometres\n",
    "null\n",
    "19\n",
    "7\n",
    "John Aalberg\n",
    "M\n",
    "31\n",
    "183\n",
    "72\n",
    "United States\n",
    "USA\n",
    "1992 Winter\n",
    "1992\n",
    "Winter\n",
    "Albertville\n",
    "Cross Country Skiing\n",
    "Cross Country Skiing Men's 50 kilometres\n",
    "null\n",
    "20\n",
    "7\n",
    "John Aalberg\n",
    "M\n",
    "31\n",
    "183\n",
    "72\n",
    "United States\n",
    "USA\n",
    "1992 Winter\n",
    "1992\n",
    "Winter\n",
    "Albertville\n",
    "Cross Country Skiing\n",
    "Cross Country Skiing Men's 10/15 kilometres Pursuit\n",
    "null\n",
    "21\n",
    "7\n",
    "John Aalberg\n",
    "M\n",
    "31\n",
    "183\n",
    "72\n",
    "United States\n",
    "USA\n",
    "1992 Winter\n",
    "1992\n",
    "Winter\n",
    "Albertville\n",
    "Cross Country Skiing\n",
    "Cross Country Skiing Men's 4 x 10 kilometres Relay\n",
    "null\n",
    "22\n",
    "7\n",
    "John Aalberg\n",
    "M\n",
    "33\n",
    "183\n",
    "72\n",
    "United States\n",
    "USA\n",
    "1994 Winter\n",
    "1994\n",
    "Winter\n",
    "Lillehammer\n",
    "Cross Country Skiing\n",
    "Cross Country Skiing Men's 10 kilometres\n",
    "null\n",
    "23\n",
    "7\n",
    "John Aalberg\n",
    "M\n",
    "33\n",
    "183\n",
    "72\n",
    "United States\n",
    "USA\n",
    "1994 Winter\n",
    "1994\n",
    "Winter\n",
    "Lillehammer\n",
    "Cross Country Skiing\n",
    "Cross Country Skiing Men's 30 kilometres\n",
    "null\n",
    "24\n",
    "7\n",
    "John Aalberg\n",
    "M\n",
    "33\n",
    "183\n",
    "72\n",
    "United States\n",
    "USA\n",
    "1994 Winter\n",
    "1994\n",
    "Winter\n",
    "Lillehammer\n",
    "Cross Country Skiing\n",
    "Cross Country Skiing Men's 10/15 kilometres Pursuit\n",
    "null\n",
    "25\n",
    "7\n",
    "John Aalberg\n",
    "M\n",
    "33\n",
    "183\n",
    "72\n",
    "United States\n",
    "USA\n",
    "1994 Winter\n",
    "1994\n",
    "Winter\n",
    "Lillehammer\n",
    "Cross Country Skiing\n",
    "Cross Country Skiing Men's 4 x 10 kilometres Relay\n",
    "null\n",
    "26\n",
    "8\n",
    "Cornelia \"Cor\" Aalten (-Strannood)\n",
    "F\n",
    "18\n",
    "168\n",
    "null\n",
    "Netherlands\n",
    "NED\n",
    "1932 Summer\n",
    "1932\n",
    "Summer\n",
    "Los Angeles\n",
    "Athletics\n",
    "Athletics Women's 100 metres\n",
    "null\n",
    "27\n",
    "8\n",
    "Cornelia \"Cor\" Aalten (-Strannood)\n",
    "F\n",
    "18\n",
    "168\n",
    "null\n",
    "Netherlands\n",
    "NED\n",
    "1932 Summer\n",
    "1932\n",
    "Summer\n",
    "Los Angeles\n",
    "Athletics\n",
    "Athletics Women's 4 x 100 metres Relay\n",
    "null\n",
    "28\n",
    "9\n",
    "Antti Sami Aalto\n",
    "M\n",
    "26\n",
    "186\n",
    "96\n",
    "Finland\n",
    "FIN\n",
    "2002 Winter\n",
    "2002\n",
    "Winter\n",
    "Salt Lake City\n",
    "Ice Hockey\n",
    "Ice Hockey Men's Ice Hockey\n",
    "null\n",
    "29\n",
    "10\n",
    "Einar Ferdinand \"Einari\" Aalto\n",
    "M\n",
    "26\n",
    "null\n",
    "null\n",
    "Finland\n",
    "FIN\n",
    "1952 Summer\n",
    "1952\n",
    "Summer\n",
    "Helsinki\n",
    "Swimming\n",
    "Swimming Men's 400 metres Freestyle\n",
    "null\n",
    "30\n",
    "11\n",
    "Jorma Ilmari Aalto\n",
    "M\n",
    "22\n",
    "182\n",
    "76.5\n",
    "Finland\n",
    "FIN\n",
    "1980 Winter\n",
    "1980\n",
    "Winter\n",
    "Lake Placid\n",
    "Cross Country Skiing\n",
    "Cross Country Skiing Men's 30 kilometres\n",
    "null\n",
    "31\n",
    "12\n",
    "Jyri Tapani Aalto\n",
    "M\n",
    "31\n",
    "172\n",
    "70\n",
    "Finland\n",
    "FIN\n",
    "2000 Summer\n",
    "2000\n",
    "Summer\n",
    "Sydney\n",
    "Badminton\n",
    "Badminton Men's Singles\n",
    "null\n",
    "32\n",
    "13\n",
    "Minna Maarit Aalto\n",
    "F\n",
    "30\n",
    "159\n",
    "55.5\n",
    "Finland\n",
    "FIN\n",
    "1996 Summer\n",
    "1996\n",
    "Summer\n",
    "Atlanta\n",
    "Sailing\n",
    "Sailing Women's Windsurfer\n",
    "null\n",
    "33\n",
    "13\n",
    "Minna Maarit Aalto\n",
    "F\n",
    "34\n",
    "159\n",
    "55.5\n",
    "Finland\n",
    "FIN\n",
    "2000 Summer\n",
    "2000\n",
    "Summer\n",
    "Sydney\n",
    "Sailing\n",
    "Sailing Women's Windsurfer\n",
    "null\n",
    "34\n",
    "14\n",
    "Pirjo Hannele Aalto (Mattila-)\n",
    "F\n",
    "32\n",
    "171\n",
    "65\n",
    "Finland\n",
    "FIN\n",
    "1994 Winter\n",
    "1994\n",
    "Winter\n",
    "Lillehammer\n",
    "Biathlon\n",
    "Biathlon Women's 7.5 kilometres Sprint\n",
    "null\n",
    "35\n",
    "15\n",
    "Arvo Ossian Aaltonen\n",
    "M\n",
    "22\n",
    "null\n",
    "null\n",
    "Finland\n",
    "FIN\n",
    "1912 Summer\n",
    "1912\n",
    "Summer\n",
    "Stockholm\n",
    "Swimming\n",
    "Swimming Men's 200 metres Breaststroke\n",
    "null\n",
    "36\n",
    "15\n",
    "Arvo Ossian Aaltonen\n",
    "M\n",
    "22\n",
    "null\n",
    "null\n",
    "Finland\n",
    "FIN\n",
    "1912 Summer\n",
    "1912\n",
    "Summer\n",
    "Stockholm\n",
    "Swimming\n",
    "Swimming Men's 400 metres Breaststroke\n",
    "null\n",
    "37\n",
    "15\n",
    "Arvo Ossian Aaltonen\n",
    "M\n",
    "30\n",
    "null\n",
    "null\n",
    "Finland\n",
    "FIN\n",
    "1920 Summer\n",
    "1920\n",
    "Summer\n",
    "Antwerpen\n",
    "Swimming\n",
    "Swimming Men's 200 metres Breaststroke\n",
    "Bronze\n",
    "38\n",
    "15\n",
    "Arvo Ossian Aaltonen\n",
    "M\n",
    "30\n",
    "null\n",
    "null\n",
    "Finland\n",
    "FIN\n",
    "1920 Summer\n",
    "1920\n",
    "Summer\n",
    "Antwerpen\n",
    "Swimming\n",
    "Swimming Men's 400 metres Breaststroke\n",
    "Bronze\n",
    "39\n",
    "15\n",
    "Arvo Ossian Aaltonen\n",
    "M\n",
    "34\n",
    "null\n",
    "null\n",
    "Finland\n",
    "FIN\n",
    "1924 Summer\n",
    "1924\n",
    "Summer\n",
    "Paris\n",
    "Swimming\n",
    "Swimming Men's 200 metres Breaststroke\n",
    "null\n",
    "40\n",
    "16\n",
    "Juhamatti Tapio Aaltonen\n",
    "M\n",
    "28\n",
    "184\n",
    "85\n",
    "Finland\n",
    "FIN\n",
    "2014 Winter\n",
    "2014\n",
    "Winter\n",
    "Sochi\n",
    "Ice Hockey\n",
    "Ice Hockey Men's Ice Hockey\n",
    "Bronze\n",
    "41\n",
    "17\n",
    "Paavo Johannes Aaltonen\n",
    "M\n",
    "28\n",
    "175\n",
    "64\n",
    "Finland\n",
    "FIN\n",
    "1948 Summer\n",
    "1948\n",
    "Summer\n",
    "London\n",
    "Gymnastics\n",
    "Gymnastics Men's Individual All-Around\n",
    "Bronze\n",
    "42\n",
    "17\n",
    "Paavo Johannes Aaltonen\n",
    "M\n",
    "28\n",
    "175\n",
    "64\n",
    "Finland\n",
    "FIN\n",
    "1948 Summer\n",
    "1948\n",
    "Summer\n",
    "London\n",
    "Gymnastics\n",
    "Gymnastics Men's Team All-Around\n",
    "Gold\n",
    "43\n",
    "17\n",
    "Paavo Johannes Aaltonen\n",
    "M\n",
    "28\n",
    "175\n",
    "64\n",
    "Finland\n",
    "FIN\n",
    "1948 Summer\n",
    "1948\n",
    "Summer\n",
    "London\n",
    "Gymnastics\n",
    "Gymnastics Men's Floor Exercise\n",
    "null\n",
    "44\n",
    "17\n",
    "Paavo Johannes Aaltonen\n",
    "M\n",
    "28\n",
    "175\n",
    "64\n",
    "Finland\n",
    "FIN\n",
    "1948 Summer\n",
    "1948\n",
    "Summer\n",
    "London\n",
    "Gymnastics\n",
    "Gymnastics Men's Horse Vault\n",
    "Gold\n",
    "45\n",
    "17\n",
    "Paavo Johannes Aaltonen\n",
    "M\n",
    "28\n",
    "175\n",
    "64\n",
    "Finland\n",
    "FIN\n",
    "1948 Summer\n",
    "1948\n",
    "Summer\n",
    "London\n",
    "Gymnastics\n",
    "Gymnastics Men's Parallel Bars\n",
    "null\n",
    "46\n",
    "17\n",
    "Paavo Johannes Aaltonen\n",
    "M\n",
    "28\n",
    "175\n",
    "64\n",
    "Finland\n",
    "FIN\n",
    "1948 Summer\n",
    "1948\n",
    "Summer\n",
    "London\n",
    "Gymnastics\n",
    "Gymnastics Men's Horizontal Bar\n",
    "null\n",
    "47\n",
    "17\n",
    "Paavo Johannes Aaltonen\n",
    "M\n",
    "28\n",
    "175\n",
    "64\n",
    "Finland\n",
    "FIN\n",
    "1948 Summer\n",
    "1948\n",
    "Summer\n",
    "London\n",
    "Gymnastics\n",
    "Gymnastics Men's Rings\n",
    "null\n",
    "48\n",
    "17\n",
    "Paavo Johannes Aaltonen\n",
    "M\n",
    "28\n",
    "175\n",
    "64\n",
    "Finland\n",
    "FIN\n",
    "1948 Summer\n",
    "1948\n",
    "Summer\n",
    "London\n",
    "Gymnastics\n",
    "Gymnastics Men's Pommelled Horse\n",
    "Gold\n",
    "49\n",
    "17\n",
    "Paavo Johannes Aaltonen\n",
    "M\n",
    "32\n",
    "175\n",
    "64\n",
    "Finland\n",
    "FIN\n",
    "1952 Summer\n",
    "1952\n",
    "Summer\n",
    "Helsinki\n",
    "Gymnastics\n",
    "Gymnastics Men's Individual All-Around\n",
    "null\n",
    "50\n",
    "17\n",
    "Paavo Johannes Aaltonen\n",
    "M\n",
    "32\n",
    "175\n",
    "64\n",
    "Finland\n",
    "FIN\n",
    "1952 Summer\n",
    "1952\n",
    "Summer\n",
    "Helsinki\n",
    "Gymnastics\n",
    "Gymnastics Men's Team All-Around\n",
    "Bronze\n",
    "51\n",
    "17\n",
    "Paavo Johannes Aaltonen\n",
    "M\n",
    "32\n",
    "175\n",
    "64\n",
    "Finland\n",
    "FIN\n",
    "1952 Summer\n",
    "1952\n",
    "Summer\n",
    "Helsinki\n",
    "Gymnastics\n",
    "Gymnastics Men's Floor Exercise\n",
    "null\n",
    "52\n",
    "17\n",
    "Paavo Johannes Aaltonen\n",
    "M\n",
    "32\n",
    "175\n",
    "64\n",
    "Finland\n",
    "FIN\n",
    "1952 Summer\n",
    "1952\n",
    "Summer\n",
    "Helsinki\n",
    "Gymnastics\n",
    "Gymnastics Men's Horse Vault\n",
    "null\n",
    "53\n",
    "17\n",
    "Paavo Johannes Aaltonen\n",
    "M\n",
    "32\n",
    "175\n",
    "64\n",
    "Finland\n",
    "FIN\n",
    "1952 Summer\n",
    "1952\n",
    "Summer\n",
    "Helsinki\n",
    "Gymnastics\n",
    "Gymnastics Men's Parallel Bars\n",
    "null\n",
    "54\n",
    "17\n",
    "Paavo Johannes Aaltonen\n",
    "M\n",
    "32\n",
    "175\n",
    "64\n",
    "Finland\n",
    "FIN\n",
    "1952 Summer\n",
    "1952\n",
    "Summer\n",
    "Helsinki\n",
    "Gymnastics\n",
    "Gymnastics Men's Horizontal Bar\n",
    "null\n",
    "55\n",
    "17\n",
    "Paavo Johannes Aaltonen\n",
    "M\n",
    "32\n",
    "175\n",
    "64\n",
    "Finland\n",
    "FIN\n",
    "1952 Summer\n",
    "1952\n",
    "Summer\n",
    "Helsinki\n",
    "Gymnastics\n",
    "Gymnastics Men's Rings\n",
    "null\n",
    "56\n",
    "17\n",
    "Paavo Johannes Aaltonen\n",
    "M\n",
    "32\n",
    "175\n",
    "64\n",
    "Finland\n",
    "FIN\n",
    "1952 Summer\n",
    "1952\n",
    "Summer\n",
    "Helsinki\n",
    "Gymnastics\n",
    "Gymnastics Men's Pommelled Horse\n",
    "null\n",
    "57\n",
    "18\n",
    "Timo Antero Aaltonen\n",
    "M\n",
    "31\n",
    "189\n",
    "130\n",
    "Finland\n",
    "FIN\n",
    "2000 Summer\n",
    "2000\n",
    "Summer\n",
    "Sydney\n",
    "Athletics\n",
    "Athletics Men's Shot Put\n",
    "null\n",
    "58\n",
    "19\n",
    "Win Valdemar Aaltonen\n",
    "M\n",
    "54\n",
    "null\n",
    "null\n",
    "Finland\n",
    "FIN\n",
    "1948 Summer\n",
    "1948\n",
    "Summer\n",
    "London\n",
    "Art Competitions\n",
    "Art Competitions Mixed Sculpturing, Unknown Event\n",
    "null\n",
    "59\n",
    "20\n",
    "Kjetil Andr Aamodt\n",
    "M\n",
    "20\n",
    "176\n",
    "85\n",
    "Norway\n",
    "NOR\n",
    "1992 Winter\n",
    "1992\n",
    "Winter\n",
    "Albertville\n",
    "Alpine Skiing\n",
    "Alpine Skiing Men's Downhill\n",
    "null\n",
    "60\n",
    "20\n",
    "Kjetil Andr Aamodt\n",
    "M\n",
    "20\n",
    "176\n",
    "85\n",
    "Norway\n",
    "NOR\n",
    "1992 Winter\n",
    "1992\n",
    "Winter\n",
    "Albertville\n",
    "Alpine Skiing\n",
    "Alpine Skiing Men's Super G\n",
    "Gold\n",
    "61\n",
    "20\n",
    "Kjetil Andr Aamodt\n",
    "M\n",
    "20\n",
    "176\n",
    "85\n",
    "Norway\n",
    "NOR\n",
    "1992 Winter\n",
    "1992\n",
    "Winter\n",
    "Albertville\n",
    "Alpine Skiing\n",
    "Alpine Skiing Men's Giant Slalom\n",
    "Bronze\n",
    "62\n",
    "20\n",
    "Kjetil Andr Aamodt\n",
    "M\n",
    "20\n",
    "176\n",
    "85\n",
    "Norway\n",
    "NOR\n",
    "1992 Winter\n",
    "1992\n",
    "Winter\n",
    "Albertville\n",
    "Alpine Skiing\n",
    "Alpine Skiing Men's Slalom\n",
    "null\n",
    "63\n",
    "20\n",
    "Kjetil Andr Aamodt\n",
    "M\n",
    "22\n",
    "176\n",
    "85\n",
    "Norway\n",
    "NOR\n",
    "1994 Winter\n",
    "1994\n",
    "Winter\n",
    "Lillehammer\n",
    "Alpine Skiing\n",
    "Alpine Skiing Men's Downhill\n",
    "Silver\n",
    "64\n",
    "20\n",
    "Kjetil Andr Aamodt\n",
    "M\n",
    "22\n",
    "176\n",
    "85\n",
    "Norway\n",
    "NOR\n",
    "1994 Winter\n",
    "1994\n",
    "Winter\n",
    "Lillehammer\n",
    "Alpine Skiing\n",
    "Alpine Skiing Men's Super G\n",
    "Bronze\n",
    "65\n",
    "20\n",
    "Kjetil Andr Aamodt\n",
    "M\n",
    "22\n",
    "176\n",
    "85\n",
    "Norway\n",
    "NOR\n",
    "1994 Winter\n",
    "1994\n",
    "Winter\n",
    "Lillehammer\n",
    "Alpine Skiing\n",
    "Alpine Skiing Men's Giant Slalom\n",
    "null\n",
    "66\n",
    "20\n",
    "Kjetil Andr Aamodt\n",
    "M\n",
    "22\n",
    "176\n",
    "85\n",
    "Norway\n",
    "NOR\n",
    "1994 Winter\n",
    "1994\n",
    "Winter\n",
    "Lillehammer\n",
    "Alpine Skiing\n",
    "Alpine Skiing Men's Slalom\n",
    "null\n",
    "67\n",
    "20\n",
    "Kjetil Andr Aamodt\n",
    "M\n",
    "22\n",
    "176\n",
    "85\n",
    "Norway\n",
    "NOR\n",
    "1994 Winter\n",
    "1994\n",
    "Winter\n",
    "Lillehammer\n",
    "Alpine Skiing\n",
    "Alpine Skiing Men's Combined\n",
    "Silver\n",
    "68\n",
    "20\n",
    "Kjetil Andr Aamodt\n",
    "M\n",
    "26\n",
    "176\n",
    "85\n",
    "Norway\n",
    "NOR\n",
    "1998 Winter\n",
    "1998\n",
    "Winter\n",
    "Nagano\n",
    "Alpine Skiing\n",
    "Alpine Skiing Men's Downhill\n",
    "null\n",
    "69\n",
    "20\n",
    "Kjetil Andr Aamodt\n",
    "M\n",
    "26\n",
    "176\n",
    "85\n",
    "Norway\n",
    "NOR\n",
    "1998 Winter\n",
    "1998\n",
    "Winter\n",
    "Nagano\n",
    "Alpine Skiing\n",
    "Alpine Skiing Men's Super G\n",
    "null\n",
    "70\n",
    "20\n",
    "Kjetil Andr Aamodt\n",
    "M\n",
    "26\n",
    "176\n",
    "85\n",
    "Norway\n",
    "NOR\n",
    "1998 Winter\n",
    "1998\n",
    "Winter\n",
    "Nagano\n",
    "Alpine Skiing\n",
    "Alpine Skiing Men's Giant Slalom\n",
    "null\n",
    "71\n",
    "20\n",
    "Kjetil Andr Aamodt\n",
    "M\n",
    "26\n",
    "176\n",
    "85\n",
    "Norway\n",
    "NOR\n",
    "1998 Winter\n",
    "1998\n",
    "Winter\n",
    "Nagano\n",
    "Alpine Skiing\n",
    "Alpine Skiing Men's Combined\n",
    "null\n",
    "72\n",
    "20\n",
    "Kjetil Andr Aamodt\n",
    "M\n",
    "30\n",
    "176\n",
    "85\n",
    "Norway\n",
    "NOR\n",
    "2002 Winter\n",
    "2002\n",
    "Winter\n",
    "Salt Lake City\n",
    "Alpine Skiing\n",
    "Alpine Skiing Men's Downhill\n",
    "null\n",
    "73\n",
    "20\n",
    "Kjetil Andr Aamodt\n",
    "M\n",
    "30\n",
    "176\n",
    "85\n",
    "Norway\n",
    "NOR\n",
    "2002 Winter\n",
    "2002\n",
    "Winter\n",
    "Salt Lake City\n",
    "Alpine Skiing\n",
    "Alpine Skiing Men's Super G\n",
    "Gold\n",
    "74\n",
    "20\n",
    "Kjetil Andr Aamodt\n",
    "M\n",
    "30\n",
    "176\n",
    "85\n",
    "Norway\n",
    "NOR\n",
    "2002 Winter\n",
    "2002\n",
    "Winter\n",
    "Salt Lake City\n",
    "Alpine Skiing\n",
    "Alpine Skiing Men's Giant Slalom\n",
    "null\n",
    "75\n",
    "20\n",
    "Kjetil Andr Aamodt\n",
    "M\n",
    "30\n",
    "176\n",
    "85\n",
    "Norway\n",
    "NOR\n",
    "2002 Winter\n",
    "2002\n",
    "Winter\n",
    "Salt Lake City\n",
    "Alpine Skiing\n",
    "Alpine Skiing Men's Slalom\n",
    "null\n",
    "76\n",
    "20\n",
    "Kjetil Andr Aamodt\n",
    "M\n",
    "30\n",
    "176\n",
    "85\n",
    "Norway\n",
    "NOR\n",
    "2002 Winter\n",
    "2002\n",
    "Winter\n",
    "Salt Lake City\n",
    "Alpine Skiing\n",
    "Alpine Skiing Men's Combined\n",
    "Gold\n",
    "77\n",
    "20\n",
    "Kjetil Andr Aamodt\n",
    "M\n",
    "34\n",
    "176\n",
    "85\n",
    "Norway\n",
    "NOR\n",
    "2006 Winter\n",
    "2006\n",
    "Winter\n",
    "Torino\n",
    "Alpine Skiing\n",
    "Alpine Skiing Men's Downhill\n",
    "null\n",
    "78\n",
    "20\n",
    "Kjetil Andr Aamodt\n",
    "M\n",
    "34\n",
    "176\n",
    "85\n",
    "Norway\n",
    "NOR\n",
    "2006 Winter\n",
    "2006\n",
    "Winter\n",
    "Torino\n",
    "Alpine Skiing\n",
    "Alpine Skiing Men's Super G\n",
    "Gold\n",
    "79\n",
    "21\n",
    "Ragnhild Margrethe Aamodt\n",
    "F\n",
    "27\n",
    "163\n",
    "null\n",
    "Norway\n",
    "NOR\n",
    "2008 Summer\n",
    "2008\n",
    "Summer\n",
    "Beijing\n",
    "Handball\n",
    "Handball Women's Handball\n",
    "Gold\n",
    "80\n",
    "22\n",
    "Andreea Aanei\n",
    "F\n",
    "22\n",
    "170\n",
    "125\n",
    "Romania\n",
    "ROU\n",
    "2016 Summer\n",
    "2016\n",
    "Summer\n",
    "Rio de Janeiro\n",
    "Weightlifting\n",
    "Weightlifting Women's Super-Heavyweight\n",
    "null\n",
    "81\n",
    "23\n",
    "Fritz Aanes\n",
    "M\n",
    "22\n",
    "187\n",
    "89\n",
    "Norway\n",
    "NOR\n",
    "2000 Summer\n",
    "2000\n",
    "Summer\n",
    "Sydney\n",
    "Wrestling\n",
    "Wrestling Men's Light-Heavyweight, Greco-Roman\n",
    "null\n",
    "82\n",
    "23\n",
    "Fritz Aanes\n",
    "M\n",
    "26\n",
    "187\n",
    "89\n",
    "Norway\n",
    "NOR\n",
    "2004 Summer\n",
    "2004\n",
    "Summer\n",
    "Athina\n",
    "Wrestling\n",
    "Wrestling Men's Light-Heavyweight, Greco-Roman\n",
    "null\n",
    "83\n",
    "24\n",
    "Nils Egil Aaness\n",
    "M\n",
    "24\n",
    "null\n",
    "null\n",
    "Norway\n",
    "NOR\n",
    "1960 Winter\n",
    "1960\n",
    "Winter\n",
    "Squaw Valley\n",
    "Speed Skating\n",
    "Speed Skating Men's 500 metres\n",
    "null\n",
    "84\n",
    "24\n",
    "Nils Egil Aaness\n",
    "M\n",
    "24\n",
    "null\n",
    "null\n",
    "Norway\n",
    "NOR\n",
    "1960 Winter\n",
    "1960\n",
    "Winter\n",
    "Squaw Valley\n",
    "Speed Skating\n",
    "Speed Skating Men's 1,500 metres\n",
    "null\n",
    "85\n",
    "24\n",
    "Nils Egil Aaness\n",
    "M\n",
    "27\n",
    "null\n",
    "null\n",
    "Norway\n",
    "NOR\n",
    "1964 Winter\n",
    "1964\n",
    "Winter\n",
    "Innsbruck\n",
    "Speed Skating\n",
    "Speed Skating Men's 1,500 metres\n",
    "null\n",
    "86\n",
    "25\n",
    "Alf Lied Aanning\n",
    "M\n",
    "24\n",
    "null\n",
    "null\n",
    "Norway\n",
    "NOR\n",
    "1920 Summer\n",
    "1920\n",
    "Summer\n",
    "Antwerpen\n",
    "Gymnastics\n",
    "Gymnastics Men's Team All-Around, Free System\n",
    "Silver\n",
    "87\n",
    "26\n",
    "Agnes Erika Aanonsen (-Eyde)\n",
    "F\n",
    "17\n",
    "169\n",
    "65\n",
    "Norway\n",
    "NOR\n",
    "1984 Winter\n",
    "1984\n",
    "Winter\n",
    "Sarajevo\n",
    "Luge\n",
    "Luge Women's Singles\n",
    "null\n",
    "88\n",
    "27\n",
    "Johan Aantjes\n",
    "M\n",
    "26\n",
    "null\n",
    "null\n",
    "Netherlands\n",
    "NED\n",
    "1984 Summer\n",
    "1984\n",
    "Summer\n",
    "Los Angeles\n",
    "Water Polo\n",
    "Water Polo Men's Water Polo\n",
    "null\n",
    "89\n",
    "28\n",
    "Jan-Erik Aarberg\n",
    "M\n",
    "43\n",
    "170\n",
    "77\n",
    "Norway\n",
    "NOR\n",
    "1968 Summer\n",
    "1968\n",
    "Summer\n",
    "Mexico City\n",
    "Sailing\n",
    "Sailing Mixed Three Person Keelboat\n",
    "null\n",
    "90\n",
    "28\n",
    "Jan-Erik Aarberg\n",
    "M\n",
    "47\n",
    "170\n",
    "77\n",
    "Norway\n",
    "NOR\n",
    "1972 Summer\n",
    "1972\n",
    "Summer\n",
    "Munich\n",
    "Sailing\n",
    "Sailing Mixed Three Person Keelboat\n",
    "null\n",
    "91\n",
    "29\n",
    "Willemien Aardenburg\n",
    "F\n",
    "22\n",
    "null\n",
    "null\n",
    "Netherlands\n",
    "NED\n",
    "1988 Summer\n",
    "1988\n",
    "Summer\n",
    "Seoul\n",
    "Hockey\n",
    "Hockey Women's Hockey\n",
    "Bronze\n",
    "92\n",
    "30\n",
    "Pepijn Aardewijn\n",
    "M\n",
    "26\n",
    "189\n",
    "72\n",
    "Netherlands\n",
    "NED\n",
    "1996 Summer\n",
    "1996\n",
    "Summer\n",
    "Atlanta\n",
    "Rowing\n",
    "Rowing Men's Lightweight Double Sculls\n",
    "Silver\n",
    "93\n",
    "30\n",
    "Pepijn Aardewijn\n",
    "M\n",
    "30\n",
    "189\n",
    "72\n",
    "Netherlands\n",
    "NED\n",
    "2000 Summer\n",
    "2000\n",
    "Summer\n",
    "Sydney\n",
    "Rowing\n",
    "Rowing Men's Lightweight Double Sculls\n",
    "null\n",
    "94\n",
    "31\n",
    "Evald rma (rman-)\n",
    "M\n",
    "24\n",
    "174\n",
    "70\n",
    "Estonia\n",
    "EST\n",
    "1936 Summer\n",
    "1936\n",
    "Summer\n",
    "Berlin\n",
    "Athletics\n",
    "Athletics Men's Pole Vault\n",
    "null\n",
    "95\n",
    "32\n",
    "Olav Augunson Aarnes\n",
    "M\n",
    "23\n",
    "null\n",
    "null\n",
    "Norway\n",
    "NOR\n",
    "1912 Summer\n",
    "1912\n",
    "Summer\n",
    "Stockholm\n",
    "Athletics\n",
    "Athletics Men's High Jump\n",
    "null\n",
    "96\n",
    "33\n",
    "Mika Lauri Aarnikka\n",
    "M\n",
    "24\n",
    "187\n",
    "76\n",
    "Finland\n",
    "FIN\n",
    "1992 Summer\n",
    "1992\n",
    "Summer\n",
    "Barcelona\n",
    "Sailing\n",
    "Sailing Men's Two Person Dinghy\n",
    "null\n",
    "97\n",
    "33\n",
    "Mika Lauri Aarnikka\n",
    "M\n",
    "28\n",
    "187\n",
    "76\n",
    "Finland\n",
    "FIN\n",
    "1996 Summer\n",
    "1996\n",
    "Summer\n",
    "Atlanta\n",
    "Sailing\n",
    "Sailing Men's Two Person Dinghy\n",
    "null\n",
    "98\n",
    "34\n",
    "Jamale (Djamel-) Aarrass (Ahrass-)\n",
    "M\n",
    "30\n",
    "187\n",
    "76\n",
    "France\n",
    "FRA\n",
    "2012 Summer\n",
    "2012\n",
    "Summer\n",
    "London\n",
    "Athletics\n",
    "Athletics Men's 1,500 metres\n",
    "null\n",
    "99\n",
    "35\n",
    "Dagfinn Sverre Aarskog\n",
    "M\n",
    "24\n",
    "190\n",
    "98\n",
    "Norway\n",
    "NOR\n",
    "1998 Winter\n",
    "1998\n",
    "Winter\n",
    "Nagano\n",
    "Bobsleigh\n",
    "Bobsleigh Men's Four\n",
    "null\n",
    "Data Dictionary\n",
    "Column\tExplanation\n",
    "id\tUnique number for each athlete\n",
    "name\tAthlete's name\n",
    "sex\tM or F\n",
    "age\tAge of the athlete\n",
    "height\tIn centimeters\n",
    "weight\tIn kilograms\n",
    "team\tTeam name\n",
    "noc\tNational Olympic Committee 3\n",
    "games\tYear and season\n",
    "year\tInteger\n",
    "season\tSummer or Winter\n",
    "city\tHost city\n",
    "sport\tSport\n",
    "event\tEvent\n",
    "medal\tGold, Silver, Bronze, or NA\n",
    "Source and license of the dataset. The dataset is a consolidated version of data from www.sports-reference.com.\n",
    "\n",
    "Don't know where to start?\n",
    "Challenges are brief tasks designed to help you practice specific skills:\n",
    "\n",
    "🗺️ Explore: In which year and city did the Netherlands win the highest number of medals in their history?\n",
    "📊 Visualize: Create a plot visualizing the relationship between the number of athletes countries send to an event and the number of medals they receive.\n",
    "🔎 Analyze: In which sports does the height of an athlete increase their chances of earning a medal?\n",
    "Scenarios are broader questions to help you develop an end-to-end project for your portfolio:\n",
    "\n",
    "You are working as a data analyst for an international judo club. The owner of the club is looking for new ways to leverage data for competition. One idea they have had is to use past competition data to estimate the threat of future opponents. They have provided you with a dataset of past Olympic data and want to know whether you can use information such as the height, weight, age, and national origin of a judo competitor to estimate the probability that they will earn a medal.\n",
    "\n",
    "You will need to prepare a report that is accessible to a broad audience. It should outline your steps, findings, and conclusions.\n",
    "\n",
    "✍️ If you have an idea for an interesting Scenario or Challenge, or have feedback on our existing ones, let us know! You can submit feedback by pressing the question mark in the top right corner of the screen and selecting \"Give Feedback\". Include the phrase \"Content Feedback\" to help us flag it in our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loan Data\n",
    "This dataset consists of more than 9,500 loans with information on the loan structure, the borrower, and whether the loan was pain back in full. This data was extracted from LendingClub.com, which is a company that connects borrowers with investors.\n",
    "\n",
    "Not sure where to begin? Scroll to the bottom to find challenges!\n",
    "\n",
    "ort pandas as pd\n",
    "loan_data = pd.read_csv(\"loan_data.csv\")\n",
    "print(loan_data.shape)\n",
    "loan_data.head(100)\n",
    "import pandas as pd\n",
    "loan_data = pd.read_csv(\"loan_data.csv\")\n",
    "print(loan_data.shape)\n",
    "loan_data.head(100)\n",
    "(9578, 14)\n",
    "credit.policy\n",
    "purpose\n",
    "int.rate\n",
    "installment\n",
    "log.annual.inc\n",
    "dti\n",
    "fico\n",
    "days.with.cr.line\n",
    "revol.bal\n",
    "revol.util\n",
    "inq.last.6mths\n",
    "delinq.2yrs\n",
    "pub.rec\n",
    "not.fully.paid\n",
    "0\n",
    "1\n",
    "debt_consolidation\n",
    "0.1189\n",
    "829.1\n",
    "11.35040654\n",
    "19.48\n",
    "737\n",
    "5639.958333\n",
    "28854\n",
    "52.1\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "1\n",
    "1\n",
    "credit_card\n",
    "0.1071\n",
    "228.22\n",
    "11.08214255\n",
    "14.29\n",
    "707\n",
    "2760\n",
    "33623\n",
    "76.7\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "2\n",
    "1\n",
    "debt_consolidation\n",
    "0.1357\n",
    "366.86\n",
    "10.37349118\n",
    "11.63\n",
    "682\n",
    "4710\n",
    "3511\n",
    "25.6\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "3\n",
    "1\n",
    "debt_consolidation\n",
    "0.1008\n",
    "162.34\n",
    "11.35040654\n",
    "8.1\n",
    "712\n",
    "2699.958333\n",
    "33667\n",
    "73.2\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "4\n",
    "1\n",
    "credit_card\n",
    "0.1426\n",
    "102.92\n",
    "11.29973224\n",
    "14.97\n",
    "667\n",
    "4066\n",
    "4740\n",
    "39.5\n",
    "0\n",
    "1\n",
    "0\n",
    "0\n",
    "5\n",
    "1\n",
    "credit_card\n",
    "0.0788\n",
    "125.13\n",
    "11.90496755\n",
    "16.98\n",
    "727\n",
    "6120.041667\n",
    "50807\n",
    "51\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "6\n",
    "1\n",
    "debt_consolidation\n",
    "0.1496\n",
    "194.02\n",
    "10.71441777\n",
    "4\n",
    "667\n",
    "3180.041667\n",
    "3839\n",
    "76.8\n",
    "0\n",
    "0\n",
    "1\n",
    "1\n",
    "7\n",
    "1\n",
    "all_other\n",
    "0.1114\n",
    "131.22\n",
    "11.00209984\n",
    "11.08\n",
    "722\n",
    "5116\n",
    "24220\n",
    "68.6\n",
    "0\n",
    "0\n",
    "0\n",
    "1\n",
    "8\n",
    "1\n",
    "home_improvement\n",
    "0.1134\n",
    "87.19\n",
    "11.40756495\n",
    "17.25\n",
    "682\n",
    "3989\n",
    "69909\n",
    "51.1\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "9\n",
    "1\n",
    "debt_consolidation\n",
    "0.1221\n",
    "84.12\n",
    "10.20359214\n",
    "10\n",
    "707\n",
    "2730.041667\n",
    "5630\n",
    "23\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "10\n",
    "1\n",
    "debt_consolidation\n",
    "0.1347\n",
    "360.43\n",
    "10.4341158\n",
    "22.09\n",
    "677\n",
    "6713.041667\n",
    "13846\n",
    "71\n",
    "2\n",
    "0\n",
    "1\n",
    "0\n",
    "11\n",
    "1\n",
    "debt_consolidation\n",
    "0.1324\n",
    "253.58\n",
    "11.83500896\n",
    "9.16\n",
    "662\n",
    "4298\n",
    "5122\n",
    "18.2\n",
    "2\n",
    "1\n",
    "0\n",
    "0\n",
    "12\n",
    "1\n",
    "debt_consolidation\n",
    "0.0859\n",
    "316.11\n",
    "10.93310697\n",
    "15.49\n",
    "767\n",
    "6519.958333\n",
    "6068\n",
    "16.7\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "13\n",
    "1\n",
    "small_business\n",
    "0.0714\n",
    "92.82\n",
    "11.51292546\n",
    "6.5\n",
    "747\n",
    "4384\n",
    "3021\n",
    "4.8\n",
    "0\n",
    "1\n",
    "0\n",
    "0\n",
    "14\n",
    "1\n",
    "debt_consolidation\n",
    "0.0863\n",
    "209.54\n",
    "9.487972109\n",
    "9.73\n",
    "727\n",
    "1559.958333\n",
    "6282\n",
    "44.6\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "15\n",
    "1\n",
    "major_purchase\n",
    "0.1103\n",
    "327.53\n",
    "10.73891524\n",
    "13.04\n",
    "702\n",
    "8159.958333\n",
    "5394\n",
    "53.4\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "16\n",
    "1\n",
    "all_other\n",
    "0.1317\n",
    "77.69\n",
    "10.52277288\n",
    "2.26\n",
    "672\n",
    "3895.958333\n",
    "2211\n",
    "88.4\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "17\n",
    "1\n",
    "credit_card\n",
    "0.0894\n",
    "476.58\n",
    "11.60823564\n",
    "7.07\n",
    "797\n",
    "6510.958333\n",
    "7586\n",
    "52.7\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "18\n",
    "1\n",
    "debt_consolidation\n",
    "0.1039\n",
    "584.12\n",
    "10.49127422\n",
    "3.8\n",
    "712\n",
    "2760\n",
    "8311\n",
    "59.8\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "19\n",
    "1\n",
    "major_purchase\n",
    "0.1513\n",
    "173.65\n",
    "11.00209984\n",
    "2.74\n",
    "667\n",
    "1126.958333\n",
    "591\n",
    "84.4\n",
    "3\n",
    "0\n",
    "0\n",
    "0\n",
    "20\n",
    "1\n",
    "all_other\n",
    "0.08\n",
    "188.02\n",
    "11.22524339\n",
    "16.08\n",
    "772\n",
    "4888.958333\n",
    "29797\n",
    "23.2\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "21\n",
    "1\n",
    "all_other\n",
    "0.0863\n",
    "474.42\n",
    "10.81977828\n",
    "2.59\n",
    "797\n",
    "11951\n",
    "5656\n",
    "27.6\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "22\n",
    "1\n",
    "credit_card\n",
    "0.1355\n",
    "339.6\n",
    "11.51292546\n",
    "7.94\n",
    "662\n",
    "1939.958333\n",
    "21162\n",
    "57.7\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "23\n",
    "1\n",
    "credit_card\n",
    "0.0788\n",
    "484.85\n",
    "11.73606902\n",
    "7.05\n",
    "782\n",
    "5640.041667\n",
    "16931\n",
    "34.6\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "24\n",
    "1\n",
    "debt_consolidation\n",
    "0.1229\n",
    "320.19\n",
    "11.26446411\n",
    "8.8\n",
    "672\n",
    "3760.958333\n",
    "4822\n",
    "58.1\n",
    "0\n",
    "0\n",
    "1\n",
    "0\n",
    "25\n",
    "1\n",
    "all_other\n",
    "0.0901\n",
    "159.03\n",
    "12.4292162\n",
    "10\n",
    "712\n",
    "1553.958333\n",
    "14354\n",
    "36.6\n",
    "0\n",
    "2\n",
    "0\n",
    "0\n",
    "26\n",
    "1\n",
    "all_other\n",
    "0.0743\n",
    "155.38\n",
    "11.08214255\n",
    "0.28\n",
    "802\n",
    "4649.958333\n",
    "1576\n",
    "5.7\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "27\n",
    "1\n",
    "debt_consolidation\n",
    "0.1375\n",
    "255.43\n",
    "9.998797732\n",
    "14.29\n",
    "662\n",
    "1318.958333\n",
    "4175\n",
    "51.5\n",
    "0\n",
    "1\n",
    "0\n",
    "0\n",
    "28\n",
    "1\n",
    "all_other\n",
    "0.0743\n",
    "155.38\n",
    "12.20607265\n",
    "0.28\n",
    "772\n",
    "4516.958333\n",
    "3164\n",
    "13.7\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "29\n",
    "1\n",
    "all_other\n",
    "0.0743\n",
    "155.38\n",
    "12.20607265\n",
    "3.72\n",
    "812\n",
    "6778.958333\n",
    "85607\n",
    "0.7\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "30\n",
    "1\n",
    "debt_consolidation\n",
    "0.0807\n",
    "156.84\n",
    "11.51292546\n",
    "2.3\n",
    "742\n",
    "3148.958333\n",
    "9698\n",
    "19.4\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "31\n",
    "1\n",
    "credit_card\n",
    "0.1028\n",
    "275.38\n",
    "9.798127037\n",
    "6.4\n",
    "692\n",
    "7469.958333\n",
    "8847\n",
    "26.9\n",
    "1\n",
    "1\n",
    "0\n",
    "0\n",
    "32\n",
    "1\n",
    "home_improvement\n",
    "0.0743\n",
    "155.38\n",
    "11.91839057\n",
    "0\n",
    "777\n",
    "7128.958333\n",
    "6053\n",
    "19.5\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "33\n",
    "1\n",
    "home_improvement\n",
    "0.0807\n",
    "78.42\n",
    "11.60823564\n",
    "11.33\n",
    "762\n",
    "6059.958333\n",
    "7274\n",
    "13.1\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "34\n",
    "1\n",
    "credit_card\n",
    "0.087\n",
    "158.3\n",
    "11.22524339\n",
    "15.55\n",
    "757\n",
    "4770\n",
    "66033\n",
    "23\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "35\n",
    "1\n",
    "all_other\n",
    "0.0743\n",
    "155.38\n",
    "11.46163217\n",
    "3.83\n",
    "772\n",
    "2535\n",
    "3660\n",
    "6.8\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "36\n",
    "1\n",
    "credit_card\n",
    "0.0743\n",
    "155.38\n",
    "11.35040654\n",
    "0.31\n",
    "787\n",
    "3540\n",
    "216\n",
    "0.6\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "37\n",
    "1\n",
    "credit_card\n",
    "0.1122\n",
    "164.23\n",
    "11.28978191\n",
    "1.21\n",
    "672\n",
    "3990\n",
    "27185\n",
    "16.1\n",
    "3\n",
    "0\n",
    "1\n",
    "0\n",
    "38\n",
    "1\n",
    "debt_consolidation\n",
    "0.0996\n",
    "161.25\n",
    "10.77895629\n",
    "8.03\n",
    "707\n",
    "4320\n",
    "28329\n",
    "48.6\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "39\n",
    "1\n",
    "home_improvement\n",
    "0.0933\n",
    "159.77\n",
    "12.10071213\n",
    "11.93\n",
    "717\n",
    "4498.958333\n",
    "60568\n",
    "39.2\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "40\n",
    "1\n",
    "home_improvement\n",
    "0.0807\n",
    "156.84\n",
    "12.10071213\n",
    "5.55\n",
    "742\n",
    "4019\n",
    "40934\n",
    "26.3\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "41\n",
    "1\n",
    "debt_consolidation\n",
    "0.0743\n",
    "155.38\n",
    "11.69524702\n",
    "2.29\n",
    "777\n",
    "3540\n",
    "8379\n",
    "16.9\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "42\n",
    "1\n",
    "small_business\n",
    "0.087\n",
    "94.98\n",
    "10.1266311\n",
    "14.54\n",
    "752\n",
    "2460\n",
    "3660\n",
    "7.8\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "43\n",
    "1\n",
    "home_improvement\n",
    "0.1028\n",
    "97.2\n",
    "12.20607265\n",
    "0\n",
    "677\n",
    "3779.958333\n",
    "0\n",
    "0\n",
    "0\n",
    "1\n",
    "0\n",
    "0\n",
    "44\n",
    "1\n",
    "credit_card\n",
    "0.1122\n",
    "164.23\n",
    "10.30895266\n",
    "18.64\n",
    "702\n",
    "5190\n",
    "15840\n",
    "47.1\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "45\n",
    "1\n",
    "credit_card\n",
    "0.0838\n",
    "167.02\n",
    "11.21182037\n",
    "14.37\n",
    "797\n",
    "5610.958333\n",
    "6844\n",
    "14.4\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "46\n",
    "1\n",
    "educational\n",
    "0.0775\n",
    "168.6\n",
    "8.987196821\n",
    "3\n",
    "762\n",
    "6659.958333\n",
    "1321\n",
    "16.5\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "47\n",
    "1\n",
    "all_other\n",
    "0.0838\n",
    "160.72\n",
    "10.37349118\n",
    "14.78\n",
    "772\n",
    "4379\n",
    "4737\n",
    "23.9\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "48\n",
    "1\n",
    "credit_card\n",
    "0.0901\n",
    "159.03\n",
    "11.19821472\n",
    "9.96\n",
    "722\n",
    "3089.958333\n",
    "23489\n",
    "37.6\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "49\n",
    "1\n",
    "debt_consolidation\n",
    "0.0743\n",
    "178.69\n",
    "11.73606902\n",
    "0.27\n",
    "792\n",
    "7488.958333\n",
    "2817\n",
    "10.2\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "50\n",
    "1\n",
    "credit_card\n",
    "0.0743\n",
    "155.38\n",
    "10.71441777\n",
    "1.12\n",
    "782\n",
    "5807\n",
    "1783\n",
    "0\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "51\n",
    "1\n",
    "small_business\n",
    "0.1028\n",
    "323.98\n",
    "10.217422\n",
    "10.7\n",
    "712\n",
    "1379\n",
    "3534\n",
    "54.4\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "52\n",
    "1\n",
    "small_business\n",
    "0.1059\n",
    "813.65\n",
    "10.77895629\n",
    "4.05\n",
    "762\n",
    "2729.958333\n",
    "2422\n",
    "23.3\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "53\n",
    "1\n",
    "educational\n",
    "0.0996\n",
    "112.87\n",
    "9.510444964\n",
    "16.44\n",
    "712\n",
    "1649.958333\n",
    "2864\n",
    "41.1\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "54\n",
    "1\n",
    "debt_consolidation\n",
    "0.1596\n",
    "94.88\n",
    "10.86283777\n",
    "12.57\n",
    "627\n",
    "1409\n",
    "3075\n",
    "92.3\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "55\n",
    "1\n",
    "small_business\n",
    "0.1059\n",
    "398.69\n",
    "11.00209984\n",
    "17.12\n",
    "747\n",
    "14008.95833\n",
    "17214\n",
    "8.1\n",
    "2\n",
    "1\n",
    "0\n",
    "0\n",
    "56\n",
    "1\n",
    "credit_card\n",
    "0.1154\n",
    "39.6\n",
    "9.903487553\n",
    "2.04\n",
    "662\n",
    "1139\n",
    "1153\n",
    "75.8\n",
    "2\n",
    "0\n",
    "0\n",
    "0\n",
    "57\n",
    "1\n",
    "credit_card\n",
    "0.1343\n",
    "678.08\n",
    "11.88448902\n",
    "10.15\n",
    "682\n",
    "4209.958333\n",
    "41674\n",
    "74.1\n",
    "0\n",
    "0\n",
    "0\n",
    "1\n",
    "58\n",
    "1\n",
    "credit_card\n",
    "0.1249\n",
    "401.37\n",
    "11.22524339\n",
    "15\n",
    "682\n",
    "4479\n",
    "43039\n",
    "93.4\n",
    "3\n",
    "0\n",
    "0\n",
    "0\n",
    "59\n",
    "1\n",
    "all_other\n",
    "0.0964\n",
    "205.45\n",
    "11.00209984\n",
    "7.72\n",
    "712\n",
    "2293.958333\n",
    "3362\n",
    "69.3\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "60\n",
    "1\n",
    "debt_consolidation\n",
    "0.1186\n",
    "397.75\n",
    "11.08214255\n",
    "7.14\n",
    "682\n",
    "8730\n",
    "11868\n",
    "71.9\n",
    "0\n",
    "2\n",
    "0\n",
    "0\n",
    "61\n",
    "1\n",
    "all_other\n",
    "0.0743\n",
    "155.38\n",
    "10.59663473\n",
    "2.55\n",
    "772\n",
    "8189.958333\n",
    "2562\n",
    "14\n",
    "2\n",
    "0\n",
    "0\n",
    "0\n",
    "62\n",
    "1\n",
    "all_other\n",
    "0.1028\n",
    "113.39\n",
    "9.903487553\n",
    "1.5\n",
    "682\n",
    "1410\n",
    "1882\n",
    "32.4\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "63\n",
    "1\n",
    "debt_consolidation\n",
    "0.1186\n",
    "99.44\n",
    "10.49551521\n",
    "7.47\n",
    "667\n",
    "2550\n",
    "1064\n",
    "70.9\n",
    "2\n",
    "0\n",
    "1\n",
    "0\n",
    "64\n",
    "1\n",
    "credit_card\n",
    "0.0807\n",
    "109.79\n",
    "11.49272276\n",
    "8.52\n",
    "752\n",
    "4979.958333\n",
    "6432\n",
    "6\n",
    "0\n",
    "1\n",
    "0\n",
    "0\n",
    "65\n",
    "1\n",
    "all_other\n",
    "0.1059\n",
    "32.55\n",
    "10.43382164\n",
    "14.47\n",
    "687\n",
    "1110\n",
    "4485\n",
    "36.9\n",
    "1\n",
    "0\n",
    "0\n",
    "1\n",
    "66\n",
    "1\n",
    "all_other\n",
    "0.0996\n",
    "112.87\n",
    "11.26446411\n",
    "11.06\n",
    "702\n",
    "4979.958333\n",
    "16310\n",
    "20.5\n",
    "0\n",
    "1\n",
    "0\n",
    "0\n",
    "67\n",
    "1\n",
    "debt_consolidation\n",
    "0.0964\n",
    "160.51\n",
    "11.77528973\n",
    "8.55\n",
    "712\n",
    "5940\n",
    "20969\n",
    "40.2\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "68\n",
    "1\n",
    "debt_consolidation\n",
    "0.1375\n",
    "340.57\n",
    "10.46310334\n",
    "17.38\n",
    "707\n",
    "2699.958333\n",
    "22213\n",
    "65.9\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "69\n",
    "1\n",
    "home_improvement\n",
    "0.0775\n",
    "156.11\n",
    "12.32385568\n",
    "6.48\n",
    "752\n",
    "5339.958333\n",
    "2160\n",
    "2.3\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "70\n",
    "1\n",
    "credit_card\n",
    "0.0743\n",
    "155.38\n",
    "11.15625052\n",
    "4.56\n",
    "812\n",
    "9809\n",
    "813\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "71\n",
    "1\n",
    "home_improvement\n",
    "0.0807\n",
    "235.25\n",
    "11.91839057\n",
    "2.72\n",
    "757\n",
    "5202\n",
    "23678\n",
    "24.4\n",
    "4\n",
    "0\n",
    "0\n",
    "0\n",
    "72\n",
    "1\n",
    "debt_consolidation\n",
    "0.0996\n",
    "257.99\n",
    "10.77895629\n",
    "15.13\n",
    "722\n",
    "2640\n",
    "11289\n",
    "47.8\n",
    "3\n",
    "0\n",
    "0\n",
    "0\n",
    "73\n",
    "1\n",
    "small_business\n",
    "0.1501\n",
    "225.37\n",
    "12.26904744\n",
    "6.45\n",
    "677\n",
    "6240\n",
    "56411\n",
    "75.3\n",
    "0\n",
    "0\n",
    "0\n",
    "1\n",
    "74\n",
    "1\n",
    "home_improvement\n",
    "0.0964\n",
    "80.26\n",
    "11.22524339\n",
    "20\n",
    "772\n",
    "4920.958333\n",
    "269\n",
    "3.8\n",
    "3\n",
    "0\n",
    "0\n",
    "1\n",
    "75\n",
    "1\n",
    "home_improvement\n",
    "0.128\n",
    "84\n",
    "10.37349118\n",
    "20\n",
    "682\n",
    "5492\n",
    "3408\n",
    "35.1\n",
    "1\n",
    "0\n",
    "0\n",
    "1\n",
    "76\n",
    "1\n",
    "debt_consolidation\n",
    "0.1122\n",
    "507.46\n",
    "10.59663473\n",
    "13.5\n",
    "712\n",
    "5368.958333\n",
    "6513\n",
    "34.3\n",
    "3\n",
    "0\n",
    "1\n",
    "1\n",
    "77\n",
    "1\n",
    "educational\n",
    "0.0743\n",
    "124.31\n",
    "10.59663473\n",
    "3.45\n",
    "772\n",
    "1739.958333\n",
    "330\n",
    "11\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "78\n",
    "1\n",
    "credit_card\n",
    "0.087\n",
    "63.32\n",
    "11.15625052\n",
    "6.07\n",
    "712\n",
    "2550\n",
    "5967\n",
    "19.8\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "79\n",
    "1\n",
    "all_other\n",
    "0.1091\n",
    "130.79\n",
    "9.798127037\n",
    "18\n",
    "707\n",
    "2192\n",
    "5533\n",
    "79.6\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "80\n",
    "1\n",
    "home_improvement\n",
    "0.0775\n",
    "93.67\n",
    "11.73606902\n",
    "2.14\n",
    "762\n",
    "8370\n",
    "21050\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "81\n",
    "1\n",
    "all_other\n",
    "0.0775\n",
    "93.67\n",
    "10.81977828\n",
    "5.35\n",
    "762\n",
    "8370\n",
    "21050\n",
    "0.7\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "82\n",
    "1\n",
    "all_other\n",
    "0.087\n",
    "88.65\n",
    "11.69524702\n",
    "15\n",
    "742\n",
    "3026\n",
    "8155\n",
    "14.9\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "83\n",
    "1\n",
    "all_other\n",
    "0.0964\n",
    "128.41\n",
    "11.51292546\n",
    "15\n",
    "722\n",
    "3026\n",
    "8155\n",
    "14.9\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "84\n",
    "1\n",
    "home_improvement\n",
    "0.087\n",
    "94.98\n",
    "11.03488966\n",
    "12.23\n",
    "737\n",
    "2729.958333\n",
    "3678\n",
    "30.7\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "85\n",
    "1\n",
    "debt_consolidation\n",
    "0.1217\n",
    "179.8\n",
    "10.46310334\n",
    "7.1\n",
    "662\n",
    "2944\n",
    "2040\n",
    "36.2\n",
    "2\n",
    "0\n",
    "0\n",
    "0\n",
    "86\n",
    "1\n",
    "credit_card\n",
    "0.1533\n",
    "444.05\n",
    "11.00209984\n",
    "19.52\n",
    "667\n",
    "2700.958333\n",
    "33074\n",
    "68.8\n",
    "2\n",
    "0\n",
    "0\n",
    "0\n",
    "87\n",
    "1\n",
    "all_other\n",
    "0.0775\n",
    "124.89\n",
    "11.03165864\n",
    "3.46\n",
    "747\n",
    "2519\n",
    "1918\n",
    "17.1\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "88\n",
    "1\n",
    "credit_card\n",
    "0.1533\n",
    "644.3\n",
    "11.40756495\n",
    "5.24\n",
    "662\n",
    "1770\n",
    "22379\n",
    "62.2\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "89\n",
    "1\n",
    "small_business\n",
    "0.0964\n",
    "642.02\n",
    "11.47210347\n",
    "2.86\n",
    "737\n",
    "2190\n",
    "10296\n",
    "42.5\n",
    "0\n",
    "0\n",
    "0\n",
    "1\n",
    "90\n",
    "1\n",
    "small_business\n",
    "0.1154\n",
    "104.76\n",
    "9.61580548\n",
    "12.24\n",
    "672\n",
    "1289.958333\n",
    "2867\n",
    "55.1\n",
    "3\n",
    "0\n",
    "0\n",
    "1\n",
    "91\n",
    "1\n",
    "debt_consolidation\n",
    "0.1122\n",
    "157.66\n",
    "10.30895266\n",
    "12.36\n",
    "682\n",
    "1770\n",
    "12310\n",
    "86.1\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "92\n",
    "1\n",
    "debt_consolidation\n",
    "0.087\n",
    "383.87\n",
    "11.91839057\n",
    "4.8\n",
    "767\n",
    "13349.95833\n",
    "61921\n",
    "39.1\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "93\n",
    "1\n",
    "credit_card\n",
    "0.1122\n",
    "217.6\n",
    "11.38509209\n",
    "18\n",
    "702\n",
    "3725\n",
    "16738\n",
    "26.1\n",
    "2\n",
    "0\n",
    "0\n",
    "0\n",
    "94\n",
    "1\n",
    "all_other\n",
    "0.0807\n",
    "297.99\n",
    "11.05089001\n",
    "3.49\n",
    "792\n",
    "5939\n",
    "40384\n",
    "9.2\n",
    "1\n",
    "0\n",
    "0\n",
    "0\n",
    "95\n",
    "1\n",
    "debt_consolidation\n",
    "0.0775\n",
    "156.11\n",
    "10.71441777\n",
    "5\n",
    "762\n",
    "2131\n",
    "1359\n",
    "5.6\n",
    "2\n",
    "0\n",
    "0\n",
    "0\n",
    "96\n",
    "1\n",
    "debt_consolidation\n",
    "0.1154\n",
    "329.95\n",
    "12.32385568\n",
    "20\n",
    "727\n",
    "10292\n",
    "107501\n",
    "44.8\n",
    "2\n",
    "0\n",
    "0\n",
    "0\n",
    "97\n",
    "1\n",
    "all_other\n",
    "0.0807\n",
    "313.67\n",
    "12.06104687\n",
    "0.72\n",
    "787\n",
    "8010\n",
    "3054\n",
    "5.6\n",
    "3\n",
    "0\n",
    "0\n",
    "0\n",
    "98\n",
    "1\n",
    "all_other\n",
    "0.0807\n",
    "297.99\n",
    "11.40756495\n",
    "2.44\n",
    "792\n",
    "8717.958333\n",
    "40384\n",
    "9.6\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "99\n",
    "1\n",
    "home_improvement\n",
    "0.0933\n",
    "239.65\n",
    "11.44035477\n",
    "9.66\n",
    "792\n",
    "4529\n",
    "128000\n",
    "0\n",
    "3\n",
    "0\n",
    "0\n",
    "0\n",
    "Data dictionary\n",
    "Variable\tExplanation\n",
    "0\tcredit_policy\t1 if the customer meets the credit underwriting criteria; 0 otherwise.\n",
    "1\tpurpose\tThe purpose of the loan.\n",
    "2\tint_rate\tThe interest rate of the loan (more risky borrowers are assigned higher interest rates).\n",
    "3\tinstallment\tThe monthly installments owed by the borrower if the loan is funded.\n",
    "4\tlog_annual_inc\tThe natural log of the self-reported annual income of the borrower.\n",
    "5\tdti\tThe debt-to-income ratio of the borrower (amount of debt divided by annual income).\n",
    "6\tfico\tThe FICO credit score of the borrower.\n",
    "7\tdays_with_cr_line\tThe number of days the borrower has had a credit line.\n",
    "8\trevol_bal\tThe borrower's revolving balance (amount unpaid at the end of the credit card billing cycle).\n",
    "9\trevol_util\tThe borrower's revolving line utilization rate (the amount of the credit line used relative to total credit available).\n",
    "10\tinq_last_6mths\tThe borrower's number of inquiries by creditors in the last 6 months.\n",
    "11\tdelinq_2yrs\tThe number of times the borrower had been 30+ days past due on a payment in the past 2 years.\n",
    "12\tpub_rec\tThe borrower's number of derogatory public records.\n",
    "13\tnot_fully_paid\t1 if the loan is not fully paid; 0 otherwise.\n",
    "Source of dataset.\n",
    "\n",
    "Don't know where to start?\n",
    "Challenges are brief tasks designed to help you practice specific skills:\n",
    "\n",
    "🗺️ Explore: Generate a correlation matrix between the numeric columns. What columns are positively and negatively correlated with each other? Does it change if you segment it by the purpose of the loan?\n",
    "📊 Visualize: Plot histograms for every numeric column with a color element to segment the bars by not_fully_paid.\n",
    "🔎 Analyze: Do loans with the same purpose have similar qualities not shared by loans with differing purposes? You can consider only fully paid loans.\n",
    "Scenarios are broader questions to help you develop an end-to-end project for your portfolio:\n",
    "\n",
    "You recently got a job as a machine learning scientist at a startup that wants to automate loan approvals. As your first project, your manager would like you to build a classifier to predict whether a loan will be paid back based on this data. There are two things to note. First, there is class imbalance; there are fewer examples of loans not fully paid. Second, it's more important to accurately predict whether a loan will not be paid back rather than if a loan is paid back. Your manager will want to know how you accounted for this in training and evaluation your model.\n",
    "\n",
    "You will need to prepare a report that is accessible to a broad audience. It will need to outline your motivation, analysis steps, findings, and conclusions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure where to begin? Scroll to the bottom to find challenges!\n",
    "import pandas as pd\n",
    "loan_data = pd.read_csv(\"loan_data.csv\")\n",
    "print(loan_data.shape)\n",
    "loan_data.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "netflix_header\n",
    "Netflix Top 10: Analyzing Weekly Chart-Toppers\n",
    "This dataset comprises Netflix's weekly top 10 lists for the most-watched TV shows and films worldwide. The data spans from June 28, 2021, to August 27, 2023.\n",
    "\n",
    "This workspace is pre-loaded with two CSV files.\n",
    "\n",
    "netflix_top10.csv contains columns such as show_title, category, weekly_rank, and several view metrics.\n",
    "netflix_top10_country.csv has information about a show or film's performance by country, contained in the columns cumulative_weeks_in_top_10 and weekly_rank.\n",
    "We've added some guiding questions for analyzing this exciting dataset! Feel free to make this workspace yours by adding and removing cells, or editing any of the existing cells.\n",
    "\n",
    "Source: Netflix\n",
    "\n",
    "Explore this dataset\n",
    "To get you started with your analysis...\n",
    "\n",
    "Combine the different categories of top 10 lists in a single weekly top 10 list spanning all categories\n",
    "Are there consistent trends or patterns in the content format (tv, film) that make it to the top 10 over different weeks or months?\n",
    "Explore your country's top 10 trends. Are there unique preferences or regional factors that set your country's list apart from others?\n",
    "Visualize popularity ranking over time through time series plots\n",
    "🔍 Scenario: Understanding the Impact of Content Duration on Netflix's Top 10 Lists\n",
    "This scenario helps you develop an end-to-end project for your portfolio.\n",
    "\n",
    "Background: As a data scientist at Netflix, you're tasked with exploring the dataset containing weekly top 10 lists of the most-watched TV shows and films. For example, you're tasked to find out what the relationship is between duration and ranking over time. Answering this question can inform content creators and strategists on how to optimize their offerings for the platform.\n",
    "\n",
    "Objective: Determine if there's a correlation between content duration and its likelihood of making it to the top 10 lists.\n",
    "\n",
    "You can query the pre-loaded CSV files using SQL directly. Here’s a sample query:\n",
    "\n",
    "Dataframes and CSVs\n",
    "DataFrame available as\n",
    "df\n",
    "SELECT show_title, MAX(cumulative_weeks_in_top_10) as max_cumulative_weeks_in_top_10\n",
    "FROM 'netflix_top10_country.csv'\n",
    "WHERE country_name = 'Argentina'\n",
    "GROUP BY show_title\n",
    "ORDER BY max_cumulative_weeks_in_top_10 DESC\n",
    "LIMIT 3;\n",
    "show_title\n",
    "max_cumulative_weeks_in_top_10\n",
    "0\n",
    "Pasión de Gavilanes\n",
    "82\n",
    "1\n",
    "Café con aroma de mujer\n",
    "31\n",
    "2\n",
    "Yo soy Betty, la fea\n",
    "31\n",
    "import pandas as pd\n",
    "global_top_10 = pd.read_csv(\"netflix_top10.csv\", index_col=0)\n",
    "print(global_top_10.shape)\n",
    "global_top_10.head()\n",
    "(4520, 10)\n",
    "week\n",
    "category\n",
    "weekly_rank\n",
    "show_title\n",
    "season_title\n",
    "weekly_hours_viewed\n",
    "runtime\n",
    "weekly_views\n",
    "cumulative_weeks_in_top_10\n",
    "is_staggered_launch\n",
    "episode_launch_details\n",
    "2023-08-27\n",
    "Films (English)\n",
    "1\n",
    "The Monkey King\n",
    "null\n",
    "23200000\n",
    "1.6167\n",
    "14400000\n",
    "2\n",
    "False\n",
    "null\n",
    "2023-08-27\n",
    "Films (English)\n",
    "2\n",
    "Heart of Stone\n",
    "null\n",
    "28500000\n",
    "2.1\n",
    "13600000\n",
    "3\n",
    "False\n",
    "null\n",
    "2023-08-27\n",
    "Films (English)\n",
    "3\n",
    "You Are So Not Invited to My Bat Mitzvah\n",
    "null\n",
    "21300000\n",
    "1.7333\n",
    "12300000\n",
    "1\n",
    "False\n",
    "null\n",
    "2023-08-27\n",
    "Films (English)\n",
    "4\n",
    "Street Kings\n",
    "null\n",
    "10300000\n",
    "1.8167\n",
    "5700000\n",
    "2\n",
    "False\n",
    "null\n",
    "2023-08-27\n",
    "Films (English)\n",
    "5\n",
    "The Boss Baby\n",
    "null\n",
    "9000000\n",
    "1.6333\n",
    "5500000\n",
    "10\n",
    "False\n",
    "null\n",
    "\n",
    "countries_top_10 = pd.read_csv(\"netflix_top10_country.csv\", index_col=0)\n",
    "print(countries_top_10.shape)\n",
    "countries_top_10.head()\n",
    "(210880, 7)\n",
    "country_name\n",
    "country_iso2\n",
    "week\n",
    "category\n",
    "weekly_rank\n",
    "show_title\n",
    "season_title\n",
    "cumulative_weeks_in_top_10\n",
    "Argentina\n",
    "AR\n",
    "2023-08-27\n",
    "Films\n",
    "1\n",
    "On the Line\n",
    "null\n",
    "1\n",
    "Argentina\n",
    "AR\n",
    "2023-08-27\n",
    "Films\n",
    "2\n",
    "Half Brothers\n",
    "null\n",
    "2\n",
    "Argentina\n",
    "AR\n",
    "2023-08-27\n",
    "Films\n",
    "3\n",
    "Street Kings\n",
    "null\n",
    "3\n",
    "Argentina\n",
    "AR\n",
    "2023-08-27\n",
    "Films\n",
    "4\n",
    "You Are So Not Invited to My Bat Mitzvah\n",
    "null\n",
    "1\n",
    "Argentina\n",
    "AR\n",
    "2023-08-27\n",
    "Films\n",
    "5\n",
    "Heart of Stone\n",
    "null\n",
    "3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nba_shootin_plyer\n",
    "NBA Shooting Data\n",
    "This dataset contains shooting statistics for four different players during the 2021 NBA Playoffs.\n",
    "\n",
    "Not sure where to begin? Scroll to the bottom to find challenges!\n",
    "\n",
    "import pandas as pd\n",
    "nba = pd.read_csv(\"nba_players_shooting.csv\", index_col=0)\n",
    "print(nba.shape)\n",
    "nba.head(100)\n",
    "(776, 6)\n",
    "INDEX\n",
    "SHOOTER\n",
    "X\n",
    "Y\n",
    "RANGE\n",
    "DEFENDER\n",
    "SCORE\n",
    "0\n",
    "Seth Curry\n",
    "-3.8089872646\n",
    "5.583621033\n",
    "(0, 4)\n",
    "Chris Paul\n",
    "MADE\n",
    "1\n",
    "Seth Curry\n",
    "-0.1042785819\n",
    "5.9191379991\n",
    "(0, 4)\n",
    "Trae Young\n",
    "MADE\n",
    "2\n",
    "Seth Curry\n",
    "-0.5258639739\n",
    "8.9122394703\n",
    "(0, 4)\n",
    "Trae Young\n",
    "MADE\n",
    "3\n",
    "Seth Curry\n",
    "-0.7906163067\n",
    "7.5037819957\n",
    "(0, 4)\n",
    "Chris Paul\n",
    "MADE\n",
    "4\n",
    "Seth Curry\n",
    "2.1190741346\n",
    "5.3205339414\n",
    "(0, 4)\n",
    "Trae Young\n",
    "MADE\n",
    "5\n",
    "Seth Curry\n",
    "0.7536142849\n",
    "7.0535536809\n",
    "(0, 4)\n",
    "Chris Paul\n",
    "MADE\n",
    "6\n",
    "Seth Curry\n",
    "-2.5350783377\n",
    "5.626915972\n",
    "(0, 4)\n",
    "Russell Westbrook\n",
    "MADE\n",
    "7\n",
    "Seth Curry\n",
    "-3.2368627197\n",
    "6.6196334807\n",
    "(0, 4)\n",
    "Chris Paul\n",
    "MADE\n",
    "8\n",
    "Seth Curry\n",
    "2.0457652115\n",
    "8.1421816677\n",
    "(0, 4)\n",
    "Chris Paul\n",
    "MADE\n",
    "9\n",
    "Seth Curry\n",
    "2.8216087465\n",
    "6.4268624992\n",
    "(0, 4)\n",
    "Chris Paul\n",
    "MADE\n",
    "10\n",
    "Seth Curry\n",
    "-3.5653972645\n",
    "5.8572311794\n",
    "(0, 4)\n",
    "Trae Young\n",
    "MADE\n",
    "11\n",
    "Seth Curry\n",
    "0.271522194\n",
    "5.3975501768\n",
    "(0, 4)\n",
    "Trae Young\n",
    "MISSED\n",
    "12\n",
    "Seth Curry\n",
    "-2.3730942603\n",
    "6.1300969936\n",
    "(0, 4)\n",
    "Russell Westbrook\n",
    "MISSED\n",
    "13\n",
    "Seth Curry\n",
    "-1.6396772583\n",
    "5.7373457978\n",
    "(0, 4)\n",
    "Russell Westbrook\n",
    "MISSED\n",
    "14\n",
    "Seth Curry\n",
    "-2.0995531982\n",
    "6.5594888722\n",
    "(0, 4)\n",
    "Russell Westbrook\n",
    "MISSED\n",
    "15\n",
    "Seth Curry\n",
    "0.8408279011\n",
    "8.3303729005\n",
    "(0, 4)\n",
    "Russell Westbrook\n",
    "MISSED\n",
    "16\n",
    "Seth Curry\n",
    "-5.7835693886\n",
    "8.2165361123\n",
    "(5, 9)\n",
    "Trae Young\n",
    "MADE\n",
    "17\n",
    "Seth Curry\n",
    "1.1219727886\n",
    "11.9426899611\n",
    "(5, 9)\n",
    "Russell Westbrook\n",
    "MADE\n",
    "18\n",
    "Seth Curry\n",
    "-2.5044678957\n",
    "12.1404921005\n",
    "(5, 9)\n",
    "Trae Young\n",
    "MADE\n",
    "19\n",
    "Seth Curry\n",
    "2.8879326089\n",
    "10.6325971515\n",
    "(5, 9)\n",
    "Trae Young\n",
    "MADE\n",
    "20\n",
    "Seth Curry\n",
    "3.1126125659\n",
    "11.8344228611\n",
    "(5, 9)\n",
    "Chris Paul\n",
    "MADE\n",
    "21\n",
    "Seth Curry\n",
    "6.026322988\n",
    "8.5369633913\n",
    "(5, 9)\n",
    "Trae Young\n",
    "MADE\n",
    "22\n",
    "Seth Curry\n",
    "-4.587841331\n",
    "11.7457429101\n",
    "(5, 9)\n",
    "Trae Young\n",
    "MADE\n",
    "23\n",
    "Seth Curry\n",
    "4.9387133083\n",
    "9.6341000961\n",
    "(5, 9)\n",
    "Trae Young\n",
    "MADE\n",
    "24\n",
    "Seth Curry\n",
    "-4.5297301394\n",
    "10.4106328484\n",
    "(5, 9)\n",
    "Trae Young\n",
    "MISSED\n",
    "25\n",
    "Seth Curry\n",
    "4.8592809373\n",
    "9.3975504125\n",
    "(5, 9)\n",
    "Chris Paul\n",
    "MISSED\n",
    "26\n",
    "Seth Curry\n",
    "-7.6734013057\n",
    "5.516084297\n",
    "(5, 9)\n",
    "Russell Westbrook\n",
    "MISSED\n",
    "27\n",
    "Seth Curry\n",
    "7.0877743792\n",
    "7.1824587968\n",
    "(5, 9)\n",
    "Russell Westbrook\n",
    "MISSED\n",
    "28\n",
    "Seth Curry\n",
    "-12.2233347832\n",
    "6.9363947407\n",
    "(10, 14)\n",
    "Trae Young\n",
    "MADE\n",
    "29\n",
    "Seth Curry\n",
    "9.099430561\n",
    "15.5908045752\n",
    "(10, 14)\n",
    "Trae Young\n",
    "MADE\n",
    "30\n",
    "Seth Curry\n",
    "7.3511068619\n",
    "14.5697257576\n",
    "(10, 14)\n",
    "Trae Young\n",
    "MADE\n",
    "31\n",
    "Seth Curry\n",
    "12.2034055655\n",
    "9.2422825868\n",
    "(10, 14)\n",
    "Trae Young\n",
    "MADE\n",
    "32\n",
    "Seth Curry\n",
    "-3.0583529732\n",
    "17.0251033672\n",
    "(10, 14)\n",
    "Trae Young\n",
    "MADE\n",
    "33\n",
    "Seth Curry\n",
    "-13.6518105178\n",
    "5.7756122526\n",
    "(10, 14)\n",
    "Russell Westbrook\n",
    "MISSED\n",
    "34\n",
    "Seth Curry\n",
    "3.4263173472\n",
    "17.3493709834\n",
    "(10, 14)\n",
    "Russell Westbrook\n",
    "MISSED\n",
    "35\n",
    "Seth Curry\n",
    "12.6666165598\n",
    "5.6788065744\n",
    "(10, 14)\n",
    "Russell Westbrook\n",
    "MISSED\n",
    "36\n",
    "Seth Curry\n",
    "-10.1084846459\n",
    "11.5163151692\n",
    "(10, 14)\n",
    "Russell Westbrook\n",
    "MISSED\n",
    "37\n",
    "Seth Curry\n",
    "12.0919957428\n",
    "5.9242985148\n",
    "(10, 14)\n",
    "Chris Paul\n",
    "MISSED\n",
    "38\n",
    "Seth Curry\n",
    "12.1922651028\n",
    "6.1825293866\n",
    "(10, 14)\n",
    "Russell Westbrook\n",
    "MISSED\n",
    "39\n",
    "Seth Curry\n",
    "-12.5312592329\n",
    "9.5000860622\n",
    "(10, 14)\n",
    "Chris Paul\n",
    "MISSED\n",
    "40\n",
    "Seth Curry\n",
    "13.2533341323\n",
    "16.3553723367\n",
    "(15, 19)\n",
    "Chris Paul\n",
    "MADE\n",
    "41\n",
    "Seth Curry\n",
    "17.2736614907\n",
    "7.6189331949\n",
    "(15, 19)\n",
    "Trae Young\n",
    "MADE\n",
    "42\n",
    "Seth Curry\n",
    "16.7727383314\n",
    "10.4554308062\n",
    "(15, 19)\n",
    "Russell Westbrook\n",
    "MADE\n",
    "43\n",
    "Seth Curry\n",
    "11.3026583502\n",
    "16.8199266148\n",
    "(15, 19)\n",
    "Chris Paul\n",
    "MADE\n",
    "44\n",
    "Seth Curry\n",
    "15.7353696699\n",
    "11.4702997881\n",
    "(15, 19)\n",
    "Russell Westbrook\n",
    "MADE\n",
    "45\n",
    "Seth Curry\n",
    "-5.4585672008\n",
    "22.1335870236\n",
    "(15, 19)\n",
    "Trae Young\n",
    "MADE\n",
    "46\n",
    "Seth Curry\n",
    "14.3139890653\n",
    "13.6875871452\n",
    "(15, 19)\n",
    "Russell Westbrook\n",
    "MADE\n",
    "47\n",
    "Seth Curry\n",
    "-2.9467634017\n",
    "23.0332152775\n",
    "(15, 19)\n",
    "Trae Young\n",
    "MADE\n",
    "48\n",
    "Seth Curry\n",
    "-2.0956491628\n",
    "22.9320198953\n",
    "(15, 19)\n",
    "Trae Young\n",
    "MADE\n",
    "49\n",
    "Seth Curry\n",
    "5.629904584\n",
    "22.7206447711\n",
    "(15, 19)\n",
    "Trae Young\n",
    "MADE\n",
    "50\n",
    "Seth Curry\n",
    "-16.0883111332\n",
    "9.9120279214\n",
    "(15, 19)\n",
    "Trae Young\n",
    "MADE\n",
    "51\n",
    "Seth Curry\n",
    "-13.5878248941\n",
    "14.0812711983\n",
    "(15, 19)\n",
    "Trae Young\n",
    "MADE\n",
    "52\n",
    "Seth Curry\n",
    "-15.1237517193\n",
    "11.6731922142\n",
    "(15, 19)\n",
    "Russell Westbrook\n",
    "MADE\n",
    "53\n",
    "Seth Curry\n",
    "4.9841609257\n",
    "21.7844973007\n",
    "(15, 19)\n",
    "Russell Westbrook\n",
    "MISSED\n",
    "54\n",
    "Seth Curry\n",
    "5.3869254794\n",
    "22.122142681\n",
    "(15, 19)\n",
    "Russell Westbrook\n",
    "MISSED\n",
    "55\n",
    "Seth Curry\n",
    "14.6385626926\n",
    "11.5868232679\n",
    "(15, 19)\n",
    "Trae Young\n",
    "MISSED\n",
    "56\n",
    "Seth Curry\n",
    "5.3144927207\n",
    "21.5191793837\n",
    "(15, 19)\n",
    "Trae Young\n",
    "MISSED\n",
    "57\n",
    "Seth Curry\n",
    "-14.4748272237\n",
    "12.723421567\n",
    "(15, 19)\n",
    "Russell Westbrook\n",
    "MISSED\n",
    "58\n",
    "Seth Curry\n",
    "-21.1627386707\n",
    "13.6819958933\n",
    "(20, 24)\n",
    "Trae Young\n",
    "MADE\n",
    "59\n",
    "Seth Curry\n",
    "-21.3373212532\n",
    "9.6926157536\n",
    "(20, 24)\n",
    "Trae Young\n",
    "MADE\n",
    "60\n",
    "Seth Curry\n",
    "-11.8320068194\n",
    "22.080053436\n",
    "(20, 24)\n",
    "Chris Paul\n",
    "MADE\n",
    "61\n",
    "Seth Curry\n",
    "-15.6417482485\n",
    "20.9646400086\n",
    "(20, 24)\n",
    "Chris Paul\n",
    "MADE\n",
    "62\n",
    "Seth Curry\n",
    "0.1728643098\n",
    "26.8368508777\n",
    "(20, 24)\n",
    "Trae Young\n",
    "MADE\n",
    "63\n",
    "Seth Curry\n",
    "21.9998344444\n",
    "11.6652236116\n",
    "(20, 24)\n",
    "Trae Young\n",
    "MADE\n",
    "64\n",
    "Seth Curry\n",
    "11.4834316517\n",
    "23.1563154751\n",
    "(20, 24)\n",
    "Trae Young\n",
    "MADE\n",
    "65\n",
    "Seth Curry\n",
    "-20.738588149\n",
    "6.3967967791\n",
    "(20, 24)\n",
    "Chris Paul\n",
    "MADE\n",
    "66\n",
    "Seth Curry\n",
    "-20.2422066744\n",
    "13.5144600757\n",
    "(20, 24)\n",
    "Russell Westbrook\n",
    "MADE\n",
    "67\n",
    "Seth Curry\n",
    "13.7465739055\n",
    "24.4640080886\n",
    "(20, 24)\n",
    "Chris Paul\n",
    "MADE\n",
    "68\n",
    "Seth Curry\n",
    "15.3593959293\n",
    "22.9583017052\n",
    "(20, 24)\n",
    "Trae Young\n",
    "MADE\n",
    "69\n",
    "Seth Curry\n",
    "-0.8784654594\n",
    "28.5069696994\n",
    "(20, 24)\n",
    "Trae Young\n",
    "MADE\n",
    "70\n",
    "Seth Curry\n",
    "-22.0182437756\n",
    "12.3705244742\n",
    "(20, 24)\n",
    "Trae Young\n",
    "MADE\n",
    "71\n",
    "Seth Curry\n",
    "14.2963877491\n",
    "23.4429867791\n",
    "(20, 24)\n",
    "Trae Young\n",
    "MADE\n",
    "72\n",
    "Seth Curry\n",
    "5.6754760221\n",
    "27.277406821\n",
    "(20, 24)\n",
    "Trae Young\n",
    "MADE\n",
    "73\n",
    "Seth Curry\n",
    "-13.3290361486\n",
    "22.0622189934\n",
    "(20, 24)\n",
    "Trae Young\n",
    "MADE\n",
    "74\n",
    "Seth Curry\n",
    "-19.8257874137\n",
    "13.9063446538\n",
    "(20, 24)\n",
    "Chris Paul\n",
    "MADE\n",
    "75\n",
    "Seth Curry\n",
    "-9.0022945249\n",
    "24.1641923943\n",
    "(20, 24)\n",
    "Chris Paul\n",
    "MADE\n",
    "76\n",
    "Seth Curry\n",
    "-13.078475726\n",
    "20.5779720347\n",
    "(20, 24)\n",
    "Chris Paul\n",
    "MADE\n",
    "77\n",
    "Seth Curry\n",
    "20.0362326167\n",
    "15.1067835713\n",
    "(20, 24)\n",
    "Trae Young\n",
    "MADE\n",
    "78\n",
    "Seth Curry\n",
    "3.0381148312\n",
    "27.5796813746\n",
    "(20, 24)\n",
    "Russell Westbrook\n",
    "MISSED\n",
    "79\n",
    "Seth Curry\n",
    "-8.2541630455\n",
    "24.582278951\n",
    "(20, 24)\n",
    "Russell Westbrook\n",
    "MISSED\n",
    "80\n",
    "Seth Curry\n",
    "17.1625470001\n",
    "21.3423165191\n",
    "(20, 24)\n",
    "Chris Paul\n",
    "MISSED\n",
    "81\n",
    "Seth Curry\n",
    "-17.6249776294\n",
    "20.9509200534\n",
    "(20, 24)\n",
    "Russell Westbrook\n",
    "MISSED\n",
    "82\n",
    "Seth Curry\n",
    "19.8928287215\n",
    "13.4636748643\n",
    "(20, 24)\n",
    "Chris Paul\n",
    "MISSED\n",
    "83\n",
    "Seth Curry\n",
    "1.5119342113\n",
    "28.1053472318\n",
    "(20, 24)\n",
    "Russell Westbrook\n",
    "MISSED\n",
    "84\n",
    "Seth Curry\n",
    "-10.797996905\n",
    "23.1438329364\n",
    "(20, 24)\n",
    "Chris Paul\n",
    "MISSED\n",
    "85\n",
    "Seth Curry\n",
    "-11.6307274048\n",
    "21.9521193985\n",
    "(20, 24)\n",
    "Russell Westbrook\n",
    "MISSED\n",
    "86\n",
    "Seth Curry\n",
    "17.1974276986\n",
    "20.4595584764\n",
    "(20, 24)\n",
    "Russell Westbrook\n",
    "MISSED\n",
    "87\n",
    "Seth Curry\n",
    "17.7230582678\n",
    "15.5803566291\n",
    "(20, 24)\n",
    "Chris Paul\n",
    "MISSED\n",
    "88\n",
    "Seth Curry\n",
    "-13.8500670618\n",
    "29.5998316852\n",
    "(25, 29)\n",
    "Chris Paul\n",
    "MADE\n",
    "89\n",
    "Seth Curry\n",
    "-5.5444219237\n",
    "31.6354458829\n",
    "(25, 29)\n",
    "Chris Paul\n",
    "MADE\n",
    "90\n",
    "Seth Curry\n",
    "14.3327408566\n",
    "27.1157099805\n",
    "(25, 29)\n",
    "Russell Westbrook\n",
    "MADE\n",
    "91\n",
    "Seth Curry\n",
    "-24.2320519623\n",
    "5.8291528529\n",
    "(25, 29)\n",
    "Russell Westbrook\n",
    "MADE\n",
    "92\n",
    "Seth Curry\n",
    "23.6315697912\n",
    "19.8198885479\n",
    "(25, 29)\n",
    "Trae Young\n",
    "MADE\n",
    "93\n",
    "Seth Curry\n",
    "21.735183323\n",
    "23.0453075363\n",
    "(25, 29)\n",
    "Russell Westbrook\n",
    "MADE\n",
    "94\n",
    "Seth Curry\n",
    "23.1038782028\n",
    "19.0685243447\n",
    "(25, 29)\n",
    "Trae Young\n",
    "MADE\n",
    "95\n",
    "Seth Curry\n",
    "23.2587397059\n",
    "18.2484226823\n",
    "(25, 29)\n",
    "Chris Paul\n",
    "MADE\n",
    "96\n",
    "Seth Curry\n",
    "-24.2320519623\n",
    "15.2095131382\n",
    "(25, 29)\n",
    "Trae Young\n",
    "MADE\n",
    "97\n",
    "Seth Curry\n",
    "23.952741768\n",
    "10.0405893304\n",
    "(25, 29)\n",
    "Chris Paul\n",
    "MADE\n",
    "98\n",
    "Seth Curry\n",
    "23.7183453125\n",
    "15.8197130259\n",
    "(25, 29)\n",
    "Chris Paul\n",
    "MADE\n",
    "99\n",
    "Seth Curry\n",
    "-0.8565501786\n",
    "33.8539132503\n",
    "(25, 29)\n",
    "Trae Young\n",
    "MADE\n",
    "Data Dictionary\n",
    "variable\tclass\tdescription\n",
    "SHOOTER\tString\tName of the player taking the shot\n",
    "X\tfloat\tHorizontal distance of the shot taken from the basket in ft\n",
    "Y\tfloat\tVertical distance of the shot taken from the basket in ft\n",
    "RANGE\tString\tRadius range of the shot taken from the basket in ft\n",
    "DEFENDER\tString\tName of the player defending the shot\n",
    "SCORE\tString\t'MADE' if shot is scored, else 'MISSED'\n",
    "Source of dataset.\n",
    "\n",
    "Don't know where to start?\n",
    "Challenges are brief tasks designed to help you practice specific skills:\n",
    "\n",
    "🗺️ Explore: At what range is each player most likely to score a shot?\n",
    "📊 Visualize: Plot the shots made by the X and Y position on the court. For each shot, differentiate between the four different players.\n",
    "🔎 Analyze: Are players more likely to score a shot the closer they get to the basket?\n",
    "Scenarios are broader questions to help you develop an end-to-end project for your portfolio:\n",
    "\n",
    "A university basketball team has hired you to use data to improve their performance. They want to know whether it's possible to use past data to provide tailored recommendations to players.\n",
    "\n",
    "As a test, you have been provided with NBA shooting data for four players. The manager of the university team has asked you whether it is possible to provide data-driven recommendations for each player based on their likelihood of making a shot. You must also include how reliable your findings are, as well as advice for each player.\n",
    "\n",
    "You will need to prepare a report that is accessible to a broad audience. It will need to outline your steps, findings, and conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBA Shooting Data\n",
    "# This dataset contains shooting statistics for four different players during the 2021 NBA Playoffs.\n",
    "\n",
    "# Not sure where to begin? Scroll to the bottom to find challenges!\n",
    "\n",
    "import pandas as pd\n",
    "nba = pd.read_csv(\"nba_players_shooting.csv\", index_col=0)\n",
    "print(nba.shape)\n",
    "nba.head(100)\n",
    "import pandas as pd\n",
    "nba = pd.read_csv(\"nba_players_shooting.csv\", index_col=0)\n",
    "print(nba.shape)\n",
    "nba.head(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "World Population Data\n",
    "This dataset has the total population numbers for every country from 1960 to 2020. Additionally, there is a table that contains country information, including region, income group, and any special notes.\n",
    "\n",
    "Not sure where to begin? Scroll to the bottom to find challenges!\n",
    "\n",
    "import pandas as pd\n",
    "world_pop = pd.read_csv(\"world_pop_data.csv\")\n",
    "world_pop\n",
    "Country Code\n",
    "Indicator Name\n",
    "Indicator Code\n",
    "1960\n",
    "1961\n",
    "1962\n",
    "1963\n",
    "1964\n",
    "1965\n",
    "1966\n",
    "1967\n",
    "1968\n",
    "1969\n",
    "1970\n",
    "1971\n",
    "1972\n",
    "1973\n",
    "1974\n",
    "1975\n",
    "1976\n",
    "1977\n",
    "1978\n",
    "1979\n",
    "1980\n",
    "1981\n",
    "1982\n",
    "1983\n",
    "1984\n",
    "1985\n",
    "1986\n",
    "1987\n",
    "1988\n",
    "1989\n",
    "1990\n",
    "1991\n",
    "1992\n",
    "1993\n",
    "1994\n",
    "1995\n",
    "1996\n",
    "1997\n",
    "1998\n",
    "1999\n",
    "2000\n",
    "2001\n",
    "2002\n",
    "2003\n",
    "2004\n",
    "2005\n",
    "2006\n",
    "2007\n",
    "2008\n",
    "2009\n",
    "2010\n",
    "2011\n",
    "2012\n",
    "2013\n",
    "2014\n",
    "2015\n",
    "2016\n",
    "2017\n",
    "2018\n",
    "2019\n",
    "2020\n",
    "0\n",
    "ABW\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "54208\n",
    "55434\n",
    "56234\n",
    "56699\n",
    "57029\n",
    "57357\n",
    "57702\n",
    "58044\n",
    "58377\n",
    "58734\n",
    "59070\n",
    "59442\n",
    "59849\n",
    "60236\n",
    "60527\n",
    "60653\n",
    "60586\n",
    "60366\n",
    "60102\n",
    "59972\n",
    "60097\n",
    "60561\n",
    "61341\n",
    "62213\n",
    "62826\n",
    "63024\n",
    "62645\n",
    "61838\n",
    "61072\n",
    "61033\n",
    "62152\n",
    "64623\n",
    "68240\n",
    "72495\n",
    "76705\n",
    "80324\n",
    "83211\n",
    "85450\n",
    "87280\n",
    "89009\n",
    "90866\n",
    "92892\n",
    "94992\n",
    "97016\n",
    "98744\n",
    "100028\n",
    "100830\n",
    "101226\n",
    "101362\n",
    "101452\n",
    "101665\n",
    "102050\n",
    "102565\n",
    "103165\n",
    "103776\n",
    "104339\n",
    "104865\n",
    "105361\n",
    "105846\n",
    "106310\n",
    "106766\n",
    "1\n",
    "AFE\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "130836765\n",
    "134159786\n",
    "137614644\n",
    "141202036\n",
    "144920186\n",
    "148769974\n",
    "152752671\n",
    "156876454\n",
    "161156430\n",
    "165611760\n",
    "170257189\n",
    "175100167\n",
    "180141148\n",
    "185376550\n",
    "190800796\n",
    "196409937\n",
    "202205766\n",
    "208193045\n",
    "214368393\n",
    "220740384\n",
    "227305945\n",
    "234058404\n",
    "240999134\n",
    "248146290\n",
    "255530063\n",
    "263161451\n",
    "271050065\n",
    "279184536\n",
    "287524258\n",
    "296024639\n",
    "304648010\n",
    "313394693\n",
    "322270073\n",
    "331265579\n",
    "340379934\n",
    "349605660\n",
    "358953595\n",
    "368440591\n",
    "378098393\n",
    "387977990\n",
    "398113044\n",
    "408522129\n",
    "419223717\n",
    "430246635\n",
    "441630149\n",
    "453404076\n",
    "465581372\n",
    "478166911\n",
    "491173160\n",
    "504604672\n",
    "518468229\n",
    "532760424\n",
    "547482863\n",
    "562601578\n",
    "578075373\n",
    "593871847\n",
    "609978946\n",
    "626392880\n",
    "643090131\n",
    "660046272\n",
    "677243299\n",
    "2\n",
    "AFG\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "8996967\n",
    "9169406\n",
    "9351442\n",
    "9543200\n",
    "9744772\n",
    "9956318\n",
    "10174840\n",
    "10399936\n",
    "10637064\n",
    "10893772\n",
    "11173654\n",
    "11475450\n",
    "11791222\n",
    "12108963\n",
    "12412960\n",
    "12689164\n",
    "12943093\n",
    "13171294\n",
    "13341199\n",
    "13411060\n",
    "13356500\n",
    "13171679\n",
    "12882518\n",
    "12537732\n",
    "12204306\n",
    "11938204\n",
    "11736177\n",
    "11604538\n",
    "11618008\n",
    "11868873\n",
    "12412311\n",
    "13299016\n",
    "14485543\n",
    "15816601\n",
    "17075728\n",
    "18110662\n",
    "18853444\n",
    "19357126\n",
    "19737770\n",
    "20170847\n",
    "20779957\n",
    "21606992\n",
    "22600774\n",
    "23680871\n",
    "24726689\n",
    "25654274\n",
    "26433058\n",
    "27100542\n",
    "27722281\n",
    "28394806\n",
    "29185511\n",
    "30117411\n",
    "31161378\n",
    "32269592\n",
    "33370804\n",
    "34413603\n",
    "35383028\n",
    "36296111\n",
    "37171922\n",
    "38041757\n",
    "38928341\n",
    "3\n",
    "AFW\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "96396419\n",
    "98407221\n",
    "100506960\n",
    "102691339\n",
    "104953470\n",
    "107289875\n",
    "109701811\n",
    "112195950\n",
    "114781116\n",
    "117468741\n",
    "120269044\n",
    "123184308\n",
    "126218502\n",
    "129384954\n",
    "132699537\n",
    "136173544\n",
    "139813171\n",
    "143615715\n",
    "147571063\n",
    "151663853\n",
    "155882270\n",
    "160223588\n",
    "164689764\n",
    "169279422\n",
    "173991851\n",
    "178826553\n",
    "183785612\n",
    "188868567\n",
    "194070079\n",
    "199382783\n",
    "204803865\n",
    "210332267\n",
    "215976366\n",
    "221754806\n",
    "227692136\n",
    "233807627\n",
    "240114179\n",
    "246613750\n",
    "253302310\n",
    "260170348\n",
    "267214544\n",
    "274433894\n",
    "281842480\n",
    "289469530\n",
    "297353098\n",
    "305520588\n",
    "313985474\n",
    "322741656\n",
    "331772330\n",
    "341050537\n",
    "350556886\n",
    "360285439\n",
    "370243017\n",
    "380437896\n",
    "390882979\n",
    "401586651\n",
    "412551299\n",
    "423769930\n",
    "435229381\n",
    "446911598\n",
    "458803476\n",
    "4\n",
    "AGO\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "5454938\n",
    "5531451\n",
    "5608499\n",
    "5679409\n",
    "5734995\n",
    "5770573\n",
    "5781305\n",
    "5774440\n",
    "5771973\n",
    "5803677\n",
    "5890360\n",
    "6041239\n",
    "6248965\n",
    "6497283\n",
    "6761623\n",
    "7023994\n",
    "7279630\n",
    "7533814\n",
    "7790774\n",
    "8058112\n",
    "8341290\n",
    "8640478\n",
    "8952971\n",
    "9278104\n",
    "9614756\n",
    "9961993\n",
    "10320116\n",
    "10689247\n",
    "11068051\n",
    "11454784\n",
    "11848385\n",
    "12248901\n",
    "12657361\n",
    "13075044\n",
    "13503753\n",
    "13945205\n",
    "14400722\n",
    "14871572\n",
    "15359600\n",
    "15866871\n",
    "16395477\n",
    "16945753\n",
    "17519418\n",
    "18121477\n",
    "18758138\n",
    "19433604\n",
    "20149905\n",
    "20905360\n",
    "21695636\n",
    "22514275\n",
    "23356247\n",
    "24220660\n",
    "25107925\n",
    "26015786\n",
    "26941773\n",
    "27884380\n",
    "28842482\n",
    "29816769\n",
    "30809787\n",
    "31825299\n",
    "32866268\n",
    "5\n",
    "ALB\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "1608800\n",
    "1659800\n",
    "1711319\n",
    "1762621\n",
    "1814135\n",
    "1864791\n",
    "1914573\n",
    "1965598\n",
    "2022272\n",
    "2081695\n",
    "2135479\n",
    "2187853\n",
    "2243126\n",
    "2296752\n",
    "2350124\n",
    "2404831\n",
    "2458526\n",
    "2513546\n",
    "2566266\n",
    "2617832\n",
    "2671997\n",
    "2726056\n",
    "2784278\n",
    "2843960\n",
    "2904429\n",
    "2964762\n",
    "3022635\n",
    "3083605\n",
    "3142336\n",
    "3227943\n",
    "3286542\n",
    "3266790\n",
    "3247039\n",
    "3227287\n",
    "3207536\n",
    "3187784\n",
    "3168033\n",
    "3148281\n",
    "3128530\n",
    "3108778\n",
    "3089027\n",
    "3060173\n",
    "3051010\n",
    "3039616\n",
    "3026939\n",
    "3011487\n",
    "2992547\n",
    "2970017\n",
    "2947314\n",
    "2927519\n",
    "2913021\n",
    "2905195\n",
    "2900401\n",
    "2895092\n",
    "2889104\n",
    "2880703\n",
    "2876101\n",
    "2873457\n",
    "2866376\n",
    "2854191\n",
    "2837743\n",
    "6\n",
    "AND\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "13410\n",
    "14378\n",
    "15379\n",
    "16407\n",
    "17466\n",
    "18542\n",
    "19646\n",
    "20760\n",
    "21886\n",
    "23053\n",
    "24275\n",
    "25571\n",
    "26885\n",
    "28232\n",
    "29515\n",
    "30705\n",
    "31782\n",
    "32769\n",
    "33744\n",
    "34825\n",
    "36063\n",
    "37498\n",
    "39115\n",
    "40854\n",
    "42706\n",
    "44593\n",
    "46520\n",
    "48459\n",
    "50433\n",
    "52452\n",
    "54508\n",
    "56666\n",
    "58882\n",
    "60974\n",
    "62676\n",
    "63860\n",
    "64363\n",
    "64318\n",
    "64140\n",
    "64368\n",
    "65390\n",
    "67344\n",
    "70048\n",
    "73180\n",
    "76250\n",
    "78871\n",
    "80995\n",
    "82682\n",
    "83860\n",
    "84461\n",
    "84454\n",
    "83748\n",
    "82427\n",
    "80770\n",
    "79213\n",
    "77993\n",
    "77295\n",
    "76997\n",
    "77008\n",
    "77146\n",
    "77265\n",
    "7\n",
    "ARB\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "92197715\n",
    "94724540\n",
    "97334438\n",
    "100034191\n",
    "102832792\n",
    "105736428\n",
    "108758634\n",
    "111899335\n",
    "115136161\n",
    "118437193\n",
    "121785630\n",
    "125164720\n",
    "128598743\n",
    "132161302\n",
    "135952270\n",
    "140040580\n",
    "144453278\n",
    "149161836\n",
    "154111160\n",
    "159218539\n",
    "164420771\n",
    "169698978\n",
    "175061794\n",
    "180505967\n",
    "186035280\n",
    "191650328\n",
    "197338140\n",
    "203084958\n",
    "208889669\n",
    "214753965\n",
    "222653371\n",
    "228731671\n",
    "232956364\n",
    "239243294\n",
    "245449429\n",
    "253107302\n",
    "259000937\n",
    "264822167\n",
    "270575777\n",
    "276393809\n",
    "282344141\n",
    "288432153\n",
    "294665202\n",
    "301113869\n",
    "307862846\n",
    "314965776\n",
    "322452764\n",
    "330290752\n",
    "338395936\n",
    "346629179\n",
    "354890097\n",
    "363156846\n",
    "371437642\n",
    "379696477\n",
    "387899835\n",
    "396028301\n",
    "404042892\n",
    "411942825\n",
    "419851989\n",
    "427870273\n",
    "436080728\n",
    "8\n",
    "ARE\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "92417\n",
    "100801\n",
    "112112\n",
    "125130\n",
    "138049\n",
    "149855\n",
    "159979\n",
    "169768\n",
    "182620\n",
    "203103\n",
    "234512\n",
    "277463\n",
    "330968\n",
    "394625\n",
    "467457\n",
    "548295\n",
    "637926\n",
    "735347\n",
    "835498\n",
    "931752\n",
    "1019507\n",
    "1096602\n",
    "1164816\n",
    "1228457\n",
    "1293970\n",
    "1366165\n",
    "1446386\n",
    "1533526\n",
    "1627068\n",
    "1725676\n",
    "1828437\n",
    "1937159\n",
    "2052892\n",
    "2173135\n",
    "2294377\n",
    "2415099\n",
    "2539121\n",
    "2671361\n",
    "2813214\n",
    "2966029\n",
    "3134067\n",
    "3302722\n",
    "3478769\n",
    "3711931\n",
    "4068577\n",
    "4588222\n",
    "5300172\n",
    "6168846\n",
    "7089486\n",
    "7917368\n",
    "8549998\n",
    "8946778\n",
    "9141598\n",
    "9197908\n",
    "9214182\n",
    "9262896\n",
    "9360975\n",
    "9487206\n",
    "9630966\n",
    "9770526\n",
    "9890400\n",
    "9\n",
    "ARG\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "20481781\n",
    "20817270\n",
    "21153042\n",
    "21488916\n",
    "21824427\n",
    "22159644\n",
    "22494031\n",
    "22828872\n",
    "23168268\n",
    "23517613\n",
    "23880564\n",
    "24259564\n",
    "24653172\n",
    "25056475\n",
    "25462305\n",
    "25865775\n",
    "26264681\n",
    "26661397\n",
    "27061041\n",
    "27471046\n",
    "27896532\n",
    "28338514\n",
    "28794550\n",
    "29262049\n",
    "29737097\n",
    "30216284\n",
    "30698964\n",
    "31184411\n",
    "31668939\n",
    "32148137\n",
    "32618648\n",
    "33079002\n",
    "33529320\n",
    "33970103\n",
    "34402669\n",
    "34828168\n",
    "35246376\n",
    "35657438\n",
    "36063451\n",
    "36467218\n",
    "36870796\n",
    "37275644\n",
    "37681743\n",
    "38087866\n",
    "38491970\n",
    "38892924\n",
    "39289876\n",
    "39684303\n",
    "40080159\n",
    "40482786\n",
    "40788453\n",
    "41261490\n",
    "41733271\n",
    "42202935\n",
    "42669500\n",
    "43131966\n",
    "43590368\n",
    "44044811\n",
    "44494502\n",
    "44938712\n",
    "45376763\n",
    "10\n",
    "ARM\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "1874119\n",
    "1941498\n",
    "2009524\n",
    "2077584\n",
    "2145004\n",
    "2211316\n",
    "2276038\n",
    "2339133\n",
    "2401142\n",
    "2462938\n",
    "2525067\n",
    "2587716\n",
    "2650484\n",
    "2712780\n",
    "2773750\n",
    "2832752\n",
    "2889583\n",
    "2944375\n",
    "2997419\n",
    "3049107\n",
    "3099759\n",
    "3148096\n",
    "3193696\n",
    "3238592\n",
    "3285593\n",
    "3335935\n",
    "3392264\n",
    "3451947\n",
    "3504667\n",
    "3536473\n",
    "3538164\n",
    "3505249\n",
    "3442820\n",
    "3363111\n",
    "3283664\n",
    "3217349\n",
    "3168213\n",
    "3133081\n",
    "3108691\n",
    "3089020\n",
    "3069597\n",
    "3050686\n",
    "3033976\n",
    "3017938\n",
    "3000715\n",
    "2981262\n",
    "2958301\n",
    "2932615\n",
    "2907615\n",
    "2888094\n",
    "2877314\n",
    "2876536\n",
    "2884239\n",
    "2897593\n",
    "2912403\n",
    "2925559\n",
    "2936147\n",
    "2944789\n",
    "2951741\n",
    "2957728\n",
    "2963234\n",
    "11\n",
    "ASM\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "20127\n",
    "20605\n",
    "21246\n",
    "22029\n",
    "22850\n",
    "23675\n",
    "24473\n",
    "25235\n",
    "25980\n",
    "26698\n",
    "27362\n",
    "27982\n",
    "28564\n",
    "29103\n",
    "29595\n",
    "30045\n",
    "30455\n",
    "30834\n",
    "31262\n",
    "31842\n",
    "32648\n",
    "33697\n",
    "34969\n",
    "36413\n",
    "37946\n",
    "39521\n",
    "41114\n",
    "42741\n",
    "44346\n",
    "45894\n",
    "47351\n",
    "48682\n",
    "49900\n",
    "51025\n",
    "52099\n",
    "53158\n",
    "54209\n",
    "55227\n",
    "56180\n",
    "57049\n",
    "57816\n",
    "58496\n",
    "59077\n",
    "59495\n",
    "59684\n",
    "59557\n",
    "59109\n",
    "58367\n",
    "57490\n",
    "56675\n",
    "56084\n",
    "55755\n",
    "55669\n",
    "55717\n",
    "55791\n",
    "55806\n",
    "55739\n",
    "55617\n",
    "55461\n",
    "55312\n",
    "55197\n",
    "12\n",
    "ATG\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "54132\n",
    "55005\n",
    "55849\n",
    "56701\n",
    "57641\n",
    "58699\n",
    "59912\n",
    "61240\n",
    "62523\n",
    "63553\n",
    "64184\n",
    "64354\n",
    "64134\n",
    "63649\n",
    "63108\n",
    "62671\n",
    "62353\n",
    "62162\n",
    "62038\n",
    "61948\n",
    "61861\n",
    "61789\n",
    "61780\n",
    "61779\n",
    "61784\n",
    "61785\n",
    "61754\n",
    "61713\n",
    "61758\n",
    "62007\n",
    "62533\n",
    "63363\n",
    "64459\n",
    "65777\n",
    "67201\n",
    "68672\n",
    "70176\n",
    "71707\n",
    "73219\n",
    "74674\n",
    "76007\n",
    "77212\n",
    "78298\n",
    "79311\n",
    "80347\n",
    "81462\n",
    "82715\n",
    "84029\n",
    "85394\n",
    "86743\n",
    "88030\n",
    "89250\n",
    "90407\n",
    "91510\n",
    "92562\n",
    "93571\n",
    "94520\n",
    "95425\n",
    "96282\n",
    "97115\n",
    "97928\n",
    "13\n",
    "AUS\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "10276477\n",
    "10483000\n",
    "10742000\n",
    "10950000\n",
    "11167000\n",
    "11388000\n",
    "11651000\n",
    "11799000\n",
    "12009000\n",
    "12263000\n",
    "12507000\n",
    "12937000\n",
    "13177000\n",
    "13380000\n",
    "13723000\n",
    "13893000\n",
    "14033000\n",
    "14192000\n",
    "14358000\n",
    "14514000\n",
    "14692000\n",
    "14927000\n",
    "15178000\n",
    "15369000\n",
    "15544000\n",
    "15758000\n",
    "16018400\n",
    "16263900\n",
    "16532200\n",
    "16814400\n",
    "17065100\n",
    "17284000\n",
    "17495000\n",
    "17667000\n",
    "17855000\n",
    "18072000\n",
    "18311000\n",
    "18517000\n",
    "18711000\n",
    "18926000\n",
    "19153000\n",
    "19413000\n",
    "19651400\n",
    "19895400\n",
    "20127400\n",
    "20394800\n",
    "20697900\n",
    "20827600\n",
    "21249200\n",
    "21691700\n",
    "22031750\n",
    "22340024\n",
    "22733465\n",
    "23128129\n",
    "23475686\n",
    "23815995\n",
    "24190907\n",
    "24601860\n",
    "24982688\n",
    "25365745\n",
    "25687041\n",
    "14\n",
    "AUT\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "7047539\n",
    "7086299\n",
    "7129864\n",
    "7175811\n",
    "7223801\n",
    "7270889\n",
    "7322066\n",
    "7376998\n",
    "7415403\n",
    "7441055\n",
    "7467086\n",
    "7500482\n",
    "7544201\n",
    "7586115\n",
    "7599038\n",
    "7578903\n",
    "7565525\n",
    "7568430\n",
    "7562305\n",
    "7549425\n",
    "7549433\n",
    "7568710\n",
    "7574140\n",
    "7561910\n",
    "7561434\n",
    "7564985\n",
    "7569794\n",
    "7574586\n",
    "7585317\n",
    "7619567\n",
    "7677850\n",
    "7754891\n",
    "7840709\n",
    "7905633\n",
    "7936118\n",
    "7948278\n",
    "7959017\n",
    "7968041\n",
    "7976789\n",
    "7992324\n",
    "8011566\n",
    "8042293\n",
    "8081957\n",
    "8121423\n",
    "8171966\n",
    "8227829\n",
    "8268641\n",
    "8295487\n",
    "8321496\n",
    "8343323\n",
    "8363404\n",
    "8391643\n",
    "8429991\n",
    "8479823\n",
    "8546356\n",
    "8642699\n",
    "8736668\n",
    "8797566\n",
    "8840521\n",
    "8879920\n",
    "8917205\n",
    "15\n",
    "AZE\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "3895398\n",
    "4030325\n",
    "4171428\n",
    "4315127\n",
    "4456691\n",
    "4592601\n",
    "4721528\n",
    "4843872\n",
    "4960237\n",
    "5071930\n",
    "5180032\n",
    "5284518\n",
    "5385266\n",
    "5483088\n",
    "5579071\n",
    "5674129\n",
    "5768730\n",
    "5863138\n",
    "5957927\n",
    "6053635\n",
    "6150735\n",
    "6249312\n",
    "6349555\n",
    "6452067\n",
    "6557581\n",
    "6666447\n",
    "6778631\n",
    "6893486\n",
    "7010027\n",
    "7126877\n",
    "7159000\n",
    "7271000\n",
    "7382000\n",
    "7495000\n",
    "7597000\n",
    "7685000\n",
    "7763000\n",
    "7838250\n",
    "7913000\n",
    "7982750\n",
    "8048600\n",
    "8111200\n",
    "8171950\n",
    "8234100\n",
    "8306500\n",
    "8391850\n",
    "8484550\n",
    "8581300\n",
    "8763400\n",
    "8947243\n",
    "9054332\n",
    "9173082\n",
    "9295784\n",
    "9416801\n",
    "9535079\n",
    "9649341\n",
    "9757812\n",
    "9854033\n",
    "9939771\n",
    "10024283\n",
    "10110116\n",
    "16\n",
    "BDI\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "2797925\n",
    "2852438\n",
    "2907320\n",
    "2964416\n",
    "3026292\n",
    "3094378\n",
    "3170496\n",
    "3253215\n",
    "3336930\n",
    "3413909\n",
    "3479070\n",
    "3530000\n",
    "3569655\n",
    "3605120\n",
    "3646428\n",
    "3700879\n",
    "3770870\n",
    "3854446\n",
    "3949264\n",
    "4051239\n",
    "4157296\n",
    "4266520\n",
    "4379727\n",
    "4497544\n",
    "4621096\n",
    "4750832\n",
    "4886745\n",
    "5027143\n",
    "5168703\n",
    "5307069\n",
    "5438959\n",
    "5564923\n",
    "5685569\n",
    "5798054\n",
    "5898964\n",
    "5987044\n",
    "6060110\n",
    "6122130\n",
    "6185564\n",
    "6267132\n",
    "6378871\n",
    "6525546\n",
    "6704118\n",
    "6909161\n",
    "7131688\n",
    "7364857\n",
    "7607850\n",
    "7862226\n",
    "8126104\n",
    "8397661\n",
    "8675606\n",
    "8958406\n",
    "9245992\n",
    "9540302\n",
    "9844301\n",
    "10160034\n",
    "10488002\n",
    "10827010\n",
    "11175379\n",
    "11530577\n",
    "11890781\n",
    "17\n",
    "BEL\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "9153489\n",
    "9183948\n",
    "9220578\n",
    "9289770\n",
    "9378113\n",
    "9463667\n",
    "9527807\n",
    "9580991\n",
    "9618756\n",
    "9646032\n",
    "9655549\n",
    "9673162\n",
    "9711115\n",
    "9741720\n",
    "9772419\n",
    "9800700\n",
    "9818227\n",
    "9830358\n",
    "9839534\n",
    "9848382\n",
    "9859242\n",
    "9858982\n",
    "9856303\n",
    "9855520\n",
    "9855372\n",
    "9858308\n",
    "9861823\n",
    "9870234\n",
    "9901664\n",
    "9937697\n",
    "9967379\n",
    "10004486\n",
    "10045158\n",
    "10084475\n",
    "10115603\n",
    "10136811\n",
    "10156637\n",
    "10181245\n",
    "10203008\n",
    "10226419\n",
    "10251250\n",
    "10286570\n",
    "10332785\n",
    "10376133\n",
    "10421137\n",
    "10478617\n",
    "10547958\n",
    "10625700\n",
    "10709973\n",
    "10796493\n",
    "10895586\n",
    "11038264\n",
    "11106932\n",
    "11159407\n",
    "11209057\n",
    "11274196\n",
    "11331422\n",
    "11375158\n",
    "11427054\n",
    "11488980\n",
    "11555997\n",
    "18\n",
    "BEN\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "2431617\n",
    "2465865\n",
    "2502897\n",
    "2542864\n",
    "2585961\n",
    "2632361\n",
    "2682159\n",
    "2735308\n",
    "2791588\n",
    "2850657\n",
    "2912338\n",
    "2976575\n",
    "3043563\n",
    "3113681\n",
    "3187413\n",
    "3265167\n",
    "3347169\n",
    "3433445\n",
    "3523933\n",
    "3618519\n",
    "3717161\n",
    "3820126\n",
    "3927717\n",
    "4039940\n",
    "4156819\n",
    "4278502\n",
    "4404504\n",
    "4535262\n",
    "4672844\n",
    "4820020\n",
    "4978489\n",
    "5149496\n",
    "5331805\n",
    "5521761\n",
    "5714215\n",
    "5905552\n",
    "6094272\n",
    "6281644\n",
    "6470275\n",
    "6664102\n",
    "6865946\n",
    "7076728\n",
    "7295400\n",
    "7520556\n",
    "7750003\n",
    "7982223\n",
    "8216893\n",
    "8454790\n",
    "8696915\n",
    "8944713\n",
    "9199254\n",
    "9460829\n",
    "9729254\n",
    "10004594\n",
    "10286839\n",
    "10575962\n",
    "10872072\n",
    "11175192\n",
    "11485035\n",
    "11801151\n",
    "12123198\n",
    "19\n",
    "BFA\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "4829289\n",
    "4894580\n",
    "4960328\n",
    "5027811\n",
    "5098891\n",
    "5174874\n",
    "5256360\n",
    "5343025\n",
    "5434046\n",
    "5528172\n",
    "5624592\n",
    "5723378\n",
    "5825174\n",
    "5930493\n",
    "6040045\n",
    "6154554\n",
    "6274032\n",
    "6398933\n",
    "6530820\n",
    "6671656\n",
    "6822837\n",
    "6985166\n",
    "7158259\n",
    "7340910\n",
    "7531239\n",
    "7727908\n",
    "7930689\n",
    "8140080\n",
    "8356313\n",
    "8579818\n",
    "8811033\n",
    "9050086\n",
    "9297110\n",
    "9552473\n",
    "9816584\n",
    "10089880\n",
    "10372734\n",
    "10665552\n",
    "10968722\n",
    "11282696\n",
    "11607951\n",
    "11944589\n",
    "12293097\n",
    "12654624\n",
    "13030576\n",
    "13421935\n",
    "13829173\n",
    "14252029\n",
    "14689725\n",
    "15141098\n",
    "15605211\n",
    "16081915\n",
    "16571252\n",
    "17072791\n",
    "17586029\n",
    "18110616\n",
    "18646350\n",
    "19193236\n",
    "19751466\n",
    "20321383\n",
    "20903278\n",
    "20\n",
    "BGD\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "48013505\n",
    "49362834\n",
    "50752150\n",
    "52202008\n",
    "53741721\n",
    "55385114\n",
    "57157651\n",
    "59034250\n",
    "60918452\n",
    "62679765\n",
    "64232486\n",
    "65531635\n",
    "66625706\n",
    "67637541\n",
    "68742222\n",
    "70066310\n",
    "71652386\n",
    "73463593\n",
    "75450033\n",
    "77529040\n",
    "79639498\n",
    "81767516\n",
    "83932132\n",
    "86142490\n",
    "88416529\n",
    "90764180\n",
    "93187593\n",
    "95671159\n",
    "98186350\n",
    "100695496\n",
    "103171957\n",
    "105599125\n",
    "107983708\n",
    "110350641\n",
    "112737684\n",
    "115169933\n",
    "117649927\n",
    "120160571\n",
    "122682818\n",
    "125189655\n",
    "127657862\n",
    "130088709\n",
    "132478077\n",
    "134791598\n",
    "136986429\n",
    "139035505\n",
    "140921154\n",
    "142660381\n",
    "144304164\n",
    "145924795\n",
    "147575433\n",
    "149273134\n",
    "151005733\n",
    "152761413\n",
    "154517385\n",
    "156256287\n",
    "157977151\n",
    "159685421\n",
    "161376713\n",
    "163046173\n",
    "164689383\n",
    "21\n",
    "BGR\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "7867374\n",
    "7943118\n",
    "8012946\n",
    "8078145\n",
    "8144340\n",
    "8204168\n",
    "8258057\n",
    "8310226\n",
    "8369603\n",
    "8434172\n",
    "8489574\n",
    "8536395\n",
    "8576200\n",
    "8620967\n",
    "8678745\n",
    "8720742\n",
    "8758599\n",
    "8804183\n",
    "8814032\n",
    "8825940\n",
    "8861535\n",
    "8891117\n",
    "8917457\n",
    "8939738\n",
    "8960679\n",
    "8960547\n",
    "8958171\n",
    "8971359\n",
    "8981446\n",
    "8876972\n",
    "8718289\n",
    "8632367\n",
    "8540164\n",
    "8472313\n",
    "8443591\n",
    "8406067\n",
    "8362826\n",
    "8312068\n",
    "8256786\n",
    "8210624\n",
    "8170172\n",
    "8009142\n",
    "7837161\n",
    "7775327\n",
    "7716860\n",
    "7658972\n",
    "7601022\n",
    "7545338\n",
    "7492561\n",
    "7444443\n",
    "7395599\n",
    "7348328\n",
    "7305888\n",
    "7265115\n",
    "7223938\n",
    "7177991\n",
    "7127822\n",
    "7075947\n",
    "7025037\n",
    "6975761\n",
    "6927288\n",
    "22\n",
    "BHR\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "162429\n",
    "167899\n",
    "173140\n",
    "178142\n",
    "182888\n",
    "187432\n",
    "191785\n",
    "196060\n",
    "200652\n",
    "206037\n",
    "212607\n",
    "220311\n",
    "229151\n",
    "239527\n",
    "251908\n",
    "266540\n",
    "283746\n",
    "303169\n",
    "323468\n",
    "342804\n",
    "359897\n",
    "374120\n",
    "385953\n",
    "396451\n",
    "407233\n",
    "419428\n",
    "433478\n",
    "448981\n",
    "465198\n",
    "481081\n",
    "495927\n",
    "509762\n",
    "523082\n",
    "536212\n",
    "549590\n",
    "563698\n",
    "578661\n",
    "594927\n",
    "613697\n",
    "636540\n",
    "664610\n",
    "697550\n",
    "735140\n",
    "778708\n",
    "829846\n",
    "889157\n",
    "958423\n",
    "1035924\n",
    "1114645\n",
    "1185075\n",
    "1240864\n",
    "1278153\n",
    "1299942\n",
    "1315029\n",
    "1336073\n",
    "1371853\n",
    "1425793\n",
    "1494077\n",
    "1569440\n",
    "1641164\n",
    "1701583\n",
    "23\n",
    "BHS\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "109532\n",
    "115119\n",
    "121092\n",
    "127340\n",
    "133705\n",
    "140060\n",
    "146381\n",
    "152621\n",
    "158648\n",
    "164265\n",
    "169376\n",
    "173894\n",
    "177863\n",
    "181519\n",
    "185105\n",
    "188895\n",
    "192903\n",
    "197100\n",
    "201482\n",
    "205991\n",
    "210589\n",
    "215321\n",
    "220182\n",
    "225095\n",
    "229913\n",
    "234579\n",
    "239020\n",
    "243265\n",
    "247451\n",
    "251738\n",
    "256227\n",
    "261007\n",
    "266028\n",
    "271065\n",
    "275849\n",
    "280179\n",
    "283980\n",
    "287363\n",
    "290600\n",
    "294063\n",
    "298045\n",
    "302618\n",
    "307657\n",
    "313123\n",
    "318893\n",
    "324848\n",
    "331032\n",
    "337387\n",
    "343680\n",
    "349600\n",
    "354936\n",
    "359583\n",
    "363581\n",
    "367162\n",
    "370625\n",
    "374200\n",
    "377923\n",
    "381749\n",
    "385635\n",
    "389486\n",
    "393248\n",
    "24\n",
    "BIH\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "3225664\n",
    "3288604\n",
    "3353228\n",
    "3417573\n",
    "3478999\n",
    "3535632\n",
    "3586630\n",
    "3632678\n",
    "3675448\n",
    "3717476\n",
    "3760536\n",
    "3805286\n",
    "3851153\n",
    "3897259\n",
    "3942221\n",
    "3985107\n",
    "4025271\n",
    "4063188\n",
    "4100355\n",
    "4138816\n",
    "4179850\n",
    "4222479\n",
    "4265183\n",
    "4307907\n",
    "4350566\n",
    "4392135\n",
    "4435925\n",
    "4479516\n",
    "4509462\n",
    "4507819\n",
    "4463422\n",
    "4369320\n",
    "4233673\n",
    "4078940\n",
    "3936527\n",
    "3829049\n",
    "3764419\n",
    "3736070\n",
    "3734338\n",
    "3743353\n",
    "3751176\n",
    "3755514\n",
    "3759389\n",
    "3762179\n",
    "3764194\n",
    "3765332\n",
    "3765422\n",
    "3762791\n",
    "3754261\n",
    "3735945\n",
    "3705478\n",
    "3661173\n",
    "3604972\n",
    "3542598\n",
    "3482106\n",
    "3429362\n",
    "3386263\n",
    "3351534\n",
    "3323929\n",
    "3300998\n",
    "3280815\n",
    "25\n",
    "BLR\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "8198000\n",
    "8271216\n",
    "8351928\n",
    "8437232\n",
    "8524224\n",
    "8610000\n",
    "8696496\n",
    "8785648\n",
    "8874552\n",
    "8960304\n",
    "9040000\n",
    "9115576\n",
    "9188968\n",
    "9257272\n",
    "9317584\n",
    "9367000\n",
    "9411000\n",
    "9463000\n",
    "9525000\n",
    "9584000\n",
    "9643000\n",
    "9710000\n",
    "9776000\n",
    "9843000\n",
    "9910000\n",
    "9975000\n",
    "10043000\n",
    "10111000\n",
    "10140000\n",
    "10170000\n",
    "10189000\n",
    "10194000\n",
    "10216000\n",
    "10239000\n",
    "10227000\n",
    "10194000\n",
    "10160000\n",
    "10117000\n",
    "10069000\n",
    "10026738\n",
    "9979610\n",
    "9928549\n",
    "9865548\n",
    "9796749\n",
    "9730146\n",
    "9663915\n",
    "9604924\n",
    "9560953\n",
    "9527985\n",
    "9506765\n",
    "9490583\n",
    "9473172\n",
    "9464495\n",
    "9465997\n",
    "9474511\n",
    "9489616\n",
    "9501534\n",
    "9498264\n",
    "9483499\n",
    "9417849\n",
    "9398861\n",
    "26\n",
    "BLZ\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "92068\n",
    "94700\n",
    "97392\n",
    "100165\n",
    "103069\n",
    "106120\n",
    "109348\n",
    "112707\n",
    "116065\n",
    "119269\n",
    "122184\n",
    "124795\n",
    "127152\n",
    "129294\n",
    "131305\n",
    "133264\n",
    "135143\n",
    "136990\n",
    "138975\n",
    "141302\n",
    "144142\n",
    "147572\n",
    "151486\n",
    "155820\n",
    "160341\n",
    "164918\n",
    "169569\n",
    "174333\n",
    "179023\n",
    "183471\n",
    "187554\n",
    "191136\n",
    "194324\n",
    "197625\n",
    "201679\n",
    "206962\n",
    "213660\n",
    "221575\n",
    "230248\n",
    "238979\n",
    "247310\n",
    "255068\n",
    "262387\n",
    "269428\n",
    "276516\n",
    "283798\n",
    "291338\n",
    "299031\n",
    "306822\n",
    "314655\n",
    "322465\n",
    "330236\n",
    "338001\n",
    "345707\n",
    "353366\n",
    "360926\n",
    "368399\n",
    "375775\n",
    "383071\n",
    "390351\n",
    "397621\n",
    "27\n",
    "BMU\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "44400\n",
    "45500\n",
    "46600\n",
    "47700\n",
    "48900\n",
    "50100\n",
    "51000\n",
    "52000\n",
    "53000\n",
    "54000\n",
    "55000\n",
    "54600\n",
    "54200\n",
    "53800\n",
    "53400\n",
    "53000\n",
    "53200\n",
    "53400\n",
    "53600\n",
    "53800\n",
    "54670\n",
    "55050\n",
    "55449\n",
    "55930\n",
    "56423\n",
    "56898\n",
    "57382\n",
    "57849\n",
    "58347\n",
    "58841\n",
    "59326\n",
    "59021\n",
    "58595\n",
    "58910\n",
    "59320\n",
    "59746\n",
    "60129\n",
    "60497\n",
    "60943\n",
    "61285\n",
    "61833\n",
    "62504\n",
    "62912\n",
    "63325\n",
    "63740\n",
    "64154\n",
    "64523\n",
    "64888\n",
    "65273\n",
    "65636\n",
    "65124\n",
    "64564\n",
    "64798\n",
    "65001\n",
    "65138\n",
    "65237\n",
    "64554\n",
    "63873\n",
    "63919\n",
    "63913\n",
    "63903\n",
    "28\n",
    "BOL\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "3656961\n",
    "3728954\n",
    "3802996\n",
    "3879191\n",
    "3957759\n",
    "4038865\n",
    "4122517\n",
    "4208683\n",
    "4297522\n",
    "4389248\n",
    "4484004\n",
    "4581752\n",
    "4682392\n",
    "4785916\n",
    "4892294\n",
    "5001413\n",
    "5113458\n",
    "5228253\n",
    "5344946\n",
    "5462413\n",
    "5579932\n",
    "5697098\n",
    "5814344\n",
    "5932812\n",
    "6054126\n",
    "6179460\n",
    "6309129\n",
    "6442824\n",
    "6580318\n",
    "6721117\n",
    "6864839\n",
    "7011456\n",
    "7160917\n",
    "7312857\n",
    "7466792\n",
    "7622334\n",
    "7779268\n",
    "7937453\n",
    "8096761\n",
    "8257066\n",
    "8418270\n",
    "8580244\n",
    "8742822\n",
    "8905820\n",
    "9069044\n",
    "9232301\n",
    "9395449\n",
    "9558438\n",
    "9721457\n",
    "9884790\n",
    "10048597\n",
    "10212951\n",
    "10377677\n",
    "10542375\n",
    "10706517\n",
    "10869732\n",
    "11031822\n",
    "11192853\n",
    "11353140\n",
    "11513102\n",
    "11673029\n",
    "29\n",
    "BRA\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "72179235\n",
    "74311338\n",
    "76514329\n",
    "78772647\n",
    "81064572\n",
    "83373533\n",
    "85696502\n",
    "88035815\n",
    "90387079\n",
    "92746607\n",
    "95113265\n",
    "97482928\n",
    "99859388\n",
    "102259497\n",
    "104706193\n",
    "107216209\n",
    "109790943\n",
    "112425392\n",
    "115121158\n",
    "117878412\n",
    "120694012\n",
    "123570327\n",
    "126498322\n",
    "129448815\n",
    "132383569\n",
    "135274083\n",
    "138108915\n",
    "140891606\n",
    "143627505\n",
    "146328305\n",
    "149003225\n",
    "151648007\n",
    "154259382\n",
    "156849086\n",
    "159432717\n",
    "162019889\n",
    "164614682\n",
    "167209046\n",
    "169785253\n",
    "172318674\n",
    "174790339\n",
    "177196051\n",
    "179537523\n",
    "181809244\n",
    "184006479\n",
    "186127108\n",
    "188167353\n",
    "190130445\n",
    "192030362\n",
    "193886505\n",
    "195713637\n",
    "197514541\n",
    "199287292\n",
    "201035904\n",
    "202763744\n",
    "204471759\n",
    "206163056\n",
    "207833825\n",
    "209469320\n",
    "211049519\n",
    "212559409\n",
    "30\n",
    "BRB\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "230985\n",
    "231718\n",
    "232623\n",
    "233632\n",
    "234588\n",
    "235415\n",
    "236084\n",
    "236661\n",
    "237241\n",
    "237963\n",
    "238895\n",
    "240093\n",
    "241523\n",
    "243076\n",
    "244643\n",
    "246158\n",
    "247584\n",
    "248931\n",
    "250200\n",
    "251347\n",
    "252388\n",
    "253296\n",
    "254078\n",
    "254791\n",
    "255493\n",
    "256260\n",
    "257117\n",
    "258012\n",
    "258970\n",
    "259961\n",
    "260933\n",
    "261912\n",
    "262890\n",
    "263869\n",
    "264893\n",
    "265955\n",
    "267047\n",
    "268183\n",
    "269334\n",
    "270455\n",
    "271511\n",
    "272494\n",
    "273423\n",
    "274331\n",
    "275283\n",
    "276320\n",
    "277475\n",
    "278701\n",
    "279946\n",
    "281107\n",
    "282131\n",
    "282987\n",
    "283698\n",
    "284294\n",
    "284825\n",
    "285327\n",
    "285798\n",
    "286229\n",
    "286640\n",
    "287021\n",
    "287371\n",
    "31\n",
    "BRN\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "81707\n",
    "85560\n",
    "89484\n",
    "93540\n",
    "97819\n",
    "102390\n",
    "107274\n",
    "112446\n",
    "117897\n",
    "123596\n",
    "129530\n",
    "135672\n",
    "142015\n",
    "148516\n",
    "155069\n",
    "161635\n",
    "168173\n",
    "174717\n",
    "181201\n",
    "187596\n",
    "193880\n",
    "200027\n",
    "206064\n",
    "212073\n",
    "218176\n",
    "224440\n",
    "230917\n",
    "237565\n",
    "244405\n",
    "251456\n",
    "258714\n",
    "266208\n",
    "273888\n",
    "281684\n",
    "289452\n",
    "297112\n",
    "304620\n",
    "311962\n",
    "319135\n",
    "326214\n",
    "333166\n",
    "340037\n",
    "346777\n",
    "353295\n",
    "359434\n",
    "365112\n",
    "370262\n",
    "374967\n",
    "379418\n",
    "383902\n",
    "388634\n",
    "393687\n",
    "398997\n",
    "404414\n",
    "409778\n",
    "414914\n",
    "419791\n",
    "424481\n",
    "428960\n",
    "433296\n",
    "437483\n",
    "32\n",
    "BTN\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "223284\n",
    "228849\n",
    "234552\n",
    "240529\n",
    "246961\n",
    "253993\n",
    "261664\n",
    "269944\n",
    "278731\n",
    "287886\n",
    "297307\n",
    "306957\n",
    "316822\n",
    "326986\n",
    "337491\n",
    "348395\n",
    "359721\n",
    "371424\n",
    "383324\n",
    "395192\n",
    "406883\n",
    "418107\n",
    "428938\n",
    "439823\n",
    "451471\n",
    "464264\n",
    "478683\n",
    "494313\n",
    "509532\n",
    "522173\n",
    "530801\n",
    "534637\n",
    "534525\n",
    "532590\n",
    "531905\n",
    "534629\n",
    "541471\n",
    "551713\n",
    "564378\n",
    "577886\n",
    "591014\n",
    "603643\n",
    "616025\n",
    "627840\n",
    "638809\n",
    "648744\n",
    "657404\n",
    "664873\n",
    "671611\n",
    "678329\n",
    "685502\n",
    "693297\n",
    "701582\n",
    "710235\n",
    "719053\n",
    "727885\n",
    "736706\n",
    "745563\n",
    "754396\n",
    "763094\n",
    "771612\n",
    "33\n",
    "BWA\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "502733\n",
    "512688\n",
    "523777\n",
    "535692\n",
    "547870\n",
    "559996\n",
    "571957\n",
    "584098\n",
    "596946\n",
    "611297\n",
    "627714\n",
    "646350\n",
    "667096\n",
    "689906\n",
    "714701\n",
    "741346\n",
    "769982\n",
    "800532\n",
    "832467\n",
    "865073\n",
    "897860\n",
    "930412\n",
    "962859\n",
    "996124\n",
    "1031439\n",
    "1069585\n",
    "1110948\n",
    "1154904\n",
    "1200073\n",
    "1244484\n",
    "1286756\n",
    "1326321\n",
    "1363541\n",
    "1399110\n",
    "1434061\n",
    "1469173\n",
    "1504724\n",
    "1540424\n",
    "1575827\n",
    "1610260\n",
    "1643333\n",
    "1674674\n",
    "1704637\n",
    "1734387\n",
    "1765533\n",
    "1799077\n",
    "1835911\n",
    "1875458\n",
    "1915636\n",
    "1953495\n",
    "1987106\n",
    "2015406\n",
    "2039551\n",
    "2062551\n",
    "2088619\n",
    "2120716\n",
    "2159925\n",
    "2205076\n",
    "2254067\n",
    "2303703\n",
    "2351625\n",
    "34\n",
    "CAF\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "1501668\n",
    "1526057\n",
    "1551908\n",
    "1579375\n",
    "1608618\n",
    "1639706\n",
    "1673019\n",
    "1708306\n",
    "1744198\n",
    "1778870\n",
    "1811157\n",
    "1840517\n",
    "1867786\n",
    "1894850\n",
    "1924386\n",
    "1958367\n",
    "1997017\n",
    "2039914\n",
    "2087662\n",
    "2140778\n",
    "2199359\n",
    "2264441\n",
    "2335339\n",
    "2408322\n",
    "2478382\n",
    "2542170\n",
    "2597765\n",
    "2646836\n",
    "2693974\n",
    "2745735\n",
    "2806740\n",
    "2878507\n",
    "2959236\n",
    "3046148\n",
    "3135017\n",
    "3222662\n",
    "3308235\n",
    "3392432\n",
    "3475485\n",
    "3558019\n",
    "3640421\n",
    "3722016\n",
    "3802129\n",
    "3881185\n",
    "3959883\n",
    "4038380\n",
    "4118075\n",
    "4198004\n",
    "4273368\n",
    "4337623\n",
    "4386765\n",
    "4418639\n",
    "4436411\n",
    "4447945\n",
    "4464171\n",
    "4493171\n",
    "4537683\n",
    "4596023\n",
    "4666375\n",
    "4745179\n",
    "4829764\n",
    "35\n",
    "CAN\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "17909009\n",
    "18271000\n",
    "18614000\n",
    "18964000\n",
    "19325000\n",
    "19678000\n",
    "20048000\n",
    "20412000\n",
    "20744000\n",
    "21028000\n",
    "21324000\n",
    "21962032\n",
    "22218463\n",
    "22491777\n",
    "22807969\n",
    "23143275\n",
    "23449808\n",
    "23725843\n",
    "23963203\n",
    "24201544\n",
    "24515667\n",
    "24819915\n",
    "25116942\n",
    "25366451\n",
    "25607053\n",
    "25842116\n",
    "26100278\n",
    "26446601\n",
    "26791747\n",
    "27276781\n",
    "27691138\n",
    "28037420\n",
    "28371264\n",
    "28684764\n",
    "29000663\n",
    "29302311\n",
    "29610218\n",
    "29905948\n",
    "30155173\n",
    "30401286\n",
    "30685730\n",
    "31020902\n",
    "31360079\n",
    "31644028\n",
    "31940655\n",
    "32243753\n",
    "32571174\n",
    "32889025\n",
    "33247118\n",
    "33628895\n",
    "34004889\n",
    "34339328\n",
    "34714222\n",
    "35082954\n",
    "35437435\n",
    "35702908\n",
    "36109487\n",
    "36545295\n",
    "37065178\n",
    "37593384\n",
    "38005238\n",
    "36\n",
    "CEB\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "91401764\n",
    "92232738\n",
    "93009498\n",
    "93840016\n",
    "94715795\n",
    "95440988\n",
    "96146336\n",
    "97043270\n",
    "97884022\n",
    "98606630\n",
    "99134548\n",
    "99635258\n",
    "100357161\n",
    "101112680\n",
    "101939916\n",
    "102860571\n",
    "103776068\n",
    "104616884\n",
    "105329397\n",
    "105948616\n",
    "106541316\n",
    "107129392\n",
    "107730380\n",
    "108297837\n",
    "108838073\n",
    "109338285\n",
    "109824166\n",
    "110296425\n",
    "110686740\n",
    "110801640\n",
    "110743128\n",
    "110469467\n",
    "110111454\n",
    "110041924\n",
    "110021594\n",
    "109864246\n",
    "109626194\n",
    "109422013\n",
    "109238340\n",
    "109060951\n",
    "108447824\n",
    "107660041\n",
    "106959751\n",
    "106624167\n",
    "106331716\n",
    "106041911\n",
    "105772481\n",
    "105378748\n",
    "105001883\n",
    "104800475\n",
    "104421447\n",
    "104174038\n",
    "103935318\n",
    "103713726\n",
    "103496179\n",
    "103257886\n",
    "102994278\n",
    "102740078\n",
    "102538451\n",
    "102398537\n",
    "102246330\n",
    "37\n",
    "CHE\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "5327827\n",
    "5434294\n",
    "5573815\n",
    "5694247\n",
    "5789228\n",
    "5856472\n",
    "5918002\n",
    "5991785\n",
    "6067714\n",
    "6136387\n",
    "6180877\n",
    "6213399\n",
    "6260956\n",
    "6307347\n",
    "6341405\n",
    "6338632\n",
    "6302504\n",
    "6281174\n",
    "6281738\n",
    "6294365\n",
    "6319408\n",
    "6354074\n",
    "6391309\n",
    "6418773\n",
    "6441865\n",
    "6470365\n",
    "6504124\n",
    "6545106\n",
    "6593386\n",
    "6646912\n",
    "6715519\n",
    "6799978\n",
    "6875364\n",
    "6938265\n",
    "6993795\n",
    "7040687\n",
    "7071850\n",
    "7088906\n",
    "7110001\n",
    "7143991\n",
    "7184250\n",
    "7229854\n",
    "7284753\n",
    "7339001\n",
    "7389625\n",
    "7437115\n",
    "7483934\n",
    "7551117\n",
    "7647675\n",
    "7743831\n",
    "7824909\n",
    "7912398\n",
    "7996861\n",
    "8089346\n",
    "8188649\n",
    "8282396\n",
    "8373338\n",
    "8451840\n",
    "8514329\n",
    "8575280\n",
    "8636896\n",
    "38\n",
    "CHI\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "109419\n",
    "110398\n",
    "111464\n",
    "112591\n",
    "113777\n",
    "114989\n",
    "116229\n",
    "117469\n",
    "118725\n",
    "119975\n",
    "121200\n",
    "122405\n",
    "123611\n",
    "124722\n",
    "125680\n",
    "126420\n",
    "126908\n",
    "127186\n",
    "127395\n",
    "127691\n",
    "128204\n",
    "128986\n",
    "129976\n",
    "131153\n",
    "132445\n",
    "133800\n",
    "135242\n",
    "136762\n",
    "138250\n",
    "139580\n",
    "140677\n",
    "141469\n",
    "142006\n",
    "142407\n",
    "142867\n",
    "143480\n",
    "144348\n",
    "145387\n",
    "146500\n",
    "147553\n",
    "148439\n",
    "149097\n",
    "149591\n",
    "150070\n",
    "150722\n",
    "151674\n",
    "152999\n",
    "154644\n",
    "156441\n",
    "158187\n",
    "159718\n",
    "160990\n",
    "162051\n",
    "163039\n",
    "164107\n",
    "165387\n",
    "166922\n",
    "168666\n",
    "170496\n",
    "172264\n",
    "173859\n",
    "39\n",
    "CHL\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "8132988\n",
    "8303804\n",
    "8476895\n",
    "8650390\n",
    "8821855\n",
    "8989607\n",
    "9152848\n",
    "9312091\n",
    "9468851\n",
    "9625304\n",
    "9783134\n",
    "9942716\n",
    "10103675\n",
    "10265827\n",
    "10428803\n",
    "10592310\n",
    "10756876\n",
    "10922777\n",
    "11089165\n",
    "11254877\n",
    "11419350\n",
    "11582020\n",
    "11743909\n",
    "11907955\n",
    "12078137\n",
    "12257238\n",
    "12445833\n",
    "12642917\n",
    "12847712\n",
    "13058758\n",
    "13274617\n",
    "13495255\n",
    "13719818\n",
    "13944934\n",
    "14166346\n",
    "14380864\n",
    "14587367\n",
    "14786227\n",
    "14977736\n",
    "15162801\n",
    "15342350\n",
    "15516112\n",
    "15684413\n",
    "15849649\n",
    "16014972\n",
    "16182713\n",
    "16354507\n",
    "16530201\n",
    "16708255\n",
    "16886184\n",
    "17062531\n",
    "17233584\n",
    "17400359\n",
    "17571511\n",
    "17758969\n",
    "17969356\n",
    "18209072\n",
    "18470435\n",
    "18729166\n",
    "18952035\n",
    "19116209\n",
    "40\n",
    "CHN\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "667070000\n",
    "660330000\n",
    "665770000\n",
    "682335000\n",
    "698355000\n",
    "715185000\n",
    "735400000\n",
    "754550000\n",
    "774510000\n",
    "796025000\n",
    "818315000\n",
    "841105000\n",
    "862030000\n",
    "881940000\n",
    "900350000\n",
    "916395000\n",
    "930685000\n",
    "943455000\n",
    "956165000\n",
    "969005000\n",
    "981235000\n",
    "993885000\n",
    "1008630000\n",
    "1023310000\n",
    "1036825000\n",
    "1051040000\n",
    "1066790000\n",
    "1084035000\n",
    "1101630000\n",
    "1118650000\n",
    "1135185000\n",
    "1150780000\n",
    "1164970000\n",
    "1178440000\n",
    "1191835000\n",
    "1204855000\n",
    "1217550000\n",
    "1230075000\n",
    "1241935000\n",
    "1252735000\n",
    "1262645000\n",
    "1271850000\n",
    "1280400000\n",
    "1288400000\n",
    "1296075000\n",
    "1303720000\n",
    "1311020000\n",
    "1317885000\n",
    "1324655000\n",
    "1331260000\n",
    "1337705000\n",
    "1344130000\n",
    "1350695000\n",
    "1357380000\n",
    "1364270000\n",
    "1371220000\n",
    "1378665000\n",
    "1386395000\n",
    "1392730000\n",
    "1397715000\n",
    "1402112000\n",
    "41\n",
    "CIV\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "3503559\n",
    "3631547\n",
    "3770756\n",
    "3918630\n",
    "4071411\n",
    "4226843\n",
    "4383723\n",
    "4544168\n",
    "4713134\n",
    "4897470\n",
    "5102070\n",
    "5328735\n",
    "5576026\n",
    "5841513\n",
    "6121295\n",
    "6412409\n",
    "6713949\n",
    "7026497\n",
    "7350269\n",
    "7685927\n",
    "8033652\n",
    "8393689\n",
    "8764991\n",
    "9144940\n",
    "9530103\n",
    "9918204\n",
    "10307268\n",
    "10698188\n",
    "11094740\n",
    "11502453\n",
    "11924873\n",
    "12362404\n",
    "12812428\n",
    "13271638\n",
    "13735438\n",
    "14199759\n",
    "14665125\n",
    "15130674\n",
    "15589407\n",
    "16032573\n",
    "16454660\n",
    "16853027\n",
    "17231539\n",
    "17599613\n",
    "17970493\n",
    "18354513\n",
    "18754914\n",
    "19171250\n",
    "19605568\n",
    "20059147\n",
    "20532944\n",
    "21028652\n",
    "21547188\n",
    "22087506\n",
    "22647672\n",
    "23226148\n",
    "23822726\n",
    "24437475\n",
    "25069226\n",
    "25716554\n",
    "26378275\n",
    "42\n",
    "CMR\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "5176920\n",
    "5285015\n",
    "5398730\n",
    "5518104\n",
    "5643039\n",
    "5773538\n",
    "5909874\n",
    "6052419\n",
    "6201410\n",
    "6357096\n",
    "6519754\n",
    "6689659\n",
    "6867170\n",
    "7052847\n",
    "7247284\n",
    "7451057\n",
    "7664398\n",
    "7887571\n",
    "8121081\n",
    "8365560\n",
    "8621409\n",
    "8888534\n",
    "9166813\n",
    "9456496\n",
    "9757849\n",
    "10070806\n",
    "10395481\n",
    "10731058\n",
    "11075423\n",
    "11425807\n",
    "11780086\n",
    "12137912\n",
    "12499499\n",
    "12864091\n",
    "13230978\n",
    "13599984\n",
    "13970812\n",
    "14344444\n",
    "14723772\n",
    "15112598\n",
    "15513944\n",
    "15928910\n",
    "16357605\n",
    "16800869\n",
    "17259322\n",
    "17733408\n",
    "18223677\n",
    "18730283\n",
    "19252674\n",
    "19789922\n",
    "20341236\n",
    "20906392\n",
    "21485267\n",
    "22077300\n",
    "22681853\n",
    "23298376\n",
    "23926549\n",
    "24566070\n",
    "25216261\n",
    "25876387\n",
    "26545864\n",
    "43\n",
    "COD\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "15248256\n",
    "15637700\n",
    "16041187\n",
    "16461828\n",
    "16903830\n",
    "17369882\n",
    "17862052\n",
    "18378620\n",
    "18913874\n",
    "19459818\n",
    "20011033\n",
    "20564062\n",
    "21121360\n",
    "21690448\n",
    "22282127\n",
    "22903587\n",
    "23560470\n",
    "24249127\n",
    "24956387\n",
    "25663598\n",
    "26358905\n",
    "27040329\n",
    "27717293\n",
    "28403858\n",
    "29119663\n",
    "29881222\n",
    "30683877\n",
    "31528702\n",
    "32443784\n",
    "33464767\n",
    "34612023\n",
    "35908240\n",
    "37333917\n",
    "38815835\n",
    "40252973\n",
    "41576239\n",
    "42757239\n",
    "43827191\n",
    "44849968\n",
    "45919615\n",
    "47105830\n",
    "48428534\n",
    "49871670\n",
    "51425583\n",
    "53068869\n",
    "54785894\n",
    "56578046\n",
    "58453687\n",
    "60411195\n",
    "62448572\n",
    "64563853\n",
    "66755151\n",
    "69020749\n",
    "71358804\n",
    "73767445\n",
    "76244532\n",
    "78789130\n",
    "81398765\n",
    "84068092\n",
    "86790568\n",
    "89561404\n",
    "44\n",
    "COG\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "1018254\n",
    "1043119\n",
    "1069236\n",
    "1096638\n",
    "1125354\n",
    "1155389\n",
    "1186782\n",
    "1219547\n",
    "1253761\n",
    "1289519\n",
    "1326894\n",
    "1365891\n",
    "1406510\n",
    "1448632\n",
    "1492055\n",
    "1536658\n",
    "1582361\n",
    "1629218\n",
    "1677326\n",
    "1726865\n",
    "1777932\n",
    "1830629\n",
    "1884873\n",
    "1940454\n",
    "1996994\n",
    "2054308\n",
    "2112359\n",
    "2171319\n",
    "2231462\n",
    "2293161\n",
    "2356740\n",
    "2422312\n",
    "2489945\n",
    "2559880\n",
    "2632345\n",
    "2707532\n",
    "2785815\n",
    "2867283\n",
    "2951651\n",
    "3038432\n",
    "3127420\n",
    "3217930\n",
    "3310376\n",
    "3406915\n",
    "3510468\n",
    "3622775\n",
    "3745143\n",
    "3876123\n",
    "4011487\n",
    "4145400\n",
    "4273738\n",
    "4394842\n",
    "4510197\n",
    "4622757\n",
    "4736965\n",
    "4856093\n",
    "4980996\n",
    "5110701\n",
    "5244363\n",
    "5380504\n",
    "5518092\n",
    "45\n",
    "COL\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "16057714\n",
    "16567817\n",
    "17092919\n",
    "17629978\n",
    "18175187\n",
    "18725242\n",
    "19279734\n",
    "19837508\n",
    "20393704\n",
    "20942453\n",
    "21480064\n",
    "22003983\n",
    "22516429\n",
    "23024512\n",
    "23538390\n",
    "24065502\n",
    "24608102\n",
    "25164544\n",
    "25733669\n",
    "26312996\n",
    "26900508\n",
    "27496608\n",
    "28101824\n",
    "28714183\n",
    "29331230\n",
    "29951194\n",
    "30572479\n",
    "31195417\n",
    "31822527\n",
    "32457497\n",
    "33102569\n",
    "33758328\n",
    "34422568\n",
    "35091272\n",
    "35758978\n",
    "36421438\n",
    "37076387\n",
    "37723803\n",
    "38364307\n",
    "38999468\n",
    "39629965\n",
    "40255956\n",
    "40875363\n",
    "41483872\n",
    "42075953\n",
    "42647731\n",
    "43200901\n",
    "43737512\n",
    "44254972\n",
    "44750054\n",
    "45222699\n",
    "45662747\n",
    "46075721\n",
    "46495492\n",
    "46967706\n",
    "47520667\n",
    "48175048\n",
    "48909844\n",
    "49661056\n",
    "50339443\n",
    "50882884\n",
    "46\n",
    "COM\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "191122\n",
    "194149\n",
    "197202\n",
    "200371\n",
    "203762\n",
    "207429\n",
    "211477\n",
    "215900\n",
    "220574\n",
    "225324\n",
    "230055\n",
    "234645\n",
    "239229\n",
    "244207\n",
    "250107\n",
    "257285\n",
    "265958\n",
    "275903\n",
    "286628\n",
    "297451\n",
    "307831\n",
    "317617\n",
    "326944\n",
    "336088\n",
    "345455\n",
    "355337\n",
    "365765\n",
    "376647\n",
    "387964\n",
    "399638\n",
    "411598\n",
    "423873\n",
    "436454\n",
    "449270\n",
    "462280\n",
    "475394\n",
    "488625\n",
    "501955\n",
    "515382\n",
    "528853\n",
    "542358\n",
    "555895\n",
    "569480\n",
    "583213\n",
    "597230\n",
    "611625\n",
    "626427\n",
    "641624\n",
    "657227\n",
    "673251\n",
    "689696\n",
    "706578\n",
    "723865\n",
    "741511\n",
    "759390\n",
    "777435\n",
    "795597\n",
    "813890\n",
    "832322\n",
    "850891\n",
    "869595\n",
    "47\n",
    "CPV\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "201770\n",
    "205321\n",
    "210141\n",
    "216087\n",
    "222949\n",
    "230421\n",
    "238655\n",
    "247522\n",
    "256169\n",
    "263461\n",
    "268633\n",
    "271315\n",
    "271841\n",
    "271068\n",
    "270228\n",
    "270240\n",
    "271345\n",
    "273335\n",
    "276182\n",
    "279729\n",
    "283848\n",
    "288678\n",
    "294244\n",
    "300226\n",
    "306140\n",
    "311668\n",
    "316613\n",
    "321137\n",
    "325744\n",
    "331180\n",
    "337953\n",
    "346229\n",
    "355763\n",
    "366057\n",
    "376409\n",
    "386288\n",
    "395533\n",
    "404248\n",
    "412513\n",
    "420456\n",
    "428178\n",
    "435701\n",
    "442955\n",
    "449925\n",
    "456619\n",
    "463034\n",
    "469171\n",
    "475067\n",
    "480846\n",
    "486667\n",
    "492644\n",
    "498858\n",
    "505241\n",
    "511740\n",
    "518276\n",
    "524740\n",
    "531140\n",
    "537499\n",
    "543764\n",
    "549936\n",
    "555988\n",
    "48\n",
    "CRI\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "1330787\n",
    "1381187\n",
    "1433346\n",
    "1486555\n",
    "1539942\n",
    "1592834\n",
    "1645076\n",
    "1696742\n",
    "1747690\n",
    "1797891\n",
    "1847394\n",
    "1896074\n",
    "1944170\n",
    "1992522\n",
    "2042242\n",
    "2094186\n",
    "2148681\n",
    "2205618\n",
    "2264942\n",
    "2326460\n",
    "2389973\n",
    "2455593\n",
    "2523358\n",
    "2593015\n",
    "2664220\n",
    "2736719\n",
    "2810245\n",
    "2884856\n",
    "2960932\n",
    "3039015\n",
    "3119436\n",
    "3202083\n",
    "3286525\n",
    "3372298\n",
    "3458829\n",
    "3545524\n",
    "3632361\n",
    "3718952\n",
    "3803893\n",
    "3885428\n",
    "3962369\n",
    "4034074\n",
    "4100922\n",
    "4164053\n",
    "4225156\n",
    "4285504\n",
    "4345421\n",
    "4404626\n",
    "4463123\n",
    "4520739\n",
    "4577371\n",
    "4633086\n",
    "4688003\n",
    "4742111\n",
    "4795390\n",
    "4847805\n",
    "4899336\n",
    "4949955\n",
    "4999443\n",
    "5047561\n",
    "5094114\n",
    "49\n",
    "CSS\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "4194711\n",
    "4274052\n",
    "4353623\n",
    "4432240\n",
    "4508189\n",
    "4580382\n",
    "4648353\n",
    "4712556\n",
    "4773890\n",
    "4833839\n",
    "4893444\n",
    "4953088\n",
    "5012600\n",
    "5071912\n",
    "5130797\n",
    "5189174\n",
    "5246547\n",
    "5303289\n",
    "5360562\n",
    "5419911\n",
    "5482191\n",
    "5548521\n",
    "5617902\n",
    "5687113\n",
    "5751508\n",
    "5808206\n",
    "5855445\n",
    "5894820\n",
    "5930153\n",
    "5967028\n",
    "6009262\n",
    "6058334\n",
    "6112784\n",
    "6170324\n",
    "6227332\n",
    "6281204\n",
    "6331277\n",
    "6378551\n",
    "6423851\n",
    "6468461\n",
    "6513486\n",
    "6559097\n",
    "6604966\n",
    "6650981\n",
    "6696985\n",
    "6742657\n",
    "6788122\n",
    "6833394\n",
    "6878973\n",
    "6925446\n",
    "6973193\n",
    "7022367\n",
    "7072640\n",
    "7123315\n",
    "7173443\n",
    "7222197\n",
    "7269385\n",
    "7314956\n",
    "7358929\n",
    "7401389\n",
    "7442291\n",
    "50\n",
    "CUB\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "7141241\n",
    "7291201\n",
    "7453535\n",
    "7623300\n",
    "7793258\n",
    "7958171\n",
    "8115487\n",
    "8266373\n",
    "8413549\n",
    "8561391\n",
    "8712535\n",
    "8868087\n",
    "9025347\n",
    "9178809\n",
    "9320943\n",
    "9446441\n",
    "9554186\n",
    "9646142\n",
    "9724044\n",
    "9790850\n",
    "9849457\n",
    "9898891\n",
    "9940314\n",
    "9981303\n",
    "10031651\n",
    "10097910\n",
    "10183894\n",
    "10286645\n",
    "10397519\n",
    "10503967\n",
    "10596986\n",
    "10673534\n",
    "10736386\n",
    "10789312\n",
    "10838461\n",
    "10888246\n",
    "10939285\n",
    "10989730\n",
    "11038706\n",
    "11084673\n",
    "11126423\n",
    "11164676\n",
    "11199664\n",
    "11229185\n",
    "11250369\n",
    "11261586\n",
    "11261241\n",
    "11251117\n",
    "11236975\n",
    "11226711\n",
    "11225833\n",
    "11236671\n",
    "11257112\n",
    "11282722\n",
    "11306909\n",
    "11324777\n",
    "11335108\n",
    "11339255\n",
    "11338146\n",
    "11333484\n",
    "11326616\n",
    "51\n",
    "CUW\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "124826\n",
    "126125\n",
    "128414\n",
    "130860\n",
    "133148\n",
    "135266\n",
    "136682\n",
    "138140\n",
    "140298\n",
    "142581\n",
    "144739\n",
    "147389\n",
    "147710\n",
    "146912\n",
    "148351\n",
    "149129\n",
    "149399\n",
    "149459\n",
    "148341\n",
    "147851\n",
    "148041\n",
    "148629\n",
    "150101\n",
    "151159\n",
    "151940\n",
    "152711\n",
    "152662\n",
    "151456\n",
    "149254\n",
    "146937\n",
    "145400\n",
    "144403\n",
    "143912\n",
    "144299\n",
    "144630\n",
    "145139\n",
    "146306\n",
    "146956\n",
    "144472\n",
    "139428\n",
    "133860\n",
    "129047\n",
    "129205\n",
    "131897\n",
    "134192\n",
    "137658\n",
    "141239\n",
    "144056\n",
    "145880\n",
    "146833\n",
    "148703\n",
    "150831\n",
    "152088\n",
    "153822\n",
    "155909\n",
    "157980\n",
    "159664\n",
    "160175\n",
    "159336\n",
    "157441\n",
    "155014\n",
    "52\n",
    "CYM\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "7870\n",
    "8024\n",
    "8141\n",
    "8219\n",
    "8299\n",
    "8370\n",
    "8444\n",
    "8518\n",
    "8638\n",
    "8833\n",
    "9143\n",
    "9588\n",
    "10135\n",
    "10778\n",
    "11488\n",
    "12234\n",
    "13023\n",
    "13848\n",
    "14681\n",
    "15485\n",
    "16207\n",
    "16857\n",
    "17436\n",
    "18005\n",
    "18662\n",
    "19460\n",
    "20426\n",
    "21539\n",
    "22775\n",
    "24028\n",
    "25307\n",
    "26540\n",
    "27784\n",
    "29068\n",
    "30521\n",
    "32161\n",
    "34065\n",
    "36155\n",
    "38330\n",
    "40423\n",
    "42305\n",
    "43934\n",
    "45347\n",
    "46624\n",
    "47899\n",
    "49261\n",
    "50729\n",
    "52280\n",
    "53835\n",
    "55321\n",
    "56672\n",
    "57877\n",
    "58963\n",
    "59933\n",
    "60848\n",
    "61721\n",
    "62564\n",
    "63382\n",
    "64172\n",
    "64948\n",
    "65720\n",
    "53\n",
    "CYP\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "572933\n",
    "576402\n",
    "577696\n",
    "577918\n",
    "578628\n",
    "580972\n",
    "585313\n",
    "591301\n",
    "598495\n",
    "606121\n",
    "613618\n",
    "620860\n",
    "628002\n",
    "635105\n",
    "642322\n",
    "649753\n",
    "657524\n",
    "665521\n",
    "673253\n",
    "680010\n",
    "685408\n",
    "689170\n",
    "691716\n",
    "694076\n",
    "697715\n",
    "703694\n",
    "712335\n",
    "723384\n",
    "736476\n",
    "751043\n",
    "766616\n",
    "783121\n",
    "800610\n",
    "818750\n",
    "837104\n",
    "855391\n",
    "873426\n",
    "891190\n",
    "908710\n",
    "926049\n",
    "943288\n",
    "960274\n",
    "976968\n",
    "993562\n",
    "1010410\n",
    "1027657\n",
    "1045508\n",
    "1063708\n",
    "1081568\n",
    "1098089\n",
    "1112617\n",
    "1124837\n",
    "1135046\n",
    "1143866\n",
    "1152297\n",
    "1160987\n",
    "1170189\n",
    "1179685\n",
    "1189262\n",
    "1198574\n",
    "1207361\n",
    "54\n",
    "CZE\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "9602006\n",
    "9586651\n",
    "9624660\n",
    "9670685\n",
    "9727804\n",
    "9779358\n",
    "9821040\n",
    "9852899\n",
    "9876346\n",
    "9896580\n",
    "9858071\n",
    "9826815\n",
    "9867632\n",
    "9922266\n",
    "9988459\n",
    "10058620\n",
    "10125939\n",
    "10186755\n",
    "10242098\n",
    "10292341\n",
    "10304193\n",
    "10300591\n",
    "10314826\n",
    "10323856\n",
    "10330213\n",
    "10337118\n",
    "10342227\n",
    "10347318\n",
    "10355276\n",
    "10361068\n",
    "10333355\n",
    "10308578\n",
    "10319123\n",
    "10329855\n",
    "10333587\n",
    "10327253\n",
    "10315241\n",
    "10304131\n",
    "10294373\n",
    "10283860\n",
    "10255063\n",
    "10216605\n",
    "10196916\n",
    "10193998\n",
    "10197101\n",
    "10211216\n",
    "10238905\n",
    "10298828\n",
    "10384603\n",
    "10443936\n",
    "10474410\n",
    "10496088\n",
    "10510785\n",
    "10514272\n",
    "10525347\n",
    "10546059\n",
    "10566332\n",
    "10594438\n",
    "10629928\n",
    "10671870\n",
    "10698896\n",
    "55\n",
    "DEU\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "72814900\n",
    "73377632\n",
    "74025784\n",
    "74714353\n",
    "75318337\n",
    "75963695\n",
    "76600311\n",
    "76951336\n",
    "77294314\n",
    "77909682\n",
    "78169289\n",
    "78312842\n",
    "78688452\n",
    "78936666\n",
    "78967433\n",
    "78673554\n",
    "78336950\n",
    "78159814\n",
    "78091820\n",
    "78126350\n",
    "78288576\n",
    "78407907\n",
    "78333366\n",
    "78128282\n",
    "77858685\n",
    "77684873\n",
    "77720436\n",
    "77839920\n",
    "78144619\n",
    "78751283\n",
    "79433029\n",
    "80013896\n",
    "80624598\n",
    "81156363\n",
    "81438348\n",
    "81678051\n",
    "81914831\n",
    "82034771\n",
    "82047195\n",
    "82100243\n",
    "82211508\n",
    "82349925\n",
    "82488495\n",
    "82534176\n",
    "82516260\n",
    "82469422\n",
    "82376451\n",
    "82266372\n",
    "82110097\n",
    "81902307\n",
    "81776930\n",
    "80274983\n",
    "80425823\n",
    "80645605\n",
    "80982500\n",
    "81686611\n",
    "82348669\n",
    "82657002\n",
    "82905782\n",
    "83092962\n",
    "83240525\n",
    "56\n",
    "DJI\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "83634\n",
    "88503\n",
    "94203\n",
    "100618\n",
    "107582\n",
    "114976\n",
    "122876\n",
    "131405\n",
    "140462\n",
    "149889\n",
    "159662\n",
    "169372\n",
    "179237\n",
    "190569\n",
    "205180\n",
    "224177\n",
    "248556\n",
    "277472\n",
    "308008\n",
    "336080\n",
    "358960\n",
    "374934\n",
    "385268\n",
    "393800\n",
    "406018\n",
    "425608\n",
    "454359\n",
    "490337\n",
    "528993\n",
    "563855\n",
    "590393\n",
    "606843\n",
    "615050\n",
    "618504\n",
    "622364\n",
    "630385\n",
    "643649\n",
    "660858\n",
    "680465\n",
    "699973\n",
    "717577\n",
    "733019\n",
    "746947\n",
    "759639\n",
    "771599\n",
    "783248\n",
    "794554\n",
    "805456\n",
    "816361\n",
    "827820\n",
    "840194\n",
    "853671\n",
    "868136\n",
    "883296\n",
    "898707\n",
    "913998\n",
    "929117\n",
    "944100\n",
    "958923\n",
    "973557\n",
    "988002\n",
    "57\n",
    "DMA\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "60020\n",
    "61036\n",
    "61978\n",
    "62917\n",
    "63921\n",
    "65044\n",
    "66300\n",
    "67687\n",
    "69034\n",
    "70214\n",
    "71084\n",
    "71569\n",
    "71731\n",
    "71739\n",
    "71814\n",
    "72088\n",
    "72662\n",
    "73443\n",
    "74280\n",
    "74960\n",
    "75318\n",
    "75314\n",
    "75012\n",
    "74484\n",
    "73846\n",
    "73207\n",
    "72531\n",
    "71842\n",
    "71212\n",
    "70722\n",
    "70422\n",
    "70377\n",
    "70546\n",
    "70818\n",
    "71044\n",
    "71105\n",
    "70933\n",
    "70592\n",
    "70183\n",
    "69828\n",
    "69650\n",
    "69671\n",
    "69840\n",
    "70102\n",
    "70387\n",
    "70580\n",
    "70718\n",
    "70797\n",
    "70829\n",
    "70848\n",
    "70877\n",
    "70912\n",
    "70954\n",
    "71019\n",
    "71091\n",
    "71175\n",
    "71307\n",
    "71460\n",
    "71626\n",
    "71808\n",
    "71991\n",
    "58\n",
    "DNK\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "4579603\n",
    "4611687\n",
    "4647727\n",
    "4684483\n",
    "4722072\n",
    "4759012\n",
    "4797381\n",
    "4835354\n",
    "4864883\n",
    "4891860\n",
    "4928757\n",
    "4963126\n",
    "4991596\n",
    "5021861\n",
    "5045297\n",
    "5059862\n",
    "5072596\n",
    "5088419\n",
    "5104248\n",
    "5116801\n",
    "5123027\n",
    "5121572\n",
    "5117810\n",
    "5114297\n",
    "5111619\n",
    "5113691\n",
    "5120534\n",
    "5127024\n",
    "5129516\n",
    "5132594\n",
    "5140939\n",
    "5154298\n",
    "5171370\n",
    "5188628\n",
    "5206180\n",
    "5233373\n",
    "5263074\n",
    "5284991\n",
    "5304219\n",
    "5321799\n",
    "5339616\n",
    "5358783\n",
    "5375931\n",
    "5390574\n",
    "5404523\n",
    "5419432\n",
    "5437272\n",
    "5461438\n",
    "5493621\n",
    "5523095\n",
    "5547683\n",
    "5570572\n",
    "5591572\n",
    "5614932\n",
    "5643475\n",
    "5683483\n",
    "5728010\n",
    "5764980\n",
    "5793636\n",
    "5814422\n",
    "5831404\n",
    "59\n",
    "DOM\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "3294222\n",
    "3406282\n",
    "3521018\n",
    "3638110\n",
    "3757123\n",
    "3877768\n",
    "3999796\n",
    "4123100\n",
    "4247559\n",
    "4373127\n",
    "4499722\n",
    "4627202\n",
    "4755464\n",
    "4884460\n",
    "5014187\n",
    "5144632\n",
    "5275767\n",
    "5407496\n",
    "5539596\n",
    "5671801\n",
    "5803929\n",
    "5935895\n",
    "6067769\n",
    "6199657\n",
    "6331760\n",
    "6464229\n",
    "6596967\n",
    "6729930\n",
    "6863438\n",
    "6997877\n",
    "7133491\n",
    "7270413\n",
    "7408339\n",
    "7546467\n",
    "7683707\n",
    "7819239\n",
    "7952766\n",
    "8084407\n",
    "8214427\n",
    "8343288\n",
    "8471317\n",
    "8598599\n",
    "8724974\n",
    "8850317\n",
    "8974444\n",
    "9097262\n",
    "9218681\n",
    "9338856\n",
    "9458079\n",
    "9576736\n",
    "9695117\n",
    "9813219\n",
    "9930916\n",
    "10048226\n",
    "10165182\n",
    "10281675\n",
    "10397738\n",
    "10513111\n",
    "10627147\n",
    "10738957\n",
    "10847904\n",
    "60\n",
    "DZA\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "11057864\n",
    "11336336\n",
    "11619828\n",
    "11912800\n",
    "12221675\n",
    "12550880\n",
    "12902626\n",
    "13275020\n",
    "13663581\n",
    "14061724\n",
    "14464992\n",
    "14872253\n",
    "15285992\n",
    "15709831\n",
    "16149018\n",
    "16607706\n",
    "17085799\n",
    "17582899\n",
    "18102266\n",
    "18647801\n",
    "19221659\n",
    "19824297\n",
    "20452901\n",
    "21101875\n",
    "21763578\n",
    "22431507\n",
    "23102386\n",
    "23774287\n",
    "24443472\n",
    "25106192\n",
    "25758872\n",
    "26400468\n",
    "27028330\n",
    "27635517\n",
    "28213777\n",
    "28757788\n",
    "29266415\n",
    "29742980\n",
    "30192750\n",
    "30623406\n",
    "31042238\n",
    "31451513\n",
    "31855110\n",
    "32264159\n",
    "32692153\n",
    "33149720\n",
    "33641007\n",
    "34166976\n",
    "34730604\n",
    "35333882\n",
    "35977451\n",
    "36661438\n",
    "37383899\n",
    "38140135\n",
    "38923688\n",
    "39728020\n",
    "40551398\n",
    "41389174\n",
    "42228415\n",
    "43053054\n",
    "43851043\n",
    "61\n",
    "EAP\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "894875757\n",
    "894479503\n",
    "906413903\n",
    "929634719\n",
    "952494226\n",
    "976360559\n",
    "1003799539\n",
    "1030341030\n",
    "1057848841\n",
    "1087054621\n",
    "1117162921\n",
    "1147907376\n",
    "1176909425\n",
    "1204956207\n",
    "1231481379\n",
    "1255568332\n",
    "1277786882\n",
    "1298403379\n",
    "1318985502\n",
    "1339874993\n",
    "1360437112\n",
    "1381732250\n",
    "1405388781\n",
    "1429179263\n",
    "1451901724\n",
    "1475340043\n",
    "1500310495\n",
    "1526774417\n",
    "1553548236\n",
    "1579666262\n",
    "1605187660\n",
    "1629631622\n",
    "1652528279\n",
    "1674580221\n",
    "1696464118\n",
    "1717901663\n",
    "1738949771\n",
    "1759750554\n",
    "1779788454\n",
    "1798639494\n",
    "1816455800\n",
    "1833422980\n",
    "1849607527\n",
    "1865132738\n",
    "1880248912\n",
    "1895275475\n",
    "1909891482\n",
    "1924018171\n",
    "1938057262\n",
    "1952017377\n",
    "1965954326\n",
    "1980035204\n",
    "1994389913\n",
    "2008919318\n",
    "2023595907\n",
    "2038190024\n",
    "2053107304\n",
    "2068150870\n",
    "2081650327\n",
    "2093675075\n",
    "2105003391\n",
    "62\n",
    "EAR\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "980003345\n",
    "1003194972\n",
    "1027204700\n",
    "1051972834\n",
    "1077531237\n",
    "1103859981\n",
    "1130947721\n",
    "1158861175\n",
    "1187474694\n",
    "1216839339\n",
    "1246947673\n",
    "1277799456\n",
    "1309397210\n",
    "1341779503\n",
    "1374824941\n",
    "1408534717\n",
    "1442862522\n",
    "1477869964\n",
    "1513819992\n",
    "1551093148\n",
    "1589897378\n",
    "1630293423\n",
    "1672134835\n",
    "1715134126\n",
    "1758863284\n",
    "1803032659\n",
    "1847526576\n",
    "1892336352\n",
    "1937278919\n",
    "1982160339\n",
    "2028993568\n",
    "2073591177\n",
    "2117785781\n",
    "2161761687\n",
    "2205758867\n",
    "2249993925\n",
    "2294576653\n",
    "2339379359\n",
    "2384127937\n",
    "2428606122\n",
    "2472852816\n",
    "2516662185\n",
    "2560101448\n",
    "2603300762\n",
    "2646483605\n",
    "2689792006\n",
    "2733278857\n",
    "2776922505\n",
    "2820636607\n",
    "2864298814\n",
    "2907916947\n",
    "2951423688\n",
    "2994346938\n",
    "3037084158\n",
    "3079685581\n",
    "3122156833\n",
    "3164439749\n",
    "3206485623\n",
    "3248413593\n",
    "3290291029\n",
    "3332105361\n",
    "63\n",
    "EAS\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "1041673567\n",
    "1043656626\n",
    "1058119847\n",
    "1083884354\n",
    "1109291332\n",
    "1135760430\n",
    "1165660993\n",
    "1194423916\n",
    "1224481196\n",
    "1256501888\n",
    "1289442011\n",
    "1324084663\n",
    "1355934840\n",
    "1386816705\n",
    "1416257332\n",
    "1442992143\n",
    "1467572978\n",
    "1490458433\n",
    "1513242365\n",
    "1536454977\n",
    "1559219642\n",
    "1582802675\n",
    "1608709544\n",
    "1634587947\n",
    "1659196213\n",
    "1684377898\n",
    "1710979840\n",
    "1739049562\n",
    "1767394678\n",
    "1795112149\n",
    "1822134188\n",
    "1848265154\n",
    "1872676768\n",
    "1896190507\n",
    "1919576721\n",
    "1942464423\n",
    "1965146473\n",
    "1987315524\n",
    "2008705487\n",
    "2028650228\n",
    "2047640091\n",
    "2065912033\n",
    "2083186086\n",
    "2099602481\n",
    "2115458625\n",
    "2131146847\n",
    "2146744105\n",
    "2161785511\n",
    "2177119138\n",
    "2192059400\n",
    "2206884622\n",
    "2221672723\n",
    "2236817711\n",
    "2252044491\n",
    "2267485985\n",
    "2282863725\n",
    "2298500738\n",
    "2314112848\n",
    "2328193462\n",
    "2340673749\n",
    "2352037717\n",
    "64\n",
    "ECA\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "274647228\n",
    "278931566\n",
    "283238943\n",
    "287564681\n",
    "291878870\n",
    "296127062\n",
    "299713085\n",
    "303366987\n",
    "306934517\n",
    "310433036\n",
    "313941308\n",
    "317445710\n",
    "320945150\n",
    "324422297\n",
    "327916784\n",
    "331431777\n",
    "335100864\n",
    "338710487\n",
    "342280107\n",
    "345827999\n",
    "349436199\n",
    "353122612\n",
    "356678273\n",
    "360204222\n",
    "363975241\n",
    "367739486\n",
    "371479741\n",
    "375227253\n",
    "378837514\n",
    "382076660\n",
    "384377493\n",
    "386437795\n",
    "388125580\n",
    "389516909\n",
    "390276284\n",
    "390572206\n",
    "390932358\n",
    "391289640\n",
    "391526555\n",
    "391495413\n",
    "391586634\n",
    "391315286\n",
    "391008840\n",
    "391172281\n",
    "391535458\n",
    "391952218\n",
    "392511901\n",
    "393215573\n",
    "394392601\n",
    "396042179\n",
    "398082642\n",
    "400311983\n",
    "402565848\n",
    "405069376\n",
    "407567142\n",
    "410111294\n",
    "412578888\n",
    "414892969\n",
    "416931527\n",
    "418760880\n",
    "420249459\n",
    "65\n",
    "ECS\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "666753356\n",
    "674450666\n",
    "682397828\n",
    "690411692\n",
    "698355574\n",
    "706070460\n",
    "712830312\n",
    "719406020\n",
    "725725300\n",
    "731903738\n",
    "737524825\n",
    "743140589\n",
    "749283635\n",
    "755259488\n",
    "761029417\n",
    "766625562\n",
    "772129341\n",
    "777422111\n",
    "782653852\n",
    "787910910\n",
    "793299463\n",
    "798559755\n",
    "803342746\n",
    "807903880\n",
    "812662217\n",
    "817497806\n",
    "822470047\n",
    "827505281\n",
    "832553390\n",
    "837438555\n",
    "841520451\n",
    "845399308\n",
    "848877909\n",
    "851974036\n",
    "854082582\n",
    "855491301\n",
    "856857233\n",
    "858165406\n",
    "859298084\n",
    "860319121\n",
    "861278544\n",
    "862347956\n",
    "863936837\n",
    "866215697\n",
    "868808153\n",
    "871456632\n",
    "874157191\n",
    "877300670\n",
    "880932055\n",
    "884469396\n",
    "887926793\n",
    "890162077\n",
    "893786694\n",
    "898057341\n",
    "902404181\n",
    "906707413\n",
    "910910900\n",
    "914655081\n",
    "918082509\n",
    "920809471\n",
    "923489941\n",
    "66\n",
    "ECU\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "4543658\n",
    "4674176\n",
    "4809194\n",
    "4948991\n",
    "5093848\n",
    "5243980\n",
    "5399423\n",
    "5560013\n",
    "5725462\n",
    "5895368\n",
    "6069375\n",
    "6247421\n",
    "6429377\n",
    "6614840\n",
    "6803271\n",
    "6994331\n",
    "7187794\n",
    "7383713\n",
    "7582357\n",
    "7784074\n",
    "7989178\n",
    "8197648\n",
    "8409396\n",
    "8624594\n",
    "8843463\n",
    "9066100\n",
    "9292481\n",
    "9522362\n",
    "9755576\n",
    "9991870\n",
    "10230931\n",
    "10472476\n",
    "10716131\n",
    "10961460\n",
    "11207999\n",
    "11455205\n",
    "11703169\n",
    "11951457\n",
    "12198449\n",
    "12442109\n",
    "12681123\n",
    "12914660\n",
    "13143465\n",
    "13369678\n",
    "13596390\n",
    "13825839\n",
    "14059379\n",
    "14296554\n",
    "14535740\n",
    "14774412\n",
    "15011114\n",
    "15243885\n",
    "15474099\n",
    "15707473\n",
    "15951832\n",
    "16212022\n",
    "16491116\n",
    "16785356\n",
    "17084359\n",
    "17373657\n",
    "17643060\n",
    "67\n",
    "EGY\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "26632891\n",
    "27366239\n",
    "28112258\n",
    "28871383\n",
    "29644875\n",
    "30433024\n",
    "31237597\n",
    "32056506\n",
    "32881852\n",
    "33703127\n",
    "34513851\n",
    "35311905\n",
    "36102670\n",
    "36896551\n",
    "37708144\n",
    "38549133\n",
    "39422731\n",
    "40329589\n",
    "41275728\n",
    "42267429\n",
    "43309063\n",
    "44400113\n",
    "45539295\n",
    "46728284\n",
    "47968643\n",
    "49258726\n",
    "50602360\n",
    "51991696\n",
    "53399243\n",
    "54788680\n",
    "56134478\n",
    "57424552\n",
    "58666812\n",
    "59880656\n",
    "61095804\n",
    "62334025\n",
    "63601632\n",
    "64892269\n",
    "66200259\n",
    "67515591\n",
    "68831561\n",
    "70152662\n",
    "71485044\n",
    "72826102\n",
    "74172073\n",
    "75523576\n",
    "76873670\n",
    "78232124\n",
    "79636081\n",
    "81134789\n",
    "82761244\n",
    "84529251\n",
    "86422240\n",
    "88404652\n",
    "90424668\n",
    "92442549\n",
    "94447071\n",
    "96442590\n",
    "98423602\n",
    "100388076\n",
    "102334403\n",
    "68\n",
    "EMU\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "265203956\n",
    "267621091\n",
    "270110063\n",
    "272655396\n",
    "275163380\n",
    "277650954\n",
    "279969050\n",
    "281974870\n",
    "283866408\n",
    "285778639\n",
    "287338854\n",
    "288923399\n",
    "290874612\n",
    "292728242\n",
    "294399217\n",
    "295923529\n",
    "297251532\n",
    "298441848\n",
    "299652841\n",
    "300886487\n",
    "302184216\n",
    "303343114\n",
    "304174261\n",
    "304787497\n",
    "305299144\n",
    "305877743\n",
    "306640636\n",
    "307488377\n",
    "308516476\n",
    "309837575\n",
    "311262608\n",
    "312708132\n",
    "314162056\n",
    "315449093\n",
    "316366781\n",
    "317181461\n",
    "318003013\n",
    "318761752\n",
    "319433985\n",
    "320258891\n",
    "321310789\n",
    "322547874\n",
    "324125339\n",
    "325885957\n",
    "327682513\n",
    "329380417\n",
    "330922785\n",
    "332645162\n",
    "334274725\n",
    "335360891\n",
    "336151482\n",
    "335419656\n",
    "336159199\n",
    "337302111\n",
    "338462234\n",
    "339488382\n",
    "340481755\n",
    "341217243\n",
    "341979171\n",
    "342282081\n",
    "342949773\n",
    "69\n",
    "ERI\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "1007586\n",
    "1033320\n",
    "1060489\n",
    "1088859\n",
    "1118152\n",
    "1148188\n",
    "1178875\n",
    "1210304\n",
    "1242633\n",
    "1276122\n",
    "1310947\n",
    "1347180\n",
    "1384789\n",
    "1423749\n",
    "1463986\n",
    "1505438\n",
    "1547978\n",
    "1591622\n",
    "1636771\n",
    "1683932\n",
    "1733423\n",
    "1784557\n",
    "1836825\n",
    "1890556\n",
    "1946299\n",
    "2003942\n",
    "2064803\n",
    "2127421\n",
    "2185607\n",
    "2231144\n",
    "2258649\n",
    "2266356\n",
    "2257593\n",
    "2238631\n",
    "2218436\n",
    "2204227\n",
    "2196467\n",
    "2195192\n",
    "2206439\n",
    "2237412\n",
    "2292413\n",
    "2374721\n",
    "2481059\n",
    "2600972\n",
    "2719809\n",
    "2826653\n",
    "2918209\n",
    "2996540\n",
    "3062782\n",
    "3119920\n",
    "3170437\n",
    "3213969\n",
    "null\n",
    "null\n",
    "null\n",
    "null\n",
    "null\n",
    "null\n",
    "null\n",
    "null\n",
    "null\n",
    "70\n",
    "ESP\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "30455000\n",
    "30739250\n",
    "31023366\n",
    "31296651\n",
    "31609195\n",
    "31954292\n",
    "32283194\n",
    "32682947\n",
    "33113134\n",
    "33441054\n",
    "33814531\n",
    "34224490\n",
    "34604469\n",
    "34988947\n",
    "35373335\n",
    "35757900\n",
    "36137812\n",
    "36511638\n",
    "36864898\n",
    "37191330\n",
    "37491165\n",
    "37758631\n",
    "37986012\n",
    "38171525\n",
    "38330364\n",
    "38469512\n",
    "38584624\n",
    "38684815\n",
    "38766939\n",
    "38827764\n",
    "38867322\n",
    "38966376\n",
    "39157685\n",
    "39361262\n",
    "39549108\n",
    "39724050\n",
    "39889852\n",
    "40057389\n",
    "40223509\n",
    "40386875\n",
    "40567864\n",
    "40850412\n",
    "41431558\n",
    "42187645\n",
    "42921895\n",
    "43653155\n",
    "44397319\n",
    "45226803\n",
    "45954106\n",
    "46362946\n",
    "46576897\n",
    "46742697\n",
    "46773055\n",
    "46620045\n",
    "46480882\n",
    "46444832\n",
    "46484062\n",
    "46593236\n",
    "46797754\n",
    "47133521\n",
    "47351567\n",
    "71\n",
    "EST\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "1211537\n",
    "1225077\n",
    "1241623\n",
    "1258857\n",
    "1277086\n",
    "1294566\n",
    "1308597\n",
    "1318946\n",
    "1331214\n",
    "1345249\n",
    "1360076\n",
    "1376955\n",
    "1392518\n",
    "1405951\n",
    "1418169\n",
    "1429352\n",
    "1439576\n",
    "1450211\n",
    "1460188\n",
    "1468333\n",
    "1477219\n",
    "1487666\n",
    "1498414\n",
    "1508745\n",
    "1518617\n",
    "1528781\n",
    "1540190\n",
    "1552221\n",
    "1561900\n",
    "1568131\n",
    "1569174\n",
    "1561314\n",
    "1533091\n",
    "1494128\n",
    "1462514\n",
    "1436634\n",
    "1415594\n",
    "1399535\n",
    "1386156\n",
    "1390244\n",
    "1396985\n",
    "1388115\n",
    "1379350\n",
    "1370720\n",
    "1362550\n",
    "1354775\n",
    "1346810\n",
    "1340680\n",
    "1337090\n",
    "1334515\n",
    "1331475\n",
    "1327439\n",
    "1322696\n",
    "1317997\n",
    "1314545\n",
    "1315407\n",
    "1315790\n",
    "1317384\n",
    "1321977\n",
    "1326898\n",
    "1331057\n",
    "72\n",
    "ETH\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "22151284\n",
    "22671193\n",
    "23221385\n",
    "23798418\n",
    "24397010\n",
    "25013634\n",
    "25641040\n",
    "26280135\n",
    "26944386\n",
    "27652715\n",
    "28415080\n",
    "29248650\n",
    "30140799\n",
    "31036670\n",
    "31861353\n",
    "32566855\n",
    "33128151\n",
    "33577240\n",
    "33993301\n",
    "34487806\n",
    "35141703\n",
    "35984531\n",
    "36995246\n",
    "38142679\n",
    "39374346\n",
    "40652146\n",
    "41965696\n",
    "43329238\n",
    "44757205\n",
    "46272308\n",
    "47887864\n",
    "49609976\n",
    "51423591\n",
    "53295556\n",
    "55180993\n",
    "57047906\n",
    "58883531\n",
    "60697443\n",
    "62507724\n",
    "64343008\n",
    "66224809\n",
    "68159422\n",
    "70142090\n",
    "72170581\n",
    "74239508\n",
    "76346310\n",
    "78489205\n",
    "80674343\n",
    "82916236\n",
    "85233923\n",
    "87639962\n",
    "90139928\n",
    "92726982\n",
    "95385793\n",
    "98094264\n",
    "100835453\n",
    "103603461\n",
    "106399926\n",
    "109224410\n",
    "112078727\n",
    "114963583\n",
    "73\n",
    "EUU\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "356906098\n",
    "359998408\n",
    "363200480\n",
    "366516509\n",
    "369850237\n",
    "373032729\n",
    "376039129\n",
    "378917964\n",
    "381605442\n",
    "384216977\n",
    "386322917\n",
    "388391948\n",
    "390995000\n",
    "393524745\n",
    "395949381\n",
    "398316579\n",
    "400473497\n",
    "402425493\n",
    "404271789\n",
    "406051882\n",
    "407875838\n",
    "409551911\n",
    "410895608\n",
    "411974424\n",
    "412931321\n",
    "413924499\n",
    "415076367\n",
    "416301843\n",
    "417653046\n",
    "419073139\n",
    "420477999\n",
    "421730520\n",
    "422963890\n",
    "424341124\n",
    "425399126\n",
    "426203356\n",
    "426896861\n",
    "427538048\n",
    "428109867\n",
    "428815486\n",
    "429328622\n",
    "429895628\n",
    "430881947\n",
    "432415932\n",
    "434040244\n",
    "435581949\n",
    "436998045\n",
    "438468397\n",
    "439876674\n",
    "440917801\n",
    "441532415\n",
    "440746989\n",
    "441395937\n",
    "442469469\n",
    "443576675\n",
    "444543761\n",
    "445487730\n",
    "446186344\n",
    "446915113\n",
    "447196538\n",
    "447794691\n",
    "74\n",
    "FCS\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "208280083\n",
    "213082100\n",
    "218087139\n",
    "223284589\n",
    "228670583\n",
    "234243249\n",
    "240005133\n",
    "245962230\n",
    "252116419\n",
    "258473157\n",
    "265037231\n",
    "271800288\n",
    "278767597\n",
    "285961194\n",
    "293417700\n",
    "301154406\n",
    "309196463\n",
    "317520801\n",
    "326050578\n",
    "334679714\n",
    "343334222\n",
    "352006783\n",
    "360736356\n",
    "369569548\n",
    "378570396\n",
    "387802210\n",
    "397235010\n",
    "406892515\n",
    "416947257\n",
    "427617018\n",
    "440937023\n",
    "453319269\n",
    "466482751\n",
    "480128168\n",
    "493815711\n",
    "507247502\n",
    "520303326\n",
    "533112558\n",
    "545728172\n",
    "558627550\n",
    "572303917\n",
    "586760366\n",
    "601848734\n",
    "617468591\n",
    "633460044\n",
    "649704454\n",
    "666174377\n",
    "682901570\n",
    "700030438\n",
    "717553793\n",
    "735448194\n",
    "753858934\n",
    "772742559\n",
    "791943328\n",
    "811249811\n",
    "830536041\n",
    "849739853\n",
    "868954263\n",
    "888488369\n",
    "908739462\n",
    "930006546\n",
    "75\n",
    "FIN\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "4429634\n",
    "4461005\n",
    "4491443\n",
    "4523309\n",
    "4548543\n",
    "4563732\n",
    "4580869\n",
    "4605744\n",
    "4626469\n",
    "4623785\n",
    "4606307\n",
    "4612124\n",
    "4639657\n",
    "4666081\n",
    "4690574\n",
    "4711440\n",
    "4725664\n",
    "4738902\n",
    "4752528\n",
    "4764690\n",
    "4779535\n",
    "4799964\n",
    "4826933\n",
    "4855787\n",
    "4881803\n",
    "4902206\n",
    "4918154\n",
    "4932123\n",
    "4946481\n",
    "4964371\n",
    "4986431\n",
    "5013740\n",
    "5041992\n",
    "5066447\n",
    "5088333\n",
    "5107790\n",
    "5124573\n",
    "5139835\n",
    "5153498\n",
    "5165474\n",
    "5176209\n",
    "5188008\n",
    "5200598\n",
    "5213014\n",
    "5228172\n",
    "5246096\n",
    "5266268\n",
    "5288720\n",
    "5313399\n",
    "5338871\n",
    "5363352\n",
    "5388272\n",
    "5413971\n",
    "5438972\n",
    "5461512\n",
    "5479531\n",
    "5495303\n",
    "5508214\n",
    "5515525\n",
    "5521606\n",
    "5530719\n",
    "76\n",
    "FJI\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "393480\n",
    "407244\n",
    "421673\n",
    "436303\n",
    "450538\n",
    "463960\n",
    "476399\n",
    "487973\n",
    "498944\n",
    "509708\n",
    "520561\n",
    "531622\n",
    "542843\n",
    "554143\n",
    "565418\n",
    "576626\n",
    "587561\n",
    "598299\n",
    "609383\n",
    "621584\n",
    "635312\n",
    "651032\n",
    "668281\n",
    "685497\n",
    "700478\n",
    "711770\n",
    "718642\n",
    "721776\n",
    "722924\n",
    "724602\n",
    "728575\n",
    "735398\n",
    "744470\n",
    "754962\n",
    "765607\n",
    "775428\n",
    "784389\n",
    "792736\n",
    "800148\n",
    "806302\n",
    "811011\n",
    "813923\n",
    "815257\n",
    "816078\n",
    "817864\n",
    "821606\n",
    "827869\n",
    "836185\n",
    "845356\n",
    "853636\n",
    "859816\n",
    "863451\n",
    "865065\n",
    "865602\n",
    "866447\n",
    "868632\n",
    "872406\n",
    "877460\n",
    "883490\n",
    "889955\n",
    "896444\n",
    "77\n",
    "FRA\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "46621688\n",
    "47240526\n",
    "47904879\n",
    "48582624\n",
    "49230585\n",
    "49818019\n",
    "50330268\n",
    "50775788\n",
    "51175507\n",
    "51561833\n",
    "51957747\n",
    "52371320\n",
    "52793151\n",
    "53207744\n",
    "53592235\n",
    "53931390\n",
    "54220033\n",
    "54467709\n",
    "54691866\n",
    "54917110\n",
    "55161510\n",
    "55430278\n",
    "55718940\n",
    "56023775\n",
    "56337675\n",
    "56654703\n",
    "56976126\n",
    "57302642\n",
    "57627106\n",
    "57940199\n",
    "58235716\n",
    "58559309\n",
    "58851216\n",
    "59106758\n",
    "59327200\n",
    "59541904\n",
    "59753095\n",
    "59964841\n",
    "60186284\n",
    "60496708\n",
    "60912500\n",
    "61357432\n",
    "61805266\n",
    "62244880\n",
    "62704901\n",
    "63179356\n",
    "63621376\n",
    "64016227\n",
    "64374979\n",
    "64707035\n",
    "65027505\n",
    "65342789\n",
    "65659814\n",
    "65998685\n",
    "66312067\n",
    "66548272\n",
    "66724104\n",
    "66918020\n",
    "67101930\n",
    "67248926\n",
    "67391582\n",
    "78\n",
    "FRO\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "34624\n",
    "35074\n",
    "35521\n",
    "35965\n",
    "36409\n",
    "36843\n",
    "37283\n",
    "37705\n",
    "38131\n",
    "38560\n",
    "39008\n",
    "39456\n",
    "39915\n",
    "40395\n",
    "40847\n",
    "41292\n",
    "41724\n",
    "42133\n",
    "42539\n",
    "42941\n",
    "43339\n",
    "43725\n",
    "44101\n",
    "44483\n",
    "44895\n",
    "45328\n",
    "45812\n",
    "46340\n",
    "46833\n",
    "47159\n",
    "47276\n",
    "47123\n",
    "46746\n",
    "46277\n",
    "45848\n",
    "45625\n",
    "45624\n",
    "45789\n",
    "46094\n",
    "46428\n",
    "46738\n",
    "46999\n",
    "47235\n",
    "47439\n",
    "47600\n",
    "47731\n",
    "47793\n",
    "47825\n",
    "47822\n",
    "47814\n",
    "47803\n",
    "47815\n",
    "47843\n",
    "47901\n",
    "47965\n",
    "48055\n",
    "48173\n",
    "48326\n",
    "48497\n",
    "48677\n",
    "48865\n",
    "79\n",
    "FSM\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "44510\n",
    "45939\n",
    "47372\n",
    "48856\n",
    "50476\n",
    "52224\n",
    "54192\n",
    "56308\n",
    "58386\n",
    "60146\n",
    "61415\n",
    "62089\n",
    "62275\n",
    "62268\n",
    "62456\n",
    "63122\n",
    "64360\n",
    "66079\n",
    "68196\n",
    "70515\n",
    "72936\n",
    "75422\n",
    "78015\n",
    "80630\n",
    "83192\n",
    "85641\n",
    "87904\n",
    "89985\n",
    "91984\n",
    "94062\n",
    "96304\n",
    "98779\n",
    "101386\n",
    "103901\n",
    "106030\n",
    "107535\n",
    "108311\n",
    "108473\n",
    "108206\n",
    "107769\n",
    "107405\n",
    "107170\n",
    "107027\n",
    "106902\n",
    "106624\n",
    "106135\n",
    "105374\n",
    "104442\n",
    "103549\n",
    "102971\n",
    "102916\n",
    "103448\n",
    "104506\n",
    "105922\n",
    "107444\n",
    "108886\n",
    "110215\n",
    "111461\n",
    "112640\n",
    "113811\n",
    "115021\n",
    "80\n",
    "GAB\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "500922\n",
    "505793\n",
    "511285\n",
    "517573\n",
    "524891\n",
    "533357\n",
    "543119\n",
    "554054\n",
    "565763\n",
    "577644\n",
    "589317\n",
    "600608\n",
    "611705\n",
    "622914\n",
    "634739\n",
    "647538\n",
    "661398\n",
    "676264\n",
    "692078\n",
    "708788\n",
    "726335\n",
    "744695\n",
    "763932\n",
    "784056\n",
    "805117\n",
    "827107\n",
    "850052\n",
    "873871\n",
    "898472\n",
    "923714\n",
    "949493\n",
    "975785\n",
    "1002573\n",
    "1029769\n",
    "1057252\n",
    "1084951\n",
    "1112944\n",
    "1141332\n",
    "1170061\n",
    "1199058\n",
    "1228359\n",
    "1258008\n",
    "1288310\n",
    "1319946\n",
    "1353788\n",
    "1390550\n",
    "1430144\n",
    "1472565\n",
    "1518538\n",
    "1568925\n",
    "1624146\n",
    "1684629\n",
    "1749677\n",
    "1817070\n",
    "1883801\n",
    "1947690\n",
    "2007882\n",
    "2064812\n",
    "2119275\n",
    "2172578\n",
    "2225728\n",
    "81\n",
    "GBR\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "52400000\n",
    "52800000\n",
    "53250000\n",
    "53650000\n",
    "54000000\n",
    "54348050\n",
    "54648500\n",
    "54943600\n",
    "55211700\n",
    "55441750\n",
    "55663250\n",
    "55896223\n",
    "56086065\n",
    "56194527\n",
    "56229974\n",
    "56225800\n",
    "56211968\n",
    "56193492\n",
    "56196504\n",
    "56246951\n",
    "56314216\n",
    "56333829\n",
    "56313641\n",
    "56332848\n",
    "56422072\n",
    "56550268\n",
    "56681396\n",
    "56802050\n",
    "56928327\n",
    "57076711\n",
    "57247586\n",
    "57424897\n",
    "57580402\n",
    "57718614\n",
    "57865745\n",
    "58019030\n",
    "58166950\n",
    "58316954\n",
    "58487141\n",
    "58682466\n",
    "58892514\n",
    "59119673\n",
    "59370479\n",
    "59647577\n",
    "59987905\n",
    "60401206\n",
    "60846820\n",
    "61322463\n",
    "61806995\n",
    "62276270\n",
    "62766365\n",
    "63258810\n",
    "63700215\n",
    "64128273\n",
    "64602298\n",
    "65116219\n",
    "65611593\n",
    "66058859\n",
    "66460344\n",
    "66836327\n",
    "67215293\n",
    "82\n",
    "GEO\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "3645600\n",
    "3703600\n",
    "3760300\n",
    "3816100\n",
    "3870300\n",
    "3921600\n",
    "3966700\n",
    "4005800\n",
    "4042300\n",
    "4080300\n",
    "4119900\n",
    "4163000\n",
    "4205300\n",
    "4242500\n",
    "4279500\n",
    "4311200\n",
    "4342400\n",
    "4372100\n",
    "4397700\n",
    "4430200\n",
    "4467700\n",
    "4504500\n",
    "4542800\n",
    "4582900\n",
    "4622200\n",
    "4662900\n",
    "4704500\n",
    "4743500\n",
    "4790700\n",
    "4803300\n",
    "4802000\n",
    "4835900\n",
    "4873500\n",
    "4911100\n",
    "4836076\n",
    "4657722\n",
    "4491699\n",
    "4349913\n",
    "4243607\n",
    "4157192\n",
    "4077131\n",
    "4014373\n",
    "3978515\n",
    "3951736\n",
    "3927340\n",
    "3902469\n",
    "3880347\n",
    "3860158\n",
    "3848449\n",
    "3814419\n",
    "3786695\n",
    "3756441\n",
    "3728874\n",
    "3717668\n",
    "3719414\n",
    "3725276\n",
    "3727505\n",
    "3728004\n",
    "3726549\n",
    "3720161\n",
    "3714000\n",
    "83\n",
    "GHA\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "6635229\n",
    "6848291\n",
    "7071966\n",
    "7300124\n",
    "7524470\n",
    "7739463\n",
    "7941418\n",
    "8132803\n",
    "8321773\n",
    "8520018\n",
    "8735493\n",
    "8973247\n",
    "9229640\n",
    "9493552\n",
    "9749098\n",
    "9985946\n",
    "10199164\n",
    "10395453\n",
    "10590265\n",
    "10805318\n",
    "11056112\n",
    "11348287\n",
    "11676828\n",
    "12033559\n",
    "12405659\n",
    "12783617\n",
    "13164839\n",
    "13552021\n",
    "13947047\n",
    "14353409\n",
    "14773274\n",
    "15207360\n",
    "15653345\n",
    "16106756\n",
    "16561677\n",
    "17014058\n",
    "17462504\n",
    "17908977\n",
    "18357159\n",
    "18812369\n",
    "19278850\n",
    "19756929\n",
    "20246376\n",
    "20750308\n",
    "21272328\n",
    "21814648\n",
    "22379057\n",
    "22963946\n",
    "23563832\n",
    "24170943\n",
    "24779614\n",
    "25387713\n",
    "25996454\n",
    "26607641\n",
    "27224480\n",
    "27849203\n",
    "28481947\n",
    "29121464\n",
    "29767108\n",
    "30417858\n",
    "31072945\n",
    "84\n",
    "GIB\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "23420\n",
    "23808\n",
    "24307\n",
    "24889\n",
    "25478\n",
    "26079\n",
    "26631\n",
    "27172\n",
    "27693\n",
    "28165\n",
    "28601\n",
    "29007\n",
    "29353\n",
    "29657\n",
    "29886\n",
    "30062\n",
    "30179\n",
    "30228\n",
    "30224\n",
    "30172\n",
    "30068\n",
    "29902\n",
    "29698\n",
    "29481\n",
    "29284\n",
    "29151\n",
    "29096\n",
    "29114\n",
    "29164\n",
    "29187\n",
    "29149\n",
    "29021\n",
    "28844\n",
    "28680\n",
    "28605\n",
    "28688\n",
    "28970\n",
    "29404\n",
    "29945\n",
    "30526\n",
    "31081\n",
    "31604\n",
    "32097\n",
    "32556\n",
    "32930\n",
    "33222\n",
    "33420\n",
    "33524\n",
    "33570\n",
    "33562\n",
    "33585\n",
    "33608\n",
    "33653\n",
    "33694\n",
    "33726\n",
    "33742\n",
    "33738\n",
    "33723\n",
    "33715\n",
    "33706\n",
    "33691\n",
    "85\n",
    "GIN\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "3494164\n",
    "3552068\n",
    "3611428\n",
    "3672560\n",
    "3735912\n",
    "3801711\n",
    "3870204\n",
    "3941046\n",
    "4013050\n",
    "4084601\n",
    "4154606\n",
    "4223056\n",
    "4290544\n",
    "4357357\n",
    "4424030\n",
    "4491248\n",
    "4559239\n",
    "4628881\n",
    "4702371\n",
    "4782491\n",
    "4871446\n",
    "4969595\n",
    "5077073\n",
    "5195443\n",
    "5326369\n",
    "5470716\n",
    "5629420\n",
    "5801323\n",
    "5982484\n",
    "6167480\n",
    "6352282\n",
    "6534936\n",
    "6716032\n",
    "6897171\n",
    "7081119\n",
    "7269631\n",
    "7463782\n",
    "7662071\n",
    "7860772\n",
    "8054745\n",
    "8240735\n",
    "8417082\n",
    "8586077\n",
    "8753097\n",
    "8925729\n",
    "9109585\n",
    "9307421\n",
    "9518159\n",
    "9738796\n",
    "9964470\n",
    "10192168\n",
    "10420459\n",
    "10652032\n",
    "10892821\n",
    "11150970\n",
    "11432096\n",
    "11738434\n",
    "12067516\n",
    "12414292\n",
    "12771246\n",
    "13132792\n",
    "86\n",
    "GMB\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "365049\n",
    "372436\n",
    "379886\n",
    "387635\n",
    "396012\n",
    "405258\n",
    "415478\n",
    "426622\n",
    "438590\n",
    "451228\n",
    "464404\n",
    "478106\n",
    "492424\n",
    "507428\n",
    "523250\n",
    "539985\n",
    "557810\n",
    "576755\n",
    "596540\n",
    "616770\n",
    "637252\n",
    "657581\n",
    "678111\n",
    "700198\n",
    "725688\n",
    "755791\n",
    "791141\n",
    "831011\n",
    "873440\n",
    "915631\n",
    "955595\n",
    "992671\n",
    "1027476\n",
    "1060861\n",
    "1094219\n",
    "1128577\n",
    "1164091\n",
    "1200522\n",
    "1238124\n",
    "1277118\n",
    "1317708\n",
    "1360070\n",
    "1404263\n",
    "1449925\n",
    "1496524\n",
    "1543745\n",
    "1591444\n",
    "1639846\n",
    "1689288\n",
    "1740277\n",
    "1793199\n",
    "1848142\n",
    "1905020\n",
    "1963708\n",
    "2024037\n",
    "2085860\n",
    "2149134\n",
    "2213900\n",
    "2280092\n",
    "2347696\n",
    "2416664\n",
    "87\n",
    "GNB\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "616140\n",
    "622754\n",
    "628877\n",
    "635008\n",
    "641821\n",
    "649795\n",
    "658998\n",
    "669243\n",
    "680431\n",
    "692407\n",
    "704937\n",
    "718359\n",
    "732516\n",
    "746178\n",
    "757668\n",
    "765985\n",
    "770415\n",
    "771732\n",
    "772137\n",
    "774726\n",
    "781678\n",
    "793804\n",
    "810402\n",
    "830212\n",
    "851276\n",
    "872163\n",
    "892530\n",
    "912755\n",
    "933046\n",
    "953782\n",
    "975265\n",
    "997522\n",
    "1020353\n",
    "1043421\n",
    "1066345\n",
    "1088850\n",
    "1110835\n",
    "1132505\n",
    "1154372\n",
    "1177133\n",
    "1201305\n",
    "1227105\n",
    "1254454\n",
    "1283297\n",
    "1313492\n",
    "1344931\n",
    "1377582\n",
    "1411545\n",
    "1446936\n",
    "1483920\n",
    "1522603\n",
    "1562996\n",
    "1604981\n",
    "1648259\n",
    "1692433\n",
    "1737207\n",
    "1782434\n",
    "1828146\n",
    "1874304\n",
    "1920917\n",
    "1967998\n",
    "88\n",
    "GNQ\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "255338\n",
    "258786\n",
    "262219\n",
    "266005\n",
    "270616\n",
    "276296\n",
    "283506\n",
    "291786\n",
    "299416\n",
    "304000\n",
    "303986\n",
    "298852\n",
    "289508\n",
    "277656\n",
    "265762\n",
    "255808\n",
    "247969\n",
    "242160\n",
    "239681\n",
    "241977\n",
    "249931\n",
    "264370\n",
    "284638\n",
    "308208\n",
    "331554\n",
    "352116\n",
    "369024\n",
    "382977\n",
    "394973\n",
    "406620\n",
    "419188\n",
    "432844\n",
    "447269\n",
    "462637\n",
    "479099\n",
    "496768\n",
    "515844\n",
    "536459\n",
    "558496\n",
    "581765\n",
    "606180\n",
    "631662\n",
    "658388\n",
    "686670\n",
    "716949\n",
    "749527\n",
    "784494\n",
    "821686\n",
    "860839\n",
    "901589\n",
    "943640\n",
    "986861\n",
    "1031191\n",
    "1076412\n",
    "1122273\n",
    "1168575\n",
    "1215181\n",
    "1262008\n",
    "1308966\n",
    "1355982\n",
    "1402985\n",
    "89\n",
    "GRC\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "8331725\n",
    "8398050\n",
    "8448233\n",
    "8479625\n",
    "8510429\n",
    "8550333\n",
    "8613651\n",
    "8684088\n",
    "8740765\n",
    "8772764\n",
    "8792806\n",
    "8831036\n",
    "8888628\n",
    "8929086\n",
    "8962022\n",
    "9046541\n",
    "9188150\n",
    "9308479\n",
    "9429959\n",
    "9548258\n",
    "9642505\n",
    "9729350\n",
    "9789513\n",
    "9846627\n",
    "9895801\n",
    "9934300\n",
    "9967213\n",
    "10000595\n",
    "10036983\n",
    "10089498\n",
    "10196792\n",
    "10319927\n",
    "10399061\n",
    "10460415\n",
    "10512922\n",
    "10562153\n",
    "10608800\n",
    "10661259\n",
    "10720509\n",
    "10761698\n",
    "10805808\n",
    "10862132\n",
    "10902022\n",
    "10928070\n",
    "10955141\n",
    "10987314\n",
    "11020362\n",
    "11048473\n",
    "11077841\n",
    "11107017\n",
    "11121341\n",
    "11104899\n",
    "11045011\n",
    "10965211\n",
    "10892413\n",
    "10820883\n",
    "10775971\n",
    "10754679\n",
    "10732882\n",
    "10721582\n",
    "10715549\n",
    "90\n",
    "GRD\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "89927\n",
    "91324\n",
    "92481\n",
    "93409\n",
    "94122\n",
    "94634\n",
    "94936\n",
    "95012\n",
    "94929\n",
    "94723\n",
    "94476\n",
    "94212\n",
    "93984\n",
    "93661\n",
    "93190\n",
    "92494\n",
    "91469\n",
    "90219\n",
    "89104\n",
    "88603\n",
    "89032\n",
    "90610\n",
    "93132\n",
    "96020\n",
    "98472\n",
    "99953\n",
    "100183\n",
    "99425\n",
    "98126\n",
    "96932\n",
    "96328\n",
    "96462\n",
    "97161\n",
    "98234\n",
    "99345\n",
    "100286\n",
    "101003\n",
    "101567\n",
    "102023\n",
    "102429\n",
    "102837\n",
    "103242\n",
    "103636\n",
    "104003\n",
    "104346\n",
    "104658\n",
    "104938\n",
    "105183\n",
    "105457\n",
    "105787\n",
    "106227\n",
    "106786\n",
    "107452\n",
    "108172\n",
    "108900\n",
    "109603\n",
    "110263\n",
    "110874\n",
    "111449\n",
    "112002\n",
    "112519\n",
    "91\n",
    "GRL\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "32500\n",
    "33700\n",
    "35000\n",
    "36400\n",
    "37600\n",
    "39200\n",
    "40500\n",
    "41900\n",
    "43400\n",
    "44900\n",
    "46400\n",
    "47200\n",
    "48300\n",
    "49000\n",
    "49500\n",
    "49600\n",
    "49700\n",
    "49400\n",
    "49200\n",
    "49600\n",
    "50200\n",
    "51000\n",
    "51500\n",
    "52100\n",
    "52700\n",
    "53200\n",
    "53500\n",
    "54100\n",
    "54800\n",
    "55300\n",
    "55600\n",
    "55500\n",
    "55300\n",
    "55200\n",
    "55500\n",
    "55800\n",
    "55900\n",
    "56000\n",
    "56100\n",
    "56100\n",
    "56200\n",
    "56350\n",
    "56609\n",
    "56765\n",
    "56911\n",
    "56935\n",
    "56774\n",
    "56555\n",
    "56328\n",
    "56323\n",
    "56905\n",
    "56890\n",
    "56810\n",
    "56483\n",
    "56295\n",
    "56114\n",
    "56186\n",
    "56172\n",
    "56023\n",
    "56225\n",
    "56367\n",
    "92\n",
    "GTM\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "4128880\n",
    "4251911\n",
    "4378604\n",
    "4508444\n",
    "4640795\n",
    "4774984\n",
    "4910790\n",
    "5047435\n",
    "5184095\n",
    "5320100\n",
    "5455197\n",
    "5589563\n",
    "5723759\n",
    "5858466\n",
    "5994300\n",
    "6131151\n",
    "6269983\n",
    "6412667\n",
    "6561919\n",
    "6720582\n",
    "6890346\n",
    "7071186\n",
    "7262658\n",
    "7462585\n",
    "7669863\n",
    "7884034\n",
    "8104921\n",
    "8332446\n",
    "8566331\n",
    "8805995\n",
    "9050115\n",
    "9296814\n",
    "9544055\n",
    "9790619\n",
    "10037522\n",
    "10286786\n",
    "10536942\n",
    "10788362\n",
    "11046215\n",
    "11311078\n",
    "11589761\n",
    "11871565\n",
    "12147518\n",
    "12415334\n",
    "12682108\n",
    "12948292\n",
    "13213330\n",
    "13477017\n",
    "13739299\n",
    "14000190\n",
    "14259687\n",
    "14521515\n",
    "14781942\n",
    "15043981\n",
    "15306316\n",
    "15567419\n",
    "15827690\n",
    "16087418\n",
    "16346950\n",
    "16604026\n",
    "16858333\n",
    "93\n",
    "GUM\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "66733\n",
    "68065\n",
    "69605\n",
    "71289\n",
    "73047\n",
    "74828\n",
    "76611\n",
    "78412\n",
    "80212\n",
    "82040\n",
    "83875\n",
    "85729\n",
    "87580\n",
    "89459\n",
    "91378\n",
    "93351\n",
    "95380\n",
    "97477\n",
    "99629\n",
    "101837\n",
    "104134\n",
    "106482\n",
    "108901\n",
    "111403\n",
    "113953\n",
    "116576\n",
    "119228\n",
    "121923\n",
    "124674\n",
    "127525\n",
    "130480\n",
    "133553\n",
    "136693\n",
    "139813\n",
    "142806\n",
    "145559\n",
    "148061\n",
    "150303\n",
    "152274\n",
    "153955\n",
    "155323\n",
    "156395\n",
    "157173\n",
    "157723\n",
    "158094\n",
    "158406\n",
    "158649\n",
    "158848\n",
    "159037\n",
    "159232\n",
    "159439\n",
    "159690\n",
    "159990\n",
    "160415\n",
    "161007\n",
    "161851\n",
    "162948\n",
    "164281\n",
    "165770\n",
    "167295\n",
    "168783\n",
    "94\n",
    "GUY\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "571813\n",
    "589274\n",
    "606286\n",
    "622580\n",
    "637835\n",
    "651865\n",
    "664518\n",
    "675861\n",
    "686144\n",
    "695743\n",
    "704930\n",
    "713684\n",
    "721949\n",
    "729909\n",
    "737846\n",
    "745847\n",
    "754101\n",
    "762424\n",
    "770128\n",
    "776257\n",
    "780153\n",
    "781734\n",
    "781249\n",
    "778953\n",
    "775217\n",
    "770439\n",
    "764447\n",
    "757451\n",
    "750641\n",
    "745581\n",
    "743306\n",
    "744477\n",
    "748596\n",
    "754141\n",
    "758896\n",
    "761298\n",
    "760801\n",
    "757975\n",
    "753778\n",
    "749676\n",
    "746718\n",
    "745206\n",
    "744789\n",
    "745142\n",
    "745737\n",
    "746156\n",
    "746335\n",
    "746477\n",
    "746815\n",
    "747718\n",
    "749430\n",
    "752029\n",
    "755388\n",
    "759281\n",
    "763371\n",
    "767433\n",
    "771363\n",
    "775218\n",
    "779007\n",
    "782775\n",
    "786559\n",
    "95\n",
    "HIC\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "759783611\n",
    "769499360\n",
    "779465348\n",
    "789358853\n",
    "799159797\n",
    "808596600\n",
    "817426786\n",
    "825698975\n",
    "833895491\n",
    "842260894\n",
    "850563450\n",
    "860560048\n",
    "869313732\n",
    "877805133\n",
    "886141034\n",
    "894227661\n",
    "901732072\n",
    "909139852\n",
    "916611800\n",
    "924450013\n",
    "932075328\n",
    "939681156\n",
    "946904804\n",
    "953688382\n",
    "960111311\n",
    "966563677\n",
    "973188301\n",
    "979874901\n",
    "986702713\n",
    "993966356\n",
    "1001677611\n",
    "1010166079\n",
    "1016513603\n",
    "1024551207\n",
    "1032035681\n",
    "1040719622\n",
    "1047818942\n",
    "1054747601\n",
    "1061501831\n",
    "1068197515\n",
    "1074819578\n",
    "1081809509\n",
    "1089036403\n",
    "1096166107\n",
    "1103693618\n",
    "1111423301\n",
    "1119840651\n",
    "1128710968\n",
    "1138167727\n",
    "1146611428\n",
    "1154194529\n",
    "1159642350\n",
    "1166465944\n",
    "1173351852\n",
    "1180423771\n",
    "1187317397\n",
    "1194191812\n",
    "1200273639\n",
    "1205941621\n",
    "1210795776\n",
    "1214930230\n",
    "96\n",
    "HKG\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "3075605\n",
    "3168100\n",
    "3305200\n",
    "3420900\n",
    "3504600\n",
    "3597900\n",
    "3629900\n",
    "3722800\n",
    "3802700\n",
    "3863900\n",
    "3959000\n",
    "4045300\n",
    "4123600\n",
    "4241600\n",
    "4377800\n",
    "4461600\n",
    "4518000\n",
    "4583700\n",
    "4667500\n",
    "4929700\n",
    "5063100\n",
    "5183400\n",
    "5264500\n",
    "5345100\n",
    "5397900\n",
    "5456200\n",
    "5524600\n",
    "5580500\n",
    "5627600\n",
    "5686200\n",
    "5704500\n",
    "5752000\n",
    "5800500\n",
    "5901000\n",
    "6035400\n",
    "6156100\n",
    "6435500\n",
    "6489300\n",
    "6543700\n",
    "6606500\n",
    "6665000\n",
    "6714300\n",
    "6744100\n",
    "6730800\n",
    "6783500\n",
    "6813200\n",
    "6857100\n",
    "6916300\n",
    "6957800\n",
    "6972800\n",
    "7024200\n",
    "7071600\n",
    "7150100\n",
    "7178900\n",
    "7229500\n",
    "7291300\n",
    "7336600\n",
    "7391700\n",
    "7451000\n",
    "7507400\n",
    "7481800\n",
    "97\n",
    "HND\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "2038636\n",
    "2096407\n",
    "2155648\n",
    "2216712\n",
    "2280044\n",
    "2346005\n",
    "2414805\n",
    "2486418\n",
    "2560723\n",
    "2637515\n",
    "2716652\n",
    "2798125\n",
    "2882102\n",
    "2968978\n",
    "3059245\n",
    "3153252\n",
    "3251145\n",
    "3352825\n",
    "3458102\n",
    "3566661\n",
    "3678274\n",
    "3792922\n",
    "3910642\n",
    "4031333\n",
    "4154864\n",
    "4281167\n",
    "4410270\n",
    "4542218\n",
    "4677023\n",
    "4814696\n",
    "4955302\n",
    "5098594\n",
    "5244677\n",
    "5394416\n",
    "5548969\n",
    "5709010\n",
    "5874814\n",
    "6045704\n",
    "6220405\n",
    "6397140\n",
    "6574510\n",
    "6751912\n",
    "6929267\n",
    "7106323\n",
    "7282953\n",
    "7458982\n",
    "7634295\n",
    "7808520\n",
    "7980955\n",
    "8150780\n",
    "8317467\n",
    "8480670\n",
    "8640692\n",
    "8798524\n",
    "8955579\n",
    "9112904\n",
    "9270794\n",
    "9429016\n",
    "9587523\n",
    "9746115\n",
    "9904608\n",
    "98\n",
    "HPC\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "161734348\n",
    "165573136\n",
    "169567052\n",
    "173722891\n",
    "178048131\n",
    "182548798\n",
    "187227553\n",
    "192084675\n",
    "197119416\n",
    "202329958\n",
    "207714326\n",
    "213272593\n",
    "219004306\n",
    "224905337\n",
    "230970270\n",
    "237195030\n",
    "243590882\n",
    "250160343\n",
    "256880818\n",
    "263721632\n",
    "270668519\n",
    "277719154\n",
    "284905161\n",
    "292290400\n",
    "299959455\n",
    "307980326\n",
    "316358167\n",
    "325095129\n",
    "334249174\n",
    "343887503\n",
    "354047283\n",
    "364782214\n",
    "376055877\n",
    "387705086\n",
    "399503994\n",
    "411298637\n",
    "423019239\n",
    "434736157\n",
    "446608371\n",
    "458867418\n",
    "471680803\n",
    "485112670\n",
    "499113608\n",
    "513601222\n",
    "528444638\n",
    "543555314\n",
    "558909246\n",
    "574560319\n",
    "590590409\n",
    "607115601\n",
    "624219296\n",
    "641921373\n",
    "660193919\n",
    "679013004\n",
    "698336663\n",
    "718128408\n",
    "738387077\n",
    "759106118\n",
    "780234453\n",
    "801708019\n",
    "823480038\n",
    "99\n",
    "HRV\n",
    "Population, total\n",
    "SP.POP.TOTL\n",
    "4140181\n",
    "4167292\n",
    "4196712\n",
    "4225675\n",
    "4252876\n",
    "4280923\n",
    "4310701\n",
    "4338683\n",
    "4365628\n",
    "4391490\n",
    "4412252\n",
    "4431275\n",
    "4450564\n",
    "4470161\n",
    "4490660\n",
    "4512082\n",
    "4535934\n",
    "4559571\n",
    "4581085\n",
    "4594778\n",
    "4599782\n",
    "4611509\n",
    "4634234\n",
    "4658254\n",
    "4680285\n",
    "4701417\n",
    "4721446\n",
    "4739745\n",
    "4755207\n",
    "4767260\n",
    "4777368\n",
    "4689022\n",
    "4575818\n",
    "4600463\n",
    "4652024\n",
    "4620030\n",
    "4557097\n",
    "4534920\n",
    "4532135\n",
    "4512597\n",
    "4468302\n",
    "4299642\n",
    "4302174\n",
    "4303399\n",
    "4304600\n",
    "4310145\n",
    "4311159\n",
    "4310217\n",
    "4309705\n",
    "4305181\n",
    "4295427\n",
    "4280622\n",
    "4267558\n",
    "4255689\n",
    "4238389\n",
    "4203604\n",
    "4174349\n",
    "4124531\n",
    "4087843\n",
    "4065253\n",
    "4047200\n",
    "country_data = pd.read_csv('metadata_country.csv')\n",
    "country_data\n",
    "Country Code\n",
    "Region\n",
    "IncomeGroup\n",
    "SpecialNotes\n",
    "TableName\n",
    "0\n",
    "ABW\n",
    "Latin America & Caribbean\n",
    "High income\n",
    "null\n",
    "Aruba\n",
    "1\n",
    "AFE\n",
    "null\n",
    "null\n",
    "26 countries, stretching from the Red Sea in the North to the Cape of Good Hope in the South (https://www.worldbank.org/en/region/afr/eastern-and-southern-africa)\n",
    "Africa Eastern and Southern\n",
    "2\n",
    "AFG\n",
    "South Asia\n",
    "Low income\n",
    "Fiscal year end: March 20; reporting period for national accounts data: FY.\n",
    "Afghanistan\n",
    "3\n",
    "AFW\n",
    "null\n",
    "null\n",
    "22 countries, stretching from the westernmost point of Africa, across the equator, and partly along the Atlantic Ocean till the Republic of Congo in the South (https://www.worldbank.org/en/region/afr/western-and-central-africa)\n",
    "Africa Western and Central\n",
    "4\n",
    "AGO\n",
    "Sub-Saharan Africa\n",
    "Lower middle income\n",
    "null\n",
    "Angola\n",
    "5\n",
    "ALB\n",
    "Europe & Central Asia\n",
    "Upper middle income\n",
    "null\n",
    "Albania\n",
    "6\n",
    "AND\n",
    "Europe & Central Asia\n",
    "High income\n",
    "null\n",
    "Andorra\n",
    "7\n",
    "ARB\n",
    "null\n",
    "null\n",
    "Arab World aggregate. Arab World is composed of members of the League of Arab States.\n",
    "Arab World\n",
    "8\n",
    "ARE\n",
    "Middle East & North Africa\n",
    "High income\n",
    "null\n",
    "United Arab Emirates\n",
    "9\n",
    "ARG\n",
    "Latin America & Caribbean\n",
    "Upper middle income\n",
    "null\n",
    "Argentina\n",
    "10\n",
    "ARM\n",
    "Europe & Central Asia\n",
    "Upper middle income\n",
    "null\n",
    "Armenia\n",
    "11\n",
    "ASM\n",
    "East Asia & Pacific\n",
    "Upper middle income\n",
    "null\n",
    "American Samoa\n",
    "12\n",
    "ATG\n",
    "Latin America & Caribbean\n",
    "High income\n",
    "null\n",
    "Antigua and Barbuda\n",
    "13\n",
    "AUS\n",
    "East Asia & Pacific\n",
    "High income\n",
    "Fiscal year end: June 30; reporting period for national accounts data: FY.\n",
    "Australia\n",
    "14\n",
    "AUT\n",
    "Europe & Central Asia\n",
    "High income\n",
    "A simple multiplier is used to convert the national currencies of EMU members to euros. The following irrevocable euro conversion rate was adopted by the EU Council on January 1, 1999: 1 euro = 13.7603 Austrian schilling. Please note that historical data before 1999 are not actual euros and are not comparable or suitable for aggregation across countries.\n",
    "Austria\n",
    "15\n",
    "AZE\n",
    "Europe & Central Asia\n",
    "Upper middle income\n",
    "null\n",
    "Azerbaijan\n",
    "16\n",
    "BDI\n",
    "Sub-Saharan Africa\n",
    "Low income\n",
    "null\n",
    "Burundi\n",
    "17\n",
    "BEL\n",
    "Europe & Central Asia\n",
    "High income\n",
    "A simple multiplier is used to convert the national currencies of EMU members to euros. The following irrevocable euro conversion rate was adopted by the EU Council on January 1, 1999: 1 euro = 40.3399 Belgian franc. Please note that historical data before 1999 are not actual euros and are not comparable or suitable for aggregation across countries.\n",
    "Belgium\n",
    "18\n",
    "BEN\n",
    "Sub-Saharan Africa\n",
    "Lower middle income\n",
    "null\n",
    "Benin\n",
    "19\n",
    "BFA\n",
    "Sub-Saharan Africa\n",
    "Low income\n",
    "null\n",
    "Burkina Faso\n",
    "20\n",
    "BGD\n",
    "South Asia\n",
    "Lower middle income\n",
    "Fiscal year end: June 30; reporting period for national accounts data: FY.\n",
    "Bangladesh\n",
    "21\n",
    "BGR\n",
    "Europe & Central Asia\n",
    "Upper middle income\n",
    "null\n",
    "Bulgaria\n",
    "22\n",
    "BHR\n",
    "Middle East & North Africa\n",
    "High income\n",
    "null\n",
    "Bahrain\n",
    "23\n",
    "BHS\n",
    "Latin America & Caribbean\n",
    "High income\n",
    "null\n",
    "Bahamas, The\n",
    "24\n",
    "BIH\n",
    "Europe & Central Asia\n",
    "Upper middle income\n",
    "null\n",
    "Bosnia and Herzegovina\n",
    "25\n",
    "BLR\n",
    "Europe & Central Asia\n",
    "Upper middle income\n",
    "Data before 2015 were adjusted to reflect the new denomination effective from July 1, 2016 (BYN), a decrease of 10,000 times (1 BYN = 10,000 BYR)\n",
    "Belarus\n",
    "26\n",
    "BLZ\n",
    "Latin America & Caribbean\n",
    "Lower middle income\n",
    "null\n",
    "Belize\n",
    "27\n",
    "BMU\n",
    "North America\n",
    "High income\n",
    "null\n",
    "Bermuda\n",
    "28\n",
    "BOL\n",
    "Latin America & Caribbean\n",
    "Lower middle income\n",
    "null\n",
    "Bolivia\n",
    "29\n",
    "BRA\n",
    "Latin America & Caribbean\n",
    "Upper middle income\n",
    "null\n",
    "Brazil\n",
    "30\n",
    "BRB\n",
    "Latin America & Caribbean\n",
    "High income\n",
    "null\n",
    "Barbados\n",
    "31\n",
    "BRN\n",
    "East Asia & Pacific\n",
    "High income\n",
    "null\n",
    "Brunei Darussalam\n",
    "32\n",
    "BTN\n",
    "South Asia\n",
    "Lower middle income\n",
    "null\n",
    "Bhutan\n",
    "33\n",
    "BWA\n",
    "Sub-Saharan Africa\n",
    "Upper middle income\n",
    "null\n",
    "Botswana\n",
    "34\n",
    "CAF\n",
    "Sub-Saharan Africa\n",
    "Low income\n",
    "null\n",
    "Central African Republic\n",
    "35\n",
    "CAN\n",
    "North America\n",
    "High income\n",
    "Fiscal year end: March 31; reporting period for national accounts data: CY.\n",
    "Canada\n",
    "36\n",
    "CEB\n",
    "null\n",
    "null\n",
    "Central Europe and the Baltics aggregate.\n",
    "Central Europe and the Baltics\n",
    "37\n",
    "CHE\n",
    "Europe & Central Asia\n",
    "High income\n",
    "null\n",
    "Switzerland\n",
    "38\n",
    "CHI\n",
    "Europe & Central Asia\n",
    "High income\n",
    "null\n",
    "Channel Islands\n",
    "39\n",
    "CHL\n",
    "Latin America & Caribbean\n",
    "High income\n",
    "null\n",
    "Chile\n",
    "40\n",
    "CHN\n",
    "East Asia & Pacific\n",
    "Upper middle income\n",
    "On 1 July 1997 China resumed its exercise of sovereignty over Hong Kong; and on 20 December 1999 China resumed its exercise of sovereignty over Macao. Unless otherwise noted, data for China do not include data for Hong Kong SAR, China; Macao SAR, China; or Taiwan, China.\n",
    "China\n",
    "41\n",
    "CIV\n",
    "Sub-Saharan Africa\n",
    "Lower middle income\n",
    "null\n",
    "Côte d'Ivoire\n",
    "42\n",
    "CMR\n",
    "Sub-Saharan Africa\n",
    "Lower middle income\n",
    "null\n",
    "Cameroon\n",
    "43\n",
    "COD\n",
    "Sub-Saharan Africa\n",
    "Low income\n",
    "null\n",
    "Congo, Dem. Rep.\n",
    "44\n",
    "COG\n",
    "Sub-Saharan Africa\n",
    "Lower middle income\n",
    "null\n",
    "Congo, Rep.\n",
    "45\n",
    "COL\n",
    "Latin America & Caribbean\n",
    "Upper middle income\n",
    "null\n",
    "Colombia\n",
    "46\n",
    "COM\n",
    "Sub-Saharan Africa\n",
    "Lower middle income\n",
    "null\n",
    "Comoros\n",
    "47\n",
    "CPV\n",
    "Sub-Saharan Africa\n",
    "Lower middle income\n",
    "null\n",
    "Cabo Verde\n",
    "48\n",
    "CRI\n",
    "Latin America & Caribbean\n",
    "Upper middle income\n",
    "null\n",
    "Costa Rica\n",
    "49\n",
    "CSS\n",
    "null\n",
    "null\n",
    "null\n",
    "Caribbean small states\n",
    "50\n",
    "CUB\n",
    "Latin America & Caribbean\n",
    "Upper middle income\n",
    "null\n",
    "Cuba\n",
    "51\n",
    "CUW\n",
    "Latin America & Caribbean\n",
    "High income\n",
    "null\n",
    "Curaçao\n",
    "52\n",
    "CYM\n",
    "Latin America & Caribbean\n",
    "High income\n",
    "null\n",
    "Cayman Islands\n",
    "53\n",
    "CYP\n",
    "Europe & Central Asia\n",
    "High income\n",
    "A simple multiplier is used to convert the national currencies of EMU members to euros. The following irrevocable euro conversion rate entered into force on January 1, 2008: 1 euro = 0.585274 Cyprus pounds. Please note that historical data are not actual euros and are not comparable or suitable for aggregation across countries.\n",
    "Cyprus\n",
    "54\n",
    "CZE\n",
    "Europe & Central Asia\n",
    "High income\n",
    "null\n",
    "Czech Republic\n",
    "55\n",
    "DEU\n",
    "Europe & Central Asia\n",
    "High income\n",
    "null\n",
    "Germany\n",
    "56\n",
    "DJI\n",
    "Middle East & North Africa\n",
    "Lower middle income\n",
    "null\n",
    "Djibouti\n",
    "57\n",
    "DMA\n",
    "Latin America & Caribbean\n",
    "Upper middle income\n",
    "null\n",
    "Dominica\n",
    "58\n",
    "DNK\n",
    "Europe & Central Asia\n",
    "High income\n",
    "null\n",
    "Denmark\n",
    "59\n",
    "DOM\n",
    "Latin America & Caribbean\n",
    "Upper middle income\n",
    "null\n",
    "Dominican Republic\n",
    "60\n",
    "DZA\n",
    "Middle East & North Africa\n",
    "Lower middle income\n",
    "null\n",
    "Algeria\n",
    "61\n",
    "EAP\n",
    "null\n",
    "null\n",
    "null\n",
    "East Asia & Pacific (excluding high income)\n",
    "62\n",
    "EAR\n",
    "null\n",
    "null\n",
    "Early-dividend countries are mostly lower-middle-income countries further along the fertility transition. Fertility rates have fallen below four births per woman and the working-age share of the population is likely rising considerably.\n",
    "Early-demographic dividend\n",
    "63\n",
    "EAS\n",
    "null\n",
    "null\n",
    "East Asia and Pacific regional aggregate (includes all income levels).\n",
    "East Asia & Pacific\n",
    "64\n",
    "ECA\n",
    "null\n",
    "null\n",
    "null\n",
    "Europe & Central Asia (excluding high income)\n",
    "65\n",
    "ECS\n",
    "null\n",
    "null\n",
    "Europe and Central Asia regional aggregate (includes all income levels).\n",
    "Europe & Central Asia\n",
    "66\n",
    "ECU\n",
    "Latin America & Caribbean\n",
    "Upper middle income\n",
    "null\n",
    "Ecuador\n",
    "67\n",
    "EGY\n",
    "Middle East & North Africa\n",
    "Lower middle income\n",
    "Fiscal year end: June 30; reporting period for national accounts data: FY for years 1980 and after. The data from 1973 to 1979 are calendar year data.\n",
    "Egypt, Arab Rep.\n",
    "68\n",
    "EMU\n",
    "null\n",
    "null\n",
    "Euro area aggregate.\n",
    "Euro area\n",
    "69\n",
    "ERI\n",
    "Sub-Saharan Africa\n",
    "Low income\n",
    "null\n",
    "Eritrea\n",
    "70\n",
    "ESP\n",
    "Europe & Central Asia\n",
    "High income\n",
    "A simple multiplier is used to convert the national currencies of EMU members to euros. The following irrevocable euro conversion rate was adopted by the EU Council on January 1, 1999: 1 euro = 166.386 Spanish peseta. Please note that historical data before 1999 are not actual euros and are not comparable or suitable for aggregation across countries.\n",
    "Spain\n",
    "71\n",
    "EST\n",
    "Europe & Central Asia\n",
    "High income\n",
    "The following irrevocable euro conversion rate entered into force on January 1, 2011: 1 euro = 15.6466 Estonian kroon. Please note that historical data are not actual euros and are not comparable or suitable for aggregation across countries.\n",
    "Estonia\n",
    "72\n",
    "ETH\n",
    "Sub-Saharan Africa\n",
    "Low income\n",
    "Fiscal year end: July 7; reporting period for national accounts data: FY.\n",
    "Ethiopia\n",
    "73\n",
    "EUU\n",
    "null\n",
    "null\n",
    "European Union aggregate.\n",
    "European Union\n",
    "74\n",
    "FCS\n",
    "null\n",
    "null\n",
    "Fragile and conflict-affected situations aggregate. Countries are distinguished based on the nature and severity of issues they face. The classification uses the following categories: - Countries with high levels of institutional and social fragility, identified based on publicly available indicators that measure the quality of policy and institutions and manifestations of fragility. - Countries affected by violent conflict, identified based on a threshold number of conflict-related deaths relative to the population. This category includes two sub-categories based on the intensity of violence: countries in high-intensity conflict and countries in medium-intensity conflict. For more information, please visit https://www.worldbank.org/en/topic/fragilityconflictviolence/brief/harmonized-list-of-fragile-situations.\n",
    "Fragile and conflict affected situations\n",
    "75\n",
    "FIN\n",
    "Europe & Central Asia\n",
    "High income\n",
    "A simple multiplier is used to convert the national currencies of EMU members to euros. The following irrevocable euro conversion rate was adopted by the EU Council on January 1, 1999: 1 euro = 5.94573 Finnish markka. Please note that historical data before 1999 are not actual euros and are not comparable or suitable for aggregation across countries.\n",
    "Finland\n",
    "76\n",
    "FJI\n",
    "East Asia & Pacific\n",
    "Upper middle income\n",
    "null\n",
    "Fiji\n",
    "77\n",
    "FRA\n",
    "Europe & Central Asia\n",
    "High income\n",
    "The following irrevocable euro conversion rate was adopted by the EU Council on January 1, 1999: 1 euro = 6.55957 French franc. Please note that historical data before 1999 are not actual euros and are not comparable or suitable for aggregation across countries.\n",
    "France\n",
    "78\n",
    "FRO\n",
    "Europe & Central Asia\n",
    "High income\n",
    "null\n",
    "Faroe Islands\n",
    "79\n",
    "FSM\n",
    "East Asia & Pacific\n",
    "Lower middle income\n",
    "Fiscal year ends on September 30; reporting period for national accounts data: FY. Based on the Pacific and Virgin Islands Training Initiative, national accounts data have been revised.\n",
    "Micronesia, Fed. Sts.\n",
    "80\n",
    "GAB\n",
    "Sub-Saharan Africa\n",
    "Upper middle income\n",
    "null\n",
    "Gabon\n",
    "81\n",
    "GBR\n",
    "Europe & Central Asia\n",
    "High income\n",
    "null\n",
    "United Kingdom\n",
    "82\n",
    "GEO\n",
    "Europe & Central Asia\n",
    "Upper middle income\n",
    "Includes self-governed areas only, which mostly exclude Abkhazia and South Ossetia, but small areas in Abkhazia and South Ossetia are included before 2008 or 2009 because of the changes in self-governed areas.\n",
    "Georgia\n",
    "83\n",
    "GHA\n",
    "Sub-Saharan Africa\n",
    "Lower middle income\n",
    "null\n",
    "Ghana\n",
    "84\n",
    "GIB\n",
    "Europe & Central Asia\n",
    "High income\n",
    "null\n",
    "Gibraltar\n",
    "85\n",
    "GIN\n",
    "Sub-Saharan Africa\n",
    "Low income\n",
    "null\n",
    "Guinea\n",
    "86\n",
    "GMB\n",
    "Sub-Saharan Africa\n",
    "Low income\n",
    "null\n",
    "Gambia, The\n",
    "87\n",
    "GNB\n",
    "Sub-Saharan Africa\n",
    "Low income\n",
    "null\n",
    "Guinea-Bissau\n",
    "88\n",
    "GNQ\n",
    "Sub-Saharan Africa\n",
    "Upper middle income\n",
    "null\n",
    "Equatorial Guinea\n",
    "89\n",
    "GRC\n",
    "Europe & Central Asia\n",
    "High income\n",
    "A simple multiplier is used to convert the national currencies of EMU members to euros. The following irrevocable euro conversion rate was adopted by the EU Council on January 1, 1999: 1 euro = 340.75 Greek drachma. Please note that historical data before 1999 are not actual euros and are not comparable or suitable for aggregation across countries.\n",
    "Greece\n",
    "90\n",
    "GRD\n",
    "Latin America & Caribbean\n",
    "Upper middle income\n",
    "null\n",
    "Grenada\n",
    "91\n",
    "GRL\n",
    "Europe & Central Asia\n",
    "High income\n",
    "null\n",
    "Greenland\n",
    "92\n",
    "GTM\n",
    "Latin America & Caribbean\n",
    "Upper middle income\n",
    "null\n",
    "Guatemala\n",
    "93\n",
    "GUM\n",
    "East Asia & Pacific\n",
    "High income\n",
    "null\n",
    "Guam\n",
    "94\n",
    "GUY\n",
    "Latin America & Caribbean\n",
    "Upper middle income\n",
    "null\n",
    "Guyana\n",
    "95\n",
    "HIC\n",
    "null\n",
    "null\n",
    "High income group aggregate. High-income economies are those in which 2020 GNI per capita was $12,696 or more.\n",
    "High income\n",
    "96\n",
    "HKG\n",
    "East Asia & Pacific\n",
    "High income\n",
    "On 1 July 1997 China resumed its exercise of sovereignty over Hong Kong. Unless otherwise noted, data for China do not include data for Hong Kong SAR, China; Macao SAR, China; or Taiwan, China. Agriculture value added includes mining and quarrying.\n",
    "Hong Kong SAR, China\n",
    "97\n",
    "HND\n",
    "Latin America & Caribbean\n",
    "Lower middle income\n",
    "null\n",
    "Honduras\n",
    "98\n",
    "HPC\n",
    "null\n",
    "null\n",
    "Heavily indebted poor countries aggregate.\n",
    "Heavily indebted poor countries (HIPC)\n",
    "99\n",
    "HRV\n",
    "Europe & Central Asia\n",
    "High income\n",
    "null\n",
    "Croatia\n",
    "Source and license of dataset.\n",
    "\n",
    "Don't know where to start?\n",
    "Challenges are brief tasks designed to help you practice specific skills:\n",
    "\n",
    "🗺️ Explore: Which countries have experienced the highest population growth?\n",
    "📊 Visualize: Create a plot that visualizes the population growth of countries over time grouped by region.\n",
    "🔎 Analyze: How does income group affect a country's population growth?\n",
    "Scenarios are broader questions to help you develop an end-to-end project for your portfolio:\n",
    "\n",
    "You are working for a regional organization in East Asia and the Pacific that analyzes population statistics and makes recommendations to relevant governments. There are concerns that some countries in the region are experiencing a decline in populations.\n",
    "\n",
    "Your manager has asked you to prepare a population forecast for the region for 5, 10, and 15 years into the future. They have also asked you to identify the five countries with the lowest population growth (or greatest decline), and perform similar forecasts for these five countries individually.\n",
    "\n",
    "You will need to prepare a report that is accessible to a broad audience. It should outline your motivation, steps, findings, and conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video Games Sales Data\n",
    "This dataset contains records of popular video games in North America, Japan, Europe and other parts of the world. Every video game in this dataset has at least 100k global sales.\n",
    "\n",
    "Not sure where to begin? Scroll to the bottom to find challenges!\n",
    "\n",
    "import pandas as pd\n",
    "sales = pd.read_csv(\"vgsales.csv\", index_col=0)\n",
    "print(sales.shape)\n",
    "sales.head(100)\n",
    "(16598, 10)\n",
    "Rank\n",
    "Name\n",
    "Platform\n",
    "Year\n",
    "Genre\n",
    "Publisher\n",
    "NA_Sales\n",
    "EU_Sales\n",
    "JP_Sales\n",
    "Other_Sales\n",
    "Global_Sales\n",
    "1\n",
    "Wii Sports\n",
    "Wii\n",
    "2006\n",
    "Sports\n",
    "Nintendo\n",
    "41.49\n",
    "29.02\n",
    "3.77\n",
    "8.46\n",
    "82.74\n",
    "2\n",
    "Super Mario Bros.\n",
    "NES\n",
    "1985\n",
    "Platform\n",
    "Nintendo\n",
    "29.08\n",
    "3.58\n",
    "6.81\n",
    "0.77\n",
    "40.24\n",
    "3\n",
    "Mario Kart Wii\n",
    "Wii\n",
    "2008\n",
    "Racing\n",
    "Nintendo\n",
    "15.85\n",
    "12.88\n",
    "3.79\n",
    "3.31\n",
    "35.82\n",
    "4\n",
    "Wii Sports Resort\n",
    "Wii\n",
    "2009\n",
    "Sports\n",
    "Nintendo\n",
    "15.75\n",
    "11.01\n",
    "3.28\n",
    "2.96\n",
    "33\n",
    "5\n",
    "Pokemon Red/Pokemon Blue\n",
    "GB\n",
    "1996\n",
    "Role-Playing\n",
    "Nintendo\n",
    "11.27\n",
    "8.89\n",
    "10.22\n",
    "1\n",
    "31.37\n",
    "6\n",
    "Tetris\n",
    "GB\n",
    "1989\n",
    "Puzzle\n",
    "Nintendo\n",
    "23.2\n",
    "2.26\n",
    "4.22\n",
    "0.58\n",
    "30.26\n",
    "7\n",
    "New Super Mario Bros.\n",
    "DS\n",
    "2006\n",
    "Platform\n",
    "Nintendo\n",
    "11.38\n",
    "9.23\n",
    "6.5\n",
    "2.9\n",
    "30.01\n",
    "8\n",
    "Wii Play\n",
    "Wii\n",
    "2006\n",
    "Misc\n",
    "Nintendo\n",
    "14.03\n",
    "9.2\n",
    "2.93\n",
    "2.85\n",
    "29.02\n",
    "9\n",
    "New Super Mario Bros. Wii\n",
    "Wii\n",
    "2009\n",
    "Platform\n",
    "Nintendo\n",
    "14.59\n",
    "7.06\n",
    "4.7\n",
    "2.26\n",
    "28.62\n",
    "10\n",
    "Duck Hunt\n",
    "NES\n",
    "1984\n",
    "Shooter\n",
    "Nintendo\n",
    "26.93\n",
    "0.63\n",
    "0.28\n",
    "0.47\n",
    "28.31\n",
    "11\n",
    "Nintendogs\n",
    "DS\n",
    "2005\n",
    "Simulation\n",
    "Nintendo\n",
    "9.07\n",
    "11\n",
    "1.93\n",
    "2.75\n",
    "24.76\n",
    "12\n",
    "Mario Kart DS\n",
    "DS\n",
    "2005\n",
    "Racing\n",
    "Nintendo\n",
    "9.81\n",
    "7.57\n",
    "4.13\n",
    "1.92\n",
    "23.42\n",
    "13\n",
    "Pokemon Gold/Pokemon Silver\n",
    "GB\n",
    "1999\n",
    "Role-Playing\n",
    "Nintendo\n",
    "9\n",
    "6.18\n",
    "7.2\n",
    "0.71\n",
    "23.1\n",
    "14\n",
    "Wii Fit\n",
    "Wii\n",
    "2007\n",
    "Sports\n",
    "Nintendo\n",
    "8.94\n",
    "8.03\n",
    "3.6\n",
    "2.15\n",
    "22.72\n",
    "15\n",
    "Wii Fit Plus\n",
    "Wii\n",
    "2009\n",
    "Sports\n",
    "Nintendo\n",
    "9.09\n",
    "8.59\n",
    "2.53\n",
    "1.79\n",
    "22\n",
    "16\n",
    "Kinect Adventures!\n",
    "X360\n",
    "2010\n",
    "Misc\n",
    "Microsoft Game Studios\n",
    "14.97\n",
    "4.94\n",
    "0.24\n",
    "1.67\n",
    "21.82\n",
    "17\n",
    "Grand Theft Auto V\n",
    "PS3\n",
    "2013\n",
    "Action\n",
    "Take-Two Interactive\n",
    "7.01\n",
    "9.27\n",
    "0.97\n",
    "4.14\n",
    "21.4\n",
    "18\n",
    "Grand Theft Auto: San Andreas\n",
    "PS2\n",
    "2004\n",
    "Action\n",
    "Take-Two Interactive\n",
    "9.43\n",
    "0.4\n",
    "0.41\n",
    "10.57\n",
    "20.81\n",
    "19\n",
    "Super Mario World\n",
    "SNES\n",
    "1990\n",
    "Platform\n",
    "Nintendo\n",
    "12.78\n",
    "3.75\n",
    "3.54\n",
    "0.55\n",
    "20.61\n",
    "20\n",
    "Brain Age: Train Your Brain in Minutes a Day\n",
    "DS\n",
    "2005\n",
    "Misc\n",
    "Nintendo\n",
    "4.75\n",
    "9.26\n",
    "4.16\n",
    "2.05\n",
    "20.22\n",
    "21\n",
    "Pokemon Diamond/Pokemon Pearl\n",
    "DS\n",
    "2006\n",
    "Role-Playing\n",
    "Nintendo\n",
    "6.42\n",
    "4.52\n",
    "6.04\n",
    "1.37\n",
    "18.36\n",
    "22\n",
    "Super Mario Land\n",
    "GB\n",
    "1989\n",
    "Platform\n",
    "Nintendo\n",
    "10.83\n",
    "2.71\n",
    "4.18\n",
    "0.42\n",
    "18.14\n",
    "23\n",
    "Super Mario Bros. 3\n",
    "NES\n",
    "1988\n",
    "Platform\n",
    "Nintendo\n",
    "9.54\n",
    "3.44\n",
    "3.84\n",
    "0.46\n",
    "17.28\n",
    "24\n",
    "Grand Theft Auto V\n",
    "X360\n",
    "2013\n",
    "Action\n",
    "Take-Two Interactive\n",
    "9.63\n",
    "5.31\n",
    "0.06\n",
    "1.38\n",
    "16.38\n",
    "25\n",
    "Grand Theft Auto: Vice City\n",
    "PS2\n",
    "2002\n",
    "Action\n",
    "Take-Two Interactive\n",
    "8.41\n",
    "5.49\n",
    "0.47\n",
    "1.78\n",
    "16.15\n",
    "26\n",
    "Pokemon Ruby/Pokemon Sapphire\n",
    "GBA\n",
    "2002\n",
    "Role-Playing\n",
    "Nintendo\n",
    "6.06\n",
    "3.9\n",
    "5.38\n",
    "0.5\n",
    "15.85\n",
    "27\n",
    "Pokemon Black/Pokemon White\n",
    "DS\n",
    "2010\n",
    "Role-Playing\n",
    "Nintendo\n",
    "5.57\n",
    "3.28\n",
    "5.65\n",
    "0.82\n",
    "15.32\n",
    "28\n",
    "Brain Age 2: More Training in Minutes a Day\n",
    "DS\n",
    "2005\n",
    "Puzzle\n",
    "Nintendo\n",
    "3.44\n",
    "5.36\n",
    "5.32\n",
    "1.18\n",
    "15.3\n",
    "29\n",
    "Gran Turismo 3: A-Spec\n",
    "PS2\n",
    "2001\n",
    "Racing\n",
    "Sony Computer Entertainment\n",
    "6.85\n",
    "5.09\n",
    "1.87\n",
    "1.16\n",
    "14.98\n",
    "30\n",
    "Call of Duty: Modern Warfare 3\n",
    "X360\n",
    "2011\n",
    "Shooter\n",
    "Activision\n",
    "9.03\n",
    "4.28\n",
    "0.13\n",
    "1.32\n",
    "14.76\n",
    "31\n",
    "Pokémon Yellow: Special Pikachu Edition\n",
    "GB\n",
    "1998\n",
    "Role-Playing\n",
    "Nintendo\n",
    "5.89\n",
    "5.04\n",
    "3.12\n",
    "0.59\n",
    "14.64\n",
    "32\n",
    "Call of Duty: Black Ops\n",
    "X360\n",
    "2010\n",
    "Shooter\n",
    "Activision\n",
    "9.67\n",
    "3.73\n",
    "0.11\n",
    "1.13\n",
    "14.64\n",
    "33\n",
    "Pokemon X/Pokemon Y\n",
    "3DS\n",
    "2013\n",
    "Role-Playing\n",
    "Nintendo\n",
    "5.17\n",
    "4.05\n",
    "4.34\n",
    "0.79\n",
    "14.35\n",
    "34\n",
    "Call of Duty: Black Ops 3\n",
    "PS4\n",
    "2015\n",
    "Shooter\n",
    "Activision\n",
    "5.77\n",
    "5.81\n",
    "0.35\n",
    "2.31\n",
    "14.24\n",
    "35\n",
    "Call of Duty: Black Ops II\n",
    "PS3\n",
    "2012\n",
    "Shooter\n",
    "Activision\n",
    "4.99\n",
    "5.88\n",
    "0.65\n",
    "2.52\n",
    "14.03\n",
    "36\n",
    "Call of Duty: Black Ops II\n",
    "X360\n",
    "2012\n",
    "Shooter\n",
    "Activision\n",
    "8.25\n",
    "4.3\n",
    "0.07\n",
    "1.12\n",
    "13.73\n",
    "37\n",
    "Call of Duty: Modern Warfare 2\n",
    "X360\n",
    "2009\n",
    "Shooter\n",
    "Activision\n",
    "8.52\n",
    "3.63\n",
    "0.08\n",
    "1.29\n",
    "13.51\n",
    "38\n",
    "Call of Duty: Modern Warfare 3\n",
    "PS3\n",
    "2011\n",
    "Shooter\n",
    "Activision\n",
    "5.54\n",
    "5.82\n",
    "0.49\n",
    "1.62\n",
    "13.46\n",
    "39\n",
    "Grand Theft Auto III\n",
    "PS2\n",
    "2001\n",
    "Action\n",
    "Take-Two Interactive\n",
    "6.99\n",
    "4.51\n",
    "0.3\n",
    "1.3\n",
    "13.1\n",
    "40\n",
    "Super Smash Bros. Brawl\n",
    "Wii\n",
    "2008\n",
    "Fighting\n",
    "Nintendo\n",
    "6.75\n",
    "2.61\n",
    "2.66\n",
    "1.02\n",
    "13.04\n",
    "41\n",
    "Call of Duty: Black Ops\n",
    "PS3\n",
    "2010\n",
    "Shooter\n",
    "Activision\n",
    "5.98\n",
    "4.44\n",
    "0.48\n",
    "1.83\n",
    "12.73\n",
    "42\n",
    "Animal Crossing: Wild World\n",
    "DS\n",
    "2005\n",
    "Simulation\n",
    "Nintendo\n",
    "2.55\n",
    "3.52\n",
    "5.33\n",
    "0.88\n",
    "12.27\n",
    "43\n",
    "Mario Kart 7\n",
    "3DS\n",
    "2011\n",
    "Racing\n",
    "Nintendo\n",
    "4.74\n",
    "3.91\n",
    "2.67\n",
    "0.89\n",
    "12.21\n",
    "44\n",
    "Halo 3\n",
    "X360\n",
    "2007\n",
    "Shooter\n",
    "Microsoft Game Studios\n",
    "7.97\n",
    "2.83\n",
    "0.13\n",
    "1.21\n",
    "12.14\n",
    "45\n",
    "Grand Theft Auto V\n",
    "PS4\n",
    "2014\n",
    "Action\n",
    "Take-Two Interactive\n",
    "3.8\n",
    "5.81\n",
    "0.36\n",
    "2.02\n",
    "11.98\n",
    "46\n",
    "Pokemon HeartGold/Pokemon SoulSilver\n",
    "DS\n",
    "2009\n",
    "Action\n",
    "Nintendo\n",
    "4.4\n",
    "2.77\n",
    "3.96\n",
    "0.77\n",
    "11.9\n",
    "47\n",
    "Super Mario 64\n",
    "N64\n",
    "1996\n",
    "Platform\n",
    "Nintendo\n",
    "6.91\n",
    "2.85\n",
    "1.91\n",
    "0.23\n",
    "11.89\n",
    "48\n",
    "Gran Turismo 4\n",
    "PS2\n",
    "2004\n",
    "Racing\n",
    "Sony Computer Entertainment\n",
    "3.01\n",
    "0.01\n",
    "1.1\n",
    "7.53\n",
    "11.66\n",
    "49\n",
    "Super Mario Galaxy\n",
    "Wii\n",
    "2007\n",
    "Platform\n",
    "Nintendo\n",
    "6.16\n",
    "3.4\n",
    "1.2\n",
    "0.76\n",
    "11.52\n",
    "50\n",
    "Pokemon Omega Ruby/Pokemon Alpha Sapphire\n",
    "3DS\n",
    "2014\n",
    "Role-Playing\n",
    "Nintendo\n",
    "4.23\n",
    "3.37\n",
    "3.08\n",
    "0.65\n",
    "11.33\n",
    "51\n",
    "Super Mario Land 2: 6 Golden Coins\n",
    "GB\n",
    "1992\n",
    "Adventure\n",
    "Nintendo\n",
    "6.16\n",
    "2.04\n",
    "2.69\n",
    "0.29\n",
    "11.18\n",
    "52\n",
    "Grand Theft Auto IV\n",
    "X360\n",
    "2008\n",
    "Action\n",
    "Take-Two Interactive\n",
    "6.76\n",
    "3.1\n",
    "0.14\n",
    "1.03\n",
    "11.02\n",
    "53\n",
    "Gran Turismo\n",
    "PS\n",
    "1997\n",
    "Racing\n",
    "Sony Computer Entertainment\n",
    "4.02\n",
    "3.87\n",
    "2.54\n",
    "0.52\n",
    "10.95\n",
    "54\n",
    "Super Mario 3D Land\n",
    "3DS\n",
    "2011\n",
    "Platform\n",
    "Nintendo\n",
    "4.89\n",
    "2.99\n",
    "2.13\n",
    "0.78\n",
    "10.79\n",
    "55\n",
    "Gran Turismo 5\n",
    "PS3\n",
    "2010\n",
    "Racing\n",
    "Sony Computer Entertainment\n",
    "2.96\n",
    "4.88\n",
    "0.81\n",
    "2.12\n",
    "10.77\n",
    "56\n",
    "Call of Duty: Modern Warfare 2\n",
    "PS3\n",
    "2009\n",
    "Shooter\n",
    "Activision\n",
    "4.99\n",
    "3.69\n",
    "0.38\n",
    "1.63\n",
    "10.69\n",
    "57\n",
    "Grand Theft Auto IV\n",
    "PS3\n",
    "2008\n",
    "Action\n",
    "Take-Two Interactive\n",
    "4.76\n",
    "3.76\n",
    "0.44\n",
    "1.62\n",
    "10.57\n",
    "58\n",
    "Super Mario All-Stars\n",
    "SNES\n",
    "1993\n",
    "Platform\n",
    "Nintendo\n",
    "5.99\n",
    "2.15\n",
    "2.12\n",
    "0.29\n",
    "10.55\n",
    "59\n",
    "Pokemon FireRed/Pokemon LeafGreen\n",
    "GBA\n",
    "2004\n",
    "Role-Playing\n",
    "Nintendo\n",
    "4.34\n",
    "2.65\n",
    "3.15\n",
    "0.35\n",
    "10.49\n",
    "60\n",
    "Super Mario 64\n",
    "DS\n",
    "2004\n",
    "Platform\n",
    "Nintendo\n",
    "5.08\n",
    "3.11\n",
    "1.25\n",
    "0.98\n",
    "10.42\n",
    "61\n",
    "Just Dance 3\n",
    "Wii\n",
    "2011\n",
    "Misc\n",
    "Ubisoft\n",
    "6.05\n",
    "3.15\n",
    "0\n",
    "1.07\n",
    "10.26\n",
    "62\n",
    "Call of Duty: Ghosts\n",
    "X360\n",
    "2013\n",
    "Shooter\n",
    "Activision\n",
    "6.72\n",
    "2.63\n",
    "0.04\n",
    "0.82\n",
    "10.21\n",
    "63\n",
    "Halo: Reach\n",
    "X360\n",
    "2010\n",
    "Shooter\n",
    "Microsoft Game Studios\n",
    "7.03\n",
    "1.98\n",
    "0.08\n",
    "0.78\n",
    "9.88\n",
    "64\n",
    "Mario Kart 64\n",
    "N64\n",
    "1996\n",
    "Racing\n",
    "Nintendo\n",
    "5.55\n",
    "1.94\n",
    "2.23\n",
    "0.15\n",
    "9.87\n",
    "65\n",
    "New Super Mario Bros. 2\n",
    "3DS\n",
    "2012\n",
    "Platform\n",
    "Nintendo\n",
    "3.66\n",
    "3.07\n",
    "2.47\n",
    "0.63\n",
    "9.82\n",
    "66\n",
    "Halo 4\n",
    "X360\n",
    "2012\n",
    "Shooter\n",
    "Microsoft Game Studios\n",
    "6.63\n",
    "2.36\n",
    "0.04\n",
    "0.73\n",
    "9.76\n",
    "67\n",
    "Final Fantasy VII\n",
    "PS\n",
    "1997\n",
    "Role-Playing\n",
    "Sony Computer Entertainment\n",
    "3.01\n",
    "2.47\n",
    "3.28\n",
    "0.96\n",
    "9.72\n",
    "68\n",
    "Call of Duty: Ghosts\n",
    "PS3\n",
    "2013\n",
    "Shooter\n",
    "Activision\n",
    "4.09\n",
    "3.73\n",
    "0.38\n",
    "1.38\n",
    "9.59\n",
    "69\n",
    "Just Dance 2\n",
    "Wii\n",
    "2010\n",
    "Misc\n",
    "Ubisoft\n",
    "5.84\n",
    "2.89\n",
    "0.01\n",
    "0.78\n",
    "9.52\n",
    "70\n",
    "Gran Turismo 2\n",
    "PS\n",
    "1999\n",
    "Racing\n",
    "Sony Computer Entertainment\n",
    "3.88\n",
    "3.42\n",
    "1.69\n",
    "0.5\n",
    "9.49\n",
    "71\n",
    "Call of Duty 4: Modern Warfare\n",
    "X360\n",
    "2007\n",
    "Shooter\n",
    "Activision\n",
    "5.91\n",
    "2.38\n",
    "0.13\n",
    "0.9\n",
    "9.32\n",
    "72\n",
    "Donkey Kong Country\n",
    "SNES\n",
    "1994\n",
    "Platform\n",
    "Nintendo\n",
    "4.36\n",
    "1.71\n",
    "3\n",
    "0.23\n",
    "9.3\n",
    "73\n",
    "Minecraft\n",
    "X360\n",
    "2013\n",
    "Misc\n",
    "Microsoft Game Studios\n",
    "5.58\n",
    "2.83\n",
    "0.02\n",
    "0.77\n",
    "9.2\n",
    "74\n",
    "Animal Crossing: New Leaf\n",
    "3DS\n",
    "2012\n",
    "Simulation\n",
    "Nintendo\n",
    "2.01\n",
    "2.32\n",
    "4.36\n",
    "0.41\n",
    "9.09\n",
    "75\n",
    "Mario Party DS\n",
    "DS\n",
    "2007\n",
    "Misc\n",
    "Nintendo\n",
    "4.46\n",
    "1.88\n",
    "1.98\n",
    "0.7\n",
    "9.02\n",
    "76\n",
    "The Elder Scrolls V: Skyrim\n",
    "X360\n",
    "2011\n",
    "Role-Playing\n",
    "Bethesda Softworks\n",
    "5.03\n",
    "2.86\n",
    "0.1\n",
    "0.85\n",
    "8.84\n",
    "77\n",
    "Super Mario Kart\n",
    "SNES\n",
    "1992\n",
    "Racing\n",
    "Nintendo\n",
    "3.54\n",
    "1.24\n",
    "3.81\n",
    "0.18\n",
    "8.76\n",
    "78\n",
    "FIFA 16\n",
    "PS4\n",
    "2015\n",
    "Sports\n",
    "Electronic Arts\n",
    "1.11\n",
    "6.06\n",
    "0.06\n",
    "1.26\n",
    "8.49\n",
    "79\n",
    "Wii Party\n",
    "Wii\n",
    "2010\n",
    "Misc\n",
    "Nintendo\n",
    "1.79\n",
    "3.53\n",
    "2.49\n",
    "0.68\n",
    "8.49\n",
    "80\n",
    "Halo 2\n",
    "XB\n",
    "2004\n",
    "Shooter\n",
    "Microsoft Game Studios\n",
    "6.82\n",
    "1.53\n",
    "0.05\n",
    "0.08\n",
    "8.49\n",
    "81\n",
    "Mario Party 8\n",
    "Wii\n",
    "2007\n",
    "Misc\n",
    "Nintendo\n",
    "3.81\n",
    "2.3\n",
    "1.58\n",
    "0.73\n",
    "8.42\n",
    "82\n",
    "Pokemon Black 2/Pokemon White 2\n",
    "DS\n",
    "2012\n",
    "Role-Playing\n",
    "Nintendo\n",
    "2.91\n",
    "1.86\n",
    "3.14\n",
    "0.43\n",
    "8.33\n",
    "83\n",
    "FIFA Soccer 13\n",
    "PS3\n",
    "2012\n",
    "Action\n",
    "Electronic Arts\n",
    "1.06\n",
    "5.05\n",
    "0.13\n",
    "2.01\n",
    "8.24\n",
    "84\n",
    "The Sims 3\n",
    "PC\n",
    "2009\n",
    "Simulation\n",
    "Electronic Arts\n",
    "0.98\n",
    "6.42\n",
    "0\n",
    "0.71\n",
    "8.11\n",
    "85\n",
    "GoldenEye 007\n",
    "N64\n",
    "1997\n",
    "Shooter\n",
    "Nintendo\n",
    "5.8\n",
    "2.01\n",
    "0.13\n",
    "0.15\n",
    "8.09\n",
    "86\n",
    "Mario & Sonic at the Olympic Games\n",
    "Wii\n",
    "2007\n",
    "Sports\n",
    "Sega\n",
    "2.58\n",
    "3.9\n",
    "0.66\n",
    "0.91\n",
    "8.06\n",
    "87\n",
    "Final Fantasy X\n",
    "PS2\n",
    "2001\n",
    "Role-Playing\n",
    "Sony Computer Entertainment\n",
    "2.91\n",
    "2.07\n",
    "2.73\n",
    "0.33\n",
    "8.05\n",
    "88\n",
    "Final Fantasy VIII\n",
    "PS\n",
    "1999\n",
    "Role-Playing\n",
    "SquareSoft\n",
    "2.28\n",
    "1.72\n",
    "3.63\n",
    "0.23\n",
    "7.86\n",
    "89\n",
    "Pokémon Platinum Version\n",
    "DS\n",
    "2008\n",
    "Role-Playing\n",
    "Nintendo\n",
    "2.82\n",
    "1.78\n",
    "2.69\n",
    "0.55\n",
    "7.84\n",
    "90\n",
    "Pac-Man\n",
    "2600\n",
    "1982\n",
    "Puzzle\n",
    "Atari\n",
    "7.28\n",
    "0.45\n",
    "0\n",
    "0.08\n",
    "7.81\n",
    "91\n",
    "Grand Theft Auto: Liberty City Stories\n",
    "PSP\n",
    "2005\n",
    "Action\n",
    "Take-Two Interactive\n",
    "2.9\n",
    "2.83\n",
    "0.24\n",
    "1.75\n",
    "7.72\n",
    "92\n",
    "Super Mario Galaxy 2\n",
    "Wii\n",
    "2010\n",
    "Platform\n",
    "Nintendo\n",
    "3.66\n",
    "2.42\n",
    "0.98\n",
    "0.64\n",
    "7.69\n",
    "93\n",
    "Star Wars Battlefront (2015)\n",
    "PS4\n",
    "2015\n",
    "Shooter\n",
    "Electronic Arts\n",
    "2.93\n",
    "3.29\n",
    "0.22\n",
    "1.23\n",
    "7.67\n",
    "94\n",
    "Call of Duty: Advanced Warfare\n",
    "PS4\n",
    "2014\n",
    "Shooter\n",
    "Activision\n",
    "2.8\n",
    "3.3\n",
    "0.14\n",
    "1.37\n",
    "7.6\n",
    "95\n",
    "The Legend of Zelda: Ocarina of Time\n",
    "N64\n",
    "1998\n",
    "Action\n",
    "Nintendo\n",
    "4.1\n",
    "1.89\n",
    "1.45\n",
    "0.16\n",
    "7.6\n",
    "96\n",
    "Crash Bandicoot 2: Cortex Strikes Back\n",
    "PS\n",
    "1997\n",
    "Platform\n",
    "Sony Computer Entertainment\n",
    "3.78\n",
    "2.17\n",
    "1.31\n",
    "0.31\n",
    "7.58\n",
    "97\n",
    "Super Mario Bros. 2\n",
    "NES\n",
    "1988\n",
    "Platform\n",
    "Nintendo\n",
    "5.39\n",
    "1.18\n",
    "0.7\n",
    "0.19\n",
    "7.46\n",
    "98\n",
    "Super Smash Bros. for Wii U and 3DS\n",
    "3DS\n",
    "2014\n",
    "Fighting\n",
    "Nintendo\n",
    "3.24\n",
    "1.35\n",
    "2.42\n",
    "0.43\n",
    "7.45\n",
    "99\n",
    "Call of Duty: World at War\n",
    "X360\n",
    "2008\n",
    "Shooter\n",
    "Activision\n",
    "4.79\n",
    "1.9\n",
    "0\n",
    "0.69\n",
    "7.37\n",
    "100\n",
    "Battlefield 3\n",
    "X360\n",
    "2011\n",
    "Shooter\n",
    "Electronic Arts\n",
    "4.46\n",
    "2.13\n",
    "0.06\n",
    "0.69\n",
    "7.34\n",
    "Data Dictionary\n",
    "Column\tExplanation\n",
    "Rank\tRanking of overall sales\n",
    "Name\tName of the game\n",
    "Platform\tPlatform of the games release (i.e. PC,PS4, etc.)\n",
    "Year\tYear the game was released in\n",
    "Genre\tGenre of the game\n",
    "Publisher\tPublisher of the game\n",
    "NA_Sales\tNumber of sales in North America (in millions)\n",
    "EU_Sales\tNumber of sales in Europe (in millions)\n",
    "JP_Sales\tNumber of sales in Japan (in millions)\n",
    "Other_Sales\tNumber of sales in other parts of the world (in millions)\n",
    "Global_Sales\tNumber of total sales (in millions)\n",
    "Source of dataset.\n",
    "\n",
    "Don't know where to start?\n",
    "Challenges are brief tasks designed to help you practice specific skills:\n",
    "\n",
    "🗺️ Explore: Which of the three seventh generation consoles (Xbox 360, Playstation 3, and Nintendo Wii) had the highest total sales globally?\n",
    "📊 Visualize: Create a plot visualizing the average sales for games in the most popular three genres. Differentiate between NA, EU, and global sales.\n",
    "🔎 Analyze: Are some genres significantly more likely to perform better or worse in Japan than others? If so, which ones?\n",
    "Scenarios are broader questions to help you develop an end-to-end project for your portfolio:\n",
    "\n",
    "You are working as a data analyst for a video game retailer based in Japan. The retailer typically orders games based on sales in North America and Europe, as the games are often released later in Japan. However, they have found that North American and European sales are not always a perfect predictor of how a game will sell in Japan.\n",
    "\n",
    "Your manager has asked you to develop a model that can predict the sales in Japan using sales in North America and Europe and other attributes such as the name of the game, the platform, the genre, and the publisher.\n",
    "\n",
    "You will need to prepare a report that is accessible to a broad audience. It should outline your motivation, steps, findings, and conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soccer Through the Ages\n",
    "This dataset contains information on international soccer games throughout the years. It includes results of soccer games and information about the players who scored the goals. The dataset contains data from 1872 up to 2023.\n",
    "\n",
    "💾 The data\n",
    "data/results.csv - CSV with results of soccer games between 1872 and 2023\n",
    "home_score - The score of the home team, excluding penalty shootouts\n",
    "away_score - The score of the away team, excluding penalty shootouts\n",
    "tournament - The name of the tournament\n",
    "city - The name of the city where the game was played\n",
    "country - The name of the country where the game was played\n",
    "neutral - Whether the game was played at a neutral venue or not\n",
    "data/shootouts.csv - CSV with results of penalty shootouts in the soccer games\n",
    "winner - The team that won the penalty shootout\n",
    "data/goalscorers.csv - CSV with information on goal scorers of some of the soccer games in the results CSV\n",
    "team - The team that scored the goal\n",
    "scorer - The player who scored the goal\n",
    "minute - The minute in the game when the goal was scored\n",
    "own_goal - Whether it was an own goal or not\n",
    "penalty - Whether the goal was scored as a penalty or not\n",
    "The following columns can be found in all datasets:\n",
    "\n",
    "date - The date of the soccer game\n",
    "home_team - The team that played at home\n",
    "away_team - The team that played away\n",
    "These shared columns fully identify the game that was played and can be used to join data between the different CSV files.\n",
    "\n",
    "Source: GitHub\n",
    "\n",
    "📊 Some guiding questions and visualization to help you explore this data:\n",
    "Which are the 15 countries that have won the most games since 1960? Show them in a horizontal bar plot.\n",
    "How many goals are scored in total in each minute of the game? Show this in a bar plot, with the minutes on the x-axis. If you're up for the challenge, you could even create an animated Plotly plot that shows how the distribution has changed over the years.\n",
    "Which 10 players have scored the most hat-tricks?\n",
    "What is the proportion of games won by each team at home and away? What is the difference between the proportions?\n",
    "How many games have been won by the home team? And by the away team?\n",
    "💼 Develop a case study for your portfolio\n",
    "After exploring the data, you can create a comprehensive case study using this dataset. We have provided an example objective below, but feel free to come up with your own - the world is your oyster!\n",
    "\n",
    "Example objective: The UEFA Euro 2024 tournament is approaching. Utilize the historical data to construct a predictive model that forecasts potential outcomes of the tournament based on the team draws. Since the draws are not known yet, you should be able to configure them as variables in your notebook.\n",
    "\n",
    "You can query the pre-loaded CSV files using SQL directly. Here’s a sample query:\n",
    "\n",
    "Dataframes and CSVs\n",
    "DataFrame available as\n",
    "results\n",
    "10\n",
    "SELECT\n",
    "    *\n",
    "FROM 'data/results.csv'\n",
    "LIMIT 10\n",
    "date\n",
    "home_team\n",
    "away_team\n",
    "home_score\n",
    "away_score\n",
    "tournament\n",
    "city\n",
    "country\n",
    "neutral\n",
    "0\n",
    "1872-11-30T00:00:00.000\n",
    "Scotland\n",
    "England\n",
    "0\n",
    "0\n",
    "Friendly\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "1\n",
    "1873-03-08T00:00:00.000\n",
    "England\n",
    "Scotland\n",
    "4\n",
    "2\n",
    "Friendly\n",
    "London\n",
    "England\n",
    "False\n",
    "2\n",
    "1874-03-07T00:00:00.000\n",
    "Scotland\n",
    "England\n",
    "2\n",
    "1\n",
    "Friendly\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "3\n",
    "1875-03-06T00:00:00.000\n",
    "England\n",
    "Scotland\n",
    "2\n",
    "2\n",
    "Friendly\n",
    "London\n",
    "England\n",
    "False\n",
    "4\n",
    "1876-03-04T00:00:00.000\n",
    "Scotland\n",
    "England\n",
    "3\n",
    "0\n",
    "Friendly\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "5\n",
    "1876-03-25T00:00:00.000\n",
    "Scotland\n",
    "Wales\n",
    "4\n",
    "0\n",
    "Friendly\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "6\n",
    "1877-03-03T00:00:00.000\n",
    "England\n",
    "Scotland\n",
    "1\n",
    "3\n",
    "Friendly\n",
    "London\n",
    "England\n",
    "False\n",
    "7\n",
    "1877-03-05T00:00:00.000\n",
    "Wales\n",
    "Scotland\n",
    "0\n",
    "2\n",
    "Friendly\n",
    "Wrexham\n",
    "Wales\n",
    "False\n",
    "8\n",
    "1878-03-02T00:00:00.000\n",
    "Scotland\n",
    "England\n",
    "7\n",
    "2\n",
    "Friendly\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "9\n",
    "1878-03-23T00:00:00.000\n",
    "Scotland\n",
    "Wales\n",
    "9\n",
    "0\n",
    "Friendly\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "You can also use SQL cells to join the tables:\n",
    "\n",
    "Dataframes and CSVs\n",
    "DataFrame available as\n",
    "goalscorers_joined\n",
    "SELECT\n",
    "    *\n",
    "FROM 'data/goalscorers.csv'\n",
    "INNER JOIN 'data/results.csv' USING (date, home_team, away_team)\n",
    "LEFT JOIN 'data/shootouts.csv' USING (date, home_team, away_team)\n",
    "LIMIT 10\n",
    "date\n",
    "home_team\n",
    "away_team\n",
    "team\n",
    "scorer\n",
    "minute\n",
    "own_goal\n",
    "penalty\n",
    "home_score\n",
    "away_score\n",
    "tournament\n",
    "city\n",
    "country\n",
    "neutral\n",
    "winner\n",
    "0\n",
    "1916-07-02T00:00:00.000\n",
    "Chile\n",
    "Uruguay\n",
    "Uruguay\n",
    "José Piendibene\n",
    "75\n",
    "False\n",
    "False\n",
    "0\n",
    "4\n",
    "Copa América\n",
    "Buenos Aires\n",
    "Argentina\n",
    "True\n",
    "null\n",
    "1\n",
    "1916-07-06T00:00:00.000\n",
    "Argentina\n",
    "Chile\n",
    "Argentina\n",
    "Alberto Marcovecchio\n",
    "81\n",
    "False\n",
    "False\n",
    "6\n",
    "1\n",
    "Copa América\n",
    "Buenos Aires\n",
    "Argentina\n",
    "False\n",
    "null\n",
    "2\n",
    "1916-07-10T00:00:00.000\n",
    "Argentina\n",
    "Brazil\n",
    "Brazil\n",
    "Manoel Alencar Monte\n",
    "23\n",
    "False\n",
    "False\n",
    "1\n",
    "1\n",
    "Copa América\n",
    "Buenos Aires\n",
    "Argentina\n",
    "False\n",
    "null\n",
    "3\n",
    "1916-07-12T00:00:00.000\n",
    "Brazil\n",
    "Uruguay\n",
    "Uruguay\n",
    "Jose Tognola\n",
    "77\n",
    "False\n",
    "False\n",
    "1\n",
    "2\n",
    "Copa América\n",
    "Buenos Aires\n",
    "Argentina\n",
    "True\n",
    "null\n",
    "4\n",
    "1917-09-30T00:00:00.000\n",
    "Uruguay\n",
    "Chile\n",
    "Uruguay\n",
    "Ángel Romano\n",
    "75\n",
    "False\n",
    "False\n",
    "4\n",
    "0\n",
    "Copa América\n",
    "Montevideo\n",
    "Uruguay\n",
    "False\n",
    "null\n",
    "5\n",
    "1917-10-03T00:00:00.000\n",
    "Argentina\n",
    "Brazil\n",
    "Argentina\n",
    "Antonio Blanco\n",
    "80\n",
    "False\n",
    "False\n",
    "4\n",
    "2\n",
    "Copa América\n",
    "Montevideo\n",
    "Uruguay\n",
    "True\n",
    "null\n",
    "6\n",
    "1917-10-06T00:00:00.000\n",
    "Argentina\n",
    "Chile\n",
    "Argentina\n",
    "Luis García\n",
    "76\n",
    "True\n",
    "False\n",
    "1\n",
    "0\n",
    "Copa América\n",
    "Montevideo\n",
    "Uruguay\n",
    "True\n",
    "null\n",
    "7\n",
    "1917-10-07T00:00:00.000\n",
    "Uruguay\n",
    "Brazil\n",
    "Uruguay\n",
    "Carlos Scarone\n",
    "86\n",
    "False\n",
    "False\n",
    "4\n",
    "0\n",
    "Copa América\n",
    "Montevideo\n",
    "Uruguay\n",
    "False\n",
    "null\n",
    "8\n",
    "1917-10-12T00:00:00.000\n",
    "Brazil\n",
    "Chile\n",
    "Brazil\n",
    "Haroldo Domingues\n",
    "59\n",
    "False\n",
    "False\n",
    "5\n",
    "0\n",
    "Copa América\n",
    "Montevideo\n",
    "Uruguay\n",
    "True\n",
    "null\n",
    "9\n",
    "1917-10-14T00:00:00.000\n",
    "Uruguay\n",
    "Argentina\n",
    "Uruguay\n",
    "Héctor Scarone\n",
    "62\n",
    "False\n",
    "False\n",
    "1\n",
    "0\n",
    "Copa América\n",
    "Montevideo\n",
    "Uruguay\n",
    "False\n",
    "null\n",
    "Alternatively, you can import the data using pandas, for example:\n",
    "\n",
    "import pandas as pd\n",
    "results = pd.read_csv(\"data/results.csv\")\n",
    "print(results.shape)\n",
    "results.head(100)\n",
    "(44934, 9)\n",
    "date\n",
    "home_team\n",
    "away_team\n",
    "home_score\n",
    "away_score\n",
    "tournament\n",
    "city\n",
    "country\n",
    "neutral\n",
    "0\n",
    "1872-11-30\n",
    "Scotland\n",
    "England\n",
    "0\n",
    "0\n",
    "Friendly\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "1\n",
    "1873-03-08\n",
    "England\n",
    "Scotland\n",
    "4\n",
    "2\n",
    "Friendly\n",
    "London\n",
    "England\n",
    "False\n",
    "2\n",
    "1874-03-07\n",
    "Scotland\n",
    "England\n",
    "2\n",
    "1\n",
    "Friendly\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "3\n",
    "1875-03-06\n",
    "England\n",
    "Scotland\n",
    "2\n",
    "2\n",
    "Friendly\n",
    "London\n",
    "England\n",
    "False\n",
    "4\n",
    "1876-03-04\n",
    "Scotland\n",
    "England\n",
    "3\n",
    "0\n",
    "Friendly\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "5\n",
    "1876-03-25\n",
    "Scotland\n",
    "Wales\n",
    "4\n",
    "0\n",
    "Friendly\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "6\n",
    "1877-03-03\n",
    "England\n",
    "Scotland\n",
    "1\n",
    "3\n",
    "Friendly\n",
    "London\n",
    "England\n",
    "False\n",
    "7\n",
    "1877-03-05\n",
    "Wales\n",
    "Scotland\n",
    "0\n",
    "2\n",
    "Friendly\n",
    "Wrexham\n",
    "Wales\n",
    "False\n",
    "8\n",
    "1878-03-02\n",
    "Scotland\n",
    "England\n",
    "7\n",
    "2\n",
    "Friendly\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "9\n",
    "1878-03-23\n",
    "Scotland\n",
    "Wales\n",
    "9\n",
    "0\n",
    "Friendly\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "10\n",
    "1879-01-18\n",
    "England\n",
    "Wales\n",
    "2\n",
    "1\n",
    "Friendly\n",
    "London\n",
    "England\n",
    "False\n",
    "11\n",
    "1879-04-05\n",
    "England\n",
    "Scotland\n",
    "5\n",
    "4\n",
    "Friendly\n",
    "London\n",
    "England\n",
    "False\n",
    "12\n",
    "1879-04-07\n",
    "Wales\n",
    "Scotland\n",
    "0\n",
    "3\n",
    "Friendly\n",
    "Wrexham\n",
    "Wales\n",
    "False\n",
    "13\n",
    "1880-03-13\n",
    "Scotland\n",
    "England\n",
    "5\n",
    "4\n",
    "Friendly\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "14\n",
    "1880-03-15\n",
    "Wales\n",
    "England\n",
    "2\n",
    "3\n",
    "Friendly\n",
    "Wrexham\n",
    "Wales\n",
    "False\n",
    "15\n",
    "1880-03-27\n",
    "Scotland\n",
    "Wales\n",
    "5\n",
    "1\n",
    "Friendly\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "16\n",
    "1881-02-26\n",
    "England\n",
    "Wales\n",
    "0\n",
    "1\n",
    "Friendly\n",
    "Blackburn\n",
    "England\n",
    "False\n",
    "17\n",
    "1881-03-12\n",
    "England\n",
    "Scotland\n",
    "1\n",
    "6\n",
    "Friendly\n",
    "London\n",
    "England\n",
    "False\n",
    "18\n",
    "1881-03-14\n",
    "Wales\n",
    "Scotland\n",
    "1\n",
    "5\n",
    "Friendly\n",
    "Wrexham\n",
    "Wales\n",
    "False\n",
    "19\n",
    "1882-02-18\n",
    "Northern Ireland\n",
    "England\n",
    "0\n",
    "13\n",
    "Friendly\n",
    "Belfast\n",
    "Ireland\n",
    "False\n",
    "20\n",
    "1882-02-25\n",
    "Wales\n",
    "Northern Ireland\n",
    "7\n",
    "1\n",
    "Friendly\n",
    "Wrexham\n",
    "Wales\n",
    "False\n",
    "21\n",
    "1882-03-11\n",
    "Scotland\n",
    "England\n",
    "5\n",
    "1\n",
    "Friendly\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "22\n",
    "1882-03-13\n",
    "Wales\n",
    "England\n",
    "5\n",
    "3\n",
    "Friendly\n",
    "Wrexham\n",
    "Wales\n",
    "False\n",
    "23\n",
    "1882-03-25\n",
    "Scotland\n",
    "Wales\n",
    "5\n",
    "0\n",
    "Friendly\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "24\n",
    "1883-02-03\n",
    "England\n",
    "Wales\n",
    "5\n",
    "0\n",
    "Friendly\n",
    "London\n",
    "England\n",
    "False\n",
    "25\n",
    "1883-02-24\n",
    "England\n",
    "Northern Ireland\n",
    "7\n",
    "0\n",
    "Friendly\n",
    "Liverpool\n",
    "England\n",
    "False\n",
    "26\n",
    "1883-03-10\n",
    "England\n",
    "Scotland\n",
    "2\n",
    "3\n",
    "Friendly\n",
    "Sheffield\n",
    "England\n",
    "False\n",
    "27\n",
    "1883-03-12\n",
    "Wales\n",
    "Scotland\n",
    "0\n",
    "3\n",
    "Friendly\n",
    "Wrexham\n",
    "Wales\n",
    "False\n",
    "28\n",
    "1883-03-17\n",
    "Northern Ireland\n",
    "Wales\n",
    "1\n",
    "1\n",
    "Friendly\n",
    "Belfast\n",
    "Ireland\n",
    "False\n",
    "29\n",
    "1884-01-26\n",
    "Northern Ireland\n",
    "Scotland\n",
    "0\n",
    "5\n",
    "British Home Championship\n",
    "Belfast\n",
    "Ireland\n",
    "False\n",
    "30\n",
    "1884-02-09\n",
    "Wales\n",
    "Northern Ireland\n",
    "6\n",
    "0\n",
    "British Home Championship\n",
    "Wrexham\n",
    "Wales\n",
    "False\n",
    "31\n",
    "1884-02-23\n",
    "Northern Ireland\n",
    "England\n",
    "1\n",
    "8\n",
    "British Home Championship\n",
    "Belfast\n",
    "Ireland\n",
    "False\n",
    "32\n",
    "1884-03-15\n",
    "Scotland\n",
    "England\n",
    "1\n",
    "0\n",
    "British Home Championship\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "33\n",
    "1884-03-17\n",
    "Wales\n",
    "England\n",
    "0\n",
    "4\n",
    "British Home Championship\n",
    "Wrexham\n",
    "Wales\n",
    "False\n",
    "34\n",
    "1884-03-29\n",
    "Scotland\n",
    "Wales\n",
    "4\n",
    "1\n",
    "British Home Championship\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "35\n",
    "1885-02-28\n",
    "England\n",
    "Northern Ireland\n",
    "4\n",
    "0\n",
    "British Home Championship\n",
    "Manchester\n",
    "England\n",
    "False\n",
    "36\n",
    "1885-03-14\n",
    "England\n",
    "Wales\n",
    "1\n",
    "1\n",
    "British Home Championship\n",
    "Blackburn\n",
    "England\n",
    "False\n",
    "37\n",
    "1885-03-14\n",
    "Scotland\n",
    "Northern Ireland\n",
    "8\n",
    "2\n",
    "British Home Championship\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "38\n",
    "1885-03-21\n",
    "England\n",
    "Scotland\n",
    "1\n",
    "1\n",
    "British Home Championship\n",
    "London\n",
    "England\n",
    "False\n",
    "39\n",
    "1885-03-23\n",
    "Wales\n",
    "Scotland\n",
    "1\n",
    "8\n",
    "British Home Championship\n",
    "Wrexham\n",
    "Wales\n",
    "False\n",
    "40\n",
    "1885-04-11\n",
    "Northern Ireland\n",
    "Wales\n",
    "2\n",
    "8\n",
    "British Home Championship\n",
    "Belfast\n",
    "Ireland\n",
    "False\n",
    "41\n",
    "1885-11-28\n",
    "United States\n",
    "Canada\n",
    "0\n",
    "1\n",
    "Friendly\n",
    "Newark\n",
    "United States\n",
    "False\n",
    "42\n",
    "1886-02-27\n",
    "Wales\n",
    "Northern Ireland\n",
    "5\n",
    "0\n",
    "British Home Championship\n",
    "Wrexham\n",
    "Wales\n",
    "False\n",
    "43\n",
    "1886-03-13\n",
    "Northern Ireland\n",
    "England\n",
    "1\n",
    "6\n",
    "British Home Championship\n",
    "Belfast\n",
    "Ireland\n",
    "False\n",
    "44\n",
    "1886-03-20\n",
    "Northern Ireland\n",
    "Scotland\n",
    "2\n",
    "7\n",
    "British Home Championship\n",
    "Belfast\n",
    "Ireland\n",
    "False\n",
    "45\n",
    "1886-03-27\n",
    "Scotland\n",
    "England\n",
    "1\n",
    "1\n",
    "British Home Championship\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "46\n",
    "1886-03-29\n",
    "Wales\n",
    "England\n",
    "1\n",
    "3\n",
    "British Home Championship\n",
    "Wrexham\n",
    "Wales\n",
    "False\n",
    "47\n",
    "1886-04-10\n",
    "Scotland\n",
    "Wales\n",
    "4\n",
    "1\n",
    "British Home Championship\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "48\n",
    "1886-11-25\n",
    "United States\n",
    "Canada\n",
    "3\n",
    "2\n",
    "Friendly\n",
    "Newark\n",
    "United States\n",
    "False\n",
    "49\n",
    "1887-02-05\n",
    "England\n",
    "Northern Ireland\n",
    "7\n",
    "0\n",
    "British Home Championship\n",
    "Sheffield\n",
    "England\n",
    "False\n",
    "50\n",
    "1887-02-19\n",
    "Scotland\n",
    "Northern Ireland\n",
    "4\n",
    "1\n",
    "British Home Championship\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "51\n",
    "1887-02-26\n",
    "England\n",
    "Wales\n",
    "4\n",
    "0\n",
    "British Home Championship\n",
    "London\n",
    "England\n",
    "False\n",
    "52\n",
    "1887-03-12\n",
    "Northern Ireland\n",
    "Wales\n",
    "4\n",
    "1\n",
    "British Home Championship\n",
    "Belfast\n",
    "Ireland\n",
    "False\n",
    "53\n",
    "1887-03-19\n",
    "England\n",
    "Scotland\n",
    "2\n",
    "3\n",
    "British Home Championship\n",
    "Blackburn\n",
    "England\n",
    "False\n",
    "54\n",
    "1887-03-21\n",
    "Wales\n",
    "Scotland\n",
    "0\n",
    "2\n",
    "British Home Championship\n",
    "Wrexham\n",
    "Wales\n",
    "False\n",
    "55\n",
    "1888-02-04\n",
    "England\n",
    "Wales\n",
    "5\n",
    "1\n",
    "British Home Championship\n",
    "Crewe\n",
    "England\n",
    "False\n",
    "56\n",
    "1888-03-03\n",
    "Wales\n",
    "Northern Ireland\n",
    "11\n",
    "0\n",
    "British Home Championship\n",
    "Wrexham\n",
    "Wales\n",
    "False\n",
    "57\n",
    "1888-03-10\n",
    "Scotland\n",
    "Wales\n",
    "5\n",
    "1\n",
    "British Home Championship\n",
    "Edinburgh\n",
    "Scotland\n",
    "False\n",
    "58\n",
    "1888-03-17\n",
    "Scotland\n",
    "England\n",
    "0\n",
    "5\n",
    "British Home Championship\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "59\n",
    "1888-03-24\n",
    "Northern Ireland\n",
    "Scotland\n",
    "2\n",
    "10\n",
    "British Home Championship\n",
    "Belfast\n",
    "Ireland\n",
    "False\n",
    "60\n",
    "1888-04-07\n",
    "Northern Ireland\n",
    "England\n",
    "1\n",
    "5\n",
    "British Home Championship\n",
    "Belfast\n",
    "Ireland\n",
    "False\n",
    "61\n",
    "1888-09-19\n",
    "Scotland\n",
    "Canada\n",
    "4\n",
    "0\n",
    "Friendly\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "62\n",
    "1889-02-23\n",
    "England\n",
    "Wales\n",
    "4\n",
    "1\n",
    "British Home Championship\n",
    "Stoke-on-Trent\n",
    "England\n",
    "False\n",
    "63\n",
    "1889-03-02\n",
    "England\n",
    "Northern Ireland\n",
    "6\n",
    "1\n",
    "British Home Championship\n",
    "Liverpool\n",
    "England\n",
    "False\n",
    "64\n",
    "1889-03-09\n",
    "Scotland\n",
    "Northern Ireland\n",
    "7\n",
    "0\n",
    "British Home Championship\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "65\n",
    "1889-04-13\n",
    "England\n",
    "Scotland\n",
    "2\n",
    "3\n",
    "British Home Championship\n",
    "London\n",
    "England\n",
    "False\n",
    "66\n",
    "1889-04-15\n",
    "Wales\n",
    "Scotland\n",
    "0\n",
    "0\n",
    "British Home Championship\n",
    "Wrexham\n",
    "Wales\n",
    "False\n",
    "67\n",
    "1889-04-27\n",
    "Northern Ireland\n",
    "Wales\n",
    "1\n",
    "3\n",
    "British Home Championship\n",
    "Belfast\n",
    "Ireland\n",
    "False\n",
    "68\n",
    "1890-02-08\n",
    "Wales\n",
    "Northern Ireland\n",
    "5\n",
    "2\n",
    "British Home Championship\n",
    "Shrewsbury\n",
    "England\n",
    "True\n",
    "69\n",
    "1890-03-15\n",
    "Northern Ireland\n",
    "England\n",
    "1\n",
    "9\n",
    "British Home Championship\n",
    "Belfast\n",
    "Ireland\n",
    "False\n",
    "70\n",
    "1890-03-15\n",
    "Wales\n",
    "England\n",
    "1\n",
    "3\n",
    "British Home Championship\n",
    "Wrexham\n",
    "Wales\n",
    "False\n",
    "71\n",
    "1890-03-22\n",
    "Scotland\n",
    "Wales\n",
    "5\n",
    "0\n",
    "British Home Championship\n",
    "Paisley\n",
    "Scotland\n",
    "False\n",
    "72\n",
    "1890-03-29\n",
    "Northern Ireland\n",
    "Scotland\n",
    "1\n",
    "4\n",
    "British Home Championship\n",
    "Belfast\n",
    "Ireland\n",
    "False\n",
    "73\n",
    "1890-04-05\n",
    "Scotland\n",
    "England\n",
    "1\n",
    "1\n",
    "British Home Championship\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "74\n",
    "1891-02-07\n",
    "Northern Ireland\n",
    "Wales\n",
    "7\n",
    "2\n",
    "British Home Championship\n",
    "Belfast\n",
    "Ireland\n",
    "False\n",
    "75\n",
    "1891-03-07\n",
    "England\n",
    "Wales\n",
    "4\n",
    "1\n",
    "British Home Championship\n",
    "Sunderland\n",
    "England\n",
    "False\n",
    "76\n",
    "1891-03-07\n",
    "England\n",
    "Northern Ireland\n",
    "6\n",
    "1\n",
    "British Home Championship\n",
    "Wolverhampton\n",
    "England\n",
    "False\n",
    "77\n",
    "1891-03-21\n",
    "Wales\n",
    "Scotland\n",
    "3\n",
    "4\n",
    "British Home Championship\n",
    "Wrexham\n",
    "Wales\n",
    "False\n",
    "78\n",
    "1891-03-28\n",
    "Scotland\n",
    "Northern Ireland\n",
    "2\n",
    "1\n",
    "British Home Championship\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "79\n",
    "1891-04-06\n",
    "England\n",
    "Scotland\n",
    "2\n",
    "1\n",
    "British Home Championship\n",
    "Blackburn\n",
    "England\n",
    "False\n",
    "80\n",
    "1892-02-27\n",
    "Wales\n",
    "Northern Ireland\n",
    "1\n",
    "1\n",
    "British Home Championship\n",
    "Bangor\n",
    "Wales\n",
    "False\n",
    "81\n",
    "1892-03-05\n",
    "Northern Ireland\n",
    "England\n",
    "0\n",
    "2\n",
    "British Home Championship\n",
    "Belfast\n",
    "Ireland\n",
    "False\n",
    "82\n",
    "1892-03-05\n",
    "Wales\n",
    "England\n",
    "0\n",
    "2\n",
    "British Home Championship\n",
    "Wrexham\n",
    "Wales\n",
    "False\n",
    "83\n",
    "1892-03-19\n",
    "Northern Ireland\n",
    "Scotland\n",
    "2\n",
    "3\n",
    "British Home Championship\n",
    "Belfast\n",
    "Ireland\n",
    "False\n",
    "84\n",
    "1892-03-26\n",
    "Scotland\n",
    "Wales\n",
    "6\n",
    "1\n",
    "British Home Championship\n",
    "Edinburgh\n",
    "Scotland\n",
    "False\n",
    "85\n",
    "1892-04-02\n",
    "Scotland\n",
    "England\n",
    "1\n",
    "4\n",
    "British Home Championship\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "86\n",
    "1893-02-25\n",
    "England\n",
    "Northern Ireland\n",
    "6\n",
    "1\n",
    "British Home Championship\n",
    "Birmingham\n",
    "England\n",
    "False\n",
    "87\n",
    "1893-03-13\n",
    "England\n",
    "Wales\n",
    "6\n",
    "0\n",
    "British Home Championship\n",
    "Stoke-on-Trent\n",
    "England\n",
    "False\n",
    "88\n",
    "1893-03-18\n",
    "Wales\n",
    "Scotland\n",
    "0\n",
    "8\n",
    "British Home Championship\n",
    "Wrexham\n",
    "Wales\n",
    "False\n",
    "89\n",
    "1893-03-25\n",
    "Scotland\n",
    "Northern Ireland\n",
    "6\n",
    "1\n",
    "British Home Championship\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "90\n",
    "1893-04-01\n",
    "England\n",
    "Scotland\n",
    "5\n",
    "2\n",
    "British Home Championship\n",
    "Richmond\n",
    "England\n",
    "False\n",
    "91\n",
    "1894-02-24\n",
    "Wales\n",
    "Northern Ireland\n",
    "4\n",
    "1\n",
    "British Home Championship\n",
    "Swansea\n",
    "Wales\n",
    "False\n",
    "92\n",
    "1894-03-03\n",
    "Northern Ireland\n",
    "England\n",
    "2\n",
    "2\n",
    "British Home Championship\n",
    "Belfast\n",
    "Ireland\n",
    "False\n",
    "93\n",
    "1894-03-12\n",
    "Wales\n",
    "England\n",
    "1\n",
    "5\n",
    "British Home Championship\n",
    "Wrexham\n",
    "Wales\n",
    "False\n",
    "94\n",
    "1894-03-24\n",
    "Scotland\n",
    "Wales\n",
    "5\n",
    "2\n",
    "British Home Championship\n",
    "Kilmarnock\n",
    "Scotland\n",
    "False\n",
    "95\n",
    "1894-03-31\n",
    "Northern Ireland\n",
    "Scotland\n",
    "1\n",
    "2\n",
    "British Home Championship\n",
    "Belfast\n",
    "Ireland\n",
    "False\n",
    "96\n",
    "1894-04-07\n",
    "Scotland\n",
    "England\n",
    "2\n",
    "2\n",
    "British Home Championship\n",
    "Glasgow\n",
    "Scotland\n",
    "False\n",
    "97\n",
    "1895-03-09\n",
    "England\n",
    "Northern Ireland\n",
    "9\n",
    "0\n",
    "British Home Championship\n",
    "Derby\n",
    "England\n",
    "False\n",
    "98\n",
    "1895-03-16\n",
    "Northern Ireland\n",
    "Wales\n",
    "2\n",
    "2\n",
    "British Home Championship\n",
    "Belfast\n",
    "Ireland\n",
    "False\n",
    "99\n",
    "1895-03-18\n",
    "England\n",
    "Wales\n",
    "1\n",
    "1\n",
    "British Home Championship\n",
    "London\n",
    "England\n",
    "False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stock Exchange Data\n",
    "This dataset consists of stock exchange data since 1965 for several indexes. It contains the daily stock prices along with the volume traded each day.\n",
    "\n",
    "Not sure where to begin? Scroll to the bottom to find challenges!\n",
    "\n",
    "import pandas as pd\n",
    "stock = pd.read_csv(\"stock_data.csv\", index_col=None)\n",
    "print(stock.shape)\n",
    "stock.head(100)\n",
    "(104224, 9)\n",
    "Index\n",
    "Date\n",
    "Open\n",
    "High\n",
    "Low\n",
    "Close\n",
    "Adj Close\n",
    "Volume\n",
    "CloseUSD\n",
    "0\n",
    "HSI\n",
    "1986-12-31\n",
    "2568.300049\n",
    "2568.300049\n",
    "2568.300049\n",
    "2568.300049\n",
    "2568.300049\n",
    "0\n",
    "333.87900637\n",
    "1\n",
    "HSI\n",
    "1987-01-02\n",
    "2540.100098\n",
    "2540.100098\n",
    "2540.100098\n",
    "2540.100098\n",
    "2540.100098\n",
    "0\n",
    "330.21301274\n",
    "2\n",
    "HSI\n",
    "1987-01-05\n",
    "2552.399902\n",
    "2552.399902\n",
    "2552.399902\n",
    "2552.399902\n",
    "2552.399902\n",
    "0\n",
    "331.81198726\n",
    "3\n",
    "HSI\n",
    "1987-01-06\n",
    "2583.899902\n",
    "2583.899902\n",
    "2583.899902\n",
    "2583.899902\n",
    "2583.899902\n",
    "0\n",
    "335.90698726\n",
    "4\n",
    "HSI\n",
    "1987-01-07\n",
    "2607.100098\n",
    "2607.100098\n",
    "2607.100098\n",
    "2607.100098\n",
    "2607.100098\n",
    "0\n",
    "338.92301274\n",
    "5\n",
    "HSI\n",
    "1987-01-08\n",
    "2603.300049\n",
    "2603.300049\n",
    "2603.300049\n",
    "2603.300049\n",
    "2603.300049\n",
    "0\n",
    "338.42900637\n",
    "6\n",
    "HSI\n",
    "1987-01-09\n",
    "2561.699951\n",
    "2561.699951\n",
    "2561.699951\n",
    "2561.699951\n",
    "2561.699951\n",
    "0\n",
    "333.02099363\n",
    "7\n",
    "HSI\n",
    "1987-01-12\n",
    "2614.899902\n",
    "2614.899902\n",
    "2614.899902\n",
    "2614.899902\n",
    "2614.899902\n",
    "0\n",
    "339.93698726\n",
    "8\n",
    "HSI\n",
    "1987-01-13\n",
    "2590.800049\n",
    "2590.800049\n",
    "2590.800049\n",
    "2590.800049\n",
    "2590.800049\n",
    "0\n",
    "336.80400637\n",
    "9\n",
    "HSI\n",
    "1987-01-14\n",
    "2578.199951\n",
    "2578.199951\n",
    "2578.199951\n",
    "2578.199951\n",
    "2578.199951\n",
    "0\n",
    "335.16599363\n",
    "10\n",
    "HSI\n",
    "1987-01-15\n",
    "2559.100098\n",
    "2559.100098\n",
    "2559.100098\n",
    "2559.100098\n",
    "2559.100098\n",
    "0\n",
    "332.68301274\n",
    "11\n",
    "HSI\n",
    "1987-01-16\n",
    "2542.600098\n",
    "2542.600098\n",
    "2542.600098\n",
    "2542.600098\n",
    "2542.600098\n",
    "0\n",
    "330.53801274\n",
    "12\n",
    "HSI\n",
    "1987-01-19\n",
    "2460.5\n",
    "2460.5\n",
    "2460.5\n",
    "2460.5\n",
    "2460.5\n",
    "0\n",
    "319.865\n",
    "13\n",
    "HSI\n",
    "1987-01-20\n",
    "2449.899902\n",
    "2449.899902\n",
    "2449.899902\n",
    "2449.899902\n",
    "2449.899902\n",
    "0\n",
    "318.48698726\n",
    "14\n",
    "HSI\n",
    "1987-01-21\n",
    "2533.899902\n",
    "2533.899902\n",
    "2533.899902\n",
    "2533.899902\n",
    "2533.899902\n",
    "0\n",
    "329.40698726\n",
    "15\n",
    "HSI\n",
    "1987-01-22\n",
    "2536.899902\n",
    "2536.899902\n",
    "2536.899902\n",
    "2536.899902\n",
    "2536.899902\n",
    "0\n",
    "329.79698726\n",
    "16\n",
    "HSI\n",
    "1987-01-23\n",
    "2499.399902\n",
    "2499.399902\n",
    "2499.399902\n",
    "2499.399902\n",
    "2499.399902\n",
    "0\n",
    "324.92198726\n",
    "17\n",
    "HSI\n",
    "1987-01-26\n",
    "2484.399902\n",
    "2484.399902\n",
    "2484.399902\n",
    "2484.399902\n",
    "2484.399902\n",
    "0\n",
    "322.97198726\n",
    "18\n",
    "HSI\n",
    "1987-01-27\n",
    "2524\n",
    "2524\n",
    "2524\n",
    "2524\n",
    "2524\n",
    "0\n",
    "328.12\n",
    "19\n",
    "HSI\n",
    "1987-01-28\n",
    "2553.300049\n",
    "2553.300049\n",
    "2553.300049\n",
    "2553.300049\n",
    "2553.300049\n",
    "0\n",
    "331.92900637\n",
    "20\n",
    "HSI\n",
    "1987-02-02\n",
    "2585.199951\n",
    "2585.199951\n",
    "2585.199951\n",
    "2585.199951\n",
    "2585.199951\n",
    "0\n",
    "336.07599363\n",
    "21\n",
    "HSI\n",
    "1987-02-03\n",
    "2606.399902\n",
    "2606.399902\n",
    "2606.399902\n",
    "2606.399902\n",
    "2606.399902\n",
    "0\n",
    "338.83198726\n",
    "22\n",
    "HSI\n",
    "1987-02-04\n",
    "2636.600098\n",
    "2636.600098\n",
    "2636.600098\n",
    "2636.600098\n",
    "2636.600098\n",
    "0\n",
    "342.75801274\n",
    "23\n",
    "HSI\n",
    "1987-02-05\n",
    "2672.399902\n",
    "2672.399902\n",
    "2672.399902\n",
    "2672.399902\n",
    "2672.399902\n",
    "0\n",
    "347.41198726\n",
    "24\n",
    "HSI\n",
    "1987-02-06\n",
    "2673.600098\n",
    "2673.600098\n",
    "2673.600098\n",
    "2673.600098\n",
    "2673.600098\n",
    "0\n",
    "347.56801274\n",
    "25\n",
    "HSI\n",
    "1987-02-09\n",
    "2713.699951\n",
    "2713.699951\n",
    "2713.699951\n",
    "2713.699951\n",
    "2713.699951\n",
    "0\n",
    "352.78099363\n",
    "26\n",
    "HSI\n",
    "1987-02-10\n",
    "2694.899902\n",
    "2694.899902\n",
    "2694.899902\n",
    "2694.899902\n",
    "2694.899902\n",
    "0\n",
    "350.33698726\n",
    "27\n",
    "HSI\n",
    "1987-02-11\n",
    "2739.5\n",
    "2739.5\n",
    "2739.5\n",
    "2739.5\n",
    "2739.5\n",
    "0\n",
    "356.135\n",
    "28\n",
    "HSI\n",
    "1987-02-12\n",
    "2754.699951\n",
    "2754.699951\n",
    "2754.699951\n",
    "2754.699951\n",
    "2754.699951\n",
    "0\n",
    "358.11099363\n",
    "29\n",
    "HSI\n",
    "1987-02-13\n",
    "2740.5\n",
    "2740.5\n",
    "2740.5\n",
    "2740.5\n",
    "2740.5\n",
    "0\n",
    "356.265\n",
    "30\n",
    "HSI\n",
    "1987-02-16\n",
    "2766.100098\n",
    "2766.100098\n",
    "2766.100098\n",
    "2766.100098\n",
    "2766.100098\n",
    "0\n",
    "359.59301274\n",
    "31\n",
    "HSI\n",
    "1987-02-17\n",
    "2792.100098\n",
    "2792.100098\n",
    "2792.100098\n",
    "2792.100098\n",
    "2792.100098\n",
    "0\n",
    "362.97301274\n",
    "32\n",
    "HSI\n",
    "1987-02-18\n",
    "2801.5\n",
    "2801.5\n",
    "2801.5\n",
    "2801.5\n",
    "2801.5\n",
    "0\n",
    "364.195\n",
    "33\n",
    "HSI\n",
    "1987-02-19\n",
    "2775.800049\n",
    "2775.800049\n",
    "2775.800049\n",
    "2775.800049\n",
    "2775.800049\n",
    "0\n",
    "360.85400637\n",
    "34\n",
    "HSI\n",
    "1987-02-20\n",
    "2827.399902\n",
    "2827.399902\n",
    "2827.399902\n",
    "2827.399902\n",
    "2827.399902\n",
    "0\n",
    "367.56198726\n",
    "35\n",
    "HSI\n",
    "1987-02-23\n",
    "2879\n",
    "2879\n",
    "2879\n",
    "2879\n",
    "2879\n",
    "0\n",
    "374.27\n",
    "36\n",
    "HSI\n",
    "1987-02-24\n",
    "2848.199951\n",
    "2848.199951\n",
    "2848.199951\n",
    "2848.199951\n",
    "2848.199951\n",
    "0\n",
    "370.26599363\n",
    "37\n",
    "HSI\n",
    "1987-02-25\n",
    "2873.600098\n",
    "2873.600098\n",
    "2873.600098\n",
    "2873.600098\n",
    "2873.600098\n",
    "0\n",
    "373.56801274\n",
    "38\n",
    "HSI\n",
    "1987-02-26\n",
    "2843.600098\n",
    "2843.600098\n",
    "2843.600098\n",
    "2843.600098\n",
    "2843.600098\n",
    "0\n",
    "369.66801274\n",
    "39\n",
    "HSI\n",
    "1987-02-27\n",
    "2877.899902\n",
    "2877.899902\n",
    "2877.899902\n",
    "2877.899902\n",
    "2877.899902\n",
    "0\n",
    "374.12698726\n",
    "40\n",
    "HSI\n",
    "1987-03-02\n",
    "2894.300049\n",
    "2894.300049\n",
    "2894.300049\n",
    "2894.300049\n",
    "2894.300049\n",
    "0\n",
    "376.25900637\n",
    "41\n",
    "HSI\n",
    "1987-03-03\n",
    "2939.100098\n",
    "2939.100098\n",
    "2939.100098\n",
    "2939.100098\n",
    "2939.100098\n",
    "0\n",
    "382.08301274\n",
    "42\n",
    "HSI\n",
    "1987-03-04\n",
    "2890.899902\n",
    "2890.899902\n",
    "2890.899902\n",
    "2890.899902\n",
    "2890.899902\n",
    "0\n",
    "375.81698726\n",
    "43\n",
    "HSI\n",
    "1987-03-05\n",
    "2798.399902\n",
    "2798.399902\n",
    "2798.399902\n",
    "2798.399902\n",
    "2798.399902\n",
    "0\n",
    "363.79198726\n",
    "44\n",
    "HSI\n",
    "1987-03-06\n",
    "2798.600098\n",
    "2798.600098\n",
    "2798.600098\n",
    "2798.600098\n",
    "2798.600098\n",
    "0\n",
    "363.81801274\n",
    "45\n",
    "HSI\n",
    "1987-03-09\n",
    "2820.399902\n",
    "2820.399902\n",
    "2820.399902\n",
    "2820.399902\n",
    "2820.399902\n",
    "0\n",
    "366.65198726\n",
    "46\n",
    "HSI\n",
    "1987-03-10\n",
    "2731.100098\n",
    "2731.100098\n",
    "2731.100098\n",
    "2731.100098\n",
    "2731.100098\n",
    "0\n",
    "355.04301274\n",
    "47\n",
    "HSI\n",
    "1987-03-11\n",
    "2760.899902\n",
    "2760.899902\n",
    "2760.899902\n",
    "2760.899902\n",
    "2760.899902\n",
    "0\n",
    "358.91698726\n",
    "48\n",
    "HSI\n",
    "1987-03-12\n",
    "2750.100098\n",
    "2750.100098\n",
    "2750.100098\n",
    "2750.100098\n",
    "2750.100098\n",
    "0\n",
    "357.51301274\n",
    "49\n",
    "HSI\n",
    "1987-03-13\n",
    "2721.199951\n",
    "2721.199951\n",
    "2721.199951\n",
    "2721.199951\n",
    "2721.199951\n",
    "0\n",
    "353.75599363\n",
    "50\n",
    "HSI\n",
    "1987-03-16\n",
    "2669.600098\n",
    "2669.600098\n",
    "2669.600098\n",
    "2669.600098\n",
    "2669.600098\n",
    "0\n",
    "347.04801274\n",
    "51\n",
    "HSI\n",
    "1987-03-17\n",
    "2629.300049\n",
    "2629.300049\n",
    "2629.300049\n",
    "2629.300049\n",
    "2629.300049\n",
    "0\n",
    "341.80900637\n",
    "52\n",
    "HSI\n",
    "1987-03-18\n",
    "2724\n",
    "2724\n",
    "2724\n",
    "2724\n",
    "2724\n",
    "0\n",
    "354.12\n",
    "53\n",
    "HSI\n",
    "1987-03-19\n",
    "2721.300049\n",
    "2721.300049\n",
    "2721.300049\n",
    "2721.300049\n",
    "2721.300049\n",
    "0\n",
    "353.76900637\n",
    "54\n",
    "HSI\n",
    "1987-03-20\n",
    "2780.600098\n",
    "2780.600098\n",
    "2780.600098\n",
    "2780.600098\n",
    "2780.600098\n",
    "0\n",
    "361.47801274\n",
    "55\n",
    "HSI\n",
    "1987-03-23\n",
    "2813.399902\n",
    "2813.399902\n",
    "2813.399902\n",
    "2813.399902\n",
    "2813.399902\n",
    "0\n",
    "365.74198726\n",
    "56\n",
    "HSI\n",
    "1987-03-24\n",
    "2828.100098\n",
    "2828.100098\n",
    "2828.100098\n",
    "2828.100098\n",
    "2828.100098\n",
    "0\n",
    "367.65301274\n",
    "57\n",
    "HSI\n",
    "1987-03-25\n",
    "2831.5\n",
    "2831.5\n",
    "2831.5\n",
    "2831.5\n",
    "2831.5\n",
    "0\n",
    "368.095\n",
    "58\n",
    "HSI\n",
    "1987-03-26\n",
    "2783.100098\n",
    "2783.100098\n",
    "2783.100098\n",
    "2783.100098\n",
    "2783.100098\n",
    "0\n",
    "361.80301274\n",
    "59\n",
    "HSI\n",
    "1987-03-27\n",
    "2798.699951\n",
    "2798.699951\n",
    "2798.699951\n",
    "2798.699951\n",
    "2798.699951\n",
    "0\n",
    "363.83099363\n",
    "60\n",
    "HSI\n",
    "1987-03-30\n",
    "2774.899902\n",
    "2774.899902\n",
    "2774.899902\n",
    "2774.899902\n",
    "2774.899902\n",
    "0\n",
    "360.73698726\n",
    "61\n",
    "HSI\n",
    "1987-03-31\n",
    "2713.800049\n",
    "2713.800049\n",
    "2713.800049\n",
    "2713.800049\n",
    "2713.800049\n",
    "0\n",
    "352.79400637\n",
    "62\n",
    "HSI\n",
    "1987-04-01\n",
    "2695.899902\n",
    "2695.899902\n",
    "2695.899902\n",
    "2695.899902\n",
    "2695.899902\n",
    "0\n",
    "350.46698726\n",
    "63\n",
    "HSI\n",
    "1987-04-02\n",
    "2709.399902\n",
    "2709.399902\n",
    "2709.399902\n",
    "2709.399902\n",
    "2709.399902\n",
    "0\n",
    "352.22198726\n",
    "64\n",
    "HSI\n",
    "1987-04-03\n",
    "2680\n",
    "2680\n",
    "2680\n",
    "2680\n",
    "2680\n",
    "0\n",
    "348.4\n",
    "65\n",
    "HSI\n",
    "1987-04-07\n",
    "2664.699951\n",
    "2664.699951\n",
    "2664.699951\n",
    "2664.699951\n",
    "2664.699951\n",
    "0\n",
    "346.41099363\n",
    "66\n",
    "HSI\n",
    "1987-04-08\n",
    "2729.600098\n",
    "2729.600098\n",
    "2729.600098\n",
    "2729.600098\n",
    "2729.600098\n",
    "0\n",
    "354.84801274\n",
    "67\n",
    "HSI\n",
    "1987-04-09\n",
    "2785.5\n",
    "2785.5\n",
    "2785.5\n",
    "2785.5\n",
    "2785.5\n",
    "0\n",
    "362.115\n",
    "68\n",
    "HSI\n",
    "1987-04-10\n",
    "2766.399902\n",
    "2766.399902\n",
    "2766.399902\n",
    "2766.399902\n",
    "2766.399902\n",
    "0\n",
    "359.63198726\n",
    "69\n",
    "HSI\n",
    "1987-04-13\n",
    "2708.899902\n",
    "2708.899902\n",
    "2708.899902\n",
    "2708.899902\n",
    "2708.899902\n",
    "0\n",
    "352.15698726\n",
    "70\n",
    "HSI\n",
    "1987-04-14\n",
    "2668.399902\n",
    "2668.399902\n",
    "2668.399902\n",
    "2668.399902\n",
    "2668.399902\n",
    "0\n",
    "346.89198726\n",
    "71\n",
    "HSI\n",
    "1987-04-15\n",
    "2693.600098\n",
    "2693.600098\n",
    "2693.600098\n",
    "2693.600098\n",
    "2693.600098\n",
    "0\n",
    "350.16801274\n",
    "72\n",
    "HSI\n",
    "1987-04-16\n",
    "2721.100098\n",
    "2721.100098\n",
    "2721.100098\n",
    "2721.100098\n",
    "2721.100098\n",
    "0\n",
    "353.74301274\n",
    "73\n",
    "HSI\n",
    "1987-04-21\n",
    "2713.399902\n",
    "2713.399902\n",
    "2713.399902\n",
    "2713.399902\n",
    "2713.399902\n",
    "0\n",
    "352.74198726\n",
    "74\n",
    "HSI\n",
    "1987-04-22\n",
    "2716.899902\n",
    "2716.899902\n",
    "2716.899902\n",
    "2716.899902\n",
    "2716.899902\n",
    "0\n",
    "353.19698726\n",
    "75\n",
    "HSI\n",
    "1987-04-23\n",
    "2719.899902\n",
    "2719.899902\n",
    "2719.899902\n",
    "2719.899902\n",
    "2719.899902\n",
    "0\n",
    "353.58698726\n",
    "76\n",
    "HSI\n",
    "1987-04-24\n",
    "2707.600098\n",
    "2707.600098\n",
    "2707.600098\n",
    "2707.600098\n",
    "2707.600098\n",
    "0\n",
    "351.98801274\n",
    "77\n",
    "HSI\n",
    "1987-04-27\n",
    "2636.5\n",
    "2636.5\n",
    "2636.5\n",
    "2636.5\n",
    "2636.5\n",
    "0\n",
    "342.745\n",
    "78\n",
    "HSI\n",
    "1987-04-28\n",
    "2617.100098\n",
    "2617.100098\n",
    "2617.100098\n",
    "2617.100098\n",
    "2617.100098\n",
    "0\n",
    "340.22301274\n",
    "79\n",
    "HSI\n",
    "1987-04-29\n",
    "2589.5\n",
    "2589.5\n",
    "2589.5\n",
    "2589.5\n",
    "2589.5\n",
    "0\n",
    "336.635\n",
    "80\n",
    "HSI\n",
    "1987-04-30\n",
    "2659.899902\n",
    "2659.899902\n",
    "2659.899902\n",
    "2659.899902\n",
    "2659.899902\n",
    "0\n",
    "345.78698726\n",
    "81\n",
    "HSI\n",
    "1987-05-01\n",
    "2685.399902\n",
    "2685.399902\n",
    "2685.399902\n",
    "2685.399902\n",
    "2685.399902\n",
    "0\n",
    "349.10198726\n",
    "82\n",
    "HSI\n",
    "1987-05-04\n",
    "2695.800049\n",
    "2695.800049\n",
    "2695.800049\n",
    "2695.800049\n",
    "2695.800049\n",
    "0\n",
    "350.45400637\n",
    "83\n",
    "HSI\n",
    "1987-05-05\n",
    "2713.800049\n",
    "2713.800049\n",
    "2713.800049\n",
    "2713.800049\n",
    "2713.800049\n",
    "0\n",
    "352.79400637\n",
    "84\n",
    "HSI\n",
    "1987-05-06\n",
    "2792.899902\n",
    "2792.899902\n",
    "2792.899902\n",
    "2792.899902\n",
    "2792.899902\n",
    "0\n",
    "363.07698726\n",
    "85\n",
    "HSI\n",
    "1987-05-07\n",
    "2785.600098\n",
    "2785.600098\n",
    "2785.600098\n",
    "2785.600098\n",
    "2785.600098\n",
    "0\n",
    "362.12801274\n",
    "86\n",
    "HSI\n",
    "1987-05-08\n",
    "2805.100098\n",
    "2805.100098\n",
    "2805.100098\n",
    "2805.100098\n",
    "2805.100098\n",
    "0\n",
    "364.66301274\n",
    "87\n",
    "HSI\n",
    "1987-05-11\n",
    "2846.899902\n",
    "2846.899902\n",
    "2846.899902\n",
    "2846.899902\n",
    "2846.899902\n",
    "0\n",
    "370.09698726\n",
    "88\n",
    "HSI\n",
    "1987-05-12\n",
    "2853.399902\n",
    "2853.399902\n",
    "2853.399902\n",
    "2853.399902\n",
    "2853.399902\n",
    "0\n",
    "370.94198726\n",
    "89\n",
    "HSI\n",
    "1987-05-13\n",
    "2833.5\n",
    "2833.5\n",
    "2833.5\n",
    "2833.5\n",
    "2833.5\n",
    "0\n",
    "368.355\n",
    "90\n",
    "HSI\n",
    "1987-05-14\n",
    "2853.899902\n",
    "2853.899902\n",
    "2853.899902\n",
    "2853.899902\n",
    "2853.899902\n",
    "0\n",
    "371.00698726\n",
    "91\n",
    "HSI\n",
    "1987-05-15\n",
    "2872.199951\n",
    "2872.199951\n",
    "2872.199951\n",
    "2872.199951\n",
    "2872.199951\n",
    "0\n",
    "373.38599363\n",
    "92\n",
    "HSI\n",
    "1987-05-18\n",
    "2875.399902\n",
    "2875.399902\n",
    "2875.399902\n",
    "2875.399902\n",
    "2875.399902\n",
    "0\n",
    "373.80198726\n",
    "93\n",
    "HSI\n",
    "1987-05-19\n",
    "2881.5\n",
    "2881.5\n",
    "2881.5\n",
    "2881.5\n",
    "2881.5\n",
    "0\n",
    "374.595\n",
    "94\n",
    "HSI\n",
    "1987-05-20\n",
    "2887.300049\n",
    "2887.300049\n",
    "2887.300049\n",
    "2887.300049\n",
    "2887.300049\n",
    "0\n",
    "375.34900637\n",
    "95\n",
    "HSI\n",
    "1987-05-21\n",
    "2871\n",
    "2871\n",
    "2871\n",
    "2871\n",
    "2871\n",
    "0\n",
    "373.23\n",
    "96\n",
    "HSI\n",
    "1987-05-22\n",
    "2897.600098\n",
    "2897.600098\n",
    "2897.600098\n",
    "2897.600098\n",
    "2897.600098\n",
    "0\n",
    "376.68801274\n",
    "97\n",
    "HSI\n",
    "1987-05-25\n",
    "2897.600098\n",
    "2897.600098\n",
    "2897.600098\n",
    "2897.600098\n",
    "2897.600098\n",
    "0\n",
    "376.68801274\n",
    "98\n",
    "HSI\n",
    "1987-05-26\n",
    "2935.899902\n",
    "2935.899902\n",
    "2935.899902\n",
    "2935.899902\n",
    "2935.899902\n",
    "0\n",
    "381.66698726\n",
    "99\n",
    "HSI\n",
    "1987-05-27\n",
    "2941.600098\n",
    "2941.600098\n",
    "2941.600098\n",
    "2941.600098\n",
    "2941.600098\n",
    "0\n",
    "382.40801274\n",
    "Data Dictionary\n",
    "Column\tExplanation\n",
    "Index\tTicker symbol for indexes\n",
    "Date\tData of observation\n",
    "Open\tOpening price\n",
    "High\tHighest price during trading day\n",
    "Low\tLowest price during trading day\n",
    "Close\tClose price\n",
    "Adj Close\tClose price adjusted for stock splits and dividends\n",
    "Volume\tNumber of shares traded during trading day\n",
    "CloseUSD\tClose price in terms of USD\n",
    "Source of dataset.\n",
    "\n",
    "Don't know where to start?\n",
    "Challenges are brief tasks designed to help you practice specific skills:\n",
    "\n",
    "🗺️ Explore: Which index has produced the highest average annual return?\n",
    "📊 Visualize: Create a plot visualizing a 30 day moving average for an index of your choosing.\n",
    "🔎 Analyze: Compare the volatilities of the indexes included in the dataset.\n",
    "Scenarios are broader questions to help you develop an end-to-end project for your portfolio:\n",
    "\n",
    "You are working for an investment firm that is looking to invest in index funds. They have provided you with a dataset containing the returns of 13 different indexes. Your manager has asked you to make short-term forecasts for several of the most promising indexes to help them decide which would be a good fund to include. Your analysis should also include a discussion of the associated risks and volatility of each fund you focus on.\n",
    "\n",
    "You will need to prepare a report that is accessible to a broad audience. It should outline your motivation, steps, findings, and conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lego Database\n",
    "👋 Welcome to your workspace! Here, you can run SQL queries, write Python code, and add text in Markdown. This workspace is automatically connected to a PostgreSQL database containing Lego set information (source).\n",
    "\n",
    "You can click the \"Browse tables\" button in the upper righthand corner of the cell below to view the available tables.\n",
    "\n",
    "To demonstrate the power of this combination, there is a query and a visualization of every Star Wars Lego set, rendered in Plotly.\n",
    "\n",
    "Lego\n",
    "DataFrame available as\n",
    "star_wars_sets\n",
    "WITH star_wars_sets AS (\n",
    "    SELECT set_num\n",
    "    FROM themes \n",
    "    INNER JOIN sets\n",
    "        ON themes.id = sets.theme_id\n",
    "    WHERE parent_id = (\n",
    "        SELECT id FROM themes \n",
    "        WHERE name = 'Star Wars' \n",
    "            AND parent_id IS NULL)\n",
    "),\n",
    " \n",
    "star_wars_sets_info AS (\n",
    "    SELECT\n",
    "        set_num,\n",
    "        set_name, \n",
    "        year, \n",
    "        num_parts, \n",
    "        color_name, \n",
    "        rgb,\n",
    "        ROW_NUMBER() OVER(PARTITION BY set_num ORDER BY number_per_color DESC) AS color_rank\n",
    "    FROM (\n",
    "        SELECT\n",
    "            sets.set_num AS set_num,\n",
    "            rgb, \n",
    "            colors.name AS color_name, \n",
    "            sets.name AS set_name, \n",
    "            year, \n",
    "            num_parts, \n",
    "            SUM(quantity) AS number_per_color\n",
    "        FROM inventory_parts\n",
    "            INNER JOIN inventories\n",
    "                ON inventory_parts.inventory_id = inventories.id\n",
    "            INNER JOIN sets\n",
    "                ON inventories.set_num = sets.set_num\n",
    "            INNER JOIN colors\n",
    "                ON inventory_parts.color_id = colors.id\n",
    "        WHERE sets.set_num IN (SELECT set_num FROM star_wars_sets)\n",
    "        GROUP BY sets.set_num, rgb, color_name, set_name, year, num_parts) AS sub\n",
    ")\n",
    " \n",
    "SELECT * FROM star_wars_sets_info \n",
    "WHERE color_rank = 1\n",
    "set_num\n",
    "set_name\n",
    "year\n",
    "num_parts\n",
    "color_name\n",
    "rgb\n",
    "color_rank\n",
    "0\n",
    "10123-1\n",
    "Cloud City\n",
    "2003\n",
    "707\n",
    "Dark Gray\n",
    "6D6E5C\n",
    "1\n",
    "1\n",
    "10131-1\n",
    "TIE Fighter Collection\n",
    "2004\n",
    "688\n",
    "Blue\n",
    "0055BF\n",
    "1\n",
    "2\n",
    "10144-1\n",
    "Sandcrawler\n",
    "2005\n",
    "1679\n",
    "Reddish Brown\n",
    "582A12\n",
    "1\n",
    "3\n",
    "10178-1\n",
    "Motorized Walking AT-AT\n",
    "2007\n",
    "1137\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "4\n",
    "10195-1\n",
    "Republic Dropship with AT-OT\n",
    "2009\n",
    "1757\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "5\n",
    "10198-1\n",
    "Tantive IV\n",
    "2009\n",
    "1408\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "6\n",
    "10225-1\n",
    "R2-D2\n",
    "2012\n",
    "2130\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "7\n",
    "10236-1\n",
    "Ewok Village\n",
    "2013\n",
    "2023\n",
    "Reddish Brown\n",
    "582A12\n",
    "1\n",
    "8\n",
    "2853590-1\n",
    "Stormtrooper\n",
    "2009\n",
    "5\n",
    "Chrome Silver\n",
    "E0E0E0\n",
    "1\n",
    "9\n",
    "2856197-1\n",
    "Shadow ARF Trooper\n",
    "2011\n",
    "5\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "10\n",
    "30004-1\n",
    "Battle Droid on STAP\n",
    "2009\n",
    "23\n",
    "Blue\n",
    "0055BF\n",
    "1\n",
    "11\n",
    "30005-1\n",
    "Imperial Speeder Bike\n",
    "2009\n",
    "33\n",
    "Reddish Brown\n",
    "582A12\n",
    "1\n",
    "12\n",
    "30006-1\n",
    "Clone Walker\n",
    "2009\n",
    "31\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "13\n",
    "30244-1\n",
    "Anakin's Jedi Intercepter\n",
    "2014\n",
    "45\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "14\n",
    "30272-1\n",
    "A-Wing Starfighter\n",
    "2015\n",
    "58\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "15\n",
    "30274-1\n",
    "AT-DP\n",
    "2015\n",
    "65\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "16\n",
    "30275-1\n",
    "TIE Advanced Prototype\n",
    "2015\n",
    "47\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "17\n",
    "30277-1\n",
    "First Order Star Destroyer\n",
    "2016\n",
    "56\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "18\n",
    "30278-1\n",
    "Poe's X-wing Fighter\n",
    "2015\n",
    "64\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "19\n",
    "30279-1\n",
    "Kylo Ren's Command Shuttle\n",
    "2016\n",
    "43\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "20\n",
    "30496-1\n",
    "U-Wing Fighter\n",
    "2017\n",
    "55\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "21\n",
    "30605-1\n",
    "Finn (FN-2187)\n",
    "2016\n",
    "3\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "22\n",
    "3219-1\n",
    "TIE Fighter - Mini\n",
    "2003\n",
    "12\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "23\n",
    "3340-1\n",
    "Star Wars #1 - Sith Minifig Pack\n",
    "2000\n",
    "30\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "24\n",
    "3341-1\n",
    "Star Wars #2 - Luke/Han/Boba Minifig Pack\n",
    "2000\n",
    "25\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "25\n",
    "3342-1\n",
    "Star Wars #3 - Troopers/Chewie Minifig Pack\n",
    "2000\n",
    "25\n",
    "Dark Gray\n",
    "6D6E5C\n",
    "1\n",
    "26\n",
    "3343-1\n",
    "Star Wars #4 - Battle Droid Minifig Pack\n",
    "2000\n",
    "33\n",
    "Tan\n",
    "E4CD9E\n",
    "1\n",
    "27\n",
    "4475-1\n",
    "Jabba's Message\n",
    "2003\n",
    "46\n",
    "Brown\n",
    "583927\n",
    "1\n",
    "28\n",
    "4476-1\n",
    "Jabba's Prize\n",
    "2003\n",
    "40\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "29\n",
    "4477-1\n",
    "T-16 Skyhopper\n",
    "2003\n",
    "98\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "30\n",
    "4478-1\n",
    "Geonosian Fighter, Black Box\n",
    "2003\n",
    "171\n",
    "Dark Gray\n",
    "6D6E5C\n",
    "1\n",
    "31\n",
    "4478-2\n",
    "Geonosian Fighter Blue Box\n",
    "2004\n",
    "153\n",
    "Dark Gray\n",
    "6D6E5C\n",
    "1\n",
    "32\n",
    "4479-1\n",
    "TIE Bomber\n",
    "2003\n",
    "229\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "33\n",
    "4480-1\n",
    "Jabba's Palace\n",
    "2003\n",
    "234\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "34\n",
    "4482-1\n",
    "AT-TE\n",
    "2003\n",
    "650\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "35\n",
    "4483-1\n",
    "AT-AT, black box\n",
    "2003\n",
    "1070\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "36\n",
    "4483-2\n",
    "AT-AT, blue box\n",
    "2004\n",
    "1070\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "37\n",
    "4484-1\n",
    "X-wing Fighter & TIE Advanced - Mini\n",
    "2003\n",
    "72\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "38\n",
    "4485-1\n",
    "Sebulba's Podracer & Anakin's Podracer - Mini\n",
    "2003\n",
    "72\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "39\n",
    "4486-1\n",
    "AT-ST & Snowspeeder - Mini\n",
    "2003\n",
    "76\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "40\n",
    "4487-1\n",
    "Jedi Starfighter & Slave I - Mini\n",
    "2003\n",
    "53\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "41\n",
    "4488-1\n",
    "Millennium Falcon - Mini\n",
    "2003\n",
    "87\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "42\n",
    "4489-1\n",
    "AT-AT - Mini\n",
    "2003\n",
    "98\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "43\n",
    "4490-1\n",
    "Republic Gunship - Mini\n",
    "2003\n",
    "102\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "44\n",
    "4491-1\n",
    "Trade Federation MTT - Mini\n",
    "2003\n",
    "99\n",
    "Brown\n",
    "583927\n",
    "1\n",
    "45\n",
    "4492-1\n",
    "Imperial Star Destroyer - Mini\n",
    "2004\n",
    "87\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "46\n",
    "4493-1\n",
    "Sith Infiltrator - Mini\n",
    "2004\n",
    "55\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "47\n",
    "4494-1\n",
    "Imperial Shuttle - Mini\n",
    "2004\n",
    "82\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "48\n",
    "4495-1\n",
    "AT-TE - Mini\n",
    "2004\n",
    "63\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "49\n",
    "4500-1\n",
    "Rebel Snowspeeder [Redesign] - Blue box\n",
    "2003\n",
    "216\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "50\n",
    "4500-2\n",
    "Rebel Snowspeeder (redesign), Original Trilogy Edition box\n",
    "2004\n",
    "211\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "51\n",
    "4501-1\n",
    "Mos Eisley Cantina, Blue box\n",
    "2003\n",
    "197\n",
    "Tan\n",
    "E4CD9E\n",
    "1\n",
    "52\n",
    "4501-2\n",
    "Mos Eisley Cantina, Original Trilogy Edition box\n",
    "2004\n",
    "194\n",
    "Dark Red\n",
    "720E0F\n",
    "1\n",
    "53\n",
    "4502-1\n",
    "X-wing Fighter (Dagobah), Blue box\n",
    "2003\n",
    "566\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "54\n",
    "4502-2\n",
    "X-wing Fighter (Dagobah), Original Trilogy Edition box\n",
    "2004\n",
    "565\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "55\n",
    "4504-1\n",
    "Millennium Falcon (Redesign), Blue box\n",
    "2003\n",
    "996\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "56\n",
    "4504-2\n",
    "Millennium Falcon (Redesign), Original Trilogy Edition box\n",
    "2003\n",
    "979\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "57\n",
    "4591726-1\n",
    "Stormtrooper\n",
    "2009\n",
    "1\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "58\n",
    "4597068-1\n",
    "Boba Fett\n",
    "2010\n",
    "1\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "59\n",
    "4649858-1\n",
    "Shadow ARF Trooper Promotional Polybag\n",
    "2011\n",
    "5\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "60\n",
    "5000062-1\n",
    "Darth Maul\n",
    "2012\n",
    "3\n",
    "Trans-Red\n",
    "C91A09\n",
    "1\n",
    "61\n",
    "5000063-1\n",
    "TC-14\n",
    "2012\n",
    "3\n",
    "Chrome Silver\n",
    "E0E0E0\n",
    "1\n",
    "62\n",
    "5001709-1\n",
    "Clone Trooper Lieutenant\n",
    "2013\n",
    "4\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "63\n",
    "5002122-1\n",
    "TC-4\n",
    "2014\n",
    "3\n",
    "Dark Red\n",
    "720E0F\n",
    "1\n",
    "64\n",
    "5002939-1\n",
    "The Phantom\n",
    "2015\n",
    "23\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "65\n",
    "5004406-1\n",
    "First Order General\n",
    "2016\n",
    "4\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "66\n",
    "5004408-1\n",
    "Rebel A-wing Pilot\n",
    "2016\n",
    "5\n",
    "Aqua\n",
    "B3D7D1\n",
    "1\n",
    "67\n",
    "6005188-1\n",
    "Darth Maul\n",
    "2012\n",
    "8\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "68\n",
    "6205-1\n",
    "V-wing Fighter\n",
    "2006\n",
    "118\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "69\n",
    "6206-1\n",
    "TIE Interceptor\n",
    "2006\n",
    "212\n",
    "Blue\n",
    "0055BF\n",
    "1\n",
    "70\n",
    "6207-1\n",
    "A-wing Fighter\n",
    "2006\n",
    "193\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "71\n",
    "6208-1\n",
    "B-wing Fighter\n",
    "2006\n",
    "439\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "72\n",
    "6209-1\n",
    "Slave I (2nd edition)\n",
    "2006\n",
    "537\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "73\n",
    "6210-1\n",
    "Jabba's Sail Barge\n",
    "2006\n",
    "781\n",
    "Reddish Brown\n",
    "582A12\n",
    "1\n",
    "74\n",
    "6211-1\n",
    "Imperial Star Destroyer\n",
    "2006\n",
    "1388\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "75\n",
    "6212-1\n",
    "X-wing Fighter\n",
    "2006\n",
    "437\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "76\n",
    "65771-1\n",
    "Episode III Collectors' Set\n",
    "2005\n",
    "5\n",
    "[No Color]\n",
    "05131D\n",
    "1\n",
    "77\n",
    "6963-1\n",
    "X-wing Fighter - Mini (Kabaya Box)\n",
    "2004\n",
    "41\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "78\n",
    "6963-2\n",
    "X-wing Fighter - Mini (Polybag)\n",
    "2004\n",
    "41\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "79\n",
    "6964-1\n",
    "Boba Fett's Slave I - Mini (Kabaya Box)\n",
    "2004\n",
    "25\n",
    "Sand Green\n",
    "A0BCAC\n",
    "1\n",
    "80\n",
    "6964-2\n",
    "Boba Fett's Slave I - Mini (Polybag)\n",
    "2004\n",
    "25\n",
    "Sand Green\n",
    "A0BCAC\n",
    "1\n",
    "81\n",
    "6965-1\n",
    "TIE Interceptor - Mini (Kabaya Box)\n",
    "2004\n",
    "32\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "82\n",
    "6966-1\n",
    "Jedi Starfighter - Mini\n",
    "2005\n",
    "38\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "83\n",
    "6966-2\n",
    "Jedi Starfighter - Mini - Korean Duracell promo package with 8 AA batteries\n",
    "2005\n",
    "38\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "84\n",
    "6967-1\n",
    "ARC-170 Starfighter - Mini\n",
    "2005\n",
    "42\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "85\n",
    "6967-2\n",
    "ARC-170 Starfighter - Mini - Korean Duracell promo package with 8 AA batteries\n",
    "2005\n",
    "42\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "86\n",
    "6968-1\n",
    "Mini Wookiee Attack\n",
    "2005\n",
    "48\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "87\n",
    "7101-1\n",
    "Lightsaber Duel\n",
    "1999\n",
    "52\n",
    "Dark Gray\n",
    "6D6E5C\n",
    "1\n",
    "88\n",
    "7103-1\n",
    "Jedi Duel\n",
    "2002\n",
    "82\n",
    "Dark Gray\n",
    "6D6E5C\n",
    "1\n",
    "89\n",
    "7104-1\n",
    "Desert Skiff\n",
    "2000\n",
    "55\n",
    "Brown\n",
    "583927\n",
    "1\n",
    "90\n",
    "7106-1\n",
    "Droid Escape\n",
    "2001\n",
    "45\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "91\n",
    "7110-1\n",
    "Landspeeder\n",
    "1999\n",
    "49\n",
    "Tan\n",
    "E4CD9E\n",
    "1\n",
    "92\n",
    "7111-1\n",
    "Droid Fighter\n",
    "1999\n",
    "62\n",
    "Brown\n",
    "583927\n",
    "1\n",
    "93\n",
    "7113-1\n",
    "Tusken Raider Encounter\n",
    "2002\n",
    "93\n",
    "Tan\n",
    "E4CD9E\n",
    "1\n",
    "94\n",
    "7115-1\n",
    "Gungan Patrol\n",
    "2000\n",
    "79\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "95\n",
    "7119-1\n",
    "Twin-Pod Cloud Car\n",
    "2002\n",
    "118\n",
    "Red\n",
    "C91A09\n",
    "1\n",
    "96\n",
    "7121-1\n",
    "Naboo Swamp\n",
    "1999\n",
    "82\n",
    "Brown\n",
    "583927\n",
    "1\n",
    "97\n",
    "7124-1\n",
    "Flash Speeder\n",
    "2000\n",
    "107\n",
    "Green\n",
    "237841\n",
    "1\n",
    "98\n",
    "7126-1\n",
    "Battle Droid Carrier\n",
    "2001\n",
    "133\n",
    "Tan\n",
    "E4CD9E\n",
    "1\n",
    "99\n",
    "7127-1\n",
    "Imperial AT-ST\n",
    "2001\n",
    "107\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "import plotly.express as px\n",
    " \n",
    "# Create color map\n",
    "colors = dict(zip(star_wars_sets.set_num, star_wars_sets.rgb.apply(lambda x: \"#\" + x)))\n",
    " \n",
    "# Create a strip plot of the data\n",
    "fig = px.strip(\n",
    "    star_wars_sets,\n",
    "    x=\"year\",\n",
    "    y=\"num_parts\",\n",
    "    color=\"set_num\",\n",
    "    color_discrete_map=colors,\n",
    "    custom_data=[\"set_name\", \"year\", \"num_parts\", \"color_name\"],\n",
    ").update_traces(dict(marker_line_width=0.5, marker_line_color=\"black\"))\n",
    " \n",
    "# Customize data to show\n",
    "fig.update_traces(\n",
    "    hovertemplate=\"<br>\".join(\n",
    "        [\n",
    "            \"<b>%{customdata[0]}</b>\",\n",
    "            \"<b>Year:</b> %{customdata[1]}\",\n",
    "            \"<b>Number of parts:</b> %{customdata[2]}\",\n",
    "            \"<b>Most common color in set:</b> %{customdata[3]}<extra></extra>\",\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    " \n",
    "# Update the layout and show the figure\n",
    "fig.update_layout(\n",
    "    title=\"Star Wars Lego Sets<br><sup>Year of Release, Number of Parts, and Most Common Color</sup>\",\n",
    "    title_x=0.5,\n",
    "    xaxis_title=\"Year of Release\",\n",
    "    yaxis_title=\"Number of Parts\",\n",
    "    template=\"plotly_white\",\n",
    "    coloraxis_colorbar_title_text=\"Total Inventory\",\n",
    "    showlegend=False\n",
    ")\n",
    " \n",
    "fig.show()\n",
    "\n",
    "This is an interactive plot! Hover over different sets to see the number of parts and most common color!\n",
    "\n",
    "💪 Now it's your turn to construct your own queries and analyze the data! Remember, you can review the tables in the database at any point using the \"Browse tables\" button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lego Database\n",
    "👋 Welcome to your workspace! Here, you can run SQL queries, write Python code, and add text in Markdown. This workspace is automatically connected to a PostgreSQL database containing Lego set information (source).\n",
    "\n",
    "You can click the \"Browse tables\" button in the upper righthand corner of the cell below to view the available tables.\n",
    "\n",
    "To demonstrate the power of this combination, there is a query and a visualization of every Star Wars Lego set, rendered in Plotly.\n",
    "\n",
    "Lego\n",
    "DataFrame available as\n",
    "star_wars_sets\n",
    "WITH star_wars_sets AS (\n",
    "    SELECT set_num\n",
    "    FROM themes \n",
    "    INNER JOIN sets\n",
    "        ON themes.id = sets.theme_id\n",
    "    WHERE parent_id = (\n",
    "        SELECT id FROM themes \n",
    "        WHERE name = 'Star Wars' \n",
    "            AND parent_id IS NULL)\n",
    "),\n",
    " \n",
    "star_wars_sets_info AS (\n",
    "    SELECT\n",
    "        set_num,\n",
    "        set_name, \n",
    "        year, \n",
    "        num_parts, \n",
    "        color_name, \n",
    "        rgb,\n",
    "        ROW_NUMBER() OVER(PARTITION BY set_num ORDER BY number_per_color DESC) AS color_rank\n",
    "    FROM (\n",
    "        SELECT\n",
    "            sets.set_num AS set_num,\n",
    "            rgb, \n",
    "            colors.name AS color_name, \n",
    "            sets.name AS set_name, \n",
    "            year, \n",
    "            num_parts, \n",
    "            SUM(quantity) AS number_per_color\n",
    "        FROM inventory_parts\n",
    "            INNER JOIN inventories\n",
    "                ON inventory_parts.inventory_id = inventories.id\n",
    "            INNER JOIN sets\n",
    "                ON inventories.set_num = sets.set_num\n",
    "            INNER JOIN colors\n",
    "                ON inventory_parts.color_id = colors.id\n",
    "        WHERE sets.set_num IN (SELECT set_num FROM star_wars_sets)\n",
    "        GROUP BY sets.set_num, rgb, color_name, set_name, year, num_parts) AS sub\n",
    ")\n",
    " \n",
    "SELECT * FROM star_wars_sets_info \n",
    "WHERE color_rank = 1\n",
    "set_num\n",
    "set_name\n",
    "year\n",
    "num_parts\n",
    "color_name\n",
    "rgb\n",
    "color_rank\n",
    "0\n",
    "10123-1\n",
    "Cloud City\n",
    "2003\n",
    "707\n",
    "Dark Gray\n",
    "6D6E5C\n",
    "1\n",
    "1\n",
    "10131-1\n",
    "TIE Fighter Collection\n",
    "2004\n",
    "688\n",
    "Blue\n",
    "0055BF\n",
    "1\n",
    "2\n",
    "10144-1\n",
    "Sandcrawler\n",
    "2005\n",
    "1679\n",
    "Reddish Brown\n",
    "582A12\n",
    "1\n",
    "3\n",
    "10178-1\n",
    "Motorized Walking AT-AT\n",
    "2007\n",
    "1137\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "4\n",
    "10195-1\n",
    "Republic Dropship with AT-OT\n",
    "2009\n",
    "1757\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "5\n",
    "10198-1\n",
    "Tantive IV\n",
    "2009\n",
    "1408\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "6\n",
    "10225-1\n",
    "R2-D2\n",
    "2012\n",
    "2130\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "7\n",
    "10236-1\n",
    "Ewok Village\n",
    "2013\n",
    "2023\n",
    "Reddish Brown\n",
    "582A12\n",
    "1\n",
    "8\n",
    "2853590-1\n",
    "Stormtrooper\n",
    "2009\n",
    "5\n",
    "Chrome Silver\n",
    "E0E0E0\n",
    "1\n",
    "9\n",
    "2856197-1\n",
    "Shadow ARF Trooper\n",
    "2011\n",
    "5\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "10\n",
    "30004-1\n",
    "Battle Droid on STAP\n",
    "2009\n",
    "23\n",
    "Blue\n",
    "0055BF\n",
    "1\n",
    "11\n",
    "30005-1\n",
    "Imperial Speeder Bike\n",
    "2009\n",
    "33\n",
    "Reddish Brown\n",
    "582A12\n",
    "1\n",
    "12\n",
    "30006-1\n",
    "Clone Walker\n",
    "2009\n",
    "31\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "13\n",
    "30244-1\n",
    "Anakin's Jedi Intercepter\n",
    "2014\n",
    "45\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "14\n",
    "30272-1\n",
    "A-Wing Starfighter\n",
    "2015\n",
    "58\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "15\n",
    "30274-1\n",
    "AT-DP\n",
    "2015\n",
    "65\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "16\n",
    "30275-1\n",
    "TIE Advanced Prototype\n",
    "2015\n",
    "47\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "17\n",
    "30277-1\n",
    "First Order Star Destroyer\n",
    "2016\n",
    "56\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "18\n",
    "30278-1\n",
    "Poe's X-wing Fighter\n",
    "2015\n",
    "64\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "19\n",
    "30279-1\n",
    "Kylo Ren's Command Shuttle\n",
    "2016\n",
    "43\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "20\n",
    "30496-1\n",
    "U-Wing Fighter\n",
    "2017\n",
    "55\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "21\n",
    "30605-1\n",
    "Finn (FN-2187)\n",
    "2016\n",
    "3\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "22\n",
    "3219-1\n",
    "TIE Fighter - Mini\n",
    "2003\n",
    "12\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "23\n",
    "3340-1\n",
    "Star Wars #1 - Sith Minifig Pack\n",
    "2000\n",
    "30\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "24\n",
    "3341-1\n",
    "Star Wars #2 - Luke/Han/Boba Minifig Pack\n",
    "2000\n",
    "25\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "25\n",
    "3342-1\n",
    "Star Wars #3 - Troopers/Chewie Minifig Pack\n",
    "2000\n",
    "25\n",
    "Dark Gray\n",
    "6D6E5C\n",
    "1\n",
    "26\n",
    "3343-1\n",
    "Star Wars #4 - Battle Droid Minifig Pack\n",
    "2000\n",
    "33\n",
    "Tan\n",
    "E4CD9E\n",
    "1\n",
    "27\n",
    "4475-1\n",
    "Jabba's Message\n",
    "2003\n",
    "46\n",
    "Brown\n",
    "583927\n",
    "1\n",
    "28\n",
    "4476-1\n",
    "Jabba's Prize\n",
    "2003\n",
    "40\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "29\n",
    "4477-1\n",
    "T-16 Skyhopper\n",
    "2003\n",
    "98\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "30\n",
    "4478-1\n",
    "Geonosian Fighter, Black Box\n",
    "2003\n",
    "171\n",
    "Dark Gray\n",
    "6D6E5C\n",
    "1\n",
    "31\n",
    "4478-2\n",
    "Geonosian Fighter Blue Box\n",
    "2004\n",
    "153\n",
    "Dark Gray\n",
    "6D6E5C\n",
    "1\n",
    "32\n",
    "4479-1\n",
    "TIE Bomber\n",
    "2003\n",
    "229\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "33\n",
    "4480-1\n",
    "Jabba's Palace\n",
    "2003\n",
    "234\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "34\n",
    "4482-1\n",
    "AT-TE\n",
    "2003\n",
    "650\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "35\n",
    "4483-1\n",
    "AT-AT, black box\n",
    "2003\n",
    "1070\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "36\n",
    "4483-2\n",
    "AT-AT, blue box\n",
    "2004\n",
    "1070\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "37\n",
    "4484-1\n",
    "X-wing Fighter & TIE Advanced - Mini\n",
    "2003\n",
    "72\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "38\n",
    "4485-1\n",
    "Sebulba's Podracer & Anakin's Podracer - Mini\n",
    "2003\n",
    "72\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "39\n",
    "4486-1\n",
    "AT-ST & Snowspeeder - Mini\n",
    "2003\n",
    "76\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "40\n",
    "4487-1\n",
    "Jedi Starfighter & Slave I - Mini\n",
    "2003\n",
    "53\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "41\n",
    "4488-1\n",
    "Millennium Falcon - Mini\n",
    "2003\n",
    "87\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "42\n",
    "4489-1\n",
    "AT-AT - Mini\n",
    "2003\n",
    "98\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "43\n",
    "4490-1\n",
    "Republic Gunship - Mini\n",
    "2003\n",
    "102\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "44\n",
    "4491-1\n",
    "Trade Federation MTT - Mini\n",
    "2003\n",
    "99\n",
    "Brown\n",
    "583927\n",
    "1\n",
    "45\n",
    "4492-1\n",
    "Imperial Star Destroyer - Mini\n",
    "2004\n",
    "87\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "46\n",
    "4493-1\n",
    "Sith Infiltrator - Mini\n",
    "2004\n",
    "55\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "47\n",
    "4494-1\n",
    "Imperial Shuttle - Mini\n",
    "2004\n",
    "82\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "48\n",
    "4495-1\n",
    "AT-TE - Mini\n",
    "2004\n",
    "63\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "49\n",
    "4500-1\n",
    "Rebel Snowspeeder [Redesign] - Blue box\n",
    "2003\n",
    "216\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "50\n",
    "4500-2\n",
    "Rebel Snowspeeder (redesign), Original Trilogy Edition box\n",
    "2004\n",
    "211\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "51\n",
    "4501-1\n",
    "Mos Eisley Cantina, Blue box\n",
    "2003\n",
    "197\n",
    "Tan\n",
    "E4CD9E\n",
    "1\n",
    "52\n",
    "4501-2\n",
    "Mos Eisley Cantina, Original Trilogy Edition box\n",
    "2004\n",
    "194\n",
    "Dark Red\n",
    "720E0F\n",
    "1\n",
    "53\n",
    "4502-1\n",
    "X-wing Fighter (Dagobah), Blue box\n",
    "2003\n",
    "566\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "54\n",
    "4502-2\n",
    "X-wing Fighter (Dagobah), Original Trilogy Edition box\n",
    "2004\n",
    "565\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "55\n",
    "4504-1\n",
    "Millennium Falcon (Redesign), Blue box\n",
    "2003\n",
    "996\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "56\n",
    "4504-2\n",
    "Millennium Falcon (Redesign), Original Trilogy Edition box\n",
    "2003\n",
    "979\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "57\n",
    "4591726-1\n",
    "Stormtrooper\n",
    "2009\n",
    "1\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "58\n",
    "4597068-1\n",
    "Boba Fett\n",
    "2010\n",
    "1\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "59\n",
    "4649858-1\n",
    "Shadow ARF Trooper Promotional Polybag\n",
    "2011\n",
    "5\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "60\n",
    "5000062-1\n",
    "Darth Maul\n",
    "2012\n",
    "3\n",
    "Trans-Red\n",
    "C91A09\n",
    "1\n",
    "61\n",
    "5000063-1\n",
    "TC-14\n",
    "2012\n",
    "3\n",
    "Chrome Silver\n",
    "E0E0E0\n",
    "1\n",
    "62\n",
    "5001709-1\n",
    "Clone Trooper Lieutenant\n",
    "2013\n",
    "4\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "63\n",
    "5002122-1\n",
    "TC-4\n",
    "2014\n",
    "3\n",
    "Dark Red\n",
    "720E0F\n",
    "1\n",
    "64\n",
    "5002939-1\n",
    "The Phantom\n",
    "2015\n",
    "23\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "65\n",
    "5004406-1\n",
    "First Order General\n",
    "2016\n",
    "4\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "66\n",
    "5004408-1\n",
    "Rebel A-wing Pilot\n",
    "2016\n",
    "5\n",
    "Aqua\n",
    "B3D7D1\n",
    "1\n",
    "67\n",
    "6005188-1\n",
    "Darth Maul\n",
    "2012\n",
    "8\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "68\n",
    "6205-1\n",
    "V-wing Fighter\n",
    "2006\n",
    "118\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "69\n",
    "6206-1\n",
    "TIE Interceptor\n",
    "2006\n",
    "212\n",
    "Blue\n",
    "0055BF\n",
    "1\n",
    "70\n",
    "6207-1\n",
    "A-wing Fighter\n",
    "2006\n",
    "193\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "71\n",
    "6208-1\n",
    "B-wing Fighter\n",
    "2006\n",
    "439\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "72\n",
    "6209-1\n",
    "Slave I (2nd edition)\n",
    "2006\n",
    "537\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "73\n",
    "6210-1\n",
    "Jabba's Sail Barge\n",
    "2006\n",
    "781\n",
    "Reddish Brown\n",
    "582A12\n",
    "1\n",
    "74\n",
    "6211-1\n",
    "Imperial Star Destroyer\n",
    "2006\n",
    "1388\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "75\n",
    "6212-1\n",
    "X-wing Fighter\n",
    "2006\n",
    "437\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "76\n",
    "65771-1\n",
    "Episode III Collectors' Set\n",
    "2005\n",
    "5\n",
    "[No Color]\n",
    "05131D\n",
    "1\n",
    "77\n",
    "6963-1\n",
    "X-wing Fighter - Mini (Kabaya Box)\n",
    "2004\n",
    "41\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "78\n",
    "6963-2\n",
    "X-wing Fighter - Mini (Polybag)\n",
    "2004\n",
    "41\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "79\n",
    "6964-1\n",
    "Boba Fett's Slave I - Mini (Kabaya Box)\n",
    "2004\n",
    "25\n",
    "Sand Green\n",
    "A0BCAC\n",
    "1\n",
    "80\n",
    "6964-2\n",
    "Boba Fett's Slave I - Mini (Polybag)\n",
    "2004\n",
    "25\n",
    "Sand Green\n",
    "A0BCAC\n",
    "1\n",
    "81\n",
    "6965-1\n",
    "TIE Interceptor - Mini (Kabaya Box)\n",
    "2004\n",
    "32\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "82\n",
    "6966-1\n",
    "Jedi Starfighter - Mini\n",
    "2005\n",
    "38\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "83\n",
    "6966-2\n",
    "Jedi Starfighter - Mini - Korean Duracell promo package with 8 AA batteries\n",
    "2005\n",
    "38\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "84\n",
    "6967-1\n",
    "ARC-170 Starfighter - Mini\n",
    "2005\n",
    "42\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "85\n",
    "6967-2\n",
    "ARC-170 Starfighter - Mini - Korean Duracell promo package with 8 AA batteries\n",
    "2005\n",
    "42\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "86\n",
    "6968-1\n",
    "Mini Wookiee Attack\n",
    "2005\n",
    "48\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "87\n",
    "7101-1\n",
    "Lightsaber Duel\n",
    "1999\n",
    "52\n",
    "Dark Gray\n",
    "6D6E5C\n",
    "1\n",
    "88\n",
    "7103-1\n",
    "Jedi Duel\n",
    "2002\n",
    "82\n",
    "Dark Gray\n",
    "6D6E5C\n",
    "1\n",
    "89\n",
    "7104-1\n",
    "Desert Skiff\n",
    "2000\n",
    "55\n",
    "Brown\n",
    "583927\n",
    "1\n",
    "90\n",
    "7106-1\n",
    "Droid Escape\n",
    "2001\n",
    "45\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "91\n",
    "7110-1\n",
    "Landspeeder\n",
    "1999\n",
    "49\n",
    "Tan\n",
    "E4CD9E\n",
    "1\n",
    "92\n",
    "7111-1\n",
    "Droid Fighter\n",
    "1999\n",
    "62\n",
    "Brown\n",
    "583927\n",
    "1\n",
    "93\n",
    "7113-1\n",
    "Tusken Raider Encounter\n",
    "2002\n",
    "93\n",
    "Tan\n",
    "E4CD9E\n",
    "1\n",
    "94\n",
    "7115-1\n",
    "Gungan Patrol\n",
    "2000\n",
    "79\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "95\n",
    "7119-1\n",
    "Twin-Pod Cloud Car\n",
    "2002\n",
    "118\n",
    "Red\n",
    "C91A09\n",
    "1\n",
    "96\n",
    "7121-1\n",
    "Naboo Swamp\n",
    "1999\n",
    "82\n",
    "Brown\n",
    "583927\n",
    "1\n",
    "97\n",
    "7124-1\n",
    "Flash Speeder\n",
    "2000\n",
    "107\n",
    "Green\n",
    "237841\n",
    "1\n",
    "98\n",
    "7126-1\n",
    "Battle Droid Carrier\n",
    "2001\n",
    "133\n",
    "Tan\n",
    "E4CD9E\n",
    "1\n",
    "99\n",
    "7127-1\n",
    "Imperial AT-ST\n",
    "2001\n",
    "107\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "Lego Database\n",
    "👋 Welcome to your workspace! Here, you can run SQL queries, write Python code, and add text in Markdown. This workspace is automatically connected to a PostgreSQL database containing Lego set information (source).\n",
    "\n",
    "You can click the \"Browse tables\" button in the upper righthand corner of the cell below to view the available tables.\n",
    "\n",
    "To demonstrate the power of this combination, there is a query and a visualization of every Star Wars Lego set, rendered in Plotly.\n",
    "\n",
    "Lego\n",
    "DataFrame available as\n",
    "star_wars_sets\n",
    "WITH star_wars_sets AS (\n",
    "    SELECT set_num\n",
    "    FROM themes \n",
    "    INNER JOIN sets\n",
    "        ON themes.id = sets.theme_id\n",
    "    WHERE parent_id = (\n",
    "        SELECT id FROM themes \n",
    "        WHERE name = 'Star Wars' \n",
    "            AND parent_id IS NULL)\n",
    "),\n",
    " \n",
    "star_wars_sets_info AS (\n",
    "    SELECT\n",
    "        set_num,\n",
    "        set_name, \n",
    "        year, \n",
    "        num_parts, \n",
    "        color_name, \n",
    "        rgb,\n",
    "        ROW_NUMBER() OVER(PARTITION BY set_num ORDER BY number_per_color DESC) AS color_rank\n",
    "    FROM (\n",
    "        SELECT\n",
    "            sets.set_num AS set_num,\n",
    "            rgb, \n",
    "            colors.name AS color_name, \n",
    "            sets.name AS set_name, \n",
    "            year, \n",
    "            num_parts, \n",
    "            SUM(quantity) AS number_per_color\n",
    "        FROM inventory_parts\n",
    "            INNER JOIN inventories\n",
    "                ON inventory_parts.inventory_id = inventories.id\n",
    "            INNER JOIN sets\n",
    "                ON inventories.set_num = sets.set_num\n",
    "            INNER JOIN colors\n",
    "                ON inventory_parts.color_id = colors.id\n",
    "        WHERE sets.set_num IN (SELECT set_num FROM star_wars_sets)\n",
    "        GROUP BY sets.set_num, rgb, color_name, set_name, year, num_parts) AS sub\n",
    ")\n",
    " \n",
    "SELECT * FROM star_wars_sets_info \n",
    "WHERE color_rank = 1\n",
    "set_num\n",
    "set_name\n",
    "year\n",
    "num_parts\n",
    "color_name\n",
    "rgb\n",
    "color_rank\n",
    "100\n",
    "7128-1\n",
    "Speeder Bikes\n",
    "1999\n",
    "93\n",
    "Dark Gray\n",
    "6D6E5C\n",
    "1\n",
    "101\n",
    "7130-1\n",
    "Snowspeeder\n",
    "1999\n",
    "217\n",
    "Dark Gray\n",
    "6D6E5C\n",
    "1\n",
    "102\n",
    "7131-1\n",
    "Anakin's Podracer\n",
    "1999\n",
    "136\n",
    "Blue\n",
    "0055BF\n",
    "1\n",
    "103\n",
    "7133-1\n",
    "Bounty Hunter Pursuit\n",
    "2002\n",
    "254\n",
    "Lime\n",
    "BBE90B\n",
    "1\n",
    "104\n",
    "7134-1\n",
    "A-wing Fighter\n",
    "2000\n",
    "125\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "105\n",
    "7139-1\n",
    "Ewok Attack\n",
    "2002\n",
    "121\n",
    "Brown\n",
    "583927\n",
    "1\n",
    "106\n",
    "7140-1\n",
    "X-wing Fighter\n",
    "1999\n",
    "271\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "107\n",
    "7141-1\n",
    "Naboo Fighter\n",
    "1999\n",
    "179\n",
    "Yellow\n",
    "F2CD37\n",
    "1\n",
    "108\n",
    "7142-1\n",
    "X-wing Fighter (re-release of 7140)\n",
    "2002\n",
    "273\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "109\n",
    "7143-1\n",
    "Jedi Starfighter\n",
    "2002\n",
    "139\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "110\n",
    "7144-1\n",
    "Slave I\n",
    "2000\n",
    "166\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "111\n",
    "7146-1\n",
    "TIE Fighter\n",
    "2001\n",
    "171\n",
    "Blue\n",
    "0055BF\n",
    "1\n",
    "112\n",
    "7150-1\n",
    "TIE Fighter & Y-wing\n",
    "1999\n",
    "410\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "113\n",
    "7151-1\n",
    "Sith Infiltrator\n",
    "1999\n",
    "244\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "114\n",
    "7152-1\n",
    "TIE Fighter & Y-wing (re-release of 7150)\n",
    "2002\n",
    "411\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "115\n",
    "7153-1\n",
    "Jango Fett's Slave I\n",
    "2002\n",
    "360\n",
    "Dark Gray\n",
    "6D6E5C\n",
    "1\n",
    "116\n",
    "7155-1\n",
    "Trade Federation AAT\n",
    "2000\n",
    "159\n",
    "Tan\n",
    "E4CD9E\n",
    "1\n",
    "117\n",
    "7159-1\n",
    "Star Wars Podracing Bucket\n",
    "2000\n",
    "292\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "118\n",
    "7161-1\n",
    "Gungan Sub\n",
    "1999\n",
    "379\n",
    "Blue\n",
    "0055BF\n",
    "1\n",
    "119\n",
    "7163-1\n",
    "Republic Gunship\n",
    "2002\n",
    "697\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "120\n",
    "7166-1\n",
    "Imperial Shuttle\n",
    "2001\n",
    "238\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "121\n",
    "7171-1\n",
    "Mos Espa Podrace\n",
    "1999\n",
    "913\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "122\n",
    "7180-1\n",
    "B-wing at Rebel Control Center\n",
    "2000\n",
    "346\n",
    "Dark Gray\n",
    "6D6E5C\n",
    "1\n",
    "123\n",
    "7184-1\n",
    "Trade Federation MTT\n",
    "2000\n",
    "466\n",
    "Tan\n",
    "E4CD9E\n",
    "1\n",
    "124\n",
    "7186-1\n",
    "Watto's Junkyard\n",
    "2001\n",
    "449\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "125\n",
    "7190-1\n",
    "Millennium Falcon\n",
    "2000\n",
    "671\n",
    "Dark Gray\n",
    "6D6E5C\n",
    "1\n",
    "126\n",
    "7200-1\n",
    "Final Duel I\n",
    "2002\n",
    "32\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "127\n",
    "7201-1\n",
    "Final Duel II\n",
    "2002\n",
    "26\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "128\n",
    "7203-1\n",
    "Jedi Defense I\n",
    "2002\n",
    "59\n",
    "Light Gray\n",
    "9BA19D\n",
    "1\n",
    "129\n",
    "7204-1\n",
    "Jedi Defense II\n",
    "2002\n",
    "53\n",
    "Tan\n",
    "E4CD9E\n",
    "1\n",
    "130\n",
    "7250-1\n",
    "Clone Scout Walker\n",
    "2005\n",
    "109\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "131\n",
    "7251-1\n",
    "Darth Vader Transformation\n",
    "2005\n",
    "60\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "132\n",
    "7252-1\n",
    "Droid Tri-Fighter\n",
    "2005\n",
    "147\n",
    "Sand Blue\n",
    "6074A1\n",
    "1\n",
    "133\n",
    "7255-1\n",
    "General Grievous Chase\n",
    "2005\n",
    "112\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "134\n",
    "7256-1\n",
    "Jedi Starfighter & Vulture Droid\n",
    "2005\n",
    "202\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "135\n",
    "7257-1\n",
    "Ultimate Lightsaber Duel\n",
    "2005\n",
    "278\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "136\n",
    "7258-1\n",
    "Wookiee Attack\n",
    "2005\n",
    "368\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "137\n",
    "7259-1\n",
    "ARC-170 Starfighter\n",
    "2005\n",
    "401\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "138\n",
    "7260-1\n",
    "Wookiee Catamaran\n",
    "2005\n",
    "381\n",
    "Reddish Brown\n",
    "582A12\n",
    "1\n",
    "139\n",
    "7261-1\n",
    "Clone Turbo Tank [Light-Up Mace Windu]\n",
    "2005\n",
    "803\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "140\n",
    "7261-2\n",
    "Clone Turbo Tank (with Non-Light-Up Mace Windu)\n",
    "2006\n",
    "814\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "141\n",
    "7262-1\n",
    "TIE Fighter and Y-wing (TRU exclusive re-release)\n",
    "2004\n",
    "415\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "142\n",
    "7263-1\n",
    "TIE Fighter\n",
    "2005\n",
    "159\n",
    "Blue\n",
    "0055BF\n",
    "1\n",
    "143\n",
    "7264-1\n",
    "Imperial Inspection\n",
    "2005\n",
    "378\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "144\n",
    "7283-1\n",
    "Ultimate Space Battle\n",
    "2005\n",
    "540\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "145\n",
    "75000-1\n",
    "Clone Troopers vs. Droidekas\n",
    "2013\n",
    "124\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "146\n",
    "75002-1\n",
    "AT-RT\n",
    "2013\n",
    "222\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "147\n",
    "75003-1\n",
    "A-wing Starfighter\n",
    "2013\n",
    "177\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "148\n",
    "75004-1\n",
    "Z-95 Headhunter\n",
    "2013\n",
    "371\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "149\n",
    "75005-1\n",
    "Rancor Pit\n",
    "2013\n",
    "380\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "150\n",
    "75006-1\n",
    "LEGO Star Wars Jedi Starfighter and Kamino\n",
    "2013\n",
    "61\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "151\n",
    "75008-1\n",
    "LEGO Star Wars TIE Bomber and Asteroid Field\n",
    "2013\n",
    "60\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "152\n",
    "75012-1\n",
    "BARC Speeder with Sidecar\n",
    "2013\n",
    "225\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "153\n",
    "75013-1\n",
    "Umbaran MHC (Mobile Heavy Cannon)\n",
    "2013\n",
    "491\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "154\n",
    "75014-1\n",
    "Battle Of Hoth\n",
    "2013\n",
    "415\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "155\n",
    "75015-1\n",
    "Corporate Alliance Tank Droid\n",
    "2013\n",
    "271\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "156\n",
    "75016-1\n",
    "Homing Spider Droid\n",
    "2013\n",
    "295\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "157\n",
    "75017-1\n",
    "Duel on Geonosis\n",
    "2013\n",
    "391\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "158\n",
    "75018-1\n",
    "JEK-14's Stealth Starfighter\n",
    "2013\n",
    "548\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "159\n",
    "75019-1\n",
    "AT-TE\n",
    "2013\n",
    "792\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "160\n",
    "75020-1\n",
    "Jabba’s Sail Barge\n",
    "2013\n",
    "848\n",
    "Reddish Brown\n",
    "582A12\n",
    "1\n",
    "161\n",
    "75021-1\n",
    "Republic Gunship\n",
    "2013\n",
    "1173\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "162\n",
    "75022-1\n",
    "Mandalorian Speeder\n",
    "2013\n",
    "210\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "163\n",
    "75024-1\n",
    "HH-87 Starhopper\n",
    "2014\n",
    "361\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "164\n",
    "75025-1\n",
    "Jedi Defender-class Cruiser\n",
    "2013\n",
    "925\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "165\n",
    "75030-1\n",
    "Millennium Falcon\n",
    "2014\n",
    "94\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "166\n",
    "75031-1\n",
    "TIE Interceptor\n",
    "2014\n",
    "92\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "167\n",
    "75032-1\n",
    "X-Wing Fighter\n",
    "2014\n",
    "97\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "168\n",
    "75033-1\n",
    "Star Destroyer\n",
    "2014\n",
    "97\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "169\n",
    "75048-1\n",
    "The Phantom\n",
    "2014\n",
    "233\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "170\n",
    "75052-1\n",
    "Mos Eisley Cantina\n",
    "2014\n",
    "614\n",
    "Tan\n",
    "E4CD9E\n",
    "1\n",
    "171\n",
    "75053-1\n",
    "The Ghost\n",
    "2014\n",
    "927\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "172\n",
    "75054-1\n",
    "AT-AT\n",
    "2014\n",
    "1136\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "173\n",
    "75058-1\n",
    "MTT\n",
    "2014\n",
    "951\n",
    "Reddish Brown\n",
    "582A12\n",
    "1\n",
    "174\n",
    "75059-1\n",
    "Sandcrawler\n",
    "2014\n",
    "3294\n",
    "Reddish Brown\n",
    "582A12\n",
    "1\n",
    "175\n",
    "75072-1\n",
    "ARC-170 Starfighter\n",
    "2015\n",
    "95\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "176\n",
    "75073-1\n",
    "Vulture Droid\n",
    "2015\n",
    "77\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "177\n",
    "75074-1\n",
    "Snowspeeder\n",
    "2015\n",
    "97\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "178\n",
    "75075-1\n",
    "AT-AT\n",
    "2015\n",
    "88\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "179\n",
    "75076-1\n",
    "Republic Gunship\n",
    "2015\n",
    "105\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "180\n",
    "75077-1\n",
    "Homing Spider Droid\n",
    "2015\n",
    "102\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "181\n",
    "75078-1\n",
    "Imperial Troop Transport\n",
    "2015\n",
    "141\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "182\n",
    "75079-1\n",
    "Shadow Troopers\n",
    "2015\n",
    "95\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "183\n",
    "75080-1\n",
    "AAT\n",
    "2015\n",
    "251\n",
    "Tan\n",
    "E4CD9E\n",
    "1\n",
    "184\n",
    "75081-1\n",
    "T-16 Skyhopper\n",
    "2015\n",
    "246\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "185\n",
    "75082-1\n",
    "TIE Advanced Prototype\n",
    "2015\n",
    "354\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "186\n",
    "75083-1\n",
    "AT-DP\n",
    "2015\n",
    "498\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "187\n",
    "75084-1\n",
    "Wookiee Gunship\n",
    "2015\n",
    "568\n",
    "Tan\n",
    "E4CD9E\n",
    "1\n",
    "188\n",
    "75085-1\n",
    "Hailfire Droid\n",
    "2015\n",
    "163\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "189\n",
    "75086-1\n",
    "Battle Droid Troop Carrier\n",
    "2015\n",
    "565\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "190\n",
    "75087-1\n",
    "Anakin's Custom Jedi Starfighter\n",
    "2015\n",
    "369\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "191\n",
    "75088-1\n",
    "Senate Commando Troopers\n",
    "2015\n",
    "106\n",
    "Trans-Dark Blue\n",
    "0020A0\n",
    "1\n",
    "192\n",
    "75089-1\n",
    "Geonosis Troopers\n",
    "2015\n",
    "105\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "193\n",
    "75090-1\n",
    "Ezra's Speeder Bike\n",
    "2015\n",
    "267\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "194\n",
    "75090-2\n",
    "Ezra's Speeder Bike\n",
    "2015\n",
    "253\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "195\n",
    "75091-1\n",
    "Flash Speeder\n",
    "2015\n",
    "311\n",
    "Green\n",
    "237841\n",
    "1\n",
    "196\n",
    "75092-1\n",
    "Naboo Starfighter\n",
    "2015\n",
    "440\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "197\n",
    "75093-1\n",
    "Death Star Final Duel\n",
    "2015\n",
    "722\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "198\n",
    "75096-1\n",
    "Sith Infiltrator\n",
    "2015\n",
    "659\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "199\n",
    "75106-1\n",
    "Imperial Assault Carrier\n",
    "2015\n",
    "1214\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "Lego Database\n",
    "👋 Welcome to your workspace! Here, you can run SQL queries, write Python code, and add text in Markdown. This workspace is automatically connected to a PostgreSQL database containing Lego set information (source).\n",
    "\n",
    "You can click the \"Browse tables\" button in the upper righthand corner of the cell below to view the available tables.\n",
    "\n",
    "To demonstrate the power of this combination, there is a query and a visualization of every Star Wars Lego set, rendered in Plotly.\n",
    "\n",
    "Lego\n",
    "DataFrame available as\n",
    "star_wars_sets\n",
    "WITH star_wars_sets AS (\n",
    "    SELECT set_num\n",
    "    FROM themes \n",
    "    INNER JOIN sets\n",
    "        ON themes.id = sets.theme_id\n",
    "    WHERE parent_id = (\n",
    "        SELECT id FROM themes \n",
    "        WHERE name = 'Star Wars' \n",
    "            AND parent_id IS NULL)\n",
    "),\n",
    " \n",
    "star_wars_sets_info AS (\n",
    "    SELECT\n",
    "        set_num,\n",
    "        set_name, \n",
    "        year, \n",
    "        num_parts, \n",
    "        color_name, \n",
    "        rgb,\n",
    "        ROW_NUMBER() OVER(PARTITION BY set_num ORDER BY number_per_color DESC) AS color_rank\n",
    "    FROM (\n",
    "        SELECT\n",
    "            sets.set_num AS set_num,\n",
    "            rgb, \n",
    "            colors.name AS color_name, \n",
    "            sets.name AS set_name, \n",
    "            year, \n",
    "            num_parts, \n",
    "            SUM(quantity) AS number_per_color\n",
    "        FROM inventory_parts\n",
    "            INNER JOIN inventories\n",
    "                ON inventory_parts.inventory_id = inventories.id\n",
    "            INNER JOIN sets\n",
    "                ON inventories.set_num = sets.set_num\n",
    "            INNER JOIN colors\n",
    "                ON inventory_parts.color_id = colors.id\n",
    "        WHERE sets.set_num IN (SELECT set_num FROM star_wars_sets)\n",
    "        GROUP BY sets.set_num, rgb, color_name, set_name, year, num_parts) AS sub\n",
    ")\n",
    " \n",
    "SELECT * FROM star_wars_sets_info \n",
    "WHERE color_rank = 1\n",
    "set_num\n",
    "set_name\n",
    "year\n",
    "num_parts\n",
    "color_name\n",
    "rgb\n",
    "color_rank\n",
    "200\n",
    "75113-1\n",
    "Rey\n",
    "2016\n",
    "84\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "201\n",
    "75114-1\n",
    "First Order Stormtrooper\n",
    "2016\n",
    "81\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "202\n",
    "75115-1\n",
    "Poe Dameron\n",
    "2016\n",
    "101\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "203\n",
    "75116-1\n",
    "Finn\n",
    "2016\n",
    "98\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "204\n",
    "75117-1\n",
    "Kylo Ren\n",
    "2016\n",
    "86\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "205\n",
    "75118-1\n",
    "Captain Phasma\n",
    "2016\n",
    "82\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "206\n",
    "75119-1\n",
    "Sergeant Jyn Erso\n",
    "2016\n",
    "104\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "207\n",
    "75120-1\n",
    "K-2SO\n",
    "2016\n",
    "169\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "208\n",
    "75121-1\n",
    "Imperial Death Trooper\n",
    "2016\n",
    "105\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "209\n",
    "75125-1\n",
    "Resistance X-Wing Fighter\n",
    "2016\n",
    "87\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "210\n",
    "75126-1\n",
    "First Order Snowspeeder\n",
    "2016\n",
    "91\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "211\n",
    "75127-1\n",
    "The Ghost\n",
    "2016\n",
    "104\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "212\n",
    "75128-1\n",
    "TIE Advanced Prototype\n",
    "2016\n",
    "93\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "213\n",
    "75129-1\n",
    "Wookiee Gunship\n",
    "2016\n",
    "84\n",
    "Tan\n",
    "E4CD9E\n",
    "1\n",
    "214\n",
    "75130-1\n",
    "AT-DP\n",
    "2016\n",
    "76\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "215\n",
    "75131-1\n",
    "Resistance Trooper Battle Pack\n",
    "2016\n",
    "112\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "216\n",
    "75132-1\n",
    "First Order Battle Pack\n",
    "2016\n",
    "88\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "217\n",
    "75135-1\n",
    "Obi-Wan's Jedi Interceptor\n",
    "2016\n",
    "212\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "218\n",
    "75136-1\n",
    "Droid Escape Pod\n",
    "2016\n",
    "196\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "219\n",
    "75137-1\n",
    "Carbon-Freezing Chamber\n",
    "2016\n",
    "231\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "220\n",
    "75138-1\n",
    "Hoth Attack\n",
    "2016\n",
    "230\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "221\n",
    "75139-1\n",
    "Battle on Takodana\n",
    "2016\n",
    "403\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "222\n",
    "75140-1\n",
    "Resistance Troop Transporter\n",
    "2016\n",
    "644\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "223\n",
    "75141-1\n",
    "Kanan's Speeder Bike\n",
    "2016\n",
    "226\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "224\n",
    "75142-1\n",
    "Homing Spider Droid\n",
    "2016\n",
    "309\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "225\n",
    "75145-1\n",
    "Eclipse Fighter\n",
    "2016\n",
    "363\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "226\n",
    "75147-1\n",
    "StarScavenger\n",
    "2016\n",
    "557\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "227\n",
    "75148-1\n",
    "Encounter on Jakku\n",
    "2016\n",
    "529\n",
    "Tan\n",
    "E4CD9E\n",
    "1\n",
    "228\n",
    "75149-1\n",
    "Resistance X-Wing Fighter\n",
    "2016\n",
    "738\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "229\n",
    "75150-1\n",
    "Vader’s TIE Advanced vs. A-Wing Starfighter\n",
    "2016\n",
    "700\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "230\n",
    "75151-1\n",
    "Clone Turbo Tank\n",
    "2016\n",
    "902\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "231\n",
    "75152-1\n",
    "Imperial Assault Hovertank\n",
    "2016\n",
    "385\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "232\n",
    "75153-1\n",
    "AT-ST Walker\n",
    "2016\n",
    "448\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "233\n",
    "75154-1\n",
    "TIE Striker\n",
    "2016\n",
    "541\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "234\n",
    "75155-1\n",
    "Rebel U-Wing Fighter\n",
    "2016\n",
    "657\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "235\n",
    "75156-1\n",
    "Krennic’s Imperial Shuttle\n",
    "2016\n",
    "861\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "236\n",
    "75157-1\n",
    "Captain Rex’s AT-TE\n",
    "2016\n",
    "969\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "237\n",
    "75158-1\n",
    "Rebel Combat Frigate\n",
    "2016\n",
    "934\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "238\n",
    "75160-1\n",
    "U-wing\n",
    "2017\n",
    "108\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "239\n",
    "75161-1\n",
    "Tie Striker Microfighter\n",
    "2017\n",
    "87\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "240\n",
    "75163-1\n",
    "Krennics Imperial Shuttle Microfighter\n",
    "2017\n",
    "77\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "241\n",
    "75164-1\n",
    "Rebel Trooper Battle Pack\n",
    "2017\n",
    "119\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "242\n",
    "75165-1\n",
    "Imperial Trooper Battle Pack\n",
    "2017\n",
    "112\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "243\n",
    "75166-1\n",
    "First Order Transport Speeder Battle Pack\n",
    "2017\n",
    "115\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "244\n",
    "75168-1\n",
    "Yoda's Jedi Starfighter\n",
    "2017\n",
    "262\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "245\n",
    "75169-1\n",
    "Duel on Naboo\n",
    "2017\n",
    "207\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "246\n",
    "75170-1\n",
    "The Phantom\n",
    "2017\n",
    "267\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "247\n",
    "75171-1\n",
    "Battle on Scarif\n",
    "2017\n",
    "419\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "248\n",
    "75172-1\n",
    "Y-Wing Starfighter\n",
    "2017\n",
    "688\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "249\n",
    "75173-1\n",
    "Luke's Landspeeder\n",
    "2017\n",
    "148\n",
    "Dark Red\n",
    "720E0F\n",
    "1\n",
    "250\n",
    "75180-1\n",
    "Rathtar\" Escape\n",
    "2017\n",
    "836\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "251\n",
    "75182-1\n",
    "Republic Fighter Tank\n",
    "2017\n",
    "302\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "252\n",
    "75183-1\n",
    "Darth Vader\" Transformation\n",
    "2017\n",
    "290\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "253\n",
    "75191-1\n",
    "Jedi Starfighter\" With Hyperdrive\n",
    "2017\n",
    "831\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "254\n",
    "75532-1\n",
    "Scout Trooper\" & Speeder Bike\n",
    "2017\n",
    "447\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "255\n",
    "7654-1\n",
    "Droids Battle Pack\n",
    "2007\n",
    "102\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "256\n",
    "7655-1\n",
    "Clone Troopers Battle Pack\n",
    "2007\n",
    "58\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "257\n",
    "7656-1\n",
    "General Grievous Starfighter\n",
    "2007\n",
    "232\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "258\n",
    "7657-1\n",
    "AT-ST\n",
    "2007\n",
    "243\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "259\n",
    "7658-1\n",
    "Y-wing Fighter\n",
    "2007\n",
    "454\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "260\n",
    "7659-1\n",
    "Imperial Landing Craft\n",
    "2007\n",
    "471\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "261\n",
    "7660-1\n",
    "Naboo N-1 Starfighter and Vulture Droid\n",
    "2007\n",
    "279\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "262\n",
    "7661-1\n",
    "Jedi Starfighter with Hyperdrive Booster Ring\n",
    "2007\n",
    "574\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "263\n",
    "7662-1\n",
    "Trade Federation MTT\n",
    "2007\n",
    "1326\n",
    "Reddish Brown\n",
    "582A12\n",
    "1\n",
    "264\n",
    "7663-1\n",
    "Sith Infiltrator\n",
    "2007\n",
    "310\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "265\n",
    "7664-1\n",
    "TIE Crawler\n",
    "2007\n",
    "546\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "266\n",
    "7665-1\n",
    "Republic Cruiser (Limited Edition - with R2-R7)\n",
    "2007\n",
    "920\n",
    "Red\n",
    "C91A09\n",
    "1\n",
    "267\n",
    "7666-1\n",
    "Hoth Rebel Base [Limited Edition - with K-3PO]\n",
    "2007\n",
    "557\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "268\n",
    "7667-1\n",
    "Imperial Dropship\n",
    "2008\n",
    "80\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "269\n",
    "7668-1\n",
    "Rebel Scout Speeder\n",
    "2008\n",
    "81\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "270\n",
    "7669-1\n",
    "Anakin's Jedi Starfighter\n",
    "2008\n",
    "152\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "271\n",
    "7669-2\n",
    "Anakin's Jedi Starfighter Clone Wars White Box\n",
    "2008\n",
    "144\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "272\n",
    "7670-1\n",
    "Hailfire Droid & Spider Droid\n",
    "2008\n",
    "249\n",
    "Reddish Brown\n",
    "582A12\n",
    "1\n",
    "273\n",
    "7670-2\n",
    "Hailfire Droid & Spider Droid Clone Wars White Box\n",
    "2008\n",
    "249\n",
    "Reddish Brown\n",
    "582A12\n",
    "1\n",
    "274\n",
    "7671-1\n",
    "AT-AP Walker\n",
    "2008\n",
    "392\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "275\n",
    "7672-1\n",
    "Rogue Shadow\n",
    "2008\n",
    "481\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "276\n",
    "7673-1\n",
    "Magna Guard Starfighter\n",
    "2008\n",
    "430\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "277\n",
    "7674-1\n",
    "V-19 Torrent\n",
    "2008\n",
    "470\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "278\n",
    "7675-1\n",
    "AT-TE Walker\n",
    "2008\n",
    "809\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "279\n",
    "7676-1\n",
    "Republic Attack Gunship\n",
    "2008\n",
    "1033\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "280\n",
    "7678-1\n",
    "Droid Gunship\n",
    "2008\n",
    "323\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "281\n",
    "7679-1\n",
    "Republic Fighter Tank\n",
    "2008\n",
    "595\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "282\n",
    "7680-1\n",
    "The Twilight - Limited Edition\n",
    "2008\n",
    "887\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "283\n",
    "7681-1\n",
    "Separatist Spider Droid\n",
    "2008\n",
    "214\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "284\n",
    "7748-1\n",
    "Corporate Alliance Tank Droid\n",
    "2009\n",
    "216\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "285\n",
    "7749-1\n",
    "Echo Base\n",
    "2009\n",
    "155\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "286\n",
    "7751-1\n",
    "Ahsoka's Starfighter and Vulture Droid\n",
    "2009\n",
    "290\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "287\n",
    "7752-1\n",
    "Count Dooku's Solar Sailer\n",
    "2009\n",
    "384\n",
    "Reddish Brown\n",
    "582A12\n",
    "1\n",
    "288\n",
    "7753-1\n",
    "Pirate Tank\n",
    "2009\n",
    "371\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "289\n",
    "7754-1\n",
    "Home One Mon Calamari Star Cruiser - Limited Edition\n",
    "2009\n",
    "788\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "290\n",
    "7778-1\n",
    "Midi-Scale Millennium Falcon\n",
    "2009\n",
    "355\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "291\n",
    "7868-1\n",
    "Mace Windu's Jedi Starfighter\n",
    "2011\n",
    "308\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "292\n",
    "7869-1\n",
    "Battle for Geonosis\n",
    "2011\n",
    "335\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "293\n",
    "7877-1\n",
    "Naboo Starfighter\n",
    "2011\n",
    "317\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "294\n",
    "7879-1\n",
    "Hoth Echo Base\n",
    "2011\n",
    "764\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "295\n",
    "7913-1\n",
    "Clone Trooper Battle Pack\n",
    "2011\n",
    "85\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "296\n",
    "7914-1\n",
    "Mandalorian Battle Pack\n",
    "2011\n",
    "68\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "297\n",
    "7915-1\n",
    "Imperial V-wing Starfighter\n",
    "2011\n",
    "138\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "298\n",
    "7929-1\n",
    "The Battle of Naboo\n",
    "2011\n",
    "241\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "299\n",
    "7930-1\n",
    "Bounty Hunter Gunship\n",
    "2011\n",
    "389\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "Lego Database\n",
    "👋 Welcome to your workspace! Here, you can run SQL queries, write Python code, and add text in Markdown. This workspace is automatically connected to a PostgreSQL database containing Lego set information (source).\n",
    "\n",
    "You can click the \"Browse tables\" button in the upper righthand corner of the cell below to view the available tables.\n",
    "\n",
    "To demonstrate the power of this combination, there is a query and a visualization of every Star Wars Lego set, rendered in Plotly.\n",
    "\n",
    "Lego\n",
    "DataFrame available as\n",
    "star_wars_sets\n",
    "WITH star_wars_sets AS (\n",
    "    SELECT set_num\n",
    "    FROM themes \n",
    "    INNER JOIN sets\n",
    "        ON themes.id = sets.theme_id\n",
    "    WHERE parent_id = (\n",
    "        SELECT id FROM themes \n",
    "        WHERE name = 'Star Wars' \n",
    "            AND parent_id IS NULL)\n",
    "),\n",
    " \n",
    "star_wars_sets_info AS (\n",
    "    SELECT\n",
    "        set_num,\n",
    "        set_name, \n",
    "        year, \n",
    "        num_parts, \n",
    "        color_name, \n",
    "        rgb,\n",
    "        ROW_NUMBER() OVER(PARTITION BY set_num ORDER BY number_per_color DESC) AS color_rank\n",
    "    FROM (\n",
    "        SELECT\n",
    "            sets.set_num AS set_num,\n",
    "            rgb, \n",
    "            colors.name AS color_name, \n",
    "            sets.name AS set_name, \n",
    "            year, \n",
    "            num_parts, \n",
    "            SUM(quantity) AS number_per_color\n",
    "        FROM inventory_parts\n",
    "            INNER JOIN inventories\n",
    "                ON inventory_parts.inventory_id = inventories.id\n",
    "            INNER JOIN sets\n",
    "                ON inventories.set_num = sets.set_num\n",
    "            INNER JOIN colors\n",
    "                ON inventory_parts.color_id = colors.id\n",
    "        WHERE sets.set_num IN (SELECT set_num FROM star_wars_sets)\n",
    "        GROUP BY sets.set_num, rgb, color_name, set_name, year, num_parts) AS sub\n",
    ")\n",
    " \n",
    "SELECT * FROM star_wars_sets_info \n",
    "WHERE color_rank = 1\n",
    "set_num\n",
    "set_name\n",
    "year\n",
    "num_parts\n",
    "color_name\n",
    "rgb\n",
    "color_rank\n",
    "300\n",
    "7931-1\n",
    "T6 Jedi Shuttle\n",
    "2011\n",
    "391\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "301\n",
    "7956-1\n",
    "Ewok Attack\n",
    "2011\n",
    "166\n",
    "Reddish Brown\n",
    "582A12\n",
    "1\n",
    "302\n",
    "7957-1\n",
    "Sith Nightspeeder\n",
    "2011\n",
    "213\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "303\n",
    "7959-1\n",
    "Geonosian Starfighter\n",
    "2011\n",
    "163\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "304\n",
    "7961-1\n",
    "Darth Maul's Sith Infiltrator\n",
    "2011\n",
    "478\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "305\n",
    "7962-1\n",
    "Anakin's and Sebulba's Podracers\n",
    "2011\n",
    "810\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "306\n",
    "7964-1\n",
    "Republic Frigate\n",
    "2011\n",
    "1030\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "307\n",
    "7965-1\n",
    "Millennium Falcon\n",
    "2011\n",
    "1253\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "308\n",
    "8014-1\n",
    "Clone Walker Battle Pack\n",
    "2009\n",
    "79\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "309\n",
    "8015-1\n",
    "Assassin Droids Battle Pack\n",
    "2009\n",
    "94\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "310\n",
    "8016-1\n",
    "Hyena Droid Bomber\n",
    "2009\n",
    "231\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "311\n",
    "8017-1\n",
    "Darth Vader's TIE Fighter\n",
    "2009\n",
    "250\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "312\n",
    "8018-1\n",
    "Armored Assault Tank (AAT)\n",
    "2009\n",
    "406\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "313\n",
    "8019-1\n",
    "Republic Attack Shuttle\n",
    "2009\n",
    "636\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "314\n",
    "8029-1\n",
    "Mini Snowspeeder\n",
    "2008\n",
    "66\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "315\n",
    "8036-1\n",
    "Separatist Shuttle\n",
    "2009\n",
    "258\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "316\n",
    "8037-1\n",
    "Anakin's Y-wing Starfighter\n",
    "2009\n",
    "569\n",
    "Yellow\n",
    "F2CD37\n",
    "1\n",
    "317\n",
    "8038-1\n",
    "The Battle of Endor\n",
    "2009\n",
    "890\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "318\n",
    "8039-1\n",
    "Venator-Class Republic Attack Cruiser\n",
    "2009\n",
    "1168\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "319\n",
    "8083-1\n",
    "Rebel Trooper Battle Pack\n",
    "2010\n",
    "79\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "320\n",
    "8084-1\n",
    "Snowtrooper Battle Pack\n",
    "2010\n",
    "74\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "321\n",
    "8085-1\n",
    "Freeco Speeder\n",
    "2010\n",
    "176\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "322\n",
    "8086-1\n",
    "Droid Tri-Fighter\n",
    "2010\n",
    "267\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "323\n",
    "8087-1\n",
    "TIE Defender\n",
    "2010\n",
    "304\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "324\n",
    "8088-1\n",
    "ARC-170 Starfighter\n",
    "2010\n",
    "395\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "325\n",
    "8089-1\n",
    "Hoth Wampa Cave\n",
    "2010\n",
    "296\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "326\n",
    "8091-1\n",
    "Republic Swamp Speeder - Limited Edition\n",
    "2010\n",
    "175\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "327\n",
    "8092-1\n",
    "Luke's Landspeeder\n",
    "2010\n",
    "163\n",
    "Dark Red\n",
    "720E0F\n",
    "1\n",
    "328\n",
    "8093-1\n",
    "Plo Koon’s Jedi Starfighter\n",
    "2010\n",
    "174\n",
    "Blue\n",
    "0055BF\n",
    "1\n",
    "329\n",
    "8095-1\n",
    "General Grievous’ Starfighter\n",
    "2010\n",
    "453\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "330\n",
    "8096-1\n",
    "Emperor Palpatine’s Shuttle\n",
    "2010\n",
    "591\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "331\n",
    "8097-1\n",
    "Slave I (Third Edition) [Initial Release]\n",
    "2010\n",
    "562\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "332\n",
    "8098-1\n",
    "Clone Turbo Tank\n",
    "2010\n",
    "1142\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "333\n",
    "8099-1\n",
    "Midi-Scale Imperial Star Destroyer\n",
    "2010\n",
    "423\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "334\n",
    "8128-1\n",
    "Cad Bane's Speeder\n",
    "2010\n",
    "317\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "335\n",
    "8129-1\n",
    "AT-AT Walker\n",
    "2010\n",
    "811\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "336\n",
    "9488-1\n",
    "Elite Clone Trooper & Commando Droid Battle Pack\n",
    "2012\n",
    "105\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "337\n",
    "9489-1\n",
    "Endor Rebel Trooper & Imperial Trooper Battle Pack\n",
    "2012\n",
    "77\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "338\n",
    "9490-1\n",
    "Droid Escape\n",
    "2012\n",
    "136\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "339\n",
    "9491-1\n",
    "Geonosian Cannon\n",
    "2012\n",
    "133\n",
    "Reddish Brown\n",
    "582A12\n",
    "1\n",
    "340\n",
    "9492-1\n",
    "TIE Fighter\n",
    "2012\n",
    "413\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "341\n",
    "9493-1\n",
    "X-wing Starfighter\n",
    "2012\n",
    "559\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "342\n",
    "9494-1\n",
    "Anakin's Jedi Interceptor\n",
    "2012\n",
    "299\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "343\n",
    "9495-1\n",
    "Gold Leader's Y-wing Starfighter\n",
    "2012\n",
    "457\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "344\n",
    "9496-1\n",
    "Desert Skiff\n",
    "2012\n",
    "220\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "345\n",
    "9498-1\n",
    "Saesee Tiin's Jedi Starfighter\n",
    "2012\n",
    "243\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "346\n",
    "9499-1\n",
    "Gungan Sub\n",
    "2012\n",
    "464\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "347\n",
    "9500-1\n",
    "Sith Fury-class Interceptor\n",
    "2012\n",
    "746\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "348\n",
    "9515-1\n",
    "Malevolence\n",
    "2012\n",
    "1098\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "349\n",
    "9516-1\n",
    "Jabba’s Palace\n",
    "2012\n",
    "716\n",
    "Tan\n",
    "E4CD9E\n",
    "1\n",
    "350\n",
    "9525-1\n",
    "Pre Vizsla's Mandalorian Fighter\n",
    "2012\n",
    "399\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "351\n",
    "9526-1\n",
    "Palpatine’s Arrest\n",
    "2012\n",
    "646\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "352\n",
    "9674-1\n",
    "Naboo Starfighter & Naboo\n",
    "2012\n",
    "56\n",
    "Yellow\n",
    "F2CD37\n",
    "1\n",
    "353\n",
    "9675-1\n",
    "Sebulba's Podracer & Tatooine\n",
    "2012\n",
    "80\n",
    "Orange\n",
    "FE8A18\n",
    "1\n",
    "354\n",
    "9676-1\n",
    "TIE Interceptor & Death Star\n",
    "2012\n",
    "65\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "355\n",
    "9677-1\n",
    "X-wing Starfighter & Yavin 4\n",
    "2012\n",
    "77\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "356\n",
    "9678-1\n",
    "Twin-pod Cloud Car & Bespin\n",
    "2012\n",
    "78\n",
    "Orange\n",
    "FE8A18\n",
    "1\n",
    "357\n",
    "9679-1\n",
    "AT-ST & Endor\n",
    "2012\n",
    "65\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "358\n",
    "CELEBV-1\n",
    "Fan Celebration V - CubeDude - The Bounty Hunter Edition\n",
    "2010\n",
    "497\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "359\n",
    "celebvi-1\n",
    "Boba Fett’s Slave I - Mini - Star Wars Celebration VI Exclusive\n",
    "2012\n",
    "81\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "360\n",
    "comcon001-1\n",
    "Clone Wars Pack - San Diego Comic-Con 2008 Exclusive\n",
    "2008\n",
    "12\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "361\n",
    "comcon004-1\n",
    "Collectible Display Set 1 - San Diego Comic-Con 2009 Exclusive\n",
    "2009\n",
    "5\n",
    "Trans-Purple\n",
    "A5A5CB\n",
    "1\n",
    "362\n",
    "comcon005-1\n",
    "Collectible Display Set 2 - San Diego Comic-Con 2009 Exclusive\n",
    "2009\n",
    "12\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "363\n",
    "comcon006-1\n",
    "Collectible Display Set 4 - San Diego Comic-Con 2009 Exclusive\n",
    "2009\n",
    "30\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "364\n",
    "comcon007-1\n",
    "Collectible Display Set 5 - San Diego Comic-Con 2009 Exclusive\n",
    "2009\n",
    "17\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "365\n",
    "comcon008-1\n",
    "Collectible Display Set 3 - San Diego Comic-Con 2009 Exclusive\n",
    "2009\n",
    "16\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "366\n",
    "comcon009-1\n",
    "Collectible Display Set 6 - San Diego Comic-Con 2009 Exclusive\n",
    "2009\n",
    "5\n",
    "Trans-Light Blue\n",
    "AEEFEC\n",
    "1\n",
    "367\n",
    "comcon011-1\n",
    "LEGO Star Wars Holo-Brick Archives San Diego Comic-Con 2009 Exclusive\n",
    "2009\n",
    "3\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "368\n",
    "comcon024-1\n",
    "Luke Skywalker's Landspeeder - Mini - New York Comic-Con 2012 Exclusive\n",
    "2012\n",
    "110\n",
    "Tan\n",
    "E4CD9E\n",
    "1\n",
    "369\n",
    "comcon039-1\n",
    "The Ghost Starship\n",
    "2014\n",
    "127\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "370\n",
    "sw117promo-1\n",
    "Darth Vader Polybag - 56. International Toy Fair Nuernberg (N├╝rnberg)\n",
    "2005\n",
    "2\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "371\n",
    "sw218promo-1\n",
    "Darth Vader 10 Year Anniversary Promotional Fig polybag\n",
    "2009\n",
    "2\n",
    "Metallic Silver\n",
    "A5A9B4\n",
    "1\n",
    "372\n",
    "SW911508-1\n",
    "Mini Slave 1\n",
    "2015\n",
    "20\n",
    "Dark Bluish Gray\n",
    "6C6E68\n",
    "1\n",
    "373\n",
    "SW911509-1\n",
    "Imperial Shooter\n",
    "2015\n",
    "20\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "374\n",
    "SW911510-1\n",
    "Star Destroyer and TIE Fighter foil pack\n",
    "2015\n",
    "21\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "375\n",
    "SW911612-1\n",
    "Acklay\n",
    "2016\n",
    "49\n",
    "Tan\n",
    "E4CD9E\n",
    "1\n",
    "376\n",
    "SW911719-1\n",
    "Kanan Jarrus foilpack\n",
    "2017\n",
    "6\n",
    "Trans-Light Blue\n",
    "AEEFEC\n",
    "1\n",
    "377\n",
    "SW911720-1\n",
    "The Ghost\n",
    "2017\n",
    "50\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "378\n",
    "SW911724-1\n",
    "A-Wing\n",
    "2017\n",
    "47\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "379\n",
    "SWCOMIC1-1\n",
    "X-Wing\n",
    "2015\n",
    "23\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "380\n",
    "SWMP-1\n",
    "Star Wars / M&M Mosaic - Promo Set\n",
    "2005\n",
    "5461\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "381\n",
    "tf05-1\n",
    "Lego Toy Fair 2005 Star Wars V.I.P. Gala Set\n",
    "2005\n",
    "38\n",
    "Light Bluish Gray\n",
    "A0A5A9\n",
    "1\n",
    "382\n",
    "TRUGHOST-1\n",
    "Rebel Micro Ghost\n",
    "2014\n",
    "42\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "383\n",
    "TRUTIE-1\n",
    "TIE Fighter\n",
    "2014\n",
    "12\n",
    "Black\n",
    "05131D\n",
    "1\n",
    "384\n",
    "TRUXWING-1\n",
    "X-Wing\n",
    "2014\n",
    "23\n",
    "White\n",
    "FFFFFF\n",
    "1\n",
    "385\n",
    "VP-10\n",
    "Star Wars Value Pack with Free LEGO Backpack (K-Mart Australia Exclusive)\n",
    "2003\n",
    "4\n",
    "[No Color]\n",
    "05131D\n",
    "1\n",
    " \t \t \t \t \t \t \t \n",
    " \t \t \t \t \t \t \t \n",
    " \t \t \t \t \t \t \t \n",
    " \t \t \t \t \t \t \t \n",
    " \t \t \t \t \t \t \t \n",
    " \t \t \t \t \t \t \t \n",
    " \t \t \t \t \t \t \t \n",
    " \t \t \t \t \t \t \t \n",
    " \t \t \t \t \t \t \t \n",
    " \t \t \t \t \t \t \t \n",
    " \t \t \t \t \t \t \t \n",
    " \t \t \t \t \t \t \t \n",
    " \t \t \t \t \t \t \t \n",
    " \t \t \t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bicycle Sales Database\n",
    "👋 Welcome to your workspace! Here, you can run SQL queries, write Python code, and add text in Markdown. This workspace is automatically connected to a Microsoft SQL Server database containing two schemas (sales and production), which in turn have various details about the sales and inventory for bike stores (source).\n",
    "\n",
    "You can click the \"Browse tables\" button in the upper righthand corner of the cell below to view the available tables for each schema.\n",
    "\n",
    "There is a short query and a visualization of the number of bicycles in stock by year and category rendered in Plotly to get you started.\n",
    "\n",
    "Bicycle Sales\n",
    "DataFrame available as\n",
    "bicycle_sales\n",
    "SELECT \n",
    "    model_year,\n",
    "    category_name,\n",
    "    SUM(quantity) AS total\n",
    "FROM production.products AS pp\n",
    "INNER JOIN production.categories AS pc\n",
    "    ON pp.category_id = pc.category_id\n",
    "INNER JOIN production.brands AS pb\n",
    "    ON pp.brand_id = pb.brand_id\n",
    "INNER JOIN production.stocks AS ps\n",
    "    ON pp.product_id = ps.product_id\n",
    "GROUP BY model_year, category_name\n",
    "ORDER BY model_year, category_name \n",
    "model_year\n",
    "category_name\n",
    "total\n",
    "0\n",
    "2016\n",
    "Children Bicycles\n",
    "126\n",
    "1\n",
    "2016\n",
    "Comfort Bicycles\n",
    "132\n",
    "2\n",
    "2016\n",
    "Cruisers Bicycles\n",
    "354\n",
    "3\n",
    "2016\n",
    "Cyclocross Bicycles\n",
    "108\n",
    "4\n",
    "2016\n",
    "Electric Bikes\n",
    "51\n",
    "5\n",
    "2016\n",
    "Mountain Bikes\n",
    "275\n",
    "6\n",
    "2017\n",
    "Children Bicycles\n",
    "914\n",
    "7\n",
    "2017\n",
    "Comfort Bicycles\n",
    "460\n",
    "8\n",
    "2017\n",
    "Cruisers Bicycles\n",
    "905\n",
    "9\n",
    "2017\n",
    "Cyclocross Bicycles\n",
    "46\n",
    "10\n",
    "2017\n",
    "Electric Bikes\n",
    "105\n",
    "11\n",
    "2017\n",
    "Mountain Bikes\n",
    "926\n",
    "12\n",
    "2017\n",
    "Road Bikes\n",
    "431\n",
    "13\n",
    "2018\n",
    "Children Bicycles\n",
    "1568\n",
    "14\n",
    "2018\n",
    "Comfort Bicycles\n",
    "666\n",
    "15\n",
    "2018\n",
    "Cruisers Bicycles\n",
    "2119\n",
    "16\n",
    "2018\n",
    "Cyclocross Bicycles\n",
    "260\n",
    "17\n",
    "2018\n",
    "Electric Bikes\n",
    "952\n",
    "18\n",
    "2018\n",
    "Mountain Bikes\n",
    "1453\n",
    "19\n",
    "2018\n",
    "Road Bikes\n",
    "1660\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    " \n",
    "# Create bar chart\n",
    "fig = px.bar(\n",
    "    bicycle_sales,\n",
    "    x=bicycle_sales[\"model_year\"].astype(str),\n",
    "    y=\"total\",\n",
    "    color=\"category_name\",\n",
    ")\n",
    " \n",
    "# Create labels and show plot\n",
    "fig.update_layout(\n",
    "    title=\"Bicycle Inventory<br><sup>By Quantity and Category of Product</sup>\",\n",
    "    title_x=0.5,\n",
    "    xaxis_title=\"Year\",\n",
    "    yaxis_title=\"Quantity in Stock\",\n",
    "    legend_title=\"Bicycle Category\",\n",
    "    template=\"plotly_dark\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "This is an interactive plot! Hover over different elements of the plot to explore the data!\n",
    "\n",
    "💪 Now it's your turn to construct your own queries and analyze the data! Remember, you can review the tables in the database at any point using the \"Browse tables\" button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oracle Sample Database\n",
    "Summary: this tutorial introduces you to an Oracle sample database and provides you with the links for you to download it.\n",
    "\n",
    "Note that this tutorial explains you the Oracle Sample Database. It doesn’t show you how to create this sample database in Oracle. To create this database for practicing, you follow the creating the Oracle Sample Database tutorial.\n",
    "\n",
    "Introduction to the OT Oracle sample database\n",
    "We provide you with an Oracle sample database named OT which is based on a global fictitious company that sells computer hardware including storage, motherboard, RAM, video card, and CPU.\n",
    "\n",
    "The company maintains the product information such as name, description standard cost, list price, and product line. It also tracks the inventory information for all products including warehouses where products are available. Because the company operates globally, it has warehouses in various locations around the world.\n",
    "\n",
    "The company records all customer information including name, address, and website. Each customer has at least one contact person with detailed information including name, email, and phone. The company also places a credit limit on each customer to limit the amount that customer can owe.\n",
    "\n",
    "Whenever a customer issues a purchase order, a sales order is created in the database with the pending status. When the company ships the order, the order status becomes shipped. In case the customer cancels an order, the order status becomes canceled.\n",
    "\n",
    "In addition to the sales information, the employee data is recorded with some basic information such as name, email, phone, job title, manager, and hire date.\n",
    "\n",
    "Oracle sample database diagram\n",
    "The following illustrates the sample database diagram:\n",
    "\n",
    "Oracle Sample Database\n",
    "Table Names\tDescription\tRecords\n",
    " CONTACTS\tstore contact person information of customers\t319 records\n",
    " COUNTRIES\tstore country information\t25 records\n",
    " CUSTOMERS\tstore customer master\t319 records\n",
    " EMPLOYEES\tstore employee master\t107 records\n",
    " INVENTORIES\tstore inventory information of products\t1112 records\n",
    " LOCATIONS\tstore locations of warehouses\t23 records\n",
    " ORDERS\tstore order header information\t105 records\n",
    " ORDER_ITEMS\tstore order line items\t665 records\n",
    " PRODUCT_CATEGORIES\tstore product categories\t5 records\n",
    " PRODUCTS\tstore product information\t288 records\n",
    " REGIONS\tstore regions where the company operates\t4 records\n",
    " WAREHOUSES\tstore warehouse information\t9 records\n",
    "Download Oracle Sample database\n",
    "Download the following sample database in zip file format:\n",
    "\n",
    "Download Oracle Sample Database\n",
    "\n",
    "After downloading the file, you should extract it. The zip file contains the following *.sql files:\n",
    "\n",
    " ot_create_user.sql is for creating OT user and grant privileges\n",
    " ot_schema.sql is for creating database objects such as tables, constraints, etc.\n",
    " ot_data.sql is for loading data into the tables.\n",
    " ot_drop.sql is for removing all objects in the sample database.\n",
    "The following illustrates the statements for creating the database objects.\n",
    "\n",
    "CREATE TABLE regions\n",
    "  (\n",
    "    region_id NUMBER GENERATED BY DEFAULT AS IDENTITY\n",
    "    START WITH 5 PRIMARY KEY,\n",
    "    region_name VARCHAR2( 50 ) NOT NULL\n",
    "  );\n",
    "-- countries table\n",
    "CREATE TABLE countries\n",
    "  (\n",
    "    country_id   CHAR( 2 ) PRIMARY KEY  ,\n",
    "    country_name VARCHAR2( 40 ) NOT NULL,\n",
    "    region_id    NUMBER                 , -- fk\n",
    "    CONSTRAINT fk_countries_regions FOREIGN KEY( region_id )\n",
    "      REFERENCES regions( region_id ) \n",
    "      ON DELETE CASCADE\n",
    "  );\n",
    "\n",
    "-- location\n",
    "CREATE TABLE locations\n",
    "  (\n",
    "    location_id NUMBER GENERATED BY DEFAULT AS IDENTITY START WITH 24 \n",
    "                PRIMARY KEY       ,\n",
    "    address     VARCHAR2( 255 ) NOT NULL,\n",
    "    postal_code VARCHAR2( 20 )          ,\n",
    "    city        VARCHAR2( 50 )          ,\n",
    "    state       VARCHAR2( 50 )          ,\n",
    "    country_id  CHAR( 2 )               , -- fk\n",
    "    CONSTRAINT fk_locations_countries \n",
    "      FOREIGN KEY( country_id )\n",
    "      REFERENCES countries( country_id ) \n",
    "      ON DELETE CASCADE\n",
    "  );\n",
    "-- warehouses\n",
    "CREATE TABLE warehouses\n",
    "  (\n",
    "    warehouse_id NUMBER \n",
    "                 GENERATED BY DEFAULT AS IDENTITY START WITH 10 \n",
    "                 PRIMARY KEY,\n",
    "    warehouse_name VARCHAR( 255 ) ,\n",
    "    location_id    NUMBER( 12, 0 ), -- fk\n",
    "    CONSTRAINT fk_warehouses_locations \n",
    "      FOREIGN KEY( location_id )\n",
    "      REFERENCES locations( location_id ) \n",
    "      ON DELETE CASCADE\n",
    "  );\n",
    "-- employees\n",
    "CREATE TABLE employees\n",
    "  (\n",
    "    employee_id NUMBER \n",
    "                GENERATED BY DEFAULT AS IDENTITY START WITH 108 \n",
    "                PRIMARY KEY,\n",
    "    first_name VARCHAR( 255 ) NOT NULL,\n",
    "    last_name  VARCHAR( 255 ) NOT NULL,\n",
    "    email      VARCHAR( 255 ) NOT NULL,\n",
    "    phone      VARCHAR( 50 ) NOT NULL ,\n",
    "    hire_date  DATE NOT NULL          ,\n",
    "    manager_id NUMBER( 12, 0 )        , -- fk\n",
    "    job_title  VARCHAR( 255 ) NOT NULL,\n",
    "    CONSTRAINT fk_employees_manager \n",
    "        FOREIGN KEY( manager_id )\n",
    "        REFERENCES employees( employee_id )\n",
    "        ON DELETE CASCADE\n",
    "  );\n",
    "-- product category\n",
    "CREATE TABLE product_categories\n",
    "  (\n",
    "    category_id NUMBER \n",
    "                GENERATED BY DEFAULT AS IDENTITY START WITH 6 \n",
    "                PRIMARY KEY,\n",
    "    category_name VARCHAR2( 255 ) NOT NULL\n",
    "  );\n",
    "\n",
    "-- products table\n",
    "CREATE TABLE products\n",
    "  (\n",
    "    product_id NUMBER \n",
    "               GENERATED BY DEFAULT AS IDENTITY START WITH 289 \n",
    "               PRIMARY KEY,\n",
    "    product_name  VARCHAR2( 255 ) NOT NULL,\n",
    "    description   VARCHAR2( 2000 )        ,\n",
    "    standard_cost NUMBER( 9, 2 )          ,\n",
    "    list_price    NUMBER( 9, 2 )          ,\n",
    "    category_id   NUMBER NOT NULL         ,\n",
    "    CONSTRAINT fk_products_categories \n",
    "      FOREIGN KEY( category_id )\n",
    "      REFERENCES product_categories( category_id ) \n",
    "      ON DELETE CASCADE\n",
    "  );\n",
    "-- customers\n",
    "CREATE TABLE customers\n",
    "  (\n",
    "    customer_id NUMBER \n",
    "                GENERATED BY DEFAULT AS IDENTITY START WITH 320 \n",
    "                PRIMARY KEY,\n",
    "    name         VARCHAR2( 255 ) NOT NULL,\n",
    "    address      VARCHAR2( 255 )         ,\n",
    "    website      VARCHAR2( 255 )         ,\n",
    "    credit_limit NUMBER( 8, 2 )\n",
    "  );\n",
    "-- contacts\n",
    "CREATE TABLE contacts\n",
    "  (\n",
    "    contact_id NUMBER \n",
    "               GENERATED BY DEFAULT AS IDENTITY START WITH 320 \n",
    "               PRIMARY KEY,\n",
    "    first_name  VARCHAR2( 255 ) NOT NULL,\n",
    "    last_name   VARCHAR2( 255 ) NOT NULL,\n",
    "    email       VARCHAR2( 255 ) NOT NULL,\n",
    "    phone       VARCHAR2( 20 )          ,\n",
    "    customer_id NUMBER                  ,\n",
    "    CONSTRAINT fk_contacts_customers \n",
    "      FOREIGN KEY( customer_id )\n",
    "      REFERENCES customers( customer_id ) \n",
    "      ON DELETE CASCADE\n",
    "  );\n",
    "-- orders table\n",
    "CREATE TABLE orders\n",
    "  (\n",
    "    order_id NUMBER \n",
    "             GENERATED BY DEFAULT AS IDENTITY START WITH 106 \n",
    "             PRIMARY KEY,\n",
    "    customer_id NUMBER( 6, 0 ) NOT NULL, -- fk\n",
    "    status      VARCHAR( 20 ) NOT NULL ,\n",
    "    salesman_id NUMBER( 6, 0 )         , -- fk\n",
    "    order_date  DATE NOT NULL          ,\n",
    "    CONSTRAINT fk_orders_customers \n",
    "      FOREIGN KEY( customer_id )\n",
    "      REFERENCES customers( customer_id )\n",
    "      ON DELETE CASCADE,\n",
    "    CONSTRAINT fk_orders_employees \n",
    "      FOREIGN KEY( salesman_id )\n",
    "      REFERENCES employees( employee_id ) \n",
    "      ON DELETE SET NULL\n",
    "  );\n",
    "-- order items\n",
    "CREATE TABLE order_items\n",
    "  (\n",
    "    order_id   NUMBER( 12, 0 )                                , -- fk\n",
    "    item_id    NUMBER( 12, 0 )                                ,\n",
    "    product_id NUMBER( 12, 0 ) NOT NULL                       , -- fk\n",
    "    quantity   NUMBER( 8, 2 ) NOT NULL                        ,\n",
    "    unit_price NUMBER( 8, 2 ) NOT NULL                        ,\n",
    "    CONSTRAINT pk_order_items \n",
    "      PRIMARY KEY( order_id, item_id ),\n",
    "    CONSTRAINT fk_order_items_products \n",
    "      FOREIGN KEY( product_id )\n",
    "      REFERENCES products( product_id ) \n",
    "      ON DELETE CASCADE,\n",
    "    CONSTRAINT fk_order_items_orders \n",
    "      FOREIGN KEY( order_id )\n",
    "      REFERENCES orders( order_id ) \n",
    "      ON DELETE CASCADE\n",
    "  );\n",
    "-- inventories\n",
    "CREATE TABLE inventories\n",
    "  (\n",
    "    product_id   NUMBER( 12, 0 )        , -- fk\n",
    "    warehouse_id NUMBER( 12, 0 )        , -- fk\n",
    "    quantity     NUMBER( 8, 0 ) NOT NULL,\n",
    "    CONSTRAINT pk_inventories \n",
    "      PRIMARY KEY( product_id, warehouse_id ),\n",
    "    CONSTRAINT fk_inventories_products \n",
    "      FOREIGN KEY( product_id )\n",
    "      REFERENCES products( product_id ) \n",
    "      ON DELETE CASCADE,\n",
    "    CONSTRAINT fk_inventories_warehouses \n",
    "      FOREIGN KEY( warehouse_id )\n",
    "      REFERENCES warehouses( warehouse_id ) \n",
    "      ON DELETE CASCADE\n",
    "  );\n",
    "Code language: SQL (Structured Query Language) (sql)\n",
    "In this tutorial, we have introduced you the Oracle sample database and shown you how to download it. Now, you should be ready to create the sample database in your Oracle database server for practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# computer hardware dtabase\n",
    "Computer Hardware Database\n",
    "👋 Welcome to your workspace! Here, you can run SQL queries, write Python code, and add text in Markdown. This workspace is automatically connected to an Oracle database containing order and inventory details for a computer hardware firm (source).\n",
    "\n",
    "You can click the \"Browse tables\" button in the upper righthand corner of the cell below to view the available tables.\n",
    "\n",
    "There is a short query and a visualization of the inventory stored in warehouses in each country rendered in Plotly to get you started.\n",
    "\n",
    "Computer Hardware\n",
    "DataFrame available as\n",
    "hardware_inventory\n",
    "SELECT \n",
    "    country_name AS \"country_name\",\n",
    "    SUM(quantity) AS \"total_inventory\",\n",
    "    SUM(CASE WHEN category_name = 'Video Card' THEN quantity\n",
    "        ELSE 0 END) AS \"video_card\",\n",
    "    SUM(CASE WHEN category_name = 'Mother Board' THEN quantity\n",
    "        ELSE 0 END) AS \"motherboard\",\n",
    "    SUM(CASE WHEN category_name = 'CPU' THEN quantity\n",
    "        ELSE 0 END) AS \"cpu\",\n",
    "    SUM(CASE WHEN category_name = 'Storage' THEN quantity\n",
    "        ELSE 0 END) AS \"storage\"\n",
    "FROM inventories\n",
    "INNER JOIN warehouses using(warehouse_id)\n",
    "INNER JOIN locations using(location_id)\n",
    "INNER JOIN countries using(country_id)\n",
    "INNER JOIN products using(product_id)\n",
    "INNER JOIN product_categories using (category_id)\n",
    "GROUP BY country_name\n",
    "country_name\n",
    "total_inventory\n",
    "video_card\n",
    "motherboard\n",
    "cpu\n",
    "storage\n",
    "0\n",
    "Mexico\n",
    "9039\n",
    "3185\n",
    "544\n",
    "892\n",
    "4418\n",
    "1\n",
    "United States of America\n",
    "56208\n",
    "22133\n",
    "5193\n",
    "9631\n",
    "19251\n",
    "2\n",
    "Australia\n",
    "20457\n",
    "4047\n",
    "3707\n",
    "3381\n",
    "9322\n",
    "3\n",
    "China\n",
    "13482\n",
    "2688\n",
    "1779\n",
    "2472\n",
    "6543\n",
    "4\n",
    "Canada\n",
    "12969\n",
    "4714\n",
    "946\n",
    "1295\n",
    "6014\n",
    "5\n",
    "India\n",
    "7357\n",
    "2229\n",
    "1178\n",
    "1925\n",
    "2025\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    " \n",
    "# Create choropleth map\n",
    "fig = px.choropleth(\n",
    "    hardware_inventory,\n",
    "    locations=\"country_name\",\n",
    "    color=\"total_inventory\",\n",
    "    locationmode=\"country names\",\n",
    "    custom_data=[\n",
    "        \"country_name\",\n",
    "        \"total_inventory\",\n",
    "        \"video_card\",\n",
    "        \"motherboard\",\n",
    "        \"cpu\",\n",
    "        \"storage\",\n",
    "    ],\n",
    "    color_continuous_scale=px.colors.sequential.Viridis,\n",
    ")\n",
    " \n",
    "# Customize data to show\n",
    "fig.update_traces(\n",
    "    hovertemplate=\"<br>\".join(\n",
    "        [\n",
    "            \"<b>%{customdata[0]}</b>\",\n",
    "            \"<br>\",\n",
    "            \"<b>Total Inventory:</b> %{customdata[1]}\",\n",
    "            \"<b>Video card:</b> %{customdata[2]}\",\n",
    "            \"<b>Motherboard:</b> %{customdata[3]}\",\n",
    "            \"<b>CPU:</b> %{customdata[4]}\",\n",
    "            \"<b>Storage:</b> %{customdata[5]}\",\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    " \n",
    "# Create labels and show plot\n",
    "fig.update_layout(\n",
    "    title=\"Worldwide Computer Hardware Inventory<br><sup>Total Quantity and by Category</sup>\",\n",
    "    title_x=0.5,\n",
    "    template=\"plotly_dark\",\n",
    "    coloraxis_colorbar_title_text=\"Total Inventory\",\n",
    ")\n",
    " \n",
    "fig.show()\n",
    "\n",
    "This is an interactive plot! Hover over different countries to view the total quantity of each category in storage.\n",
    "\n",
    "💪 Now it's your turn to construct your own queries and analyze the data! Remember, you can review the tables in the database at any point using the \"Browse tables\" button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# employee database\n",
    "Employee Database\n",
    "👋 Welcome to your workspace! Here, you can run SQL queries, write Python code, and add text in Markdown. This workspace is automatically connected to a MySQL database containing employee data for a fictional company (source).\n",
    "\n",
    "You can click the \"Browse tables\" button in the upper righthand corner of the cell below to view the available tables.\n",
    "\n",
    "There is a short query and a visualization of the average time an employee spends at the company (for those that left) by position rendered in Plotly to get you started.\n",
    "\n",
    "Employees\n",
    "DataFrame available as\n",
    "employee_tenures\n",
    "WITH past_employees AS (\n",
    "    SELECT \n",
    "        emp_no,\n",
    "        Min(from_date) AS start_date,\n",
    "        Max(to_date) AS end_date\n",
    "    FROM salaries\n",
    "    WHERE to_date != (SELECT MAX(to_date) FROM salaries)\n",
    "    GROUP BY emp_no),\n",
    "    \n",
    "employee_tenures AS (\n",
    "    SELECT \n",
    "        emp_no,\n",
    "        start_date,\n",
    "        end_date,\n",
    "        DATEDIFF(end_date, start_date) / 365 AS employee_tenure\n",
    "    FROM past_employees)\n",
    " \n",
    "SELECT \n",
    "    title,\n",
    "    AVG(employee_tenure) AS avg_employee_tenure\n",
    "FROM employee_tenures\n",
    "INNER JOIN titles\n",
    "    ON employee_tenures.emp_no = titles.emp_no\n",
    "    AND employee_tenures.end_date = titles.to_date\n",
    "GROUP BY title\n",
    "ORDER BY avg_employee_tenure\n",
    "title\n",
    "avg_employee_tenure\n",
    "0\n",
    "Assistant Engineer\n",
    "2.92876533\n",
    "1\n",
    "Staff\n",
    "2.98147199\n",
    "2\n",
    "Engineer\n",
    "3.26380921\n",
    "3\n",
    "Technique Leader\n",
    "4.8392571\n",
    "4\n",
    "Senior Engineer\n",
    "7.3372753\n",
    "5\n",
    "Senior Staff\n",
    "7.44855334\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    " \n",
    "# Create bar chart\n",
    "fig = px.bar(employee_tenures, x=\"avg_employee_tenure\", y=\"title\")\n",
    " \n",
    "# Create labels and show plot\n",
    "fig.update_layout(\n",
    "    title=\"Average Employee Tenure<br><sup>By Last Held Position</sup>\",\n",
    "    title_x=0.5,\n",
    "    xaxis_title=\"Average Tenure\",\n",
    "    yaxis_title=\"Position\",\n",
    "    legend_title=\"Position\",\n",
    "    template=\"plotly_dark\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "This is an interactive plot! Hover over the bars to get the precise average tenure of each position.\n",
    "\n",
    "💪 Now it's your turn to construct your own queries and analyze the data! Remember, you can review the tables in the database at any point using the \"Browse tables\" button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which \n",
    "statement \n",
    "is \n",
    "true \n",
    "about \n",
    "a \n",
    "nonlocal \n",
    "variable? \n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "A nonlocal variable in Python is used inside a nested function to refer to a variable in the nearest enclosing scope (excluding the global scope). Here’s the correct statement about a nonlocal variable:\n",
    "\n",
    "A nonlocal variable allows a nested function to modify a variable in its enclosing scope.\n",
    "This is in contrast to a local variable, which only exists within the function where it is declared, and a global variable, which is accessible throughout the entire module. The nonlocal keyword is used to declare a variable as nonlocal.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Function \n",
    "multiply\n",
    "multiplies \n",
    "the \n",
    "value \n",
    "by \n",
    "10 \n",
    "and \n",
    "function \n",
    "subtract\n",
    "subtracts \n",
    "10. \n",
    "\n",
    "my_function = [multiply, subtract, print]\n",
    "my_function[1](123)\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "In the code snippet provided:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "my_function = [multiply, subtract, print]\n",
    "my_function \n",
    "Here's what happens step by step:\n",
    "\n",
    "my_function is a list containing three function references: multiply, subtract, and print.\n",
    "my_function[1] accesses the second element of the list, which is the subtract function.\n",
    "my_function calls the subtract function with the argument 123.\n",
    "Given the description that the subtract function subtracts 10, the expected result would be:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "123 - 10 = 113\n",
    "So, the output of this code will be 113.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "\n",
    "Complete \n",
    "the \n",
    "code \n",
    "to \n",
    "return \n",
    "the \n",
    "output \n",
    "def eur_to_dol(\n",
    "?\n",
    "):\n",
    "  def wrapper(a, b):\n",
    "    return func(a, b * 1.08)\n",
    "  return \n",
    "?\n",
    "@\n",
    "?\n",
    "def budgeting(a, b):\n",
    "  print(a*b)\n",
    "budgeting(20, 34)\n",
    "734.4\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "To complete the code so that the budgeting function, when decorated with eur_to_dol, returns the correct output 734.4, follow these steps:\n",
    "\n",
    "Define the decorator eur_to_dol to wrap around budgeting.\n",
    "Use the @ syntax to apply the decorator to budgeting.\n",
    "Ensure the decorator function modifies the second argument by multiplying it by 1.08.\n",
    "Here is the completed code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "def eur_to_dol(func):\n",
    "    def wrapper(a, b):\n",
    "        return func(a, b * 1.08)\n",
    "    return wrapper\n",
    "\n",
    "@eur_to_dol\n",
    "def budgeting(a, b):\n",
    "    print(a * b)\n",
    "\n",
    "budgeting(20, 34)  # Output should be 734.4\n",
    "Explanation:\n",
    "eur_to_dol is a decorator function that takes another function func as its argument.\n",
    "Inside eur_to_dol, the wrapper function modifies the second argument b by multiplying it by 1.08 (conversion rate from euros to dollars) before passing it to the func.\n",
    "The @eur_to_dol decorator is applied to the budgeting function, so when budgeting(20, 34) is called, the second argument 34 is first converted to 34 * 1.08 = 36.72, and then 20 * 36.72 results in 734.4.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "\n",
    "Complete \n",
    "the \n",
    "code \n",
    "to \n",
    "return \n",
    "the \n",
    "output \n",
    "def compute():\n",
    "  lst = [15, 9, 55, 41, 35, 20, 62, 49]\n",
    "  def Average():\n",
    "    print(sum(lst)/ len(lst))\n",
    "  return Average\n",
    "func = compute()\n",
    "?\n",
    "[15, 9, 55, 41, 35, 20, 62, 49]\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "To complete the code so that it calculates and prints the average of the list [15, 9, 55, 41, 35, 20, 62, 49], you should call the function returned by compute().\n",
    "\n",
    "Here’s the completed code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "def compute():\n",
    "    lst = [15, 9, 55, 41, 35, 20, 62, 49]\n",
    "    def Average():\n",
    "        print(sum(lst) / len(lst))\n",
    "    return Average\n",
    "\n",
    "func = compute()\n",
    "func()  # This will print the average of the list\n",
    "Explanation:\n",
    "The compute function defines a list lst and a nested function Average that calculates and prints the average of the list.\n",
    "compute then returns the Average function.\n",
    "The line func = compute() assigns the Average function to func.\n",
    "Calling func() will execute the Average function, which prints the average of the list [15, 9, 55, 41, 35, 20, 62, 49].\n",
    "The output will be:\n",
    "\n",
    "Copy code\n",
    "35.75\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "func.__closure__.cell_contents\n",
    "\n",
    "is this the correct syntax\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "Yes, the syntax func.__closure__[0].cell_contents is correct for accessing the contents of a closure cell in Python. However, your original example involves calling func() to execute the function, which is more typical. If you want to inspect the closure (the variables captured by the nested function), you can use the __closure__ attribute.\n",
    "\n",
    "Here’s how you can use it in your example:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "def compute():\n",
    "    lst = [15, 9, 55, 41, 35, 20, 62, 49]\n",
    "    def Average():\n",
    "        print(sum(lst) / len(lst))\n",
    "    return Average\n",
    "\n",
    "func = compute()\n",
    "print(func.__closure__[0].cell_contents)  # Accesses the closure's contents (lst)\n",
    "Explanation:\n",
    "func.__closure__ is a tuple containing the cells that hold the variables captured by the closure.\n",
    "func.__closure__[0] accesses the first (and in this case, only) cell.\n",
    "func.__closure__[0].cell_contents retrieves the actual value of the variable, which would be the list lst.\n",
    "The output of print(func.__closure__[0].cell_contents) will be:\n",
    "\n",
    "csharp\n",
    "Copy code\n",
    "[15, 9, 55, 41, 35, 20, 62, 49]\n",
    "This confirms that lst is correctly captured by the closure.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "What \n",
    "is \n",
    "the \n",
    "correct \n",
    "way \n",
    "to \n",
    "define \n",
    "default \n",
    "arguments? \n",
    "my_df\n",
    "DataFrame \n",
    "has \n",
    "been \n",
    "created \n",
    "for \n",
    "you. \n",
    "Complete \n",
    "the \n",
    "code \n",
    "to \n",
    "return \n",
    "the \n",
    "output \n",
    "def add_column(col_name, values, df = \n",
    "?\n",
    "):\n",
    "  if df \n",
    "?\n",
    ":\n",
    "    df = pandas.DataFrame()\n",
    "  df[\"Tourists\"] = [\"200\", \"19\", \"74\"]\n",
    "  return df\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "Memory updated\n",
    "To complete the code with the correct syntax for defining default arguments and handling the DataFrame, you can set my_df as the default argument. Here's the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "def add_column(col_name, values, df=None):\n",
    "    if df is None:\n",
    "        df = pd.DataFrame()\n",
    "    df[col_name] = values\n",
    "    return df\n",
    "\n",
    "# Assuming my_df exists\n",
    "my_df = pd.DataFrame()\n",
    "\n",
    "# Example usage:\n",
    "result_df = add_column(\"Tourists\", [\"200\", \"19\", \"74\"], my_df)\n",
    "print(result_df)\n",
    "Explanation:\n",
    "Default Argument (df=None):\n",
    "The default value for df is set to None. This allows you to check whether a DataFrame was passed when the function is called.\n",
    "Condition to Check df:\n",
    "The line if df is None: checks if no DataFrame was provided. If so, it creates a new, empty DataFrame.\n",
    "Adding the Column:\n",
    "The new column col_name is added to the DataFrame with the provided values.\n",
    "This way, if you pass my_df to the function, the column is added to it. If no DataFrame is passed, a new DataFrame is created, and the column is added to that.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "\n",
    "Format \n",
    "the \n",
    "code \n",
    "using \n",
    "Numpy \n",
    "docstrings. \n",
    "Complete \n",
    "the \n",
    "code \n",
    "to \n",
    "return \n",
    "the \n",
    "output \n",
    "def clean_df(df, column, file_name=\"df2\"):\n",
    "  \"\"\"\n",
    "  ...\n",
    "  Parameters\n",
    "  ----------\n",
    "  \n",
    "?\n",
    " : pandas.DataFrame\n",
    "  \tdescription\n",
    "  column : str\n",
    "  file_name : str, \n",
    "?\n",
    "  \tDefault=\"df2\"\n",
    "    \n",
    "  \n",
    "?\n",
    "  _________\n",
    "  ...\n",
    "  \"\"\"\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "Here’s how you can format the code using NumPy-style docstrings and complete the function signature:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "def clean_df(df, column, file_name=\"df2\"):\n",
    "    \"\"\"\n",
    "    Cleans the DataFrame by removing a specified column and saving the cleaned DataFrame to a file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame to be cleaned.\n",
    "    column : str\n",
    "        The name of the column to be removed from the DataFrame.\n",
    "    file_name : str, optional\n",
    "        The name of the file where the cleaned DataFrame will be saved. \n",
    "        Default is \"df2\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The cleaned DataFrame with the specified column removed.\n",
    "\n",
    "    \"\"\"\n",
    "    df.drop(column, axis=1, inplace=True)\n",
    "    df.to_csv(f\"{file_name}.csv\", index=False)\n",
    "    return df\n",
    "Explanation:\n",
    "Parameters Section:\n",
    "\n",
    "df : pandas.DataFrame describes the DataFrame input.\n",
    "column : str indicates that a column name should be provided as a string.\n",
    "file_name : str, optional describes the optional parameter with a default value of \"df2\".\n",
    "Returns Section:\n",
    "\n",
    "Describes that the function returns a cleaned pandas.DataFrame.\n",
    "This docstring provides a clear, structured explanation of the function’s purpose, parameters, and return value, following the NumPy documentation style.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "How \n",
    "can \n",
    "you \n",
    "retrieve \n",
    "a \n",
    "docstring \n",
    "from \n",
    "a \n",
    "function? \n",
    "Complete \n",
    "the \n",
    "code \n",
    "to \n",
    "return \n",
    "the \n",
    "output \n",
    "def math_is_fun(x1, x2):\n",
    "  \"\"\" \n",
    "  Args:\n",
    "    x1 (int): first number\n",
    "    x2 (int): second number\n",
    "  \n",
    "  Returns:\n",
    "    int\n",
    "  \"\"\"\n",
    "  return x1 * x2\n",
    "print(\n",
    "?\n",
    ")\n",
    " \n",
    "  Args:\n",
    "    x1 (int): first number\n",
    "    x2 (int): second number\n",
    "  \n",
    "  Returns:\n",
    "    int\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "To retrieve a docstring from a function, you can use the __doc__ attribute of the function. Here's the completed code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "def math_is_fun(x1, x2):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        x1 (int): first number\n",
    "        x2 (int): second number\n",
    "\n",
    "    Returns:\n",
    "        int\n",
    "    \"\"\"\n",
    "    return x1 * x2\n",
    "\n",
    "print(math_is_fun.__doc__)\n",
    "Explanation:\n",
    "math_is_fun.__doc__ retrieves the docstring of the math_is_fun function.\n",
    "The print() function then prints out the docstring.\n",
    "The output will be:\n",
    "\n",
    "sql\n",
    "Copy code\n",
    " \n",
    "Args:\n",
    "    x1 (int): first number\n",
    "    x2 (int): second number\n",
    "\n",
    "Returns:\n",
    "    int\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
