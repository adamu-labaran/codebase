{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Drop the Latitude and Longitude columns from volunteer, storing as volunteer_cols.\n",
    "Subset volunteer_cols by dropping rows containing missing values in the category_desc, and store in a new variable called volunteer_subset.\n",
    "Take a look at the .shape attribute of volunteer_subset, to verify it worked correctly.\n",
    "'''\n",
    "\n",
    "# Drop the Latitude and Longitude columns from volunteer\n",
    "volunteer_cols = volunteer.drop(columns=['Latitude', 'Longitude'])\n",
    "\n",
    "\n",
    "# Drop rows with missing category_desc values from volunteer_cols\n",
    "volunteer_subset = volunteer_cols.dropna(subset=['category_desc'])\n",
    "\n",
    "\n",
    "# Print out the shape of the subset\n",
    "print(volunteer_subset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Search the text in the length argument for numbers and decimals using an appropriate pattern.\n",
    "Extract the matched pattern and convert it to a float.\n",
    "Apply the return_mileage() function to each row in the hiking[\"Length\"] column.\n",
    "'''\n",
    "# Write a pattern to extract numbers and decimals\n",
    "def return_mileage(length):\n",
    "    \n",
    "    # Search the text for matches\n",
    "    mile = re.____(____, ____)\n",
    "    \n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "    if mile is not None:\n",
    "        return float(____)\n",
    "        \n",
    "# Apply the function to the Length column and take a look at both columns\n",
    "hiking[\"Length_num\"] = ____.apply(____)\n",
    "print(hiking[[\"Length\", \"Length_num\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a pattern to extract numbers and decimals\n",
    "def return_mileage(length):\n",
    "    \n",
    "    # Search the text for matches\n",
    "    mile = re.search(\"\\d+\\.\\d+\", length)\n",
    "    \n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "    if mile is not None:\n",
    "        return float(mile.group(0))\n",
    "        \n",
    "# Apply the function to the Length column and take a look at both columns\n",
    "hiking[\"Length_num\"] = hiking[\"Length\"].apply(return_mileage)\n",
    "print(hiking[[\"Length\", \"Length_num\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing text\n",
    "# You'll now transform the volunteer dataset's title column into a text vector, which you'll use in a prediction task in the next exercise.\n",
    "'''\n",
    "Store the volunteer[\"title\"] column in a variable named title_text.\n",
    "Instantiate a TfidfVectorizer as tfidf_vec.\n",
    "Transform the text in title_text into a tf-idf vector using tfidf_vec\n",
    "\n",
    "# Take the title text\n",
    "title_text = ____\n",
    "\n",
    "# Create the vectorizer method\n",
    "tfidf_vec = ____\n",
    "\n",
    "# Transform the text into tf-idf vectors\n",
    "text_tfidf = tfidf_vec.____(____)\n",
    "'''\n",
    "\n",
    "# solution:\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Store the volunteer[\"title\"] column in a variable named title_text\n",
    "title_text = volunteer[\"title\"]\n",
    "\n",
    "# Create the vectorizer method\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "# Transform the text into tf-idf vectors\n",
    "text_tfidf = tfidf_vec.fit_transform(title_text)\n",
    "\n",
    "# Nice job. scikit-learn provides several methods for text vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Split the text_tfidf vector and y target variable into training and test sets, setting the stratify parameter equal to y, since the class distribution is uneven. Notice that we have to run the .toarray() method on the tf/idf vector, in order to get in it the proper format for scikit-learn.\n",
    "Fit the X_train and y_train data to the Naive Bayes model, nb.\n",
    "Print out the test set accuracy.\n",
    "\n",
    "# Split the dataset according to the class distribution of category_desc\n",
    "y = volunteer[\"category_desc\"]\n",
    "X_train, X_test, y_train, y_test = ____(____.toarray(), ____, ____=____, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb.____(____, ____)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.____(____, ____))\n",
    "'''\n",
    "# solution\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Assuming you have a DataFrame named 'volunteer' with 'title' and 'category_desc' columns\n",
    "\n",
    "# Create TF-IDF vectors\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "text_tfidf = tfidf_vec.fit_transform(volunteer['title'])\n",
    "\n",
    "# Split the dataset according to the class distribution of category_desc\n",
    "y = volunteer[\"category_desc\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y, random_state=42)\n",
    "\n",
    "# Create a Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(X_test, y_test))\n",
    "# Nice work! Notice that the model doesn't score very well. We'll work on selecting the best features for modeling in the next chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Take a look at the .head() of the hits column.\n",
    "Convert the hits column to type int.\n",
    "Take a look at the .dtypes of the dataset again, and notice that the column type has changed.\n",
    "'''\n",
    "\n",
    "# Print the head of the hits column\n",
    "print(volunteer[\"hits\"].head())\n",
    "\n",
    "# Convert the hits column to type int\n",
    "volunteer[\"hits\"] = volunteer[\"hits\"].astype(int)\n",
    "\n",
    "# Look at the dtypes of the dataset\n",
    "print(volunteer.dtypes)\n",
    "# Nice work! You can use astype to convert between a variety of types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans: Correct! Both Emergency Preparedness and Environment occur less than 50 times.\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the DataFrame is named volunteer and has been loaded\n",
    "volunteer = pd.read_csv('volunteer.csv')\n",
    "\n",
    "# Count occurrences of each description\n",
    "description_counts = volunteer['category_desc'].value_counts()\n",
    "\n",
    "# Filter descriptions occurring less than 50 times\n",
    "less_than_50 = description_counts[description_counts < 50]\n",
    "\n",
    "# Print descriptions occurring less than 50 times\n",
    "print(less_than_50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://goodboychan.github.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a list of redundant column names and store it in the to_drop variable:\n",
    "Out of all the location-related features, keep only postalcode.\n",
    "Features that have gone through the feature engineering process are redundant as well.\n",
    "Drop the columns in the to_drop list from the dataset.\n",
    "Print out the .head() of volunteer_subset to see the selected columns.\n",
    "'''\n",
    "\n",
    "# Create a list of redundant column names to drop\n",
    "to_drop = [\"category_desc\", \"created_date\", \"locality\", \"region\", \"vol_requests\"]\n",
    "\n",
    "# Drop those columns from the dataset\n",
    "volunteer_subset = volunteer.drop(to_drop, axis=1)\n",
    "\n",
    "# Print out the head of volunteer_subset\n",
    "print(volunteer_subset.head())\n",
    "# Nice job! It's often easier to collect a list of columns to drop, rather than dropping them individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for correlesion features\n",
    "'''\n",
    "Print out the Pearson correlation coefficients for each pair of features in the wine dataset.\n",
    "Drop any columns from wine that have a correlation coefficient above 0.75 with at least two other columns.\n",
    "'''\n",
    "# Print out the column correlations of the wine dataset\n",
    "print(wine.corr())\n",
    "\n",
    "# Drop that column from the DataFrame\n",
    "wine = wine.drop(\"Flavanoids\", axis=1)\n",
    "\n",
    "print(wine.head()) \n",
    "# Good work! Dropping correlated features is often an iterative process, so you may need to try different combinations in your model.PRE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "\n",
    "    # Transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]: zipped[i] for i in vector[vector_index].indices})\n",
    "\n",
    "    # Sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "# Print out the weighted words\n",
    "print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, 8, 3))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a DataFrame of features, X, with all of the columns except category_desc.\n",
    "Create a DataFrame of labels, y from the category_desc column.\n",
    "Split X and y into training and test sets, ensuring that the class distribution in the labels is the same in both sets\n",
    "Print the labels and counts in y_train using .value_counts().\n",
    "'''\n",
    "# Create a DataFrame with all columns except category_desc\n",
    "X = volunteer.drop(\"category_desc\", axis=1)\n",
    "\n",
    "# Create a category_desc labels dataset\n",
    "y = volunteer[[\"category_desc\"]]\n",
    "\n",
    "# Use stratified sampling to split up the dataset according to the y dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Print the category_desc counts from y_train\n",
    "print(y_train[\"category_desc\"].value_counts())\n",
    "# Great job! You'll use train_test_split() frequently while building models, so it's useful to be familiar with the function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "When to standardize\n",
    "Now that you've learned when it is appropriate to standardize your data, which of these scenarios is NOT a reason to standardize?\n",
    "\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "Select one answer\n",
    "\n",
    "A column you want to use for modeling has extremely high variance.\n",
    "PRESS\n",
    "1\n",
    "\n",
    "You have a dataset with several continuous columns on different scales, and you'd like to use a linear model to train the data.\n",
    "PRESS\n",
    "2\n",
    "\n",
    "The models you're working with use some sort of distance metric in a linear space.\n",
    "PRESS\n",
    "3\n",
    "\n",
    "Your dataset is comprised of categorical data\n",
    "\n",
    "Correct! Standardization is a preprocessing task performed on numerical, continuous data.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Split up the X and y sets into training and test sets, ensuring that class labels are equally distributed in both sets.\n",
    "Fit the knn model to the training features and labels.\n",
    "Print the test set accuracy of the knn model using the .score() method.\n",
    "'''\n",
    "\n",
    "# Split the dataset into training and test sets, ensuring equal distribution of class labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Initialize KNN model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Fit the knn model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data and print the accuracy\n",
    "print(knn.score(X_test, y_test))\n",
    "\n",
    "# Correct! The Proline column has an extremely high variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Print out the variance of the Proline column for reference.\n",
    "Use the np.log() function on the Proline column to create a new, log-normalized column named Proline_log.\n",
    "Print out the variance of the Proline_log column to see the difference.\n",
    "'''\n",
    "\n",
    "# Print out the variance of the Proline column\n",
    "print(wine['Proline'].var())\n",
    "\n",
    "# Apply the log normalization function to the Proline column\n",
    "wine['Proline_log'] = np.log(wine['Proline'])\n",
    "\n",
    "# Check the variance of the normalized Proline column\n",
    "print(np.var(wine[\"Proline_log\"]))\n",
    "print(wine['Proline_log'].var())\n",
    "\n",
    "# Nice work! The np.log() function is an easy way to log normalize a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Scaling data - investigating columns\n",
    "You want to use the Ash, Alcalinity of ash, and Magnesium columns in the wine dataset to train a linear model, but it's possible that these columns are all measured in different ways, which would bias a linear model.\n",
    "\n",
    "Which of the following statements about these columns is true?\n",
    "ans:The max of Ash is 3.23, the max of Alcalinity of ash is 30, and the max of Magnesium is 162.\n",
    "Correct! Understanding your data is a crucial first step before deciding on the most appropriate standardization technique.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import the StandardScaler class.\n",
    "Instantiate a StandardScaler() and store it in the variable, scaler.\n",
    "Create a subset of the wine DataFrame containing the Ash, Alcalinity of ash, and Magnesium columns, assign it to wine_subset.\n",
    "Fit and transform the standard scaler to wine_subset.\n",
    "'''\n",
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create a subset of the DataFrame with the columns you want to scale\n",
    "wine_subset = wine[['Ash', 'Alcalinity of ash', 'Magnesium']]\n",
    "\n",
    "# Fit and transform the scaler on the subset\n",
    "wine_subset_scaled = scaler.fit_transform(wine_subset)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame\n",
    "wine_subset_scaled = pd.DataFrame(wine_subset_scaled, columns=wine_subset.columns)\n",
    "\n",
    "# Print the first few rows to verify\n",
    "print(wine_subset_scaled.head())\n",
    "\n",
    "# Good job! In scikit-learn, running .fit_transform() during preprocessing will both fit the method to the data as well as transform the data in a single step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Split the dataset into training and test sets.\n",
    "Fit the knn model to the training data.\n",
    "Print out the test set accuracy of your trained knn model.\n",
    "'''\n",
    "\n",
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Initialize KNN model (assuming k=5 for example)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the KNN model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data and print the accuracy\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print(f\"Test set accuracy of KNN model: {accuracy}\")\n",
    "# Well done! This accuracy definitely isn't poor, but let's see if we can improve it by standardizing the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create the StandardScaler() method, stored in a variable named scaler.\n",
    "Scale the training and test features, being careful not to introduce data leakage.\n",
    "Fit the knn model to the scaled training data.\n",
    "Evaluate the model's performance by computing the test set accuracy.\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Instantiate a StandardScaler\n",
    "scaler = ____\n",
    "\n",
    "# Scale the training and test features\n",
    "X_train_scaled = ____.____(____)\n",
    "X_test_scaled = ____.____(____)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "____.____(____, ____)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(____.____(____, ____))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X and y are already defined as your features and target labels\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Instantiate a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the training and test features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the KNN model to the scaled training data\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Score the model on the test data and print the accuracy\n",
    "accuracy = knn.score(X_test_scaled, y_test)\n",
    "print(f\"Test set accuracy of KNN model on scaled data: {accuracy}\")\n",
    "# Excellent! That's quite the improvement, and definitely made scaling the data worthwhile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "# Correct! Timestamps can be broken into days or months, and headlines can be used for natural language processing.\n",
    "# Correct! All three of these columns will require some feature engineering before modeling.\n",
    "'''\n",
    "Store LabelEncoder() in a variable named enc.\n",
    "Using the encoder's .fit_transform() method, encode the hiking dataset's \"Accessible\" column. Call the new column Accessible_enc.\n",
    "Compare the two columns side-by-side to see the encoding.\n",
    "'''\n",
    "\n",
    "# Set up the LabelEncoder object\n",
    "enc = LabelEncoder()\n",
    "\n",
    "# Apply the encoding to the \"Accessible\" column\n",
    "hiking[\"Accessible_enc\"] = enc.fit_transform(hiking[\"Accessible\"])\n",
    "\n",
    "# Compare the two columns\n",
    "print(hiking[[\"Accessible_enc\", \"Accessible\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Call get_dummies() on the volunteer[\"category_desc\"] column to create the encoded columns and assign it to category_enc.\n",
    "Print out the .head() of the category_enc variable to take a look at the encoded columns.\n",
    "'''\n",
    "\n",
    "# Transform the category_desc column\n",
    "category_enc = pd.get_dummies(volunteer[\"category_desc\"])\n",
    "\n",
    "# Take a look at the encoded columns\n",
    "print(category_enc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Use the .loc[] method to select all rows and columns to find the .mean() of the each columns.\n",
    "Print the .head() of the DataFrame to see the mean column.\n",
    "'''\n",
    "# Use .loc to create a mean column\n",
    "running_times_5k[\"mean\"] = running_times_5k.loc[:, \"run1\":\"run5\"].mean(axis=1)\n",
    "\n",
    "# Take a look at the results\n",
    "print(running_times_5k.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Convert the start_date_date column into a pandas datetime column and store it in a new column called start_date_converted.\n",
    "Retrieve the month component of start_date_converted and store it in a new column called start_date_month.\n",
    "Print the .head() of just the start_date_converted and start_date_month columns.\n",
    "'''\n",
    "\n",
    "\n",
    "# First, convert string column to date column\n",
    "volunteer[\"start_date_converted\"] = pd.to_datetime(volunteer[\"start_date_date\"])\n",
    "\n",
    "# Extract just the month from the converted column\n",
    "volunteer[\"start_date_month\"] = volunteer[\"start_date_converted\"].dt.month\n",
    "\n",
    "# Take a look at the converted and new month columns\n",
    "print(volunteer[[\"start_date_converted\", \"start_date_month\"]].head())\n",
    "# Awesome! You can also use attributes like .day to get the day and .year to get the year from datetime columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Add parameters called original_vocab, for the tfidf_vec.vocabulary_, and top_n.\n",
    "Call pd.Series() on the zipped dictionary. This will make it easier to operate on.\n",
    "Use the .sort_values() function to sort the series and slice the index up to top_n words.\n",
    "Call the function, setting original_vocab=tfidf_vec.vocabulary_, setting vector_index=8 to grab the 9th row, and setting top_n=3, to grab the top 3 weighted words.\n",
    "'''\n",
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "\n",
    "    # Transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]: zipped[i] for i in vector[vector_index].indices})\n",
    "\n",
    "    # Sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "# Print out the weighted words\n",
    "print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, 8, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excellent! In the next exercise, you'll train a model using the filtered vector.\n",
    "'''\n",
    "'''\n",
    "Call return_weights() to return the top weighted words for that document.\n",
    "Call set() on the returned filter_list to remove duplicated numbers.\n",
    "Call words_to_filter, passing in the following parameters: vocab for the vocab parameter, tfidf_vec.vocabulary_ for the original_vocab parameter, text_tfidf for the vector parameter, and 3 to grab the top_n 3 weighted words from each document.\n",
    "Finally, pass that filtered_words set into a list to use as a filter for the text vector.\n",
    "'''\n",
    "\n",
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "    \n",
    "        # Call the return_weights function and extend filter_list\n",
    "        filtered = ____(vocab, original_vocab, vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "        \n",
    "    # Return the list in a set, so we don't get duplicate word indices\n",
    "    return ____(filter_list)\n",
    "\n",
    "# Call the function to get the list of word indices\n",
    "filtered_words = ____(____, ____, ____, ____)\n",
    "\n",
    "# Filter the columns in text_tfidf to only those in filtered_words\n",
    "filtered_text = text_tfidf[:, list(____)]\n",
    "\n",
    "'''\n",
    "# solution\n",
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(vector.shape[0]):\n",
    "        # Call the return_weights function and extend filter_list\n",
    "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "    \n",
    "    # Return the list in a set, so we don't get duplicate word indices\n",
    "    return set(filter_list)\n",
    "\n",
    "# Call the function to get the list of word indices\n",
    "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
    "\n",
    "# Filter the columns in text_tfidf to only those in filtered_words\n",
    "filtered_text = text_tfidf[:, list(filtered_words)]\n",
    "\n",
    "# Print the filtered_text to verify the results\n",
    "print(filtered_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Use train_test_split() on the filtered_text text vector, the y labels (which is the category_desc labels), and pass the y set to the stratify parameter, since we have an uneven class distribution.\n",
    "Fit the nb Naive Bayes model to X_train and y_train.\n",
    "Calculate the test set accuracy of nb.\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Split the dataset according to the class distribution of category_desc\n",
    "X_train, X_test, y_train, y_test = train_test_split(filtered_text.toarray(), y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(X_test, y_test))\n",
    "\n",
    "'''\n",
    "Awesome! You can see that our accuracy score wasn't that different from the score at the end of Chapter 3. But don't worry, this is mainly because of how small the title field is.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Instantiate a PCA object.\n",
    "Define the features (X) and labels (y) from wine, using the labels in the \"Type\" column.\n",
    "Apply PCA to X_train and X_test, ensuring no data leakage, and store the transformed values as pca_X_train and pca_X_test.\n",
    "Print out the .explained_variance_ratio_ attribute of pca to check how much variance is explained by each component.\n",
    "'''\n",
    "\n",
    "# Instantiate a PCA object\n",
    "pca = ____()\n",
    "\n",
    "# Define the features and labels from the wine dataset\n",
    "X = wine.drop(____, ____)\n",
    "y = wine[\"Type\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Apply PCA to the wine dataset X vector\n",
    "pca_X_train = ___.____(____)\n",
    "pca_X_test = ___.____(____)\n",
    "\n",
    "# Look at the percentage of variance explained by the different components\n",
    "print(____)\n",
    "\n",
    "# solution\n",
    "# github.com/dnlsyfq/py_preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Instantiate a PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# Define the features and labels from the wine dataset\n",
    "X = wine.drop('Type', axis=1)\n",
    "y = wine[\"Type\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Apply PCA to the training and test sets\n",
    "pca_X_train = pca.fit_transform(X_train)\n",
    "pca_X_test = pca.transform(X_test)  # Use transform on test set to avoid data leakage\n",
    "\n",
    "# Look at the percentage of variance explained by the different components\n",
    "print(pca.explained_variance_ratio_)\n",
    "# Excellent! In the next exercise, you'll train a model using the PCA-transformed vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "'''\n",
    "Fit the knn model to the PCA-transformed features, pca_X_train, and training labels, y_train.\n",
    "Print the test set accuracy of the knn model using pca_X_test and y_test.\n",
    "'''\n",
    "\n",
    "# Fit knn to the training data\n",
    "____\n",
    "\n",
    "# Score knn on the test data and print it out\n",
    "____\n",
    "'''\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create a KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)  # You can adjust n_neighbors as needed\n",
    "\n",
    "# Fit knn to the training data\n",
    "knn.fit(pca_X_train, y_train)\n",
    "\n",
    "# Score knn on the test data and print it out\n",
    "print(\"Test set accuracy:\", knn.score(pca_X_test, y_test))\n",
    "# Good work! PCA turned out to be a good choice for the wine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Call the .info() method on the ufo dataset.\n",
    "Convert the type of the seconds column to the float data type.\n",
    "Convert the type of the date column to the datetime data type.\n",
    "Call .info() on ufo again to see if the changes worked.\n",
    "'''\n",
    "\n",
    "# Print the DataFrame info\n",
    "print(ufo.info())\n",
    "\n",
    "# Change the type of seconds to float\n",
    "ufo[\"seconds\"] = ufo[\"seconds\"].astype(float)\n",
    "\n",
    "# Change the date column to type datetime\n",
    "ufo[\"date\"] = pd.to_datetime(ufo[\"date\"])\n",
    "\n",
    "# Check the column types\n",
    "print(ufo.info())\n",
    "# Nice job on transforming the column types! This will make feature engineering and standardization much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Count the missing values in the length_of_time, state, and type columns, in that order\n",
    "print(ufo[['length_of_time', 'state', 'type']].isna().sum())\n",
    "\n",
    "# Drop rows where length_of_time, state, or type are missing\n",
    "ufo_no_missing = ufo.dropna(subset=['length_of_time', 'state', 'type'])\n",
    "\n",
    "# Print out the shape of the new dataset\n",
    "print(ufo_no_missing.shape)\n",
    "\n",
    "'''\n",
    "\n",
    "# Count the missing values in the length_of_time, state, and type columns, in that order\n",
    "print(ufo[['length_of_time', 'state', 'type']].isna().sum())\n",
    "\n",
    "# Drop rows where length_of_time, state, or type are missing\n",
    "ufo_no_missing = ufo.dropna(subset=['length_of_time', 'state', 'type'])\n",
    "\n",
    "# Print out the shape of the new dataset\n",
    "print(ufo_no_missing.shape)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "'''\n",
    "Search time_string for numbers using an appropriate RegEx pattern.\n",
    "Use the .apply() method to call the return_minutes() on every row of the length_of_time column.\n",
    "Print out the .head() of both the length_of_time and minutes columns to compare.\n",
    "'''\n",
    "\n",
    "def return_minutes(time_string):\n",
    "\n",
    "    # Search for numbers in time_string\n",
    "    num = re.____(____, ____)\n",
    "    if num is not None:\n",
    "        return int(num.group(0))\n",
    "        \n",
    "# Apply the extraction to the length_of_time column\n",
    "ufo[\"minutes\"] = ufo[\"length_of_time\"].____\n",
    "\n",
    "# Take a look at the head of both of the columns\n",
    "print(ufo[[____]].head())\n",
    "'''\n",
    "# solution\n",
    "# Nice job! The minutes information is now in a form where it can be inputted into a model.\n",
    "import re\n",
    "\n",
    "def return_minutes(time_string):\n",
    "  \"\"\"Extracts the number of minutes from a time string.\"\"\"\n",
    "\n",
    "  # Search for numbers in time_string\n",
    "  num = re.search(r'\\d+', time_string)\n",
    "  if num is not None:\n",
    "    return int(num.group(0))\n",
    "\n",
    "# Apply the extraction to the length_of_time column\n",
    "ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(return_minutes)\n",
    "\n",
    "# Take a look at the head of both of the columns\n",
    "print(ufo[['length_of_time', 'minutes']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "Calculate the variance in the seconds and minutes columns and take a close look at the results.\n",
    "Perform log normalization on the seconds column, transforming it into a new column named seconds_log.\n",
    "Print out the variance of the seconds_log column.\n",
    "'''\n",
    "# Check the variance of the seconds and minutes columns\n",
    "print(____)\n",
    "\n",
    "# Log normalize the seconds column\n",
    "ufo[\"seconds_log\"] = ____\n",
    "\n",
    "# Print out the variance of just the seconds_log column\n",
    "print(____)\n",
    "# solition\n",
    "import numpy as np\n",
    "\n",
    "# Check the variance of the seconds and minutes columns\n",
    "print(ufo[['seconds', 'minutes']].var())\n",
    "\n",
    "# Log normalize the seconds column\n",
    "ufo['seconds_log'] = np.log(ufo['seconds'])\n",
    "\n",
    "# Print out the variance of just the seconds_log column\n",
    "print(ufo['seconds_log'].var())\n",
    "# Good work! Now it's time to engineer new features in the ufo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Using apply(), write a conditional lambda function that returns a 1 if the value is \"us\", else return 0.\n",
    "Print out the number of .unique() values in the type column.\n",
    "Using pd.get_dummies(), create a one-hot encoded set of the type column.\n",
    "Finally, use pd.concat() to concatenate the type_set encoded variables to the ufo dataset\n",
    "'''\n",
    "# Use pandas to encode us values as 1 and others as 0\n",
    "ufo[\"country_enc\"] = ufo[\"country\"].apply(lambda val: 1 if val == \"us\" else 0)\n",
    "\n",
    "# Print the number of unique type values\n",
    "print(len(ufo[\"type\"].unique()))\n",
    "\n",
    "# Create a one-hot encoded set of the type values\n",
    "type_set = pd.get_dummies(ufo[\"type\"])\n",
    "\n",
    "# Concatenate this set back to the ufo DataFrame\n",
    "ufo = pd.concat([ufo, type_set], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 5 rows of the date column\n",
    "print(ufo['date'].head())\n",
    "\n",
    "# Extract the month from the date column\n",
    "ufo['month'] = ufo['date'].dt.month\n",
    "\n",
    "# Extract the year from the date column\n",
    "ufo['year'] = ufo['date'].dt.year\n",
    "\n",
    "# Take a look at the head of all three columns\n",
    "print(ufo[['date', 'month', 'year']].head())\n",
    "# Nice job on extracting dates! The pandas series attributes .dt.month and .dt.year are extremely useful for extraction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Take a look at the head of the desc field\n",
    "print(ufo['desc'].head())\n",
    "\n",
    "# Instantiate the tfidf vectorizer object\n",
    "vec = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform desc using vec\n",
    "desc_tfidf = vec.fit_transform(ufo['desc'])\n",
    "\n",
    "# Look at the number of columns and rows\n",
    "print(desc_tfidf.shape)\n",
    "# Great! You'll notice that the text vector has a large number of columns. We'll work on selecting the features we want to use for modeling in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of features to drop   \n",
    "to_drop = [\"city\", \"country\", \"date\", \"desc\", \"lat\", \"length_of_time\", \"long\", \"minutes\", \"recorded\", \"seconds\", \"state\"]\n",
    "\n",
    "# Drop those features\n",
    "ufo_dropped = ufo.drop(to_drop, axis=1)\n",
    "\n",
    "# Let's also filter some words out of the text vector we created\n",
    "filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the features in the X set of data\n",
    "print(X.columns)\n",
    "\n",
    "# Split the X and y sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit knn to the training sets\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Print the score of knn on the test sets\n",
    "print(knn.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Use the list of filtered words we created to filter the text vector\n",
    "filtered_text = desc_tfidf[:, list(filtered_words)]\n",
    "\n",
    "# Split the X and y sets using train_test_split, setting stratify=y \n",
    "X_train, X_test, y_train, y_test = train_test_split(filtered_text.toarray(), y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit nb to the training sets\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print the score of nb on the test sets\n",
    "print(nb.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
