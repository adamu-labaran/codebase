{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "To find out the number of rows and columns of a 2D NumPy array, you can use the .shape attribute. Here's how you can do it:\n",
    "\n",
    "Example:\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "\n",
    "# Create a 2D NumPy array\n",
    "array_2D = np.array([[1, 2, 3],\n",
    "                     [4, 5, 6],\n",
    "                     [7, 8, 9]])\n",
    "\n",
    "# Get the number of rows and columns\n",
    "rows, columns = array_2D.shape\n",
    "\n",
    "print(f\"Number of rows: {rows}\")\n",
    "print(f\"Number of columns: {columns}\")\n",
    "Output:\n",
    "javascript\n",
    "Copy code\n",
    "Number of rows: 3\n",
    "Number of columns: 3\n",
    "Explanation:\n",
    "array_2D.shape returns a tuple where the first element is the number of rows and the second element is the number of columns.\n",
    "You can directly unpack this tuple into variables rows and columns.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "'''Create a SQL table from a dataframe\n",
    "A dataframe can be used to create a temporary table. A temporary table is one that will not exist after the session ends. Spark documentation also refers to this type of table as a SQL temporary view. In the documentation this is referred to as to register the dataframe as a SQL temporary view. This command is called on the dataframe itself, and creates a table if it does not already exist, replacing it with the current data from the dataframe if it does already exist.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Load csv data from the file trainsched.txt into a dataframe stored in a variable named df.\n",
    "Create a temporary table from df. Call the table \"table1\".'''\n",
    "\n",
    "\n",
    "# Load trainsched.txt\n",
    "df = spark.____.____(\"trainsched.txt\", header=True)\n",
    "\n",
    "# Create temporary table called table1\n",
    "df.____(____)\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "To load CSV data from the file trainsched.txt into a DataFrame and create a temporary SQL table (temporary view) from it in PySpark, you can use the following code:\n",
    "\n",
    "Code:\n",
    "python\n",
    "Copy code\n",
    "# Load trainsched.txt\n",
    "df = spark.read.csv(\"trainsched.txt\", header=True)\n",
    "\n",
    "# Create temporary table called table1\n",
    "df.createOrReplaceTempView(\"table1\")\n",
    "Explanation:\n",
    "Loading the CSV File:\n",
    "\n",
    "spark.read.csv(\"trainsched.txt\", header=True) loads the CSV file into a DataFrame df.\n",
    "The header=True option indicates that the first row of the CSV file contains column names.\n",
    "Creating a Temporary Table:\n",
    "\n",
    "df.createOrReplaceTempView(\"table1\") creates a temporary SQL table named table1 from the DataFrame df.\n",
    "This table can be queried using SQL syntax within the current Spark session.\n",
    "After running this code, you can perform SQL queries on table1 as if it were a table in a relational database.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "'''Determine the column names of a table\n",
    "The video lesson showed how to run an SQL query. It also showed how to inspect the column names of a Spark table using SQL. This is important to know because in practice relational tables are typically provided without additional documentation giving the table schema.\n",
    "\n",
    "Don't hesitate to refer to the slides available at the right of the console if you forget how something was done in the video.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Use a DESCRIBE query to determine the names and types of the columns in the table schedule.\n",
    "\n",
    "'''\n",
    "\n",
    "# Inspect the columns in the table schedule\n",
    "spark.sql(\"____ ____\").show()\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "To determine the names and types of the columns in the table schedule using a DESCRIBE query in PySpark, you can use the following code:\n",
    "\n",
    "Code:\n",
    "python\n",
    "Copy code\n",
    "# Inspect the columns in the table schedule\n",
    "spark.sql(\"DESCRIBE schedule\").show()\n",
    "Explanation:\n",
    "DESCRIBE schedule: This SQL query retrieves the column names and data types from the schedule table.\n",
    ".show(): This method displays the result of the query in the console.\n",
    "When you run this command, it will list all the columns in the schedule table along with their respective data types.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
