{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Save file locally\n",
    "urlretrieve(url, 'winequality-red.csv')\n",
    "\n",
    "# Read file into a DataFrame and print its head\n",
    "df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Your recent learnings\n",
    "When you left 10 hours ago, you worked on Importing data from the Internet, chapter 1 of the course Intermediate Importing Data in Python. Here is what you covered in your last lesson:\n",
    "\n",
    "You learned how to import data from the web into Python using the urllib.request module, specifically how to use the urlretrieve function to save a file from the web to your local computer. For example, you used urlretrieve to download a dataset of wine quality from the University of California, Irvine, saving it as 'winequality-red.csv'. You also discovered how to directly load a file from the web into a pandas DataFrame without saving it locally first, using pd.read_csv() with the file's URL as the argument. This method is efficient for quickly accessing and analyzing web-hosted datasets. Additionally, you explored importing non-flat files, such as Excel files, from the web using pd.read_excel(), learning to read in all sheets of an Excel file into a dictionary of DataFrames. This skill is particularly useful for working with complex datasets distributed across multiple sheets in a single file.\n",
    "\n",
    "Key points covered include:\n",
    "\n",
    "Using urlretrieve from urllib.request to download files.\n",
    "Directly loading web files into pandas DataFrames using pd.read_csv().\n",
    "Importing and working with Excel files from the web using pd.read_excel(), handling multiple sheets.\n",
    "Here's a snippet of code you worked with:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Importing flat files from the web: your turn!\n",
    "You are about to import your first file from the web! The flat file you will import will be 'winequality-red.csv' from the University of California, Irvine's Machine Learning repository. The flat file contains tabular data of physiochemical properties of red wine, such as pH, alcohol content and citric acid content, along with wine quality rating.\n",
    "\n",
    "The URL of the file is\n",
    "\n",
    "'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "After you import it, you'll check your working directory to confirm that it is there and then you'll load it into a pandas DataFrame.\n",
    "ChatGPT\n",
    "Great! Let's go through the steps to import the file from the web, check the working directory, and load the data into a pandas DataFrame.\n",
    "\n",
    "Import necessary libraries:\n",
    "\n",
    "We'll need pandas for handling the DataFrame.\n",
    "We'll use requests to download the file from the web.\n",
    "We can use os to check the working directory.\n",
    "Download the file:\n",
    "\n",
    "Use the requests library to download the CSV file from the given URL.\n",
    "Save the file to the local working directory.\n",
    "Check the working directory:\n",
    "\n",
    "Use the os library to confirm that the file has been downloaded and is present in the working directory.\n",
    "Load the file into a pandas DataFrame:\n",
    "\n",
    "Use pandas to read the CSV file and load it into a DataFrame.\n",
    "Let's execute these steps in Python.\n",
    "\n",
    "Let's run this code and see the results.\n",
    "\n",
    "It appears that there is an issue with the connection, possibly due to a network restriction in the current environment. However, you can run the same steps on your local machine to achieve the desired outcome.\n",
    "\n",
    "Here’s a summary of the steps you can follow on your local machine:\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define the URL and the local filename\n",
    "url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "local_filename = 'winequality-red.csv'\n",
    "\n",
    "# Step 2: Download the file from the web\n",
    "response = requests.get(url)\n",
    "with open(local_filename, 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "# Step 3: Check the working directory to confirm the file is there\n",
    "working_directory = os.listdir('.')\n",
    "print(\"Working Directory Contents:\", working_directory)\n",
    "\n",
    "# Step 4: Load the file into a pandas DataFrame\n",
    "df = pd.read_csv(local_filename)\n",
    "\n",
    "# Display the first few rows of the DataFrame to confirm it's loaded correctly\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Install necessary libraries if you don't have them already:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "pip install pandas requests\n",
    "Use the following Python script to download and load the CSV file into a pandas DataFrame:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL and the local filename\n",
    "url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "local_filename = 'winequality-red.csv'\n",
    "\n",
    "# Download the file from the web\n",
    "response = requests.get(url)\n",
    "with open(local_filename, 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "# Check the working directory to confirm the file is there\n",
    "working_directory = os.listdir('.')\n",
    "print(\"Working Directory Contents:\", working_directory)\n",
    "\n",
    "# Load the file into a pandas DataFrame\n",
    "df = pd.read_csv(local_filename)\n",
    "\n",
    "# Display the first few rows of the DataFrame to confirm it's loaded correctly\n",
    "print(df.head())\n",
    "This script will:\n",
    "\n",
    "Download the CSV file from the given URL.\n",
    "Save the file locally.\n",
    "Check the local directory to confirm the file is downloaded.\n",
    "Load the data into a pandas DataFrame and display the first few rows.\n",
    "You can run this script in any Python environment on your local machine. If you encounter any issues or have any questions, feel free to ask! ​\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import the function urlretrieve from the subpackage urllib.request.\n",
    "Assign the URL of the file to the variable url.\n",
    "Use the function urlretrieve() to save the file locally as 'winequality-red.csv'.\n",
    "Execute the remaining code to load 'winequality-red.csv' in a pandas DataFrame and to print its head to the shell.\n",
    "# Import package\n",
    "from ____ import ____\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "\n",
    "\n",
    "# Save file locally\n",
    "\n",
    "\n",
    "# Read file into a DataFrame and print its head\n",
    "df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "print(df.head())\n",
    "ChatGPT\n",
    "Sure! Let's update the code according to your requirements. We will use urlretrieve from the urllib.request subpackage to download the file. Here is the updated code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import package\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Save file locally\n",
    "urlretrieve(url, 'winequality-red.csv')\n",
    "\n",
    "# Read file into a DataFrame and print its head\n",
    "df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "print(df.head())\n",
    "You can run this script on your local machine to download the file and load it into a pandas DataFrame. Let me know if you need further assistance!\n",
    "\n",
    "Opening and reading flat files from the web\n",
    "You have just imported a file from the web, saved it locally and loaded it into a DataFrame. If you just wanted to load a file from the web into a DataFrame without first saving it locally, you can do that easily using pandas. In particular, you can use the function pd.read_csv() with the URL as the first argument and the separator sep as the second argument.\n",
    "\n",
    "The URL of the file, once again, is\n",
    "\n",
    "'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "ChatGPT\n",
    "You can directly load the CSV file from the web into a pandas DataFrame using the pd.read_csv() function by providing the URL. Here is how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the file\n",
    "url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Read file into a DataFrame directly from the web and print its head\n",
    "df = pd.read_csv(url, sep=';')\n",
    "print(df.head())\n",
    "This code will:\n",
    "\n",
    "Import the pandas library.\n",
    "Define the URL of the CSV file.\n",
    "Use pd.read_csv() to read the CSV file directly from the URL into a DataFrame.\n",
    "Print the first few rows of the DataFrame using df.head().\n",
    "You can run this script in any Python environment with internet access.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Assign the URL of the file to the variable url.\n",
    "Read file into a DataFrame df using pd.read_csv(), recalling that the separator in the file is ';'.\n",
    "Print the head of the DataFrame df.\n",
    "Execute the rest of the code to plot histogram of the first feature in the DataFrame df.\n",
    "\n",
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "\n",
    "\n",
    "# Read file into a DataFrame: df\n",
    "\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(____)\n",
    "\n",
    "# Plot first column of df\n",
    "df.iloc[:, 0].hist()\n",
    "plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n",
    "ChatGPT\n",
    "Sure! Here is the complete code following your instructions:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Read file into a DataFrame: df\n",
    "df = pd.read_csv(url, sep=';')\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Plot first column of df\n",
    "df.iloc[:, 0].hist()\n",
    "plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n",
    "This code will:\n",
    "\n",
    "Import the necessary libraries.\n",
    "Define the URL of the file.\n",
    "Read the CSV file into a pandas DataFrame using the URL.\n",
    "Print the first few rows of the DataFrame.\n",
    "Plot a histogram of the first column (fixed acidity), labeling the x-axis and y-axis accordingly.\n",
    "You can run this script in a Python environment with internet access to visualize the data.\n",
    "\n",
    "Importing non-flat files from the web\n",
    "Congrats! You've just loaded a flat file from the web into a DataFrame without first saving it locally using the pandas function pd.read_csv(). This function is super cool because it has close relatives that allow you to load all types of files, not only flat ones. In this interactive exercise, you'll use pd.read_excel() to import an Excel spreadsheet.\n",
    "\n",
    "The URL of the spreadsheet is\n",
    "\n",
    "'https://assets.datacamp.com/course/importing_data_into_r/latitude\n",
    "ChatGPT\n",
    "To import an Excel file from the web directly into a pandas DataFrame, you can use the pd.read_excel() function. Here's how you can do it with the provided URL.\n",
    "\n",
    "Import necessary libraries.\n",
    "Assign the URL of the Excel file to a variable.\n",
    "Use pd.read_excel() to read the file into a DataFrame.\n",
    "Print the head of the DataFrame to verify the data.\n",
    "Here’s the complete code to achieve this:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assign URL of the Excel file\n",
    "url = 'https://assets.datacamp.com/course/importing_data_into_r/latitude.xlsx'\n",
    "\n",
    "# Read file into a DataFrame\n",
    "df = pd.read_excel(url)\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(df.head())\n",
    "This code will:\n",
    "\n",
    "Import the pandas library.\n",
    "Define the URL of the Excel file.\n",
    "Use pd.read_excel() to read the Excel file directly from the URL into a DataFrame.\n",
    "Print the first few rows of the DataFrame using df.head().\n",
    "You can run this script in any Python environment with internet access. If you have any further questions or need additional help, feel free to ask!\n",
    "\n",
    "Your job is to use pd.read_excel() to read in all of its sheets, print the sheet names and then print the head of the first sheet using its name, not its index.\n",
    "\n",
    "Note that the output of pd.read_excel() is a Python dictionary with sheet names as keys and corresponding DataFrames as corresponding values.\n",
    "\n",
    "# Import package\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "\n",
    "\n",
    "# Read in all sheets of Excel file: xls\n",
    "\n",
    "\n",
    "# Print the sheetnames to the shell\n",
    "\n",
    "\n",
    "# Print the head of the first sheet (using its name, NOT its index)\n",
    "\n",
    "ChatGPT\n",
    "Sure! Here is the code to read in all sheets of the Excel file, print the sheet names, and then print the head of the first sheet using its name:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import package\n",
    "import pandas as pd\n",
    "\n",
    "# Assign URL of the file\n",
    "url = 'https://assets.datacamp.com/course/importing_data_into_r/latitude.xlsx'\n",
    "\n",
    "# Read in all sheets of the Excel file: xls\n",
    "xls = pd.read_excel(url, sheet_name=None)\n",
    "\n",
    "# Print the sheet names to the shell\n",
    "print(xls.keys())\n",
    "\n",
    "# Print the head of the first sheet (using its name, NOT its index)\n",
    "first_sheet_name = list(xls.keys())[0]\n",
    "print(xls[first_sheet_name].head())\n",
    "This code will:\n",
    "\n",
    "Import the pandas library.\n",
    "Define the URL of the Excel file.\n",
    "Use pd.read_excel() to read all sheets from the Excel file into a dictionary xls where the keys are the sheet names and the values are the DataFrames.\n",
    "Print the keys of the dictionary, which are the sheet names.\n",
    "Print the first few rows of the first sheet by accessing it via its name.\n",
    "You can run this script in any Python environment with internet access. If you encounter any issues or have any questions, feel free to ask!\n",
    "\n",
    "Performing HTTP requests in Python using urllib\n",
    "Now that you know the basics behind HTTP GET requests, it's time to perform some of your own. In this interactive exercise, you will ping our very own DataCamp servers to perform a GET request to extract information from the first coding exercise of this course, \"https://campus.datacamp.com/courses/1606/4135?ex=2\".\n",
    "\n",
    "In the next exercise, you'll extract the HTML itself. Right now, however, you are going to package and send the request and then catch the response.\n",
    "ChatGPT\n",
    "To perform HTTP GET requests in Python using urllib, you can follow these steps:\n",
    "\n",
    "Import the necessary module from urllib.\n",
    "Define the URL you want to request.\n",
    "Use urllib to perform the GET request and capture the response.\n",
    "Print the response to see the status and the headers.\n",
    "Here's the code to perform a GET request to the provided URL:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import the urlopen function from urllib.request\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Define the URL\n",
    "url = 'https://campus.datacamp.com/courses/1606/4135?ex=2'\n",
    "\n",
    "# Perform the GET request\n",
    "response = urlopen(url)\n",
    "\n",
    "# Print the response's status and headers\n",
    "print(\"Status Code:\", response.getcode())\n",
    "print(\"Headers:\", response.headers)\n",
    "This code will:\n",
    "\n",
    "Import the urlopen function from urllib.request.\n",
    "Define the URL of the DataCamp exercise.\n",
    "Perform the GET request to the URL and store the response.\n",
    "Print the status code and headers of the response.\n",
    "You can run this script in any Python environment with internet access. Let me know if you need further assistance!\n",
    "\n",
    "'''\n",
    "Import the functions urlopen and Request from the subpackage urllib.request.\n",
    "Package the request to the url \"https://campus.datacamp.com/courses/1606/4135?ex=2\" using the function Request() and assign it to request.\n",
    "Send the request and catch the response in the variable response with the function urlopen().\n",
    "Run the rest of the code to see the datatype of response and to close the connection!\n",
    "'''\n",
    "\n",
    "# Import packages\n",
    "\n",
    "\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# This packages the request: request\n",
    "\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "\n",
    "\n",
    "# Print the datatype of response\n",
    "print(type(response))\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()\n",
    "ChatGPT\n",
    "Sure! Here's how you can package the request, send it, and catch the response using urllib.request in Python:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# This packages the request: request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Print the datatype of response\n",
    "print(type(response))\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()\n",
    "This code will:\n",
    "\n",
    "Import the necessary functions from urllib.request.\n",
    "Define the URL for the request.\n",
    "Package the request using the Request function.\n",
    "Send the request and catch the response using urlopen.\n",
    "Print the type of the response object to see its datatype.\n",
    "Close the response to be polite and free up resources.\n",
    "You can run this script in any Python environment with internet access. If you have any questions or need further assistance, feel free to ask!\n",
    "\n",
    "Printing HTTP request results in Python using urllib\n",
    "You have just packaged and sent a GET request to \"https://campus.datacamp.com/courses/1606/4135?ex=2\" and then caught the response. You saw that such a response is a http.client.HTTPResponse object. The question remains: what can you do with this response?\n",
    "\n",
    "Well, as it came from an HTML page, you could read it to extract the HTML and, in fact, such a http.client.HTTPResponse object has an associated read() method. In this exercise, you'll build on your previous great work to extract the response and print the HTML.\n",
    "ChatGPT\n",
    "To extract and print the HTML content from the HTTP response, you can use the read() method of the http.client.HTTPResponse object. Here is how you can achieve this:\n",
    "\n",
    "Import the necessary functions from urllib.request.\n",
    "Specify the URL.\n",
    "Package the request using Request.\n",
    "Send the request and catch the response using urlopen.\n",
    "Use the read() method to extract the HTML content.\n",
    "Print the HTML content.\n",
    "Close the response to free up resources.\n",
    "Here is the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# This packages the request: request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Extract the response and print the HTML\n",
    "html = response.read()\n",
    "print(html)\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()\n",
    "This code will:\n",
    "\n",
    "Import the necessary functions from urllib.request.\n",
    "Define the URL for the request.\n",
    "Package the request using Request.\n",
    "Send the request and catch the response using urlopen.\n",
    "Extract the HTML content from the response using the read() method.\n",
    "Print the HTML content.\n",
    "Close the response to free up resources.\n",
    "You can run this script in any Python environment with internet access. If you have any questions or need further assistance, feel free to ask!\n",
    "\n",
    "Send the request and catch the response in the variable response with the function urlopen(), as in the previous exercise.\n",
    "Extract the response using the read() method and store the result in the variable html.\n",
    "Print the string html.\n",
    "Hit submit to perform all of the above and to close the response: be tidy!\n",
    "ChatGPT\n",
    "Sure! Here's the complete code to send the request, catch the response, extract the HTML using the read() method, and print the HTML content:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# This packages the request: request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Extract the response using the read() method\n",
    "html = response.read()\n",
    "\n",
    "# Print the string html\n",
    "print(html)\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()\n",
    "This code will:\n",
    "\n",
    "Import the necessary functions from urllib.request.\n",
    "Define the URL for the request.\n",
    "Package the request using Request.\n",
    "Send the request and catch the response using urlopen.\n",
    "Extract the HTML content from the response using the read() method and store it in the variable html.\n",
    "Print the HTML content stored in html.\n",
    "Close the response to free up resources.\n",
    "You can run this script in any Python environment with internet access. If you have any questions or need further assistance, feel free to ask!\n",
    "\n",
    "Performing HTTP requests in Python using requests\n",
    "Now that you've got your head and hands around making HTTP requests using the urllib package, you're going to figure out how to do the same using the higher-level requests library. You'll once again be pinging DataCamp servers for their \"http://www.datacamp.com/teach/documentation\" page.\n",
    "\n",
    "Note that unlike in the previous exercises using urllib, you don't have to close the connection when using requests!\n",
    "ChatGPT\n",
    "Using the requests library to perform HTTP requests is simpler and more intuitive than using urllib. Here's how you can perform a GET request to the specified URL and print the response content:\n",
    "\n",
    "Import the requests library.\n",
    "Define the URL.\n",
    "Send the GET request using requests.get().\n",
    "Print the content of the response.\n",
    "Here is the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import the requests package\n",
    "import requests\n",
    "\n",
    "# Specify the URL\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "\n",
    "# Send the GET request and catch the response\n",
    "response = requests.get(url)\n",
    "\n",
    "# Print the content of the response\n",
    "print(response.text)\n",
    "This code will:\n",
    "\n",
    "Import the requests library.\n",
    "Define the URL you want to request.\n",
    "Send a GET request to the URL and store the response.\n",
    "Print the content of the response using response.text, which returns the HTML content of the page.\n",
    "You can run this script in any Python environment with internet access. If you have any questions or need further assistance, feel free to ask!\n",
    "\n",
    "\n",
    "2/2\n",
    "\n",
    "'''\n",
    "Import the package requests.\n",
    "Assign the URL of interest to the variable url.\n",
    "Package the request to the URL, send the request and catch the response with a single function requests.get(), assigning the response to the variable r.\n",
    "Use the text attribute of the object r to return the HTML of the webpage as a string; store the result in a variable text.\n",
    "Hit submit to print the HTML of the webpage.\n",
    "'''\n",
    "\n",
    "\n",
    "# Import package\n",
    "\n",
    "\n",
    "# Specify the url: url\n",
    "\n",
    "\n",
    "# Packages the request, send the request and catch the response: r\n",
    "\n",
    "\n",
    "# Extract the response: text\n",
    "\n",
    "\n",
    "# Print the html\n",
    "print(text)\n",
    "ChatGPT\n",
    "Sure! Here is the code to achieve what you've described:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import the requests package\n",
    "import requests\n",
    "\n",
    "# Specify the URL\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "\n",
    "# Package the request, send the request, and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response: text\n",
    "text = r.text\n",
    "\n",
    "# Print the HTML\n",
    "print(text)\n",
    "This code will:\n",
    "\n",
    "Import the requests package.\n",
    "Assign the URL of interest to the variable url.\n",
    "Send a GET request to the URL using requests.get(), and store the response in the variable r.\n",
    "Extract the HTML content from the response using the text attribute and store it in the variable text.\n",
    "Print the HTML content of the webpage.\n",
    "You can run this script in any Python environment with internet access. If you have any questions or need further assistance, feel free to ask!\n",
    "\n",
    "Parsing HTML with BeautifulSoup\n",
    "In this interactive exercise, you'll learn how to use the BeautifulSoup package to parse, prettify and extract information from HTML. You'll scrape the data from the webpage of Guido van Rossum, Python's very own Benevolent Dictator for Life. In the following exercises, you'll prettify the HTML and then extract the text and the hyperlinks.\n",
    "\n",
    "The URL of interest is url = 'https://www.python.org/~guido/'\n",
    "ChatGPT\n",
    "To parse and extract information from HTML using the BeautifulSoup package, you can follow these steps:\n",
    "\n",
    "Import the necessary packages: requests to fetch the web page and BeautifulSoup to parse the HTML.\n",
    "Fetch the HTML content of the URL using requests.get().\n",
    "Create a BeautifulSoup object to parse the HTML content.\n",
    "Prettify the HTML to make it more readable.\n",
    "Extract and print the text and hyperlinks.\n",
    "Here's the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify the URL\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request, and catch the response\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response content\n",
    "html_content = r.text\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Prettify the HTML\n",
    "pretty_html = soup.prettify()\n",
    "print(pretty_html)\n",
    "\n",
    "# Extract and print all text from the HTML\n",
    "page_text = soup.get_text()\n",
    "print(page_text)\n",
    "\n",
    "# Extract and print all hyperlinks\n",
    "hyperlinks = soup.find_all('a')\n",
    "for link in hyperlinks:\n",
    "    print(link.get('href'))\n",
    "This code will:\n",
    "\n",
    "Import the requests and BeautifulSoup packages.\n",
    "Specify the URL of Guido van Rossum's page.\n",
    "Send a GET request to the URL and catch the response.\n",
    "Extract the HTML content from the response.\n",
    "Create a BeautifulSoup object to parse the HTML content.\n",
    "Prettify the HTML content and print it to make it more readable.\n",
    "Extract and print all the text from the HTML.\n",
    "Extract and print all the hyperlinks from the HTML.\n",
    "You can run this script in any Python environment with internet access. If you have any questions or need further assistance, feel free to ask!\n",
    "\n",
    "'''\n",
    "Import the function BeautifulSoup from the package bs4.\n",
    "Assign the URL of interest to the variable url.\n",
    "Package the request to the URL, send the request and catch the response with a single function requests.get(), assigning the response to the variable r.\n",
    "Use the text attribute of the object r to return the HTML of the webpage as a string; store the result in a variable html_doc.\n",
    "Create a BeautifulSoup object soup from the resulting HTML using the function BeautifulSoup().\n",
    "Use the method prettify() on soup and assign the result to pretty_soup.\n",
    "Hit submit to print to prettified HTML to your shell!\n",
    "'''\n",
    "# Import packages\n",
    "import requests\n",
    "from ____ import ____\n",
    "\n",
    "# Specify url: url\n",
    "\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "\n",
    "\n",
    "# Extracts the response as html: html_doc\n",
    "\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "\n",
    "\n",
    "# Prettify the BeautifulSoup object: pretty_soup\n",
    "\n",
    "\n",
    "# Print the response\n",
    "print(pretty_soup)\n",
    "ChatGPT\n",
    "Sure! Here's the complete code that follows your instructions:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Prettify the BeautifulSoup object: pretty_soup\n",
    "pretty_soup = soup.prettify()\n",
    "\n",
    "# Print the prettified HTML\n",
    "print(pretty_soup)\n",
    "This code will:\n",
    "\n",
    "Import the requests package and the BeautifulSoup function from bs4.\n",
    "Assign the URL of interest to the variable url.\n",
    "Send a GET request to the URL and catch the response in the variable r.\n",
    "Extract the HTML content from the response and store it in the variable html_doc.\n",
    "Create a BeautifulSoup object from the HTML content.\n",
    "Use the prettify() method on the BeautifulSoup object and assign the result to pretty_soup.\n",
    "Print the prettified HTML.\n",
    "You can run this script in any Python environment with internet access. If you have any questions or need further assistance, feel free to ask!\n",
    "\n",
    "Turning a webpage into data using BeautifulSoup: getting the text\n",
    "As promised, in the following exercises, you'll learn the basics of extracting information from HTML soup. In this exercise, you'll figure out how to extract the text from the BDFL's webpage, along with printing the webpage's title\n",
    "ChatGPT\n",
    "To extract the text and title from a webpage using BeautifulSoup, you can follow these steps:\n",
    "\n",
    "Import the necessary packages: requests to fetch the web page and BeautifulSoup to parse the HTML.\n",
    "Fetch the HTML content of the URL using requests.get().\n",
    "Create a BeautifulSoup object to parse the HTML content.\n",
    "Extract the title of the webpage using the title attribute.\n",
    "Extract all the text from the webpage using the get_text() method.\n",
    "Print the title and the extracted text.\n",
    "Here's the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify the URL\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Get the title of the webpage\n",
    "title = soup.title.string\n",
    "\n",
    "# Extract the text from the webpage\n",
    "page_text = soup.get_text()\n",
    "\n",
    "# Print the title\n",
    "print(\"Title of the webpage:\", title)\n",
    "\n",
    "# Print the extracted text\n",
    "print(\"Text from the webpage:\\n\", page_text)\n",
    "This code will:\n",
    "\n",
    "Import the requests package and the BeautifulSoup function from bs4.\n",
    "Assign the URL of Guido van Rossum's page to the variable url.\n",
    "Send a GET request to the URL and catch the response in the variable r.\n",
    "Extract the HTML content from the response and store it in the variable html_doc.\n",
    "Create a BeautifulSoup object from the HTML content.\n",
    "Extract the title of the webpage using soup.title.string.\n",
    "Extract all the text from the webpage using soup.get_text().\n",
    "Print the title and the extracted text from the webpage.\n",
    "You can run this script in any Python environment with internet access. If you have any questions or need further assistance, feel free to ask!\n",
    "\n",
    "''''\n",
    "In the sample code, the HTML response object html_doc has already been created: your first task is to Soupify it using the function BeautifulSoup() and to assign the resulting soup to the variable soup.\n",
    "Extract the title from the HTML soup soup using the attribute title and assign the result to guido_title.\n",
    "Print the title of Guido's webpage to the shell using the print() function.\n",
    "Extract the text from the HTML soup soup using the method get_text() and assign to guido_text.\n",
    "Hit submit to print the text from Guido's webpage to the shell.\n",
    "'''\n",
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "\n",
    "\n",
    "# Get the title of Guido's webpage: guido_title\n",
    "\n",
    "\n",
    "# Print the title of Guido's webpage to the shell\n",
    "\n",
    "\n",
    "# Get Guido's text: guido_text\n",
    "\n",
    "\n",
    "# Print Guido's text to the shell\n",
    "print(guido_text)\n",
    "ChatGPT\n",
    "Here's the complete code following your instructions:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Get the title of Guido's webpage: guido_title\n",
    "guido_title = soup.title.string\n",
    "\n",
    "# Print the title of Guido's webpage to the shell\n",
    "print(\"Title of Guido's webpage:\", guido_title)\n",
    "\n",
    "# Get Guido's text: guido_text\n",
    "guido_text = soup.get_text()\n",
    "\n",
    "# Print Guido's text to the shell\n",
    "print(guido_text)\n",
    "This code will:\n",
    "\n",
    "Import the requests package and the BeautifulSoup function from bs4.\n",
    "Assign the URL of Guido van Rossum's page to the variable url.\n",
    "Send a GET request to the URL and catch the response in the variable r.\n",
    "Extract the HTML content from the response and store it in the variable html_doc.\n",
    "Create a BeautifulSoup object from the HTML content and assign it to soup.\n",
    "Extract the title of the webpage using soup.title.string and assign it to guido_title.\n",
    "Print the title of Guido's webpage.\n",
    "Extract all the text from the webpage using soup.get_text() and assign it to guido_text.\n",
    "Print the text from Guido's webpage.\n",
    "You can run this script in any Python environment with internet access. If you have any questions or need further assistance, feel free to ask!\n",
    "\n",
    "\n",
    "2/2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Was this response better or worse?\n",
    "\n",
    "Better\n",
    "\n",
    "Worse\n",
    "\n",
    "Same\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
