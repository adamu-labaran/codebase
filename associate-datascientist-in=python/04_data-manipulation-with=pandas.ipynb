{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming DataFrames\n",
    "Free\n",
    "\n",
    "0%\n",
    "Let’s master the pandas basics. Learn how to inspect DataFrames and perform fundamental manipulations, including sorting rows, subsetting, and adding new columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the first few rows of the homelessness DataFrame\n",
    "print(homelessness.head())\n",
    "\n",
    "# Print information about the column types and missing values in homelessness\n",
    "print(homelessness.info())\n",
    "\n",
    "# Print the shape of the homelessness DataFrame\n",
    "print(homelessness.shape)\n",
    "\n",
    "# Print summary statistics of the homelessness DataFrame\n",
    "print(homelessness.describe())\n",
    "\n",
    "'''\n",
    "Insightful inspecting! You can see that the average number of homeless individuals in each state is about 7226. Let's explore the DataFrame further.\n",
    "\n",
    "\n",
    "'''Inspecting a DataFrame\n",
    "When you get a new DataFrame to work with, the first thing you need to do is explore it and see what it contains. There are several useful methods and attributes for this.\n",
    "\n",
    ".head() returns the first few rows (the “head” of the DataFrame).\n",
    ".info() shows information on each of the columns, such as the data type and number of missing values.\n",
    ".shape returns the number of rows and columns of the DataFrame.\n",
    ".describe() calculates a few summary statistics for each column.\n",
    "homelessness is a DataFrame containing estimates of homelessness in each U.S. state in 2018. The individual column is the number of homeless individuals not part of a family with children. The family_members column is the number of homeless individuals part of a family with children. The state_pop column is the state's total population.\n",
    "\n",
    "pandas is imported for you.\n",
    "\n",
    "Instructions 1/4\n",
    "25 XP\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "Print the head of the homelessness DataFrame'''\n",
    "# Print the head of the homelessness data\n",
    "ChatGPT\n",
    "To print the head of the homelessness DataFrame, you can use the .head() method. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Print the first few rows of the homelessness DataFrame\n",
    "print(homelessness.head())\n",
    "This code will display the first few rows of the homelessness DataFrame, giving you an overview of the initial entries in your dataset.\n",
    "\n",
    "# Print the head of the homelessness data\n",
    "# Print information about the column types and missing values in homelessness.\n",
    "print(homelessness.head())\n",
    "\n",
    "# Print information about homelessness\n",
    "____\n",
    "ChatGPT\n",
    "To print information about the column types and missing values in the homelessness DataFrame, you can use the .info() method. Here's the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Print the first few rows of the homelessness DataFrame\n",
    "print(homelessness.head())\n",
    "\n",
    "# Print information about the column types and missing values in homelessness\n",
    "print(homelessness.info())\n",
    "This will display the first few rows of the DataFrame and provide detailed information about each column, including the data type and the number of non-null values.\n",
    "\n",
    "# Print the head of the homelessness data\n",
    "print(homelessness.head())\n",
    "\n",
    "# Print information about homelessness\n",
    "print(homelessness.info())\n",
    "\n",
    "# Print the shape of homelessness\n",
    "# Print the number of rows and columns in homelessness.\n",
    "____\n",
    "ChatGPT\n",
    "To print the shape of the homelessness DataFrame, you can use the .shape attribute. Here's the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Print the first few rows of the homelessness DataFrame\n",
    "print(homelessness.head())\n",
    "\n",
    "# Print information about the column types and missing values in homelessness\n",
    "print(homelessness.info())\n",
    "\n",
    "# Print the shape of the homelessness DataFrame\n",
    "print(homelessness.shape)\n",
    "This will print the number of rows and columns in the homelessness DataFrame.\n",
    "\n",
    "\n",
    "# Print the head of the homelessness data\n",
    "print(homelessness.head())\n",
    "\n",
    "# Print information about homelessness\n",
    "print(homelessness.info())\n",
    "\n",
    "# Print the shape of homelessness\n",
    "print(homelessness.shape)\n",
    "\n",
    "# Print a description of homelessness\n",
    "# Print some summary statistics that describe the homelessness DataFrame.\n",
    "____\n",
    "ChatGPT\n",
    "To print a description of the homelessness DataFrame, you can use the .describe() method. This will provide summary statistics for the numerical columns. Here's the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Print the first few rows of the homelessness DataFrame\n",
    "print(homelessness.head())\n",
    "\n",
    "# Print information about the column types and missing values in homelessness\n",
    "print(homelessness.info())\n",
    "\n",
    "# Print the shape of the homelessness DataFrame\n",
    "print(homelessness.shape)\n",
    "\n",
    "# Print summary statistics of the homelessness DataFrame\n",
    "print(homelessness.describe())\n",
    "This will provide a detailed statistical summary of the numerical columns in the homelessness DataFrame, including the count, mean, standard deviation, minimum, and maximum values, as well as the 25th, 50th, and 75th percentiles.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Print the values of homelessness\n",
    "print(homelessness.values)\n",
    "\n",
    "# Print the column index of homelessness\n",
    "print(homelessness.columns)\n",
    "\n",
    "# Print the row index of homelessness\n",
    "print(homelessness.index)\n",
    "\n",
    "\n",
    "'''Dynamite DataFrame dissection! DataFrames have three components: values, a column index, and a row index.\n",
    "\n",
    "# Print the head of the homelessness data\n",
    "print(homelessness.head())\n",
    "\n",
    "# Print information about homelessness\n",
    "print(homelessness.info())\n",
    "\n",
    "# Print the shape of homelessness\n",
    "print(homelessness.shape)\n",
    "\n",
    "# Print a description of homelessness\n",
    "# Print some summary statistics that describe the homelessness DataFrame.\n",
    "____\n",
    "ChatGPT\n",
    "To print a description of the homelessness DataFrame, you can use the .describe() method. This will provide summary statistics for the numerical columns. Here's the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Print the first few rows of the homelessness DataFrame\n",
    "print(homelessness.head())\n",
    "\n",
    "# Print information about the column types and missing values in homelessness\n",
    "print(homelessness.info())\n",
    "\n",
    "# Print the shape of the homelessness DataFrame\n",
    "print(homelessness.shape)\n",
    "\n",
    "# Print summary statistics of the homelessness DataFrame\n",
    "print(homelessness.describe())\n",
    "This will provide a detailed statistical summary of the numerical columns in the homelessness DataFrame, including the count, mean, standard deviation, minimum, and maximum values, as well as the 25th, 50th, and 75th percentiles.\n",
    "\n",
    "'''Parts of a DataFrame\n",
    "To better understand DataFrame objects, it's useful to know that they consist of three components, stored as attributes:\n",
    "\n",
    ".values: A two-dimensional NumPy array of values.\n",
    ".columns: An index of columns: the column names.\n",
    ".index: An index for the rows: either row numbers or row names.\n",
    "You can usually think of indexes as a list of strings or numbers, though the pandas Index data type allows for more sophisticated options. (These will be covered later in the course.)\n",
    "\n",
    "homelessness is available.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import pandas using the alias pd.\n",
    "Print a 2D NumPy array of the values in homelessness.\n",
    "Print the column names of homelessness.\n",
    "Print the index of homelessness.'''\n",
    "\n",
    "\n",
    "\n",
    "# Import pandas using the alias pd\n",
    "____\n",
    "\n",
    "# Print the values of homelessness\n",
    "____\n",
    "\n",
    "# Print the column index of homelessness\n",
    "____\n",
    "\n",
    "# Print the row index of homelessness\n",
    "____\n",
    "ChatGPT\n",
    "Here's how you can access and print the various components of the homelessness DataFrame:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Print the values of homelessness\n",
    "print(homelessness.values)\n",
    "\n",
    "# Print the column index of homelessness\n",
    "print(homelessness.columns)\n",
    "\n",
    "# Print the row index of homelessness\n",
    "print(homelessness.index)\n",
    "This code will print:\n",
    "\n",
    "The 2D NumPy array of the values in homelessness.\n",
    "The column names of homelessness.\n",
    "The row index of homelessness.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Sort homelessness by individuals\n",
    "homelessness_ind = homelessness.sort_values(\"individuals\")\n",
    "\n",
    "# Print the top few rows of the sorted DataFrame\n",
    "print(homelessness_ind.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Sort homelessness by descending family members\n",
    "homelessness_fam = homelessness.sort_values(\"family_members\", ascending=False)\n",
    "\n",
    "# Print the top few rows of the sorted DataFrame\n",
    "print(homelessness_fam.head())\n",
    "# Your head is sorted! Using .sort_values() and .head() form a powerful pair.\n",
    "\n",
    "'''\n",
    "''''Sorting rows\n",
    "Finding interesting bits of data in a DataFrame is often easier if you change the order of the rows. You can sort the rows by passing a column name to .sort_values().\n",
    "\n",
    "In cases where rows have the same value (this is common if you sort on a categorical variable), you may wish to break the ties by sorting on another column. You can sort on multiple columns in this way by passing a list of column names.\n",
    "\n",
    "Sort on …\tSyntax\n",
    "one column\tdf.sort_values(\"breed\")\n",
    "multiple columns\tdf.sort_values([\"breed\", \"weight_kg\"])\n",
    "By combining .sort_values() with .head(), you can answer questions in the form, \"What are the top cases where…?\".\n",
    "\n",
    "homelessness is available and pandas is loaded as pd.\n",
    "\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "1\n",
    "Sort homelessness by the number of homeless individuals in the individuals column, from smallest to largest, and save this as homelessness_ind.\n",
    "Print the head of the sorted DataFrame.'''\n",
    "\n",
    "\n",
    "\n",
    "# Sort homelessness by individuals\n",
    "homelessness_ind = ____\n",
    "\n",
    "# Print the top few rows\n",
    "print(____)\n",
    "ChatGPT\n",
    "To sort the homelessness DataFrame by the number of homeless individuals in the individuals column, you can use the .sort_values() method. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Sort homelessness by individuals\n",
    "homelessness_ind = homelessness.sort_values(\"individuals\")\n",
    "\n",
    "# Print the top few rows of the sorted DataFrame\n",
    "print(homelessness_ind.head())\n",
    "This will sort the homelessness DataFrame by the individuals column in ascending order and print the first few rows of the sorted DataFrame.\n",
    "\n",
    "'''\n",
    "Sort homelessness by the number of homeless family_members in descending order, and save this as homelessness_fam.'''\n",
    "\n",
    "# Sort homelessness by descending family members\n",
    "homelessness_fam = ____\n",
    "\n",
    "print(homelessness_fam.head())\n",
    "ChatGPT\n",
    "To sort the homelessness DataFrame by the number of homeless family_members in descending order, you can use the .sort_values() method with the ascending parameter set to False. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Sort homelessness by descending family members\n",
    "homelessness_fam = homelessness.sort_values(\"family_members\", ascending=False)\n",
    "\n",
    "# Print the top few rows of the sorted DataFrame\n",
    "print(homelessness_fam.head())\n",
    "This code will sort the homelessness DataFrame by the family_members column in descending order and print the first few rows of the sorted DataFrame.\n",
    "\n",
    "'''Sort homelessness first by region (ascending), and then by number of family members (descending). Save this as homelessness_reg_fam.'''\n",
    "\n",
    "\n",
    "# Sort homelessness by region, then descending family members\n",
    "homelessness_reg_fam = ____\n",
    "\n",
    "# Print the top few rows\n",
    "print(homelessness_fam.head())\n",
    "ChatGPT\n",
    "To sort the homelessness DataFrame first by region in ascending order and then by family_members in descending order, you can pass a list of column names to the .sort_values() method and specify the sorting order for each column. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Sort homelessness by region (ascending), then by family members (descending)\n",
    "homelessness_reg_fam = homelessness.sort_values(by=[\"region\", \"family_members\"], ascending=[True, False])\n",
    "\n",
    "# Print the top few rows of the sorted DataFrame\n",
    "print(homelessness_reg_fam.head())\n",
    "This code will sort the homelessness DataFrame first by the region column in ascending order and then by the family_members column in descending order. The top few rows of the sorted DataFrame will be printed.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Select the individuals column\n",
    "individuals = homelessness[\"individuals\"]\n",
    "\n",
    "# Print the top few rows of the Series\n",
    "print(individuals.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Select the state and family_members columns\n",
    "state_fam = homelessness[[\"state\", \"family_members\"]]\n",
    "\n",
    "# Print the top few rows of the DataFrame\n",
    "print(state_fam.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Select only the individuals and state columns, in that order\n",
    "ind_state = homelessness[[\"individuals\", \"state\"]]\n",
    "\n",
    "# Print the top few rows of the DataFrame\n",
    "print(ind_state.head())\n",
    "\n",
    "\n",
    "'''Radical reordering! Selecting and reordering columns can make data easier to work with.\n",
    "'''Subsetting columns\n",
    "When working with data, you may not need all of the variables in your dataset. Square brackets ([]) can be used to select only the columns that matter to you in an order that makes sense to you. To select only \"col_a\" of the DataFrame df, use\n",
    "\n",
    "df[\"col_a\"]\n",
    "To select \"col_a\" and \"col_b\" of df, use\n",
    "\n",
    "df[[\"col_a\", \"col_b\"]]\n",
    "homelessness is available and pandas is loaded as pd.\n",
    "\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "1\n",
    "Create a Series called individuals that contains only the individuals column of homelessness.'''\n",
    "\n",
    "\n",
    "# Select the individuals column\n",
    "individuals = ____\n",
    "\n",
    "print(individuals.head())\n",
    "\n",
    "ChatGPT\n",
    "To create a Series called individuals that contains only the individuals column of the homelessness DataFrame, you can use square brackets. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Select the individuals column\n",
    "individuals = homelessness[\"individuals\"]\n",
    "\n",
    "# Print the top few rows of the Series\n",
    "print(individuals.head())\n",
    "This code will create a Series containing only the individuals column from the homelessness DataFrame and print the first few rows of this Series.\n",
    "\n",
    "'''Create a DataFrame called state_fam that contains only the state and family_members columns of homelessness, in that order.'''\n",
    "\n",
    "\n",
    "# Select the state and family_members columns\n",
    "state_fam = ____\n",
    "\n",
    "print(state_fam.head())\n",
    "ChatGPT\n",
    "To create a DataFrame called state_fam that contains only the state and family_members columns of the homelessness DataFrame, you can use double square brackets to select multiple columns. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Select the state and family_members columns\n",
    "state_fam = homelessness[[\"state\", \"family_members\"]]\n",
    "\n",
    "# Print the top few rows of the DataFrame\n",
    "print(state_fam.head())\n",
    "This code will create a DataFrame containing only the state and family_members columns from the homelessness DataFrame and print the first few rows of this new DataFrame.\n",
    "\n",
    "\n",
    "# Create a DataFrame called ind_state that contains the individuals and state columns of homelessness, in that order.\n",
    "\n",
    "# Select only the individuals and state columns, in that order\n",
    "ind_state = ____\n",
    "\n",
    "print(ind_state.head())\n",
    "ChatGPT\n",
    "To create a DataFrame called ind_state that contains only the individuals and state columns of the homelessness DataFrame, in that order, you can use double square brackets. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Select only the individuals and state columns, in that order\n",
    "ind_state = homelessness[[\"individuals\", \"state\"]]\n",
    "\n",
    "# Print the top few rows of the DataFrame\n",
    "print(ind_state.head())\n",
    "This code will create a DataFrame containing only the individuals and state columns from the homelessness DataFrame and print the first few rows of this new DataFrame.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subseting rows\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Filter for rows where individuals is greater than 10000\n",
    "ind_gt_10k = homelessness[homelessness[\"individuals\"] > 10000]\n",
    "\n",
    "# See the result\n",
    "print(ind_gt_10k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Filter for rows where region is Mountain\n",
    "mountain_reg = homelessness[homelessness[\"region\"] == \"Mountain\"]\n",
    "\n",
    "# See the result\n",
    "print(mountain_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Filter for rows where family_members is less than 1000 and region is Pacific\n",
    "fam_lt_1k_pac = homelessness[(homelessness[\"family_members\"] < 1000) & (homelessness[\"region\"] == \"Pacific\")]\n",
    "\n",
    "# See the result\n",
    "print(fam_lt_1k_pac)\n",
    "\n",
    "\n",
    "'''\n",
    "Superb subsetting! Using square brackets plus logical conditions is often the most powerful way of identifying interesting rows of data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "'''Inspecting a DataFrame\n",
    "When you get a new DataFrame to work with, the first thing you need to do is explore it and see what it contains. There are several useful methods and attributes for this.\n",
    "\n",
    ".head() returns the first few rows (the “head” of the DataFrame).\n",
    ".info() shows information on each of the columns, such as the data type and number of missing values.\n",
    ".shape returns the number of rows and columns of the DataFrame.\n",
    ".describe() calculates a few summary statistics for each column.\n",
    "homelessness is a DataFrame containing estimates of homelessness in each U.S. state in 2018. The individual column is the number of homeless individuals not part of a family with children. The family_members column is the number of homeless individuals part of a family with children. The state_pop column is the state's total population.\n",
    "\n",
    "pandas is imported for you.\n",
    "\n",
    "Instructions 1/4\n",
    "25 XP\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "Print the head of the homelessness DataFrame'''\n",
    "# Print the head of the homelessness data\n",
    "ChatGPT\n",
    "To print the head of the homelessness DataFrame, you can use the .head() method. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Print the first few rows of the homelessness DataFrame\n",
    "print(homelessness.head())\n",
    "This code will display the first few rows of the homelessness DataFrame, giving you an overview of the initial entries in your dataset.\n",
    "\n",
    "# Print the head of the homelessness data\n",
    "# Print information about the column types and missing values in homelessness.\n",
    "print(homelessness.head())\n",
    "\n",
    "# Print information about homelessness\n",
    "____\n",
    "ChatGPT\n",
    "To print information about the column types and missing values in the homelessness DataFrame, you can use the .info() method. Here's the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Print the first few rows of the homelessness DataFrame\n",
    "print(homelessness.head())\n",
    "\n",
    "# Print information about the column types and missing values in homelessness\n",
    "print(homelessness.info())\n",
    "This will display the first few rows of the DataFrame and provide detailed information about each column, including the data type and the number of non-null values.\n",
    "\n",
    "# Print the head of the homelessness data\n",
    "print(homelessness.head())\n",
    "\n",
    "# Print information about homelessness\n",
    "print(homelessness.info())\n",
    "\n",
    "# Print the shape of homelessness\n",
    "# Print the number of rows and columns in homelessness.\n",
    "____\n",
    "ChatGPT\n",
    "To print the shape of the homelessness DataFrame, you can use the .shape attribute. Here's the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Print the first few rows of the homelessness DataFrame\n",
    "print(homelessness.head())\n",
    "\n",
    "# Print information about the column types and missing values in homelessness\n",
    "print(homelessness.info())\n",
    "\n",
    "# Print the shape of the homelessness DataFrame\n",
    "print(homelessness.shape)\n",
    "This will print the number of rows and columns in the homelessness DataFrame.\n",
    "\n",
    "\n",
    "# Print the head of the homelessness data\n",
    "print(homelessness.head())\n",
    "\n",
    "# Print information about homelessness\n",
    "print(homelessness.info())\n",
    "\n",
    "# Print the shape of homelessness\n",
    "print(homelessness.shape)\n",
    "\n",
    "# Print a description of homelessness\n",
    "# Print some summary statistics that describe the homelessness DataFrame.\n",
    "____\n",
    "ChatGPT\n",
    "To print a description of the homelessness DataFrame, you can use the .describe() method. This will provide summary statistics for the numerical columns. Here's the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Print the first few rows of the homelessness DataFrame\n",
    "print(homelessness.head())\n",
    "\n",
    "# Print information about the column types and missing values in homelessness\n",
    "print(homelessness.info())\n",
    "\n",
    "# Print the shape of the homelessness DataFrame\n",
    "print(homelessness.shape)\n",
    "\n",
    "# Print summary statistics of the homelessness DataFrame\n",
    "print(homelessness.describe())\n",
    "This will provide a detailed statistical summary of the numerical columns in the homelessness DataFrame, including the count, mean, standard deviation, minimum, and maximum values, as well as the 25th, 50th, and 75th percentiles.\n",
    "\n",
    "'''Parts of a DataFrame\n",
    "To better understand DataFrame objects, it's useful to know that they consist of three components, stored as attributes:\n",
    "\n",
    ".values: A two-dimensional NumPy array of values.\n",
    ".columns: An index of columns: the column names.\n",
    ".index: An index for the rows: either row numbers or row names.\n",
    "You can usually think of indexes as a list of strings or numbers, though the pandas Index data type allows for more sophisticated options. (These will be covered later in the course.)\n",
    "\n",
    "homelessness is available.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import pandas using the alias pd.\n",
    "Print a 2D NumPy array of the values in homelessness.\n",
    "Print the column names of homelessness.\n",
    "Print the index of homelessness.'''\n",
    "\n",
    "\n",
    "\n",
    "# Import pandas using the alias pd\n",
    "____\n",
    "\n",
    "# Print the values of homelessness\n",
    "____\n",
    "\n",
    "# Print the column index of homelessness\n",
    "____\n",
    "\n",
    "# Print the row index of homelessness\n",
    "____\n",
    "ChatGPT\n",
    "Here's how you can access and print the various components of the homelessness DataFrame:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Print the values of homelessness\n",
    "print(homelessness.values)\n",
    "\n",
    "# Print the column index of homelessness\n",
    "print(homelessness.columns)\n",
    "\n",
    "# Print the row index of homelessness\n",
    "print(homelessness.index)\n",
    "This code will print:\n",
    "\n",
    "The 2D NumPy array of the values in homelessness.\n",
    "The column names of homelessness.\n",
    "The row index of homelessness.\n",
    "''''Sorting rows\n",
    "Finding interesting bits of data in a DataFrame is often easier if you change the order of the rows. You can sort the rows by passing a column name to .sort_values().\n",
    "\n",
    "In cases where rows have the same value (this is common if you sort on a categorical variable), you may wish to break the ties by sorting on another column. You can sort on multiple columns in this way by passing a list of column names.\n",
    "\n",
    "Sort on …\tSyntax\n",
    "one column\tdf.sort_values(\"breed\")\n",
    "multiple columns\tdf.sort_values([\"breed\", \"weight_kg\"])\n",
    "By combining .sort_values() with .head(), you can answer questions in the form, \"What are the top cases where…?\".\n",
    "\n",
    "homelessness is available and pandas is loaded as pd.\n",
    "\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "1\n",
    "Sort homelessness by the number of homeless individuals in the individuals column, from smallest to largest, and save this as homelessness_ind.\n",
    "Print the head of the sorted DataFrame.'''\n",
    "\n",
    "\n",
    "\n",
    "# Sort homelessness by individuals\n",
    "homelessness_ind = ____\n",
    "\n",
    "# Print the top few rows\n",
    "print(____)\n",
    "ChatGPT\n",
    "To sort the homelessness DataFrame by the number of homeless individuals in the individuals column, you can use the .sort_values() method. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Sort homelessness by individuals\n",
    "homelessness_ind = homelessness.sort_values(\"individuals\")\n",
    "\n",
    "# Print the top few rows of the sorted DataFrame\n",
    "print(homelessness_ind.head())\n",
    "This will sort the homelessness DataFrame by the individuals column in ascending order and print the first few rows of the sorted DataFrame.\n",
    "\n",
    "'''\n",
    "Sort homelessness by the number of homeless family_members in descending order, and save this as homelessness_fam.'''\n",
    "\n",
    "# Sort homelessness by descending family members\n",
    "homelessness_fam = ____\n",
    "\n",
    "print(homelessness_fam.head())\n",
    "ChatGPT\n",
    "To sort the homelessness DataFrame by the number of homeless family_members in descending order, you can use the .sort_values() method with the ascending parameter set to False. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Sort homelessness by descending family members\n",
    "homelessness_fam = homelessness.sort_values(\"family_members\", ascending=False)\n",
    "\n",
    "# Print the top few rows of the sorted DataFrame\n",
    "print(homelessness_fam.head())\n",
    "This code will sort the homelessness DataFrame by the family_members column in descending order and print the first few rows of the sorted DataFrame.\n",
    "\n",
    "'''Sort homelessness first by region (ascending), and then by number of family members (descending). Save this as homelessness_reg_fam.'''\n",
    "\n",
    "\n",
    "# Sort homelessness by region, then descending family members\n",
    "homelessness_reg_fam = ____\n",
    "\n",
    "# Print the top few rows\n",
    "print(homelessness_fam.head())\n",
    "ChatGPT\n",
    "To sort the homelessness DataFrame first by region in ascending order and then by family_members in descending order, you can pass a list of column names to the .sort_values() method and specify the sorting order for each column. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Sort homelessness by region (ascending), then by family members (descending)\n",
    "homelessness_reg_fam = homelessness.sort_values(by=[\"region\", \"family_members\"], ascending=[True, False])\n",
    "\n",
    "# Print the top few rows of the sorted DataFrame\n",
    "print(homelessness_reg_fam.head())\n",
    "This code will sort the homelessness DataFrame first by the region column in ascending order and then by the family_members column in descending order. The top few rows of the sorted DataFrame will be printed.\n",
    "\n",
    "'''Subsetting columns\n",
    "When working with data, you may not need all of the variables in your dataset. Square brackets ([]) can be used to select only the columns that matter to you in an order that makes sense to you. To select only \"col_a\" of the DataFrame df, use\n",
    "\n",
    "df[\"col_a\"]\n",
    "To select \"col_a\" and \"col_b\" of df, use\n",
    "\n",
    "df[[\"col_a\", \"col_b\"]]\n",
    "homelessness is available and pandas is loaded as pd.\n",
    "\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "1\n",
    "Create a Series called individuals that contains only the individuals column of homelessness.'''\n",
    "\n",
    "\n",
    "# Select the individuals column\n",
    "individuals = ____\n",
    "\n",
    "print(individuals.head())\n",
    "ChatGPT\n",
    "To create a Series called individuals that contains only the individuals column of the homelessness DataFrame, you can use square brackets. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Select the individuals column\n",
    "individuals = homelessness[\"individuals\"]\n",
    "\n",
    "# Print the top few rows of the Series\n",
    "print(individuals.head())\n",
    "This code will create a Series containing only the individuals column from the homelessness DataFrame and print the first few rows of this Series.\n",
    "\n",
    "'''Create a DataFrame called state_fam that contains only the state and family_members columns of homelessness, in that order.'''\n",
    "\n",
    "\n",
    "# Select the state and family_members columns\n",
    "state_fam = ____\n",
    "\n",
    "print(state_fam.head())\n",
    "ChatGPT\n",
    "To create a DataFrame called state_fam that contains only the state and family_members columns of the homelessness DataFrame, you can use double square brackets to select multiple columns. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Select the state and family_members columns\n",
    "state_fam = homelessness[[\"state\", \"family_members\"]]\n",
    "\n",
    "# Print the top few rows of the DataFrame\n",
    "print(state_fam.head())\n",
    "This code will create a DataFrame containing only the state and family_members columns from the homelessness DataFrame and print the first few rows of this new DataFrame.\n",
    "\n",
    "\n",
    "# Create a DataFrame called ind_state that contains the individuals and state columns of homelessness, in that order.\n",
    "\n",
    "# Select only the individuals and state columns, in that order\n",
    "ind_state = ____\n",
    "\n",
    "print(ind_state.head())\n",
    "ChatGPT\n",
    "To create a DataFrame called ind_state that contains only the individuals and state columns of the homelessness DataFrame, in that order, you can use double square brackets. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Select only the individuals and state columns, in that order\n",
    "ind_state = homelessness[[\"individuals\", \"state\"]]\n",
    "\n",
    "# Print the top few rows of the DataFrame\n",
    "print(ind_state.head())\n",
    "This code will create a DataFrame containing only the individuals and state columns from the homelessness DataFrame and print the first few rows of this new DataFrame.\n",
    "\n",
    "'''Subsetting rows\n",
    "A large part of data science is about finding which bits of your dataset are interesting. One of the simplest techniques for this is to find a subset of rows that match some criteria. This is sometimes known as filtering rows or selecting rows.\n",
    "\n",
    "There are many ways to subset a DataFrame, perhaps the most common is to use relational operators to return True or False for each row, then pass that inside square brackets.\n",
    "\n",
    "dogs[dogs[\"height_cm\"] > 60]\n",
    "dogs[dogs[\"color\"] == \"tan\"]\n",
    "You can filter for multiple conditions at once by using the \"bitwise and\" operator, &.\n",
    "\n",
    "dogs[(dogs[\"height_cm\"] > 60) & (dogs[\"color\"] == \"tan\")]\n",
    "homelessness is available and pandas is loaded as pd.\n",
    "\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "1\n",
    "Filter homelessness for cases where the number of individuals is greater than ten thousand, assigning to ind_gt_10k. View the printed result.'''\n",
    "\n",
    "\n",
    "\n",
    "# Filter for rows where individuals is greater than 10000\n",
    "ind_gt_10k = ____\n",
    "\n",
    "# See the result\n",
    "print(ind_gt_10k)\n",
    "ChatGPT\n",
    "To filter the homelessness DataFrame for cases where the number of individuals is greater than ten thousand, you can use a relational operator inside square brackets. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Filter for rows where individuals is greater than 10000\n",
    "ind_gt_10k = homelessness[homelessness[\"individuals\"] > 10000]\n",
    "\n",
    "# See the result\n",
    "print(ind_gt_10k)\n",
    "This code will create a subset of the homelessness DataFrame where the individuals column has values greater than 10,000 and then print this subset.\n",
    "\n",
    "# Filter homelessness for cases where the USA Census region is \"Mountain\", assigning to mountain_reg. View the printed result.\n",
    "\n",
    "\n",
    "\n",
    "# Filter for rows where region is Mountain\n",
    "mountain_reg = ____\n",
    "\n",
    "# See the result\n",
    "____\n",
    "ChatGPT\n",
    "To filter the homelessness DataFrame for cases where the USA Census region is \"Mountain\", you can use a relational operator inside square brackets. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Filter for rows where region is Mountain\n",
    "mountain_reg = homelessness[homelessness[\"region\"] == \"Mountain\"]\n",
    "\n",
    "# See the result\n",
    "print(mountain_reg)\n",
    "This code will create a subset of the homelessness DataFrame where the region column has the value \"Mountain\" and then print this subset.\n",
    "\n",
    "'''Filter homelessness for cases where the number of family_members is less than one thousand and the region is \"Pacific\", assigning to fam_lt_1k_pac. View the printed result.'''\n",
    "\n",
    "# Filter for rows where family_members is less than 1000 \n",
    "# and region is Pacific\n",
    "fam_lt_1k_pac = ____\n",
    "\n",
    "# See the result\n",
    "print(fam_lt_1k_pac)\n",
    "ChatGPT\n",
    "To filter the homelessness DataFrame for cases where the number of family_members is less than one thousand and the region is \"Pacific\", you can use the \"bitwise and\" operator & inside square brackets. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Filter for rows where family_members is less than 1000 and region is Pacific\n",
    "fam_lt_1k_pac = homelessness[(homelessness[\"family_members\"] < 1000) & (homelessness[\"region\"] == \"Pacific\")]\n",
    "\n",
    "# See the result\n",
    "print(fam_lt_1k_pac)\n",
    "This code will create a subset of the homelessness DataFrame where the family_members column has values less than 1000 and the region column has the value \"Pacific\", and then print this subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# The Mojave Desert states\n",
    "canu = [\"California\", \"Arizona\", \"Nevada\", \"Utah\"]\n",
    "\n",
    "# Filter for rows where the state is in the list of Mojave Desert states\n",
    "mojave_homelessness = homelessness[homelessness[\"state\"].isin(canu)]\n",
    "\n",
    "# See the result\n",
    "print(mojave_homelessness)\n",
    "\n",
    "# Success _is in_ the air! Using .isin() makes subsetting categorical variables a breeze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Subsetting rows by categorical variables\n",
    "Subsetting data based on a categorical variable often involves using the \"or\" operator (|) to select rows from multiple categories. This can get tedious when you want all states in one of three different regions, for example. Instead, use the .isin() method, which will allow you to tackle this problem by writing one condition instead of three separate ones.\n",
    "\n",
    "colors = [\"brown\", \"black\", \"tan\"]\n",
    "condition = dogs[\"color\"].isin(colors)\n",
    "dogs[condition]\n",
    "homelessness is available and pandas is loaded as pd.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Filter homelessness for cases where the USA census state is in the list of Mojave states, canu, assigning to mojave_homelessness. View the printed result.''''\n",
    "\n",
    "\n",
    "# The Mojave Desert states\n",
    "canu = [\"California\", \"Arizona\", \"Nevada\", \"Utah\"]\n",
    "\n",
    "# Filter for rows in the Mojave Desert states\n",
    "mojave_homelessness = ____\n",
    "\n",
    "# See the result\n",
    "print(mojave_homelessness)\n",
    "ChatGPT\n",
    "To filter the homelessness DataFrame for cases where the USA census state is in the list of Mojave Desert states, you can use the .isin() method. Here’s how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# The Mojave Desert states\n",
    "canu = [\"California\", \"Arizona\", \"Nevada\", \"Utah\"]\n",
    "\n",
    "# Filter for rows where the state is in the list of Mojave Desert states\n",
    "mojave_homelessness = homelessness[homelessness[\"state\"].isin(canu)]\n",
    "\n",
    "# See the result\n",
    "print(mojave_homelessness)\n",
    "This code will create a subset of the homelessness DataFrame where the state column matches any of the states in the canu list and then print this subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Add total column as the sum of individuals and family_members\n",
    "homelessness[\"total\"] = homelessness[\"individuals\"] + homelessness[\"family_members\"]\n",
    "\n",
    "# Add p_homeless column as the proportion of total homeless population to the state population\n",
    "homelessness[\"p_homeless\"] = homelessness[\"total\"] / homelessness[\"state_pop\"]\n",
    "\n",
    "# See the result\n",
    "print(homelessness)\n",
    "\n",
    "# Astounding column assignment! If your dataset doesn't have the exact columns you need, you can often make your own from what you have.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/'''Adding new columns\n",
    "You aren't stuck with just the data you are given. Instead, you can add new columns to a DataFrame. This has many names, such as transforming, mutating, and feature engineering.\n",
    "\n",
    "You can create new columns from scratch, but it is also common to derive them from other columns, for example, by adding columns together or by changing their units.\n",
    "\n",
    "homelessness is a DataFrame containing estimates of homelessness in each U.S. state in 2018. The individual column is the number of homeless individuals not part of a family with children. The family_members column is the number of homeless individuals part of a family with children. The state_pop column is the state's total population.\n",
    "\n",
    "homelessness is available and pandas is loaded as pd.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Add a new column to homelessness, named total, containing the sum of the individuals and family_members columns.\n",
    "Add another column to homelessness, named p_homeless, containing the proportion of the total homeless population to the total population in each state state_pop.'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add total col as sum of individuals and family_members\n",
    "____\n",
    "\n",
    "# Add p_homeless col as proportion of total homeless population to the state population\n",
    "____\n",
    "\n",
    "# See the result\n",
    "print(homelessness)\n",
    "ChatGPT\n",
    "To add new columns to the homelessness DataFrame, you can perform calculations using existing columns and assign the results to new columns. Here's how you can do it:\n",
    "\n",
    "Add a new column named total that contains the sum of the individuals and family_members columns.\n",
    "Add another column named p_homeless that contains the proportion of the total homeless population to the state's total population (state_pop).\n",
    "Here's the code to accomplish this:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Add total column as the sum of individuals and family_members\n",
    "homelessness[\"total\"] = homelessness[\"individuals\"] + homelessness[\"family_members\"]\n",
    "\n",
    "# Add p_homeless column as the proportion of total homeless population to the state population\n",
    "homelessness[\"p_homeless\"] = homelessness[\"total\"] / homelessness[\"state_pop\"]\n",
    "\n",
    "# See the result\n",
    "print(homelessness)\n",
    "This code will:\n",
    "\n",
    "Create a new column total in the homelessness DataFrame, which is the sum of individuals and family_members.\n",
    "Create another new column p_homeless, which is the proportion of the total homeless population to the state's total population (state_pop).\n",
    "Print the updated homelessness DataFrame to show the new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Create indiv_per_10k column as homeless individuals per 10k state pop\n",
    "homelessness[\"indiv_per_10k\"] = 10000 * homelessness[\"individuals\"] / homelessness[\"state_pop\"]\n",
    "\n",
    "# Subset rows for indiv_per_10k greater than 20\n",
    "high_homelessness = homelessness[homelessness[\"indiv_per_10k\"] > 20]\n",
    "\n",
    "# Sort high_homelessness by descending indiv_per_10k\n",
    "high_homelessness_srt = high_homelessness.sort_values(\"indiv_per_10k\", ascending=False)\n",
    "\n",
    "# From high_homelessness_srt, select the state and indiv_per_10k columns\n",
    "result = high_homelessness_srt[[\"state\", \"indiv_per_10k\"]]\n",
    "\n",
    "# See the result\n",
    "print(result)\n",
    "\n",
    "\n",
    "'''Cool combination! District of Columbia has the highest number of homeless individuals - almost 54 per ten thousand people. This is almost double the number of the next-highest state, Hawaii. If you combine new column addition, row subsetting, sorting, and column selection, you can answer lots of questions like this.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Combo-attack!\n",
    "You've seen the four most common types of data manipulation: sorting rows, subsetting columns, subsetting rows, and adding new columns. In a real-life data analysis, you can mix and match these four manipulations to answer a multitude of questions.\n",
    "\n",
    "In this exercise, you'll answer the question, \"Which state has the highest number of homeless individuals per 10,000 people in the state?\" Combine your new pandas skills to find out.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Add a column to homelessness, indiv_per_10k, containing the number of homeless individuals per ten thousand people in each state, using state_pop for state population.\n",
    "Subset rows where indiv_per_10k is higher than 20, assigning to high_homelessness.\n",
    "Sort high_homelessness by descending indiv_per_10k, assigning to high_homelessness_srt.\n",
    "Select only the state and indiv_per_10k columns of high_homelessness_srt and save as result. Look at the result.'''\n",
    "\n",
    "\n",
    "# Create indiv_per_10k col as homeless individuals per 10k state pop\n",
    "homelessness[\"indiv_per_10k\"] = 10000 * ____ / ____ \n",
    "\n",
    "# Subset rows for indiv_per_10k greater than 20\n",
    "high_homelessness = ____\n",
    "\n",
    "# Sort high_homelessness by descending indiv_per_10k\n",
    "high_homelessness_srt = ____\n",
    "\n",
    "# From high_homelessness_srt, select the state and indiv_per_10k cols\n",
    "result = ____\n",
    "\n",
    "# See the result\n",
    "print(result)\n",
    "ChatGPT\n",
    "Here's how you can combine the different types of data manipulation to answer the question, \"Which state has the highest number of homeless individuals per 10,000 people in the state?\":\n",
    "\n",
    "Create a new column indiv_per_10k containing the number of homeless individuals per 10,000 people.\n",
    "Subset rows where indiv_per_10k is higher than 20.\n",
    "Sort the filtered DataFrame by indiv_per_10k in descending order.\n",
    "Select only the state and indiv_per_10k columns from the sorted DataFrame.\n",
    "Here is the code to achieve this:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Create indiv_per_10k column as homeless individuals per 10k state pop\n",
    "homelessness[\"indiv_per_10k\"] = 10000 * homelessness[\"individuals\"] / homelessness[\"state_pop\"]\n",
    "\n",
    "# Subset rows for indiv_per_10k greater than 20\n",
    "high_homelessness = homelessness[homelessness[\"indiv_per_10k\"] > 20]\n",
    "\n",
    "# Sort high_homelessness by descending indiv_per_10k\n",
    "high_homelessness_srt = high_homelessness.sort_values(\"indiv_per_10k\", ascending=False)\n",
    "\n",
    "# From high_homelessness_srt, select the state and indiv_per_10k columns\n",
    "result = high_homelessness_srt[[\"state\", \"indiv_per_10k\"]]\n",
    "\n",
    "# See the result\n",
    "print(result)\n",
    "This code:\n",
    "\n",
    "Adds a new column indiv_per_10k to the homelessness DataFrame, which calculates the number of homeless individuals per 10,000 people.\n",
    "Filters rows where indiv_per_10k is greater than 20.\n",
    "Sorts the filtered DataFrame by indiv_per_10k in descending order.\n",
    "Selects only the state and indiv_per_10k columns and prints the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Combo-attack!\n",
    "You've seen the four most common types of data manipulation: sorting rows, subsetting columns, subsetting rows, and adding new columns. In a real-life data analysis, you can mix and match these four manipulations to answer a multitude of questions.\n",
    "\n",
    "In this exercise, you'll answer the question, \"Which state has the highest number of homeless individuals per 10,000 people in the state?\" Combine your new pandas skills to find out.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Add a column to homelessness, indiv_per_10k, containing the number of homeless individuals per ten thousand people in each state, using state_pop for state population.\n",
    "Subset rows where indiv_per_10k is higher than 20, assigning to high_homelessness.\n",
    "Sort high_homelessness by descending indiv_per_10k, assigning to high_homelessness_srt.\n",
    "Select only the state and indiv_per_10k columns of high_homelessness_srt and save as result. Look at the result.'''\n",
    "\n",
    "\n",
    "# Create indiv_per_10k col as homeless individuals per 10k state pop\n",
    "homelessness[\"indiv_per_10k\"] = 10000 * ____ / ____ \n",
    "\n",
    "# Subset rows for indiv_per_10k greater than 20\n",
    "high_homelessness = ____\n",
    "\n",
    "# Sort high_homelessness by descending indiv_per_10k\n",
    "high_homelessness_srt = ____\n",
    "\n",
    "# From high_homelessness_srt, select the state and indiv_per_10k cols\n",
    "result = ____\n",
    "\n",
    "# See the result\n",
    "print(result)\n",
    "ChatGPT\n",
    "Here's how you can combine the different types of data manipulation to answer the question, \"Which state has the highest number of homeless individuals per 10,000 people in the state?\":\n",
    "\n",
    "Create a new column indiv_per_10k containing the number of homeless individuals per 10,000 people.\n",
    "Subset rows where indiv_per_10k is higher than 20.\n",
    "Sort the filtered DataFrame by indiv_per_10k in descending order.\n",
    "Select only the state and indiv_per_10k columns from the sorted DataFrame.\n",
    "Here is the code to achieve this:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the homelessness DataFrame is already defined\n",
    "# homelessness = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Create indiv_per_10k column as homeless individuals per 10k state pop\n",
    "homelessness[\"indiv_per_10k\"] = 10000 * homelessness[\"individuals\"] / homelessness[\"state_pop\"]\n",
    "\n",
    "# Subset rows for indiv_per_10k greater than 20\n",
    "high_homelessness = homelessness[homelessness[\"indiv_per_10k\"] > 20]\n",
    "\n",
    "# Sort high_homelessness by descending indiv_per_10k\n",
    "high_homelessness_srt = high_homelessness.sort_values(\"indiv_per_10k\", ascending=False)\n",
    "\n",
    "# From high_homelessness_srt, select the state and indiv_per_10k columns\n",
    "result = high_homelessness_srt[[\"state\", \"indiv_per_10k\"]]\n",
    "\n",
    "# See the result\n",
    "print(result)\n",
    "This code:\n",
    "\n",
    "Adds a new column indiv_per_10k to the homelessness DataFrame, which calculates the number of homeless individuals per 10,000 people.\n",
    "Filters rows where indiv_per_10k is greater than 20.\n",
    "Sorts the filtered DataFrame by indiv_per_10k in descending order.\n",
    "Selects only the state and indiv_per_10k columns and prints the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Aggregating DataFrames\n",
    "\n",
    "14%\n",
    "In this chapter, you’ll calculate summary statistics on DataFrame columns, and master grouped summary statistics and pivot tables.\n",
    "'''\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the sales DataFrame is already defined\n",
    "# sales = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Print the head of the sales DataFrame\n",
    "print(sales.head())\n",
    "\n",
    "# Print the info about the sales DataFrame\n",
    "print(sales.info())\n",
    "\n",
    "# Print the mean of weekly_sales\n",
    "print(sales[\"weekly_sales\"].mean())\n",
    "\n",
    "# Print the median of weekly_sales\n",
    "print(sales[\"weekly_sales\"].median())\n",
    "\n",
    "'''The mean weekly sales amount is almost double the median weekly sales amount! This can tell you that there are a few very high sales weeks that are making the mean so much higher than the median.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Mean and median\n",
    "Summary statistics are exactly what they sound like - they summarize many numbers in one statistic. For example, mean, median, minimum, maximum, and standard deviation are summary statistics. Calculating summary statistics allows you to get a better sense of your data, even if there's a lot of it.\n",
    "\n",
    "sales is available and pandas is loaded as pd.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Explore your new DataFrame first by printing the first few rows of the sales DataFrame.\n",
    "Print information about the columns in sales.\n",
    "Print the mean of the weekly_sales column.\n",
    "Print the median of the weekly_sales column.'''\n",
    "\n",
    "# Print the head of the sales DataFrame\n",
    "print(____)\n",
    "\n",
    "# Print the info about the sales DataFrame\n",
    "print(____)\n",
    "\n",
    "# Print the mean of weekly_sales\n",
    "print(____)\n",
    "\n",
    "# Print the median of weekly_sales\n",
    "print(____)\n",
    "ChatGPT\n",
    "To explore the sales DataFrame and calculate summary statistics for the weekly_sales column, you can follow these steps:\n",
    "\n",
    "Print the first few rows of the sales DataFrame to get an overview of the data.\n",
    "Print information about the columns in the sales DataFrame.\n",
    "Print the mean of the weekly_sales column.\n",
    "Print the median of the weekly_sales column.\n",
    "Here's the code to do that:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the sales DataFrame is already defined\n",
    "# sales = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Print the head of the sales DataFrame\n",
    "print(sales.head())\n",
    "\n",
    "# Print the info about the sales DataFrame\n",
    "print(sales.info())\n",
    "\n",
    "# Print the mean of weekly_sales\n",
    "print(sales[\"weekly_sales\"].mean())\n",
    "\n",
    "# Print the median of weekly_sales\n",
    "print(sales[\"weekly_sales\"].median())\n",
    "This code will:\n",
    "\n",
    "Display the first few rows of the sales DataFrame.\n",
    "Show information about the columns and data types in the sales DataFrame.\n",
    "Calculate and print the mean of the weekly_sales column.\n",
    "Calculate and print the median of the weekly_sales column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the sales DataFrame is already defined and the date column is named 'date'\n",
    "# sales = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Print the maximum of the date column\n",
    "print(sales[\"date\"].max())\n",
    "\n",
    "# Print the minimum of the date column\n",
    "print(sales[\"date\"].min())\n",
    "\n",
    "\n",
    "'''Super summarizing! Taking the minimum and maximum of a column of dates is handy for figuring out what time period your data covers. In this case, there are data from February of 2010 to October of 2012\n",
    "'''Summarizing dates\n",
    "Summary statistics can also be calculated on date columns that have values with the data type datetime64. Some summary statistics — like mean — don't make a ton of sense on dates, but others are super helpful, for example, minimum and maximum, which allow you to see what time range your data covers.\n",
    "\n",
    "sales is available and pandas is loaded as pd.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Print the maximum of the date column.\n",
    "Print the minimum of the date column.'''\n",
    "\n",
    "\n",
    "# Print the maximum of the date column\n",
    "____\n",
    "\n",
    "# Print the minimum of the date column\n",
    "____\n",
    "ChatGPT\n",
    "To summarize the dates in the sales DataFrame, you can use the max() and min() methods. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the sales DataFrame is already defined and the date column is named 'date'\n",
    "# sales = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Print the maximum of the date column\n",
    "print(sales[\"date\"].max())\n",
    "\n",
    "# Print the minimum of the date column\n",
    "print(sales[\"date\"].min())\n",
    "This code will display the most recent and the earliest dates in the date column of the sales DataFrame.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the sales DataFrame is already defined\n",
    "# sales = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Print IQR of the temperature_c column\n",
    "print(sales[\"temperature_c\"].agg(iqr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the sales DataFrame is already defined\n",
    "# sales = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg(iqr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr, np.median]))\n",
    "\n",
    "# Excellent efficiency! The .agg() method makes it easy to compute multiple statistics on multiple columns, all in just one line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "'''Efficient summaries\n",
    "While pandas and NumPy have tons of functions, sometimes, you may need a different function to summarize your data.\n",
    "\n",
    "The .agg() method allows you to apply your own custom functions to a DataFrame, as well as apply functions to more than one column of a DataFrame at once, making your aggregations super-efficient. For example,\n",
    "\n",
    "df['column'].agg(function)\n",
    "In the custom function for this exercise, \"IQR\" is short for inter-quartile range, which is the 75th percentile minus the 25th percentile. It's an alternative to standard deviation that is helpful if your data contains outliers.\n",
    "\n",
    "sales is available and pandas is loaded as pd.\n",
    "\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "1\n",
    "Use the custom iqr function defined for you along with .agg() to print the IQR of the temperature_c column of sales.'''\n",
    "\n",
    "\n",
    "# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "    \n",
    "# Print IQR of the temperature_c column\n",
    "print(____)\n",
    "ChatGPT\n",
    "To calculate the Interquartile Range (IQR) of the temperature_c column in the sales DataFrame using the custom iqr function and .agg(), you can follow these steps:\n",
    "\n",
    "Define the custom IQR function.\n",
    "Use the .agg() method with the custom function on the temperature_c column.\n",
    "Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the sales DataFrame is already defined\n",
    "# sales = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Print IQR of the temperature_c column\n",
    "print(sales[\"temperature_c\"].agg(iqr))\n",
    "This code will:\n",
    "\n",
    "Define the iqr function to compute the interquartile range.\n",
    "Apply this function to the temperature_c column using .agg() and print the result.\n",
    "'''Update the column selection to use the custom iqr function with .agg() to print the IQR of temperature_c, fuel_price_usd_per_l, and unemployment, in that order.'''\n",
    "\n",
    "# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales[[\"temperature_c\", ____, ____]].agg(iqr))\n",
    "ChatGPT\n",
    "To calculate the IQR for multiple columns (temperature_c, fuel_price_usd_per_l, and unemployment) using the custom iqr function and the .agg() method, you need to select the columns in the DataFrame and apply the function. Here’s how to update the code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the sales DataFrame is already defined\n",
    "# sales = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg(iqr))\n",
    "This code will:\n",
    "\n",
    "Define the iqr function.\n",
    "Select the columns temperature_c, fuel_price_usd_per_l, and unemployment from the sales DataFrame.\n",
    "Apply the iqr function to each of these columns using .agg() and print the results.\n",
    "'''Update the aggregation functions called by .agg(): include iqr and np.median in that order.'''\n",
    "\n",
    "# Import NumPy and create custom IQR function\n",
    "import numpy as np\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg(iqr))\n",
    "ChatGPT\n",
    "To apply both the custom iqr function and np.median to the columns using .agg(), you need to pass a dictionary to .agg(), where the keys are the column names and the values are lists of functions to apply.\n",
    "\n",
    "Here’s how you can update your code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr, np.median]))\n",
    "In this code:\n",
    "\n",
    "The .agg() method takes a list of functions [iqr, np.median], which are applied in that order to each column.\n",
    "iqr and np.median are computed for temperature_c, fuel_price_usd_per_l, and unemployment columns, and the results are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming sales_1_1 DataFrame is already defined\n",
    "# sales_1_1 = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Sort sales_1_1 by date\n",
    "sales_1_1 = sales_1_1.sort_values(by=\"date\")\n",
    "\n",
    "# Get the cumulative sum of weekly_sales, add as cum_weekly_sales column\n",
    "sales_1_1[\"cum_weekly_sales\"] = sales_1_1[\"weekly_sales\"].cumsum()\n",
    "\n",
    "# Get the cumulative max of weekly_sales, add as cum_max_sales column\n",
    "sales_1_1[\"cum_max_sales\"] = sales_1_1[\"weekly_sales\"].cummax()\n",
    "\n",
    "# See the columns you calculated\n",
    "print(sales_1_1[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_max_sales\"]])\n",
    "\n",
    "\n",
    "'''You've accumulated success! Not all functions that calculate on columns return a single number. Some, like the cumulative statistic functions, return a whole column.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Cumulative statistics\n",
    "Cumulative statistics can also be helpful in tracking summary statistics over time. In this exercise, you'll calculate the cumulative sum and cumulative max of a department's weekly sales, which will allow you to identify what the total sales were so far as well as what the highest weekly sales were so far.\n",
    "\n",
    "A DataFrame called sales_1_1 has been created for you, which contains the sales data for department 1 of store 1. pandas is loaded as pd.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Sort the rows of sales_1_1 by the date column in ascending order.\n",
    "Get the cumulative sum of weekly_sales and add it as a new column of sales_1_1 called cum_weekly_sales.\n",
    "Get the cumulative maximum of weekly_sales, and add it as a column called cum_max_sales.\n",
    "Print the date, weekly_sales, cum_weekly_sales, and cum_max_sales columns'''\n",
    "\n",
    "\n",
    "# Sort sales_1_1 by date\n",
    "sales_1_1 = ____\n",
    "\n",
    "# Get the cumulative sum of weekly_sales, add as cum_weekly_sales col\n",
    "sales_1_1[____] = ____\n",
    "\n",
    "# Get the cumulative max of weekly_sales, add as cum_max_sales col\n",
    "____\n",
    "\n",
    "# See the columns you calculated\n",
    "print(sales_1_1[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_max_sales\"]])\n",
    "ChatGPT\n",
    "To calculate cumulative statistics such as cumulative sum and cumulative maximum of weekly sales and add these as new columns in sales_1_1, you can follow these steps:\n",
    "\n",
    "Sort the DataFrame by the date column in ascending order.\n",
    "Calculate the cumulative sum of weekly_sales and add it as a new column cum_weekly_sales.\n",
    "Calculate the cumulative maximum of weekly_sales and add it as a new column cum_max_sales.\n",
    "Print the relevant columns to verify the results.\n",
    "Here’s the code to achieve this:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming sales_1_1 DataFrame is already defined\n",
    "# sales_1_1 = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Sort sales_1_1 by date\n",
    "sales_1_1 = sales_1_1.sort_values(by=\"date\")\n",
    "\n",
    "# Get the cumulative sum of weekly_sales, add as cum_weekly_sales column\n",
    "sales_1_1[\"cum_weekly_sales\"] = sales_1_1[\"weekly_sales\"].cumsum()\n",
    "\n",
    "# Get the cumulative max of weekly_sales, add as cum_max_sales column\n",
    "sales_1_1[\"cum_max_sales\"] = sales_1_1[\"weekly_sales\"].cummax()\n",
    "\n",
    "# See the columns you calculated\n",
    "print(sales_1_1[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_max_sales\"]])\n",
    "This code:\n",
    "\n",
    "Sorts sales_1_1 by the date column to ensure that the cumulative calculations are based on the correct temporal order.\n",
    "Computes the cumulative sum and cumulative maximum of weekly_sales and adds these as new columns.\n",
    "Prints the relevant columns to display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the sales DataFrame is already defined\n",
    "# sales = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Drop duplicate store/type combinations\n",
    "store_types = sales.drop_duplicates(subset=[\"store\", \"type\"])\n",
    "print(store_types.head())\n",
    "\n",
    "# Drop duplicate store/department combinations\n",
    "store_depts = sales.drop_duplicates(subset=[\"store\", \"department\"])\n",
    "print(store_depts.head())\n",
    "\n",
    "# Subset the rows where is_holiday is True and drop duplicate dates\n",
    "holiday_dates = sales[sales[\"is_holiday\"]].drop_duplicates(subset=[\"date\"])\n",
    "\n",
    "# Print date column of holiday_dates\n",
    "print(holiday_dates[\"date\"])\n",
    "\n",
    "\n",
    "'''Dazzling duplicate dropping! The holiday weeks correspond to the Superbowl in February, Labor Day in September, Thanksgiving in November, and Christmas in December. Now that the duplicates are removed, it's time to do some counting.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Dropping duplicates\n",
    "Removing duplicates is an essential skill to get accurate counts because often, you don't want to count the same thing multiple times. In this exercise, you'll create some new DataFrames using unique values from sales.\n",
    "\n",
    "sales is available and pandas is imported as pd.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Remove rows of sales with duplicate pairs of store and type and save as store_types and print the head.\n",
    "Remove rows of sales with duplicate pairs of store and department and save as store_depts and print the head.\n",
    "Subset the rows that are holiday weeks using the is_holiday column, and drop the duplicate dates, saving as holiday_dates.\n",
    "Select the date column of holiday_dates, and print.'''\n",
    "\n",
    "\n",
    "# Drop duplicate store/type combinations\n",
    "store_types = ____\n",
    "print(store_types.head())\n",
    "\n",
    "# Drop duplicate store/department combinations\n",
    "store_depts = ____\n",
    "print(store_depts.head())\n",
    "\n",
    "# Subset the rows where is_holiday is True and drop duplicate dates\n",
    "holiday_dates = sales[sales[____]].drop_duplicates(____)\n",
    "\n",
    "# Print date col of holiday_dates\n",
    "print(____)\n",
    "ChatGPT\n",
    "Here’s how you can handle the removal of duplicates from the sales DataFrame and extract unique values:\n",
    "\n",
    "Remove duplicate rows based on combinations of columns (store and type, store and department).\n",
    "Subset the rows where is_holiday is True and drop duplicate dates.\n",
    "Print the relevant columns to verify the results.\n",
    "Here’s the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the sales DataFrame is already defined\n",
    "# sales = pd.read_csv('path_to_file.csv')  # Example of how you might load the DataFrame\n",
    "\n",
    "# Drop duplicate store/type combinations\n",
    "store_types = sales.drop_duplicates(subset=[\"store\", \"type\"])\n",
    "print(store_types.head())\n",
    "\n",
    "# Drop duplicate store/department combinations\n",
    "store_depts = sales.drop_duplicates(subset=[\"store\", \"department\"])\n",
    "print(store_depts.head())\n",
    "\n",
    "# Subset the rows where is_holiday is True and drop duplicate dates\n",
    "holiday_dates = sales[sales[\"is_holiday\"]].drop_duplicates(subset=[\"date\"])\n",
    "\n",
    "# Print date column of holiday_dates\n",
    "print(holiday_dates[\"date\"])\n",
    "In this code:\n",
    "\n",
    "drop_duplicates(subset=[\"store\", \"type\"]) removes rows with duplicate pairs of store and type.\n",
    "drop_duplicates(subset=[\"store\", \"department\"]) removes rows with duplicate pairs of store and department.\n",
    "sales[sales[\"is_holiday\"]] subsets rows where is_holiday is True, and drop_duplicates(subset=[\"date\"]) removes duplicate dates in the subset.\n",
    "Finally, print(holiday_dates[\"date\"]) prints the date column from the holiday_dates DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the store_types and store_depts DataFrames are already defined\n",
    "\n",
    "# Count the number of stores of each type\n",
    "store_counts = store_types[\"type\"].value_counts()\n",
    "print(store_counts)\n",
    "\n",
    "# Get the proportion of stores of each type\n",
    "store_props = store_types[\"type\"].value_counts(normalize=True)\n",
    "print(store_props)\n",
    "\n",
    "# Count the number of stores for each department and sort\n",
    "dept_counts_sorted = store_depts[\"department\"].value_counts().sort_values(ascending=False)\n",
    "print(dept_counts_sorted)\n",
    "\n",
    "# Get the proportion of stores in each department and sort\n",
    "dept_props_sorted = store_depts[\"department\"].value_counts(normalize=True).sort_values(ascending=False)\n",
    "print(dept_props_sorted)\n",
    "\n",
    "\n",
    "# Great counting! It looks like department 43 only exists in two stores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Counting categorical variables\n",
    "Counting is a great way to get an overview of your data and to spot curiosities that you might not notice otherwise. In this exercise, you'll count the number of each type of store and the number of each department number using the DataFrames you created in the previous exercise:\n",
    "\n",
    "# Drop duplicate store/type combinations\n",
    "store_types = sales.drop_duplicates(subset=[\"store\", \"type\"])\n",
    "\n",
    "# Drop duplicate store/department combinations\n",
    "store_depts = sales.drop_duplicates(subset=[\"store\", \"department\"])\n",
    "The store_types and store_depts DataFrames you created in the last exercise are available, and pandas is imported as pd.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Count the number of stores of each store type in store_types.\n",
    "Count the proportion of stores of each store type in store_types.\n",
    "Count the number of stores of each department in store_depts, sorting the counts in descending order.\n",
    "Count the proportion of stores of each department in store_depts, sorting the proportions in descending order.'''\n",
    "\n",
    "\n",
    "# Count the number of stores of each type\n",
    "store_counts = ____\n",
    "print(store_counts)\n",
    "\n",
    "# Get the proportion of stores of each type\n",
    "store_props = ____\n",
    "print(store_props)\n",
    "\n",
    "# Count the number of stores for each department and sort\n",
    "dept_counts_sorted = ____\n",
    "print(dept_counts_sorted)\n",
    "\n",
    "# Get the proportion of stores in each department and sort\n",
    "dept_props_sorted = ____.____(sort=____, normalize=____)\n",
    "print(dept_props_sorted)\n",
    "ChatGPT\n",
    "Here’s how you can count and calculate proportions for categorical variables in the store_types and store_depts DataFrames:\n",
    "\n",
    "Count the number of stores of each type.\n",
    "Calculate the proportion of stores of each type.\n",
    "Count the number of stores of each department, sorted in descending order.\n",
    "Calculate the proportion of stores of each department, sorted in descending order.\n",
    "Here’s the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Assuming the store_types and store_depts DataFrames are already defined\n",
    "\n",
    "# Count the number of stores of each type\n",
    "store_counts = store_types[\"type\"].value_counts()\n",
    "print(store_counts)\n",
    "\n",
    "# Get the proportion of stores of each type\n",
    "store_props = store_types[\"type\"].value_counts(normalize=True)\n",
    "print(store_props)\n",
    "\n",
    "# Count the number of stores for each department and sort\n",
    "dept_counts_sorted = store_depts[\"department\"].value_counts().sort_values(ascending=False)\n",
    "print(dept_counts_sorted)\n",
    "\n",
    "# Get the proportion of stores in each department and sort\n",
    "dept_props_sorted = store_depts[\"department\"].value_counts(normalize=True).sort_values(ascending=False)\n",
    "print(dept_props_sorted)\n",
    "In this code:\n",
    "\n",
    "value_counts() is used to count the number of occurrences for each category.\n",
    "value_counts(normalize=True) calculates the proportion of each category.\n",
    "sort_values(ascending=False) sorts the counts or proportions in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total weekly sales over the whole dataset\n",
    "sales_all = sales[\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type A stores and calculate total weekly sales\n",
    "sales_A = sales[sales[\"type\"] == \"A\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type B stores and calculate total weekly sales\n",
    "sales_B = sales[sales[\"type\"] == \"B\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type C stores and calculate total weekly sales\n",
    "sales_C = sales[sales[\"type\"] == \"C\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Get proportion for each type\n",
    "sales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all\n",
    "print(sales_propn_by_type)\n",
    "\n",
    "\n",
    "'''Marvelous mathematics! About 91% of sales occurred in stores of type A', 9% in stores of type B, and there are no sales records for stores of type C. Now see if you can do this calculation using .groupby().'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''What percent of sales occurred at each store type?\n",
    "While .groupby() is useful, you can calculate grouped summary statistics without it.\n",
    "\n",
    "Walmart distinguishes three types of stores: \"supercenters,\" \"discount stores,\" and \"neighborhood markets,\" encoded in this dataset as type \"A,\" \"B,\" and \"C.\" In this exercise, you'll calculate the total sales made at each store type, without using .groupby(). You can then use these numbers to see what proportion of Walmart's total sales were made at each type.\n",
    "\n",
    "sales is available and pandas is imported as pd.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Calculate the total weekly_sales over the whole dataset.\n",
    "Subset for type \"A\" stores, and calculate their total weekly sales.\n",
    "Do the same for type \"B\" and type \"C\" stores.\n",
    "Combine the A/B/C results into a list, and divide by sales_all to get the proportion of sales by type.'''\n",
    "\n",
    "\n",
    "# Calc total weekly sales\n",
    "sales_all = ____[\"____\"].____()\n",
    "\n",
    "# Subset for type A stores, calc total weekly sales\n",
    "sales_A = ____[____[\"____\"] == \"____\"][\"____\"].____()\n",
    "\n",
    "# Subset for type B stores, calc total weekly sales\n",
    "sales_B = ____\n",
    "\n",
    "# Subset for type C stores, calc total weekly sales\n",
    "sales_C = ____\n",
    "\n",
    "# Get proportion for each type\n",
    "sales_propn_by_type = [sales_A, ____, ____] / ____\n",
    "print(sales_propn_by_type)\n",
    "ChatGPT\n",
    "To calculate the proportion of sales for each store type without using .groupby(), follow these steps:\n",
    "\n",
    "Calculate the total weekly sales over the entire dataset.\n",
    "Subset the data for each store type and calculate the total weekly sales for each type.\n",
    "Combine these results into a list and divide by the total sales to get the proportion of sales for each type.\n",
    "Here’s how you can do this:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Calculate total weekly sales over the whole dataset\n",
    "sales_all = sales[\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type A stores and calculate total weekly sales\n",
    "sales_A = sales[sales[\"type\"] == \"A\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type B stores and calculate total weekly sales\n",
    "sales_B = sales[sales[\"type\"] == \"B\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type C stores and calculate total weekly sales\n",
    "sales_C = sales[sales[\"type\"] == \"C\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Get proportion for each type\n",
    "sales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all\n",
    "print(sales_propn_by_type)\n",
    "Explanation:\n",
    "sales[\"weekly_sales\"].sum() calculates the total sales across all store types.\n",
    "sales[sales[\"type\"] == \"A\"][\"weekly_sales\"].sum() calculates the total sales for type \"A\" stores.\n",
    "Similar calculations are performed for types \"B\" and \"C\".\n",
    "[sales_A, sales_B, sales_C] / sales_all computes the proportion of sales for each store type by dividing the sales for each type by the total sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by type and calculate total weekly sales\n",
    "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
    "\n",
    "# Calculate the proportion of sales for each type\n",
    "sales_propn_by_type = sales_by_type / sales_by_type.sum()\n",
    "print(sales_propn_by_type)\n",
    "\n",
    "# Group by type and is_holiday, then calculate the total weekly sales\n",
    "sales_by_type_is_holiday = sales.groupby([\"type\", \"is_holiday\"])[\"weekly_sales\"].sum()\n",
    "print(sales_by_type_is_holiday)\n",
    "\n",
    "\n",
    "# Great grouping! You were able to do the same calculation as in the previous exercise while writing much less code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Calculations with .groupby()\n",
    "The .groupby() method makes life much easier. In this exercise, you'll perform the same calculations as last time, except you'll use the .groupby() method. You'll also perform calculations on data grouped by two variables to see if sales differ by store type depending on if it's a holiday week or not.\n",
    "\n",
    "sales is available and pandas is loaded as pd.\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "1\n",
    "2\n",
    "Group sales by \"type\", take the sum of \"weekly_sales\", and store as sales_by_type.\n",
    "Calculate the proportion of sales at each store type by dividing by the sum of sales_by_type. Assign to sales_propn_by_type.'''\n",
    "\n",
    "# Group by type; calc total weekly sales\n",
    "sales_by_type = ____.____(\"____\")[\"____\"].____()\n",
    "\n",
    "# Get proportion for each type\n",
    "sales_propn_by_type = ____ / sum(____)\n",
    "print(sales_propn_by_type)\n",
    "ChatGPT\n",
    "To perform calculations using .groupby() and get the proportion of sales for each store type, follow these steps:\n",
    "\n",
    "Group the data by \"type\" and calculate the sum of \"weekly_sales\".\n",
    "Calculate the proportion of sales for each store type by dividing each group's sales by the total sales.\n",
    "Here’s how you can do this:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Group by type and calculate total weekly sales\n",
    "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
    "\n",
    "# Calculate the proportion of sales for each type\n",
    "sales_propn_by_type = sales_by_type / sales_by_type.sum()\n",
    "print(sales_propn_by_type)\n",
    "Explanation:\n",
    "sales.groupby(\"type\")[\"weekly_sales\"].sum() groups the data by \"type\" and calculates the total weekly sales for each store type.\n",
    "sales_by_type.sum() calculates the total sales across all store types.\n",
    "sales_by_type / sales_by_type.sum() computes the proportion of sales for each store type.\n",
    "# Group sales by \"type\" and \"is_holiday\", take the sum of weekly_sales, and store as sales_by_type_is_holiday.\n",
    "\n",
    "# From previous step\n",
    "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
    "\n",
    "# Group by type and is_holiday; calc total weekly sales\n",
    "sales_by_type_is_holiday = ____\n",
    "print(sales_by_type_is_holiday)\n",
    "ChatGPT\n",
    "To group the data by both \"type\" and \"is_holiday\", and calculate the total weekly sales for each combination, you can use .groupby() with multiple columns. Here’s how to do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Group by type and is_holiday, then calculate the total weekly sales\n",
    "sales_by_type_is_holiday = sales.groupby([\"type\", \"is_holiday\"])[\"weekly_sales\"].sum()\n",
    "print(sales_by_type_is_holiday)\n",
    "Explanation:\n",
    "sales.groupby([\"type\", \"is_holiday\"]) groups the data by both \"type\" and \"is_holiday\".\n",
    "[\"weekly_sales\"].sum() calculates the total weekly sales for each combination of \"type\" and \"is_holiday\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy with the alias np\n",
    "import numpy as np\n",
    "\n",
    "# For each store type, aggregate weekly_sales: get min, max, mean, and median\n",
    "sales_stats = sales.groupby(\"type\")[\"weekly_sales\"].agg([np.min, np.max, np.mean, np.median])\n",
    "\n",
    "# Print sales_stats\n",
    "print(sales_stats)\n",
    "\n",
    "# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\n",
    "unemp_fuel_stats = sales.groupby(\"type\")[[\"unemployment\", \"fuel_price_usd_per_l\"]].agg([np.min, np.max, np.mean, np.median])\n",
    "\n",
    "# Print unemp_fuel_stats\n",
    "print(unemp_fuel_stats)\n",
    "\n",
    "# Awesome aggregating! Notice that the minimum weekly_sales is negative because some stores had more returns than sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Multiple grouped summaries\n",
    "Earlier in this chapter, you saw that the .agg() method is useful to compute multiple statistics on multiple variables. It also works with grouped data. NumPy, which is imported as np, has many different summary statistics functions, including: np.min, np.max, np.mean, and np.median.\n",
    "\n",
    "sales is available and pandas is imported as pd.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Import numpy with the alias np.\n",
    "Get the min, max, mean, and median of weekly_sales for each store type using .groupby() and .agg(). Store this as sales_stats. Make sure to use numpy functions!\n",
    "Get the min, max, mean, and median of unemployment and fuel_price_usd_per_l for each store type. Store this as unemp_fuel_stats.'''\n",
    "\n",
    "\n",
    "\n",
    "# Import numpy with the alias np\n",
    "____\n",
    "\n",
    "# For each store type, aggregate weekly_sales: get min, max, mean, and median\n",
    "sales_stats = ____\n",
    "\n",
    "# Print sales_stats\n",
    "print(sales_stats)\n",
    "\n",
    "# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\n",
    "unemp_fuel_stats = ____\n",
    "\n",
    "# Print unemp_fuel_stats\n",
    "print(unemp_fuel_stats)\n",
    "ChatGPT\n",
    "Here’s how you can perform the required operations using .groupby() and .agg() with NumPy functions:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import numpy with the alias np\n",
    "import numpy as np\n",
    "\n",
    "# For each store type, aggregate weekly_sales: get min, max, mean, and median\n",
    "sales_stats = sales.groupby(\"type\")[\"weekly_sales\"].agg([np.min, np.max, np.mean, np.median])\n",
    "\n",
    "# Print sales_stats\n",
    "print(sales_stats)\n",
    "\n",
    "# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\n",
    "unemp_fuel_stats = sales.groupby(\"type\")[[\"unemployment\", \"fuel_price_usd_per_l\"]].agg([np.min, np.max, np.mean, np.median])\n",
    "\n",
    "# Print unemp_fuel_stats\n",
    "print(unemp_fuel_stats)\n",
    "Explanation:\n",
    "import numpy as np imports NumPy with the alias np.\n",
    "sales.groupby(\"type\")[\"weekly_sales\"].agg([np.min, np.max, np.mean, np.median]) groups the data by \"type\" and computes the minimum, maximum, mean, and median of weekly_sales.\n",
    "sales.groupby(\"type\")[[\"unemployment\", \"fuel_price_usd_per_l\"]].agg([np.min, np.max, np.mean, np.median]) does the same for unemployment and fuel_price_usd_per_l."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot for mean weekly_sales for each store type\n",
    "mean_sales_by_type = sales.pivot_table(values='weekly_sales', index='type')\n",
    "\n",
    "# Print mean_sales_by_type\n",
    "print(mean_sales_by_type)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NumPy as np\n",
    "import numpy as np\n",
    "\n",
    "# Pivot for mean and median weekly_sales for each store type\n",
    "mean_med_sales_by_type = sales.pivot_table(\n",
    "    values='weekly_sales', \n",
    "    index='type', \n",
    "    aggfunc=[np.mean, np.median]\n",
    ")\n",
    "\n",
    "# Print mean_med_sales_by_type\n",
    "print(mean_med_sales_by_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot for mean weekly_sales by store type and holiday\n",
    "mean_sales_by_type_holiday = sales.pivot_table(\n",
    "    values='weekly_sales',\n",
    "    index='type',\n",
    "    columns='is_holiday'\n",
    ")\n",
    "\n",
    "# Print mean_sales_by_type_holiday\n",
    "print(mean_sales_by_type_holiday)\n",
    "\n",
    "# Perfect pivoting! Pivot tables are another way to do the same thing as a group-by-then-summarize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Pivoting on one variable\n",
    "Pivot tables are the standard way of aggregating data in spreadsheets.\n",
    "\n",
    "In pandas, pivot tables are essentially another way of performing grouped calculations. That is, the .pivot_table() method is an alternative to .groupby().\n",
    "\n",
    "In this exercise, you'll perform calculations using .pivot_table() to replicate the calculations you performed in the last lesson using .groupby().\n",
    "\n",
    "sales is available and pandas is imported as pd.\n",
    "\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "1\n",
    "Get the mean weekly_sales by type using .pivot_table() and store as mean_sales_by_type.'''\n",
    "\n",
    "\n",
    "# Pivot for mean weekly_sales for each store type\n",
    "mean_sales_by_type = sales.____\n",
    "\n",
    "# Print mean_sales_by_type\n",
    "print(mean_sales_by_type)\n",
    "ChatGPT\n",
    "To calculate the mean weekly sales by store type using .pivot_table(), you can use the following code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Pivot for mean weekly_sales for each store type\n",
    "mean_sales_by_type = sales.pivot_table(values='weekly_sales', index='type', aggfunc='mean')\n",
    "\n",
    "# Print mean_sales_by_type\n",
    "print(mean_sales_by_type)\n",
    "Explanation:\n",
    "sales.pivot_table(values='weekly_sales', index='type', aggfunc='mean') creates a pivot table that calculates the mean of weekly_sales for each type.\n",
    "values='weekly_sales' specifies the column to aggregate.\n",
    "index='type' specifies the column to group by.\n",
    "aggfunc='mean' specifies the aggregation function (mean) to use.\n",
    "Incorrect submission\n",
    "Remember that .pivot_table() takes the mean of each group by default, so you don't need the aggfunc argument. But if you choose to inlude it, make sure that you import the package containing the aggregation function!\n",
    "ChatGPT\n",
    "You're right. By default, .pivot_table() takes the mean of each group, so specifying aggfunc='mean' is optional. Here’s the corrected code without specifying aggfunc:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Pivot for mean weekly_sales for each store type\n",
    "mean_sales_by_type = sales.pivot_table(values='weekly_sales', index='type')\n",
    "\n",
    "# Print mean_sales_by_type\n",
    "print(mean_sales_by_type)\n",
    "This code creates a pivot table that shows the mean weekly sales for each store type.\n",
    "\n",
    "# Get the mean and median (using NumPy functions) of weekly_sales by type using .pivot_table() and store as mean_med_sales_by_type.\n",
    "\n",
    "# Import NumPy as np\n",
    "import numpy as np\n",
    "\n",
    "# Pivot for mean and median weekly_sales for each store type\n",
    "mean_med_sales_by_type = sales.pivot_table(____)\n",
    "\n",
    "# Print mean_med_sales_by_type\n",
    "print(mean_med_sales_by_type)\n",
    "ChatGPT\n",
    "To calculate both the mean and median using .pivot_table(), you'll need to specify aggfunc as a list of functions. Here's how to do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import NumPy as np\n",
    "import numpy as np\n",
    "\n",
    "# Pivot for mean and median weekly_sales for each store type\n",
    "mean_med_sales_by_type = sales.pivot_table(\n",
    "    values='weekly_sales', \n",
    "    index='type', \n",
    "    aggfunc=[np.mean, np.median]\n",
    ")\n",
    "\n",
    "# Print mean_med_sales_by_type\n",
    "print(mean_med_sales_by_type)\n",
    "This code creates a pivot table that shows both the mean and median weekly sales for each store type.\n",
    "\n",
    "'''Get the mean of weekly_sales by type and is_holiday using .pivot_table() and store as mean_sales_by_type_holiday.'''\n",
    "\n",
    "# Pivot for mean weekly_sales by store type and holiday \n",
    "mean_sales_by_type_holiday = sales.pivot_table(____)\n",
    "\n",
    "# Print mean_sales_by_type_holiday\n",
    "print(mean_sales_by_type_holiday)\n",
    "ChatGPT\n",
    "To calculate the mean of weekly_sales by both type and is_holiday, you can use .pivot_table() with index set to both variables. Here's how to do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Pivot for mean weekly_sales by store type and holiday\n",
    "mean_sales_by_type_holiday = sales.pivot_table(\n",
    "    values='weekly_sales',\n",
    "    index=['type', 'is_holiday']\n",
    ")\n",
    "\n",
    "# Print mean_sales_by_type_holiday\n",
    "print(mean_sales_by_type_holiday)\n",
    "This code will give you a DataFrame where the mean weekly sales are calculated for each combination of store type and whether it is a holiday.\n",
    "\n",
    "Incorrect submission\n",
    "Check your call of sales.pivot_table(). Did you specify the argument columns?\n",
    "ChatGPT\n",
    "You're right; if you want to include both type and is_holiday in the result, you should set columns to is_holiday. Here’s how you should update the .pivot_table() call:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Pivot for mean weekly_sales by store type and holiday\n",
    "mean_sales_by_type_holiday = sales.pivot_table(\n",
    "    values='weekly_sales',\n",
    "    index='type',\n",
    "    columns='is_holiday'\n",
    ")\n",
    "\n",
    "# Print mean_sales_by_type_holiday\n",
    "print(mean_sales_by_type_holiday)\n",
    "This configuration will create a pivot table with type as the rows and is_holiday as the columns, showing the mean weekly_sales for each combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print mean weekly_sales by department and type; fill missing values with 0\n",
    "print(sales.pivot_table(\n",
    "    values='weekly_sales',\n",
    "    index='department',\n",
    "    columns='type',\n",
    "    fill_value=0\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\n",
    "print(sales.pivot_table(\n",
    "    values=\"weekly_sales\",\n",
    "    index=\"department\",\n",
    "    columns=\"type\",\n",
    "    fill_value=0,\n",
    "    margins=True\n",
    "))\n",
    "\n",
    "'''Magnificent margin making! You are now armed with pivot table skills that can help you compute summaries at multiple grouped levels in one line of code. Note the subtlety in the value of margins here. The column 'All' returns an overall mean for each department, not (A+B)/2. (A+B)/2 would be a mean of means, rather than an overall mean per department!'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Fill in missing values and sum values with pivot tables\n",
    "The .pivot_table() method has several useful arguments, including fill_value and margins.\n",
    "\n",
    "fill_value replaces missing values with a real value (known as imputation). What to replace missing values with is a topic big enough to have its own course (Dealing with Missing Data in Python), but the simplest thing to do is to substitute a dummy value.\n",
    "margins is a shortcut for when you pivoted by two variables, but also wanted to pivot by each of those variables separately: it gives the row and column totals of the pivot table contents.\n",
    "In this exercise, you'll practice using these arguments to up your pivot table skills, which will help you crunch numbers more efficiently!\n",
    "\n",
    "sales is available and pandas is imported as pd.\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "1\n",
    "Print the mean weekly_sales by department and type, filling in any missing values with 0.'''\n",
    "\n",
    "# Print mean weekly_sales by department and type; fill missing values with 0\n",
    "print(sales.pivot_table(____))\n",
    "ChatGPT\n",
    "To print the mean weekly sales by department and type, filling in any missing values with 0, you should use the fill_value argument in pivot_table(). Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Print mean weekly_sales by department and type; fill missing values with 0\n",
    "print(sales.pivot_table(\n",
    "    values='weekly_sales',\n",
    "    index='department',\n",
    "    columns='type',\n",
    "    fill_value=0\n",
    "))\n",
    "In this setup:\n",
    "\n",
    "values='weekly_sales' specifies the column to aggregate.\n",
    "index='department' and columns='type' define the rows and columns of the pivot table.\n",
    "fill_value=0 ensures that any missing values in the pivot table are replaced with 0.\n",
    "# Print the mean weekly_sales by department and type, filling in any missing values with 0 and summing all rows and columns.\n",
    "\n",
    "\n",
    "# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\n",
    "print(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", ____))\n",
    "ChatGPT\n",
    "To print the mean weekly sales by department and type, filling in any missing values with 0 and summing all rows and columns, you should use the margins argument in pivot_table() to include totals. Here’s how to do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\n",
    "print(sales.pivot_table(\n",
    "    values=\"weekly_sales\",\n",
    "    index=\"department\",\n",
    "    columns=\"type\",\n",
    "    fill_value=0,\n",
    "    margins=True\n",
    "))\n",
    "In this setup:\n",
    "\n",
    "values=\"weekly_sales\" specifies the column to aggregate.\n",
    "index=\"department\" and columns=\"type\" define the rows and columns of the pivot table.\n",
    "fill_value=0 ensures missing values are replaced with 0.\n",
    "margins=True adds a row and column to show the totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing and Indexing DataFrames\n",
    "# Indexes are supercharged row and column names. Learn how they can be combined with slicing for powerful DataFrame subsetting.\n",
    "\n",
    "# Look at temperatures\n",
    "print(temperatures)\n",
    "\n",
    "# Set the index of temperatures to city\n",
    "temperatures_ind = temperatures.set_index(\"city\")\n",
    "\n",
    "# Look at temperatures_ind\n",
    "print(temperatures_ind)\n",
    "\n",
    "# Reset the temperatures_ind index, keeping its contents\n",
    "print(temperatures_ind.reset_index())\n",
    "\n",
    "# Reset the temperatures_ind index, dropping its contents\n",
    "print(temperatures_ind.reset_index(drop=True))\n",
    "\n",
    "\n",
    "# Incredible indexing! Setting an index allows more concise code for subsetting for rows of a categorical variable via .loc[].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting and removing indexes\n",
    "pandas allows you to designate columns as an index. This enables cleaner code when taking subsets (as well as providing more efficient lookup under some circumstances).\n",
    "\n",
    "In this chapter, you'll be exploring temperatures, a DataFrame of average temperatures in cities around the world. pandas is loaded as pd.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Look at temperatures.\n",
    "Set the index of temperatures to \"city\", assigning to temperatures_ind.\n",
    "Look at temperatures_ind. How is it different from temperatures?\n",
    "Reset the index of temperatures_ind, keeping its contents.\n",
    "Reset the index of temperatures_ind, dropping its contents.\n",
    "\n",
    "ChatGPT\n",
    "Here’s how you can set, reset, and remove indexes in pandas:\n",
    "\n",
    "Set the index of temperatures to \"city\" and assign it to temperatures_ind:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "temperatures_ind = temperatures.set_index(\"city\")\n",
    "Look at temperatures_ind to see how it's different from temperatures:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_ind)\n",
    "This will show that temperatures_ind now uses the \"city\" column as its index, while temperatures still has the default integer index.\n",
    "\n",
    "Reset the index of temperatures_ind, keeping its contents:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "temperatures_reset_keep = temperatures_ind.reset_index()\n",
    "This will move the index back into a column and restore the default integer index.\n",
    "\n",
    "Reset the index of temperatures_ind, dropping its contents:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "temperatures_reset_drop = temperatures_ind.reset_index(drop=True)\n",
    "This will remove the current index completely, dropping it from the DataFrame and restoring the default integer index.\n",
    "\n",
    "Here's how it all looks together:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Set the index to \"city\"\n",
    "temperatures_ind = temperatures.set_index(\"city\")\n",
    "\n",
    "# Display the DataFrame with the new index\n",
    "print(temperatures_ind)\n",
    "\n",
    "# Reset the index, keeping the old index as a column\n",
    "temperatures_reset_keep = temperatures_ind.reset_index()\n",
    "print(temperatures_reset_keep)\n",
    "\n",
    "# Reset the index, dropping the old index\n",
    "temperatures_reset_drop = temperatures_ind.reset_index(drop=True)\n",
    "print(temperatures_reset_drop)\n",
    "This code will show you the effects of setting and resetting the index, as well as the difference between keeping and dropping the old index.\n",
    "\n",
    "'''Setting and removing indexes\n",
    "pandas allows you to designate columns as an index. This enables cleaner code when taking subsets (as well as providing more efficient lookup under some circumstances).\n",
    "\n",
    "In this chapter, you'll be exploring temperatures, a DataFrame of average temperatures in cities around the world. pandas is loaded as pd.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Look at temperatures.\n",
    "Set the index of temperatures to \"city\", assigning to temperatures_ind.\n",
    "Look at temperatures_ind. How is it different from temperatures?\n",
    "Reset the index of temperatures_ind, keeping its contents.\n",
    "Reset the index of temperatures_ind, dropping its contents.''''\n",
    "\n",
    "# Look at temperatures\n",
    "print(____)\n",
    "\n",
    "# Set the index of temperatures to city\n",
    "temperatures_ind = ____\n",
    "\n",
    "# Look at temperatures_ind\n",
    "print(____)\n",
    "\n",
    "# Reset the temperatures_ind index, keeping its contents\n",
    "print(____)\n",
    "\n",
    "# Reset the temperatures_ind index, dropping its contents\n",
    "print(____)\n",
    "ChatGPT\n",
    "Here's how to complete each instruction step-by-step:\n",
    "\n",
    "Look at temperatures:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures)\n",
    "Set the index of temperatures to \"city\", assigning it to temperatures_ind:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "temperatures_ind = temperatures.set_index(\"city\")\n",
    "Look at temperatures_ind to see how it's different from temperatures:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_ind)\n",
    "This will show that temperatures_ind now uses the \"city\" column as the index, whereas temperatures has the default integer index.\n",
    "\n",
    "Reset the index of temperatures_ind, keeping its contents:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_ind.reset_index())\n",
    "Reset the index of temperatures_ind, dropping its contents:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_ind.reset_index(drop=True))\n",
    "Here's the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Look at temperatures\n",
    "print(temperatures)\n",
    "\n",
    "# Set the index of temperatures to city\n",
    "temperatures_ind = temperatures.set_index(\"city\")\n",
    "\n",
    "# Look at temperatures_ind\n",
    "print(temperatures_ind)\n",
    "\n",
    "# Reset the temperatures_ind index, keeping its contents\n",
    "print(temperatures_ind.reset_index())\n",
    "\n",
    "# Reset the temperatures_ind index, dropping its contents\n",
    "print(temperatures_ind.reset_index(drop=True))\n",
    "This will demonstrate how setting and resetting indexes affect your DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of cities to subset on\n",
    "cities = [\"Moscow\", \"Saint Petersburg\"]\n",
    "\n",
    "# Subset temperatures using square brackets\n",
    "print(temperatures[temperatures[\"city\"].isin(cities)])\n",
    "\n",
    "# Subset temperatures_ind using .loc[]\n",
    "print(temperatures_ind.loc[cities])\n",
    "\n",
    "\n",
    "# .loc[] is used by all the best folk! Setting an index allows more concise code for subsetting rows via .loc[]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Subsetting with .loc[]\n",
    "The killer feature for indexes is .loc[]: a subsetting method that accepts index values. When you pass it a single argument, it will take a subset of rows.\n",
    "\n",
    "The code for subsetting using .loc[] can be easier to read than standard square bracket subsetting, which can make your code less burdensome to maintain.\n",
    "\n",
    "pandas is loaded as pd. temperatures and temperatures_ind are available; the latter is indexed by city.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Create a list called cities that contains \"Moscow\" and \"Saint Petersburg\".\n",
    "Use [] subsetting to filter temperatures for rows where the city column takes a value in the cities list.\n",
    "Use .loc[] subsetting to filter temperatures_ind for rows where the city is in the cities list.'''\n",
    "\n",
    "# Make a list of cities to subset on\n",
    "cities = [\"____\", \"____\"]\n",
    "\n",
    "# Subset temperatures using square brackets\n",
    "print(temperatures[____])\n",
    "\n",
    "# Subset temperatures_ind using .loc[]\n",
    "print(temperatures_ind.loc[____])\n",
    "\n",
    "ChatGPT\n",
    "Here's how you can complete the subsetting tasks using .loc[] and standard subsetting:\n",
    "\n",
    "Create a list called cities that contains \"Moscow\" and \"Saint Petersburg\":\n",
    "\n",
    "python\n",
    "Copy code\n",
    "cities = [\"Moscow\", \"Saint Petersburg\"]\n",
    "Use [] subsetting to filter temperatures for rows where the city column takes a value in the cities list:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures[temperatures[\"city\"].isin(cities)])\n",
    "Use .loc[] subsetting to filter temperatures_ind for rows where the city is in the cities list:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_ind.loc[cities])\n",
    "Here's the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Make a list of cities to subset on\n",
    "cities = [\"Moscow\", \"Saint Petersburg\"]\n",
    "\n",
    "# Subset temperatures using square brackets\n",
    "print(temperatures[temperatures[\"city\"].isin(cities)])\n",
    "\n",
    "# Subset temperatures_ind using .loc[]\n",
    "print(temperatures_ind.loc[cities])\n",
    "The temperatures[temperatures[\"city\"].isin(cities)] line filters the temperatures DataFrame using the cities list.\n",
    "The temperatures_ind.loc[cities] line filters the temperatures_ind DataFrame based on the index values in the cities list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index temperatures by country & city\n",
    "temperatures_ind = temperatures.set_index([\"country\", \"city\"])\n",
    "\n",
    "# List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore\n",
    "rows_to_keep = [(\"Brazil\", \"Rio De Janeiro\"), (\"Pakistan\", \"Lahore\")]\n",
    "\n",
    "# Subset for rows to keep\n",
    "print(temperatures_ind.loc[rows_to_keep])\n",
    "\n",
    "\n",
    "'''Magnificent multi-level indexing! Multi-level indexes can make it easy to comprehend your dataset when one category is nested inside another category.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Setting multi-level indexes\n",
    "Indexes can also be made out of multiple columns, forming a multi-level index (sometimes called a hierarchical index). There is a trade-off to using these.\n",
    "\n",
    "The benefit is that multi-level indexes make it more natural to reason about nested categorical variables. For example, in a clinical trial, you might have control and treatment groups. Then each test subject belongs to one or another group, and we can say that a test subject is nested inside the treatment group. Similarly, in the temperature dataset, the city is located in the country, so we can say a city is nested inside the country.\n",
    "\n",
    "The main downside is that the code for manipulating indexes is different from the code for manipulating columns, so you have to learn two syntaxes and keep track of how your data is represented.\n",
    "\n",
    "pandas is loaded as pd. temperatures is available.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Set the index of temperatures to the \"country\" and \"city\" columns, and assign this to temperatures_ind.\n",
    "Specify two country/city pairs to keep: \"Brazil\"/\"Rio De Janeiro\" and \"Pakistan\"/\"Lahore\", assigning to rows_to_keep.\n",
    "Print and subset temperatures_ind for rows_to_keep using .loc[].'''\n",
    "\n",
    "# Index temperatures by country & city\n",
    "temperatures_ind = ____\n",
    "\n",
    "# List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore\n",
    "rows_to_keep = [____]\n",
    "\n",
    "# Subset for rows to keep\n",
    "print(temperatures_ind.____)\n",
    "ChatGPT\n",
    "Here's how you can set and subset multi-level indexes in pandas:\n",
    "\n",
    "Set the index of temperatures to the \"country\" and \"city\" columns, and assign this to temperatures_ind:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "temperatures_ind = temperatures.set_index([\"country\", \"city\"])\n",
    "Specify two country/city pairs to keep: \"Brazil\"/\"Rio De Janeiro\" and \"Pakistan\"/\"Lahore\", assigning to rows_to_keep:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "rows_to_keep = [(\"Brazil\", \"Rio De Janeiro\"), (\"Pakistan\", \"Lahore\")]\n",
    "Print and subset temperatures_ind for rows_to_keep using .loc[]:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_ind.loc[rows_to_keep])\n",
    "Here's the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Index temperatures by country & city\n",
    "temperatures_ind = temperatures.set_index([\"country\", \"city\"])\n",
    "\n",
    "# List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore\n",
    "rows_to_keep = [(\"Brazil\", \"Rio De Janeiro\"), (\"Pakistan\", \"Lahore\")]\n",
    "\n",
    "# Subset for rows to keep\n",
    "print(temperatures_ind.loc[rows_to_keep])\n",
    "This code sets a multi-level index using the \"country\" and \"city\" columns, and then subsets the DataFrame to include only the rows corresponding to the specified country/city pairs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort temperatures_ind by index values\n",
    "print(temperatures_ind.sort_index())\n",
    "\n",
    "# Sort temperatures_ind by index values at the city level\n",
    "print(temperatures_ind.sort_index(level=\"city\"))\n",
    "\n",
    "# Sort temperatures_ind by country then descending city\n",
    "print(temperatures_ind.sort_index(level=[\"country\", \"city\"], ascending=[True, False]))\n",
    "\n",
    "# Sorted! Sorting index values is similar to sorting values in columns, except that you call .sort_index() instead of .sort_values()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Sorting by index values\n",
    "Previously, you changed the order of the rows in a DataFrame by calling .sort_values(). It's also useful to be able to sort by elements in the index. For this, you need to use .sort_index().\n",
    "\n",
    "pandas is loaded as pd. temperatures_ind has a multi-level index of country and city, and is available.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Sort temperatures_ind by the index values.\n",
    "Sort temperatures_ind by the index values at the \"city\" level.\n",
    "Sort temperatures_ind by ascending country then descending city.'''\n",
    "\n",
    "\n",
    "# Sort temperatures_ind by index values\n",
    "print(____)\n",
    "\n",
    "# Sort temperatures_ind by index values at the city level\n",
    "print(____)\n",
    "\n",
    "# Sort temperatures_ind by country then descending city\n",
    "print(____)\n",
    "ChatGPT\n",
    "To sort a DataFrame with a multi-level index, you can use .sort_index() with different levels and sorting orders. Here's how you can accomplish the tasks:\n",
    "\n",
    "Sort temperatures_ind by the index values:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_ind.sort_index())\n",
    "Sort temperatures_ind by the index values at the \"city\" level:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_ind.sort_index(level=\"city\"))\n",
    "Sort temperatures_ind by ascending \"country\" then descending \"city\":\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_ind.sort_index(level=[\"country\", \"city\"], ascending=[True, False]))\n",
    "Here's the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Sort temperatures_ind by index values\n",
    "print(temperatures_ind.sort_index())\n",
    "\n",
    "# Sort temperatures_ind by index values at the city level\n",
    "print(temperatures_ind.sort_index(level=\"city\"))\n",
    "\n",
    "# Sort temperatures_ind by country then descending city\n",
    "print(temperatures_ind.sort_index(level=[\"country\", \"city\"], ascending=[True, False]))\n",
    "This code will help you sort the DataFrame by different levels of the multi-level index and in different orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the index of temperatures_ind\n",
    "temperatures_srt = temperatures_ind.sort_index()\n",
    "\n",
    "# Subset rows from Pakistan to Russia (by country level)\n",
    "print(temperatures_srt.loc[\"Pakistan\":\"Russia\"])\n",
    "\n",
    "# Try to subset rows from Lahore to Moscow (by city level; will return nonsense)\n",
    "print(temperatures_srt.loc[\"Lahore\":\"Moscow\"])\n",
    "\n",
    "# Subset rows from Pakistan, Lahore to Russia, Moscow (by both country and city levels)\n",
    "print(temperatures_srt.loc[(\"Pakistan\", \"Lahore\"):(\"Russia\", \"Moscow\")])\n",
    "\n",
    "\n",
    "# That's a nice slice! Combining slicing with .loc[] provides a concise syntax for subsetting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Slicing index values\n",
    "Slicing lets you select consecutive elements of an object using first:last syntax. DataFrames can be sliced by index values or by row/column number; we'll start with the first case. This involves slicing inside the .loc[] method.\n",
    "\n",
    "Compared to slicing lists, there are a few things to remember.\n",
    "\n",
    "You can only slice an index if the index is sorted (using .sort_index()).\n",
    "To slice at the outer level, first and last can be strings.\n",
    "To slice at inner levels, first and last should be tuples.\n",
    "If you pass a single slice to .loc[], it will slice the rows.\n",
    "pandas is loaded as pd. temperatures_ind has country and city in the index, and is available.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Sort the index of temperatures_ind.\n",
    "Use slicing with .loc[] to get these subsets:\n",
    "from Pakistan to Russia.\n",
    "from Lahore to Moscow. (This will return nonsense.)\n",
    "from Pakistan, Lahore to Russia, Moscow.'''\n",
    "\n",
    "\n",
    "# Sort the index of temperatures_ind\n",
    "temperatures_srt = ____\n",
    "\n",
    "# Subset rows from Pakistan to Russia\n",
    "print(____)\n",
    "\n",
    "# Try to subset rows from Lahore to Moscow\n",
    "print(____)\n",
    "\n",
    "# Subset rows from Pakistan, Lahore to Russia, Moscow\n",
    "print(____)\n",
    "ChatGPT\n",
    "To perform slicing on a DataFrame with a multi-level index, you'll need to follow these steps:\n",
    "\n",
    "Sort the index:\n",
    "Ensure the DataFrame is sorted by the index to enable proper slicing.\n",
    "\n",
    "Use .loc[] for slicing:\n",
    "\n",
    "For outer-level slicing (e.g., by country), you can use strings.\n",
    "For inner-level slicing (e.g., by city), use tuples to specify the range.\n",
    "Here is the code to accomplish the tasks:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Sort the index of temperatures_ind\n",
    "temperatures_srt = temperatures_ind.sort_index()\n",
    "\n",
    "# Subset rows from Pakistan to Russia (by country level)\n",
    "print(temperatures_srt.loc[\"Pakistan\":\"Russia\"])\n",
    "\n",
    "# Try to subset rows from Lahore to Moscow (by city level; will return nonsense)\n",
    "print(temperatures_srt.loc[\"Lahore\":\"Moscow\"])\n",
    "\n",
    "# Subset rows from Pakistan, Lahore to Russia, Moscow (by both country and city levels)\n",
    "print(temperatures_srt.loc[(\"Pakistan\", \"Lahore\"):(\"Russia\", \"Moscow\")])\n",
    "Explanation:\n",
    "Sort the index:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "temperatures_srt = temperatures_ind.sort_index()\n",
    "Subset rows from Pakistan to Russia (by country):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_srt.loc[\"Pakistan\":\"Russia\"])\n",
    "Subset rows from Lahore to Moscow (by city; note this may not be meaningful without sorting by city):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_srt.loc[\"Lahore\":\"Moscow\"])\n",
    "Subset rows from Pakistan, Lahore to Russia, Moscow (by both levels):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_srt.loc[(\"Pakistan\", \"Lahore\"):(\"Russia\", \"Moscow\")])\n",
    "This code will help you slice data using multi-level indexes and understand how to access different levels of data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset rows from India, Hyderabad to Iraq, Baghdad\n",
    "print(temperatures_srt.loc[(\"India\", \"Hyderabad\"):(\"Iraq\", \"Baghdad\")])\n",
    "\n",
    "# Subset columns from date to avg_temp_c\n",
    "print(temperatures_srt.loc[:, \"date\":\"avg_temp_c\"])\n",
    "\n",
    "# Subset in both directions at once\n",
    "print(temperatures_srt.loc[(\"India\", \"Hyderabad\"):(\"Iraq\", \"Baghdad\"), \"date\":\"avg_temp_c\"])\n",
    "\n",
    "\n",
    "# Brilliant bidirectional slicing! Slicing with .loc[] lets you take subsets in both directions at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Slicing in both directions\n",
    "You've seen slicing DataFrames by rows and by columns, but since DataFrames are two-dimensional objects, it is often natural to slice both dimensions at once. That is, by passing two arguments to .loc[], you can subset by rows and columns in one go.\n",
    "\n",
    "pandas is loaded as pd. temperatures_srt is indexed by country and city, has a sorted index, and is available.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Use .loc[] slicing to subset rows from India, Hyderabad to Iraq, Baghdad.\n",
    "Use .loc[] slicing to subset columns from date to avg_temp_c.\n",
    "Slice in both directions at once from Hyderabad to Baghdad, and date to avg_temp_c.'''\n",
    "\n",
    "# Subset rows from India, Hyderabad to Iraq, Baghdad\n",
    "print(____)\n",
    "\n",
    "# Subset columns from date to avg_temp_c\n",
    "print(____)\n",
    "\n",
    "# Subset in both directions at once\n",
    "print(____)\n",
    "\n",
    "ChatGPT\n",
    "To perform slicing in both directions (rows and columns) using .loc[], follow these steps:\n",
    "\n",
    "Subset rows from India, Hyderabad to Iraq, Baghdad.\n",
    "Subset columns from date to avg_temp_c.\n",
    "Slice in both directions at once, from Hyderabad to Baghdad and date to avg_temp_c.\n",
    "Here is the code to accomplish these tasks:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Subset rows from India, Hyderabad to Iraq, Baghdad\n",
    "print(temperatures_srt.loc[\"India\":\"Iraq\"])\n",
    "\n",
    "# Subset columns from date to avg_temp_c\n",
    "print(temperatures_srt.loc[:, \"date\":\"avg_temp_c\"])\n",
    "\n",
    "# Subset in both directions at once\n",
    "print(temperatures_srt.loc[\"India\":\"Iraq\", \"date\":\"avg_temp_c\"])\n",
    "Explanation:\n",
    "Subset rows from India, Hyderabad to Iraq, Baghdad:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_srt.loc[\"India\":\"Iraq\"])\n",
    "This will return all rows from \"India\" to \"Iraq\" based on the country index. If you want to specify city ranges, ensure the index includes both levels.\n",
    "\n",
    "Subset columns from date to avg_temp_c:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_srt.loc[:, \"date\":\"avg_temp_c\"])\n",
    "This will select all columns from \"date\" to \"avg_temp_c\".\n",
    "\n",
    "Subset in both directions at once:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_srt.loc[\"India\":\"Iraq\", \"date\":\"avg_temp_c\"])\n",
    "This combines the row slicing and column slicing, selecting data from rows between \"India\" and \"Iraq\" and columns between \"date\" and \"avg_temp_c\".\n",
    "\n",
    "Ensure that your DataFrame temperatures_srt has the appropriate indexes and columns for these operations to work correctly.\n",
    "\n",
    "Incorrect submission\n",
    "Are you subsetting _from_ India, Hyderabad?\n",
    "ChatGPT\n",
    "You're right; I should have been more precise. To subset from a specific starting point to an ending point, including both indices and columns, you should follow these guidelines:\n",
    "\n",
    "Subset rows from India, Hyderabad to Iraq, Baghdad:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_srt.loc[\"India\":\"Iraq\"])\n",
    "Subset columns from date to avg_temp_c:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_srt.loc[:, \"date\":\"avg_temp_c\"])\n",
    "Subset in both directions at once, from Hyderabad to Baghdad, and date to avg_temp_c:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_srt.loc[\"India\":\"Iraq\", \"date\":\"avg_temp_c\"])\n",
    "Updated Code:\n",
    "python\n",
    "Copy code\n",
    "# Subset rows from India, Hyderabad to Iraq, Baghdad\n",
    "print(temperatures_srt.loc[\"India\":\"Iraq\"])\n",
    "\n",
    "# Subset columns from date to avg_temp_c\n",
    "print(temperatures_srt.loc[:, \"date\":\"avg_temp_c\"])\n",
    "\n",
    "# Subset in both directions at once\n",
    "print(temperatures_srt.loc[\"India\":\"Iraq\", \"date\":\"avg_temp_c\"])\n",
    "Note: If you need to be more specific about slicing starting from \"Hyderabad\" to \"Baghdad\" and columns from \"date\" to \"avg_temp_c\", ensure that your DataFrame index and columns are ordered and contain these exact values. Adjust the slicing accordingly based on the actual data in your DataFrame.\n",
    "\n",
    "'''Hint\n",
    "Slice rows with code like df.loc[(\"a\", \"b\"):(\"c\", \"d\")].\n",
    "Slice columns with code like df.loc[:, \"e\":\"f\"].\n",
    "Slice both ways with code like df.loc[(\"a\", \"b\"):(\"c\", \"d\"), \"e\":\"f\"].'''\n",
    "\n",
    "\n",
    "# Subset rows from India, Hyderabad to Iraq, Baghdad\n",
    "print(____)\n",
    "\n",
    "# Subset columns from date to avg_temp_c\n",
    "print(____)\n",
    "\n",
    "# Subset in both directions at once\n",
    "print(____)\n",
    "ChatGPT\n",
    "With the given hint, here's how to correctly slice the DataFrame:\n",
    "\n",
    "Subset rows from India, Hyderabad to Iraq, Baghdad:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_srt.loc[(\"India\", \"Hyderabad\"):(\"Iraq\", \"Baghdad\")])\n",
    "Subset columns from date to avg_temp_c:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_srt.loc[:, \"date\":\"avg_temp_c\"])\n",
    "Subset in both directions at once:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_srt.loc[(\"India\", \"Hyderabad\"):(\"Iraq\", \"Baghdad\"), \"date\":\"avg_temp_c\"])\n",
    "Updated Code:\n",
    "python\n",
    "Copy code\n",
    "# Subset rows from India, Hyderabad to Iraq, Baghdad\n",
    "print(temperatures_srt.loc[(\"India\", \"Hyderabad\"):(\"Iraq\", \"Baghdad\")])\n",
    "\n",
    "# Subset columns from date to avg_temp_c\n",
    "print(temperatures_srt.loc[:, \"date\":\"avg_temp_c\"])\n",
    "\n",
    "# Subset in both directions at once\n",
    "print(temperatures_srt.loc[(\"India\", \"Hyderabad\"):(\"Iraq\", \"Baghdad\"), \"date\":\"avg_temp_c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Boolean conditions to subset temperatures for rows in 2010 and 2011\n",
    "temperatures_bool = temperatures[(temperatures[\"date\"] >= \"2010-01-01\") & (temperatures[\"date\"] <= \"2011-12-31\")]\n",
    "print(temperatures_bool)\n",
    "\n",
    "# Set date as the index and sort the index\n",
    "temperatures_ind = temperatures.set_index(\"date\").sort_index()\n",
    "\n",
    "# Use .loc[] to subset temperatures_ind for rows in 2010 and 2011\n",
    "print(temperatures_ind.loc[\"2010-01-01\":\"2011-12-31\"])\n",
    "\n",
    "# Use .loc[] to subset temperatures_ind for rows from Aug 2010 to Feb 2011\n",
    "print(temperatures_ind.loc[\"2010-08-01\":\"2011-02-28\"])\n",
    "\n",
    "\n",
    "# Delightful data range slicing! Using .loc[] in conjunction with a date index provides an easy way to subset for rows before or after some date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Slicing time series\n",
    "Slicing is particularly useful for time series since it's a common thing to want to filter for data within a date range. Add the date column to the index, then use .loc[] to perform the subsetting. The important thing to remember is to keep your dates in ISO 8601 format, that is, \"yyyy-mm-dd\" for year-month-day, \"yyyy-mm\" for year-month, and \"yyyy\" for year.\n",
    "\n",
    "Recall from Chapter 1 that you can combine multiple Boolean conditions using logical operators, such as &. To do so in one line of code, you'll need to add parentheses () around each condition.\n",
    "\n",
    "pandas is loaded as pd and temperatures, with no index, is available.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Use Boolean conditions, not .isin() or .loc[], and the full date \"yyyy-mm-dd\", to subset temperatures for rows where the date column is in 2010 and 2011 and print the results.\n",
    "Set the index of temperatures to the date column and sort it.\n",
    "Use .loc[] to subset temperatures_ind for rows in 2010 and 2011.\n",
    "Use .loc[] to subset temperatures_ind for rows from August 2010 to February 2011.'''\n",
    "\n",
    "\n",
    "# Use Boolean conditions to subset temperatures for rows in 2010 and 2011\n",
    "temperatures_bool = ____[(____ >= ____) & (____ <= ____)]\n",
    "print(temperatures_bool)\n",
    "\n",
    "# Set date as the index and sort the index\n",
    "temperatures_ind = temperatures.____.____\n",
    "\n",
    "# Use .loc[] to subset temperatures_ind for rows in 2010 and 2011\n",
    "print(____)\n",
    "\n",
    "# Use .loc[] to subset temperatures_ind for rows from Aug 2010 to Feb 2011\n",
    "print(____)\n",
    "ChatGPT\n",
    "Here's how you can perform the required operations:\n",
    "\n",
    "Subset temperatures for rows where the date column is in 2010 and 2011 using Boolean conditions:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "temperatures_bool = temperatures[(temperatures[\"date\"] >= \"2010-01-01\") & (temperatures[\"date\"] <= \"2011-12-31\")]\n",
    "print(temperatures_bool)\n",
    "Set the index of temperatures to the date column and sort it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "temperatures_ind = temperatures.set_index(\"date\").sort_index()\n",
    "Use .loc[] to subset temperatures_ind for rows in 2010 and 2011:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_ind.loc[\"2010-01-01\":\"2011-12-31\"])\n",
    "Use .loc[] to subset temperatures_ind for rows from August 2010 to February 2011:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures_ind.loc[\"2010-08-01\":\"2011-02-28\"])\n",
    "Updated Code:\n",
    "python\n",
    "Copy code\n",
    "# Use Boolean conditions to subset temperatures for rows in 2010 and 2011\n",
    "temperatures_bool = temperatures[(temperatures[\"date\"] >= \"2010-01-01\") & (temperatures[\"date\"] <= \"2011-12-31\")]\n",
    "print(temperatures_bool)\n",
    "\n",
    "# Set date as the index and sort the index\n",
    "temperatures_ind = temperatures.set_index(\"date\").sort_index()\n",
    "\n",
    "# Use .loc[] to subset temperatures_ind for rows in 2010 and 2011\n",
    "print(temperatures_ind.loc[\"2010-01-01\":\"2011-12-31\"])\n",
    "\n",
    "# Use .loc[] to subset temperatures_ind for rows from Aug 2010 to Feb 2011\n",
    "print(temperatures_ind.loc[\"2010-08-01\":\"2011-02-28\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 23rd row, 2nd column (index 22, 1)\n",
    "print(temperatures.iloc[22, 1])\n",
    "\n",
    "# Use slicing to get the first 5 rows\n",
    "print(temperatures.iloc[0:5])\n",
    "\n",
    "# Use slicing to get columns 3 to 4\n",
    "print(temperatures.iloc[:, 2:5])\n",
    "\n",
    "# Use slicing in both directions at once\n",
    "print(temperatures.iloc[0:5, 2:5])\n",
    "# Insightful use of .iloc[]! Use .iloc[] to specify a subset using the row or column numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Subsetting by row/column number\n",
    "The most common ways to subset rows are the ways we've previously discussed: using a Boolean condition or by index labels. However, it is also occasionally useful to pass row numbers.\n",
    "\n",
    "This is done using .iloc[], and like .loc[], it can take two arguments to let you subset by rows and columns.\n",
    "\n",
    "pandas is loaded as pd. temperatures (without an index) is available.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Use .iloc[] on temperatures to take subsets.\n",
    "\n",
    "Get the 23rd row, 2nd column (index positions 22 and 1).\n",
    "Get the first 5 rows (index positions 0 to 5).\n",
    "Get all rows, columns 3 and 4 (index positions 2 to 4).\n",
    "Get the first 5 rows, columns 3 and 4.'''\n",
    "\n",
    "\n",
    "\n",
    "# Get 23rd row, 2nd column (index 22, 1)\n",
    "print(____)\n",
    "\n",
    "# Use slicing to get the first 5 rows\n",
    "print(____)\n",
    "\n",
    "# Use slicing to get columns 3 to 4\n",
    "print(____)\n",
    "\n",
    "# Use slicing in both directions at once\n",
    "print(____)\n",
    "ChatGPT\n",
    "Here’s how you can use .iloc[] to perform the required subsetting:\n",
    "\n",
    "Get the 23rd row, 2nd column (index positions 22 and 1):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures.iloc[22, 1])\n",
    "Get the first 5 rows (index positions 0 to 5):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures.iloc[0:5])\n",
    "Get all rows, columns 3 and 4 (index positions 2 to 4):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures.iloc[:, 2:5])\n",
    "Get the first 5 rows, columns 3 and 4:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(temperatures.iloc[0:5, 2:5])\n",
    "Updated Code:\n",
    "python\n",
    "Copy code\n",
    "# Get 23rd row, 2nd column (index 22, 1)\n",
    "print(temperatures.iloc[22, 1])\n",
    "\n",
    "# Use slicing to get the first 5 rows\n",
    "print(temperatures.iloc[0:5])\n",
    "\n",
    "# Use slicing to get columns 3 to 4\n",
    "print(temperatures.iloc[:, 2:5])\n",
    "\n",
    "# Use slicing in both directions at once\n",
    "print(temperatures.iloc[0:5, 2:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a year column to temperatures\n",
    "temperatures['year'] = temperatures['date'].dt.year\n",
    "\n",
    "# Pivot avg_temp_c by country and city vs year\n",
    "temp_by_country_city_vs_year = temperatures.pivot_table(\n",
    "    values='avg_temp_c', \n",
    "    index=['country', 'city'], \n",
    "    columns='year'\n",
    ")\n",
    "\n",
    "# See the result\n",
    "print(temp_by_country_city_vs_year)\n",
    "# Powerful pivoting! Now you have the pivot table, let's manipulate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Pivot temperature by city and year\n",
    "It's interesting to see how temperatures for each city change over time—looking at every month results in a big table, which can be tricky to reason about. Instead, let's look at how temperatures change by year.\n",
    "\n",
    "You can access the components of a date (year, month and day) using code of the form dataframe[\"column\"].dt.component. For example, the month component is dataframe[\"column\"].dt.month, and the year component is dataframe[\"column\"].dt.year.\n",
    "\n",
    "Once you have the year column, you can create a pivot table with the data aggregated by city and year, which you'll explore in the coming exercises.\n",
    "\n",
    "pandas is loaded as pd. temperatures is available.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Add a year column to temperatures, from the year component of the date column.\n",
    "Make a pivot table of the avg_temp_c column, with country and city as rows, and year as columns. Assign to temp_by_country_city_vs_year, and look at the result.'''\n",
    "\n",
    "\n",
    "# Add a year column to temperatures\n",
    "____\n",
    "\n",
    "# Pivot avg_temp_c by country and city vs year\n",
    "temp_by_country_city_vs_year = ____\n",
    "\n",
    "# See the result\n",
    "print(temp_by_country_city_vs_year)\n",
    "ChatGPT\n",
    "To pivot the temperature data by city and year, follow these steps:\n",
    "\n",
    "Add a year column to temperatures from the year component of the date column:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "temperatures['year'] = temperatures['date'].dt.year\n",
    "Create a pivot table of avg_temp_c with country and city as rows, and year as columns:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "temp_by_country_city_vs_year = temperatures.pivot_table(\n",
    "    values='avg_temp_c', \n",
    "    index=['country', 'city'], \n",
    "    columns='year'\n",
    ")\n",
    "Complete Code:\n",
    "python\n",
    "Copy code\n",
    "# Add a year column to temperatures\n",
    "temperatures['year'] = temperatures['date'].dt.year\n",
    "\n",
    "# Pivot avg_temp_c by country and city vs year\n",
    "temp_by_country_city_vs_year = temperatures.pivot_table(\n",
    "    values='avg_temp_c', \n",
    "    index=['country', 'city'], \n",
    "    columns='year'\n",
    ")\n",
    "\n",
    "# See the result\n",
    "print(temp_by_country_city_vs_year)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset for Egypt to India\n",
    "egypt_to_india = temp_by_country_city_vs_year.loc['Egypt':'India']\n",
    "\n",
    "# Subset for Egypt, Cairo to India, Delhi\n",
    "egypt_cairo_to_india_delhi = temp_by_country_city_vs_year.loc[('Egypt', 'Cairo'):('India', 'Delhi')]\n",
    "\n",
    "# Subset for Egypt, Cairo to India, Delhi, and 2005 to 2010\n",
    "egypt_cairo_to_india_delhi_2005_2010 = temp_by_country_city_vs_year.loc[\n",
    "    ('Egypt', 'Cairo'):('India', 'Delhi'),\n",
    "    2005:2010\n",
    "]\n",
    "# Super pivot table subsetting! Slicing is especially helpful for subsetting pivot tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Subsetting pivot tables\n",
    "A pivot table is just a DataFrame with sorted indexes, so the techniques you have learned already can be used to subset them. In particular, the .loc[] + slicing combination is often helpful.\n",
    "\n",
    "pandas is loaded as pd. temp_by_country_city_vs_year is available.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Use .loc[] on temp_by_country_city_vs_year to take subsets.\n",
    "\n",
    "From Egypt to India.\n",
    "From Egypt, Cairo to India, Delhi.\n",
    "From Egypt, Cairo to India, Delhi, and 2005 to 2010.'''\n",
    "\n",
    "# Subset for Egypt to India\n",
    "____\n",
    "\n",
    "# Subset for Egypt, Cairo to India, Delhi\n",
    "____\n",
    "\n",
    "# Subset for Egypt, Cairo to India, Delhi, and 2005 to 2010\n",
    "____\n",
    "ChatGPT\n",
    "To subset the pivot table temp_by_country_city_vs_year, use the .loc[] method with the appropriate slices for rows and columns.\n",
    "\n",
    "Subsetting Instructions:\n",
    "From Egypt to India:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "egypt_to_india = temp_by_country_city_vs_year.loc['Egypt':'India']\n",
    "From Egypt, Cairo to India, Delhi:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "egypt_cairo_to_india_delhi = temp_by_country_city_vs_year.loc[('Egypt', 'Cairo'):('India', 'Delhi')]\n",
    "From Egypt, Cairo to India, Delhi, and 2005 to 2010:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "egypt_cairo_to_india_delhi_2005_2010 = temp_by_country_city_vs_year.loc[\n",
    "    ('Egypt', 'Cairo'):('India', 'Delhi'),\n",
    "    2005:2010\n",
    "]\n",
    "Complete Code:\n",
    "python\n",
    "Copy code\n",
    "# Subset for Egypt to India\n",
    "egypt_to_india = temp_by_country_city_vs_year.loc['Egypt':'India']\n",
    "\n",
    "# Subset for Egypt, Cairo to India, Delhi\n",
    "egypt_cairo_to_india_delhi = temp_by_country_city_vs_year.loc[('Egypt', 'Cairo'):('India', 'Delhi')]\n",
    "\n",
    "# Subset for Egypt, Cairo to India, Delhi, and 2005 to 2010\n",
    "egypt_cairo_to_india_delhi_2005_2010 = temp_by_country_city_vs_year.loc[\n",
    "    ('Egypt', 'Cairo'):('India', 'Delhi'),\n",
    "    2005:2010\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the worldwide mean temperature by year\n",
    "mean_temp_by_year = temp_by_country_city_vs_year.mean()\n",
    "\n",
    "# Filter for the year that had the highest mean temperature\n",
    "print(mean_temp_by_year[mean_temp_by_year == mean_temp_by_year.max()])\n",
    "\n",
    "# Get the mean temperature by city\n",
    "mean_temp_by_city = temp_by_country_city_vs_year.mean(axis=1)\n",
    "\n",
    "# Filter for the city that had the lowest mean temperature\n",
    "print(mean_temp_by_city[mean_temp_by_city == mean_temp_by_city.min()])\n",
    "# Classy calculating! With an average temperature just above freezing, Harbin is famous for its ice sculpture festival.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''Calculating on a pivot table\n",
    "Pivot tables are filled with summary statistics, but they are only a first step to finding something insightful. Often you'll need to perform further calculations on them. A common thing to do is to find the rows or columns where the highest or lowest value occurs.\n",
    "\n",
    "Recall from Chapter 1 that you can easily subset a Series or DataFrame to find rows of interest using a logical condition inside of square brackets. For example: series[series > value].\n",
    "\n",
    "pandas is loaded as pd and the DataFrame temp_by_country_city_vs_year is available. The .head() for this DataFrame is shown below, with only a few of the year columns displayed:\n",
    "\n",
    "country\tcity\t2000\t2001\t2002\t…\t2013\n",
    "Afghanistan\tKabul\t15.823\t15.848\t15.715\t…\t16.206\n",
    "Angola\tLuanda\t24.410\t24.427\t24.791\t…\t24.554\n",
    "Australia\tMelbourne\t14.320\t14.180\t14.076\t…\t14.742\n",
    "Sydney\t17.567\t17.854\t17.734\t…\t18.090\n",
    "Bangladesh\tDhaka\t25.905\t25.931\t26.095\t…\t26.587\n",
    "Instructions\n",
    "100 XP\n",
    "Calculate the mean temperature for each year, assigning to mean_temp_by_year.\n",
    "Filter mean_temp_by_year for the year that had the highest mean temperature.\n",
    "Calculate the mean temperature for each city (across columns), assigning to mean_temp_by_city.\n",
    "Filter mean_temp_by_city for the city that had the lowest mean temperature.'''\n",
    "\n",
    "\n",
    "# Get the worldwide mean temp by year\n",
    "mean_temp_by_year = temp_by_country_city_vs_year.____\n",
    "\n",
    "# Filter for the year that had the highest mean temp\n",
    "print(mean_temp_by_year[____])\n",
    "\n",
    "# Get the mean temp by city\n",
    "mean_temp_by_city = temp_by_country_city_vs_year.____\n",
    "\n",
    "# Filter for the city that had the lowest mean temp\n",
    "print(mean_temp_by_city[____])\n",
    "ChatGPT\n",
    "Here's how you can perform the calculations and filtering on the pivot table temp_by_country_city_vs_year:\n",
    "\n",
    "Calculate the mean temperature for each year:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "mean_temp_by_year = temp_by_country_city_vs_year.mean()\n",
    "Filter for the year that had the highest mean temperature:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "year_with_highest_mean_temp = mean_temp_by_year.idxmax()\n",
    "print(mean_temp_by_year[year_with_highest_mean_temp])\n",
    "Calculate the mean temperature for each city (across columns):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "mean_temp_by_city = temp_by_country_city_vs_year.mean(axis=1)\n",
    "Filter for the city that had the lowest mean temperature:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "city_with_lowest_mean_temp = mean_temp_by_city.idxmin()\n",
    "print(mean_temp_by_city[city_with_lowest_mean_temp])\n",
    "Complete Code:\n",
    "python\n",
    "Copy code\n",
    "# Get the worldwide mean temperature by year\n",
    "mean_temp_by_year = temp_by_country_city_vs_year.mean()\n",
    "\n",
    "# Filter for the year that had the highest mean temperature\n",
    "year_with_highest_mean_temp = mean_temp_by_year.idxmax()\n",
    "print(mean_temp_by_year[year_with_highest_mean_temp])\n",
    "\n",
    "# Get the mean temperature by city\n",
    "mean_temp_by_city = temp_by_country_city_vs_year.mean(axis=1)\n",
    "\n",
    "# Filter for the city that had the lowest mean temperature\n",
    "city_with_lowest_mean_temp = mean_temp_by_city.idxmin()\n",
    "print(mean_temp_by_city[city_with_lowest_mean_temp])\n",
    "Incorrect submission\n",
    "Are you using Boolean (==) filtering to select the year that had the highest mean temperature?\n",
    "\n",
    " \n",
    "ChatGPT\n",
    "To correctly use Boolean filtering for selecting the year with the highest mean temperature and the city with the lowest mean temperature, you need to use a condition within square brackets. Here’s how you can do it:\n",
    "\n",
    "Calculate the mean temperature for each year:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "mean_temp_by_year = temp_by_country_city_vs_year.mean()\n",
    "Filter for the year that had the highest mean temperature using Boolean filtering:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(mean_temp_by_year[mean_temp_by_year == mean_temp_by_year.max()])\n",
    "Calculate the mean temperature for each city (across columns):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "mean_temp_by_city = temp_by_country_city_vs_year.mean(axis=1)\n",
    "Filter for the city that had the lowest mean temperature using Boolean filtering:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "print(mean_temp_by_city[mean_temp_by_city == mean_temp_by_city.min()])\n",
    "Complete Code:\n",
    "python\n",
    "Copy code\n",
    "# Get the worldwide mean temperature by year\n",
    "mean_temp_by_year = temp_by_country_city_vs_year.mean()\n",
    "\n",
    "# Filter for the year that had the highest mean temperature\n",
    "print(mean_temp_by_year[mean_temp_by_year == mean_temp_by_year.max()])\n",
    "\n",
    "# Get the mean temperature by city\n",
    "mean_temp_by_city = temp_by_country_city_vs_year.mean(axis=1)\n",
    "\n",
    "# Filter for the city that had the lowest mean temperature\n",
    "print(mean_temp_by_city[mean_temp_by_city == mean_temp_by_city.min()])\n",
    "This approach ensures that you are using Boolean filtering to get the exact year with the highest mean temperature and the city with the lowest mean temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and Visualizing DataFrames\n",
    "\n",
    "# Learn to visualize the contents of your DataFrames, handle missing data values, and import data from and export data to CSV files.\n",
    "\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Look at the first few rows of data\n",
    "print(avocados.head())\n",
    "\n",
    "# Get the total number of avocados sold of each size\n",
    "nb_sold_by_size = avocados.groupby('size')['nb_sold'].sum()\n",
    "\n",
    "# Create a bar plot of the number of avocados sold by size\n",
    "nb_sold_by_size.plot(kind='bar')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Bedazzling bar plot! It looks like small avocados were the most-purchased size, but large avocados were a close second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Which avocado size is most popular?\n",
    "Avocados are increasingly popular and delicious in guacamole and on toast. The Hass Avocado Board keeps track of avocado supply and demand across the USA, including the sales of three different sizes of avocado. In this exercise, you'll use a bar plot to figure out which size is the most popular.\n",
    "\n",
    "Bar plots are great for revealing relationships between categorical (size) and numeric (number sold) variables, but you'll often have to manipulate your data first in order to get the numbers you need for plotting.\n",
    "\n",
    "pandas has been imported as pd, and avocados is available.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Print the head of the avocados dataset. What columns are available?\n",
    "For each avocado size group, calculate the total number sold, storing as nb_sold_by_size.\n",
    "Create a bar plot of the number of avocados sold by size.\n",
    "Show the plot.'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Look at the first few rows of data\n",
    "print(____)\n",
    "\n",
    "# Get the total number of avocados sold of each size\n",
    "nb_sold_by_size = ____\n",
    "\n",
    "# Create a bar plot of the number of avocados sold by size\n",
    "____\n",
    "\n",
    "# Show the plot\n",
    "____\n",
    "ChatGPT\n",
    "To solve this problem, you'll need to follow these steps:\n",
    "\n",
    "Inspect the first few rows of the avocados dataset to understand its structure.\n",
    "Calculate the total number of avocados sold for each size category.\n",
    "Create a bar plot to visualize the number of avocados sold by size.\n",
    "Display the plot.\n",
    "Here’s how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Look at the first few rows of data\n",
    "print(avocados.head())\n",
    "\n",
    "# Get the total number of avocados sold of each size\n",
    "nb_sold_by_size = avocados.groupby('size')['number_sold'].sum()\n",
    "\n",
    "# Create a bar plot of the number of avocados sold by size\n",
    "nb_sold_by_size.plot(kind='bar')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "Explanation:\n",
    "\n",
    "avocados.head() displays the first few rows of the dataset, allowing you to see what columns are available.\n",
    "avocados.groupby('size')['number_sold'].sum() groups the data by the size column and sums the number_sold for each size category, resulting in a Series with the total number sold for each size.\n",
    "nb_sold_by_size.plot(kind='bar') creates a bar plot of the total number of avocados sold for each size category.\n",
    "plt.show() displays the plot.\n",
    "\n",
    "2/2\n",
    "\n",
    "Incorrect submission\n",
    "After grouping by \"size\", did you select the \"nb_sold\" column before calculating the sum?\n",
    "ChatGPT\n",
    "You are correct. I should select the \"nb_sold\" column after grouping by \"size\". Let's correct that:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Look at the first few rows of data\n",
    "print(avocados.head())\n",
    "\n",
    "# Get the total number of avocados sold of each size\n",
    "nb_sold_by_size = avocados.groupby('size')['nb_sold'].sum()\n",
    "\n",
    "# Create a bar plot of the number of avocados sold by size\n",
    "nb_sold_by_size.plot(kind='bar')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "Explanation:\n",
    "\n",
    "avocados.groupby('size')['nb_sold'].sum() groups the data by the size column and then sums the nb_sold for each size category, resulting in a Series with the total number sold for each size.\n",
    "nb_sold_by_size.plot(kind='bar') creates a bar plot of the total number of avocados sold for each size category.\n",
    "plt.show() displays the plot.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the total number of avocados sold on each date\n",
    "nb_sold_by_date = avocados.groupby('date')['nb_sold'].sum()\n",
    "\n",
    "# Create a line plot of the number of avocados sold by date\n",
    "nb_sold_by_date.plot(kind='line')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Lovely line plot! Line plots are great for visualizing something over time. Here, it looks like the number of avocados spikes around the same time each year.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Changes in sales over time\n",
    "Line plots are designed to visualize the relationship between two numeric variables, where each data values is connected to the next one. They are especially useful for visualizing the change in a number over time since each time point is naturally connected to the next time point. In this exercise, you'll visualize the change in avocado sales over three years.\n",
    "\n",
    "pandas has been imported as pd, and avocados is available.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Get the total number of avocados sold on each date. The DataFrame has two rows for each date—one for organic, and one for conventional. Save this as nb_sold_by_date.\n",
    "Create a line plot of the number of avocados sold.\n",
    "Show the plot.'''\n",
    "\n",
    "\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the total number of avocados sold on each date\n",
    "nb_sold_by_date = ____\n",
    "\n",
    "# Create a line plot of the number of avocados sold by date\n",
    "____\n",
    "\n",
    "# Show the plot\n",
    "____\n",
    "ChatGPT\n",
    "Here's how you can complete the exercise:\n",
    "\n",
    "Use the groupby method to get the total number of avocados sold on each date.\n",
    "Use plot to create a line plot.\n",
    "Show the plot using plt.show().\n",
    "Here's the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the total number of avocados sold on each date\n",
    "nb_sold_by_date = avocados.groupby('date')['nb_sold'].sum()\n",
    "\n",
    "# Create a line plot of the number of avocados sold by date\n",
    "nb_sold_by_date.plot(kind='line')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "This will generate a line plot showing the total number of avocados sold over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the avocados DataFrame is available\n",
    "# Example data for demonstration purposes\n",
    "data = {\n",
    "    'nb_sold': [1500, 1600, 1800, 1900, 2000],\n",
    "    'avg_price': [1.50, 1.60, 1.55, 1.70, 1.65]\n",
    "}\n",
    "avocados = pd.DataFrame(data)\n",
    "\n",
    "# Create a scatter plot with nb_sold on the x-axis and avg_price on the y-axis\n",
    "plt.scatter(avocados['nb_sold'], avocados['avg_price'])\n",
    "\n",
    "# Title the plot\n",
    "plt.title('Number of avocados sold vs. average price')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# updated\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the avocados DataFrame is available\n",
    "# Example data for demonstration purposes\n",
    "data = {\n",
    "    'nb_sold': [1500, 1600, 1800, 1900, 2000],\n",
    "    'avg_price': [1.50, 1.60, 1.55, 1.70, 1.65]\n",
    "}\n",
    "avocados = pd.DataFrame(data)\n",
    "\n",
    "# Create a scatter plot with nb_sold on the x-axis and avg_price on the y-axis\n",
    "avocados.plot(kind='scatter', x='nb_sold', y='avg_price', title='Number of avocados sold vs. average price')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correction\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Create a scatter plot with nb_sold on the x-axis and avg_price on the y-axis\n",
    "avocados.plot(kind='scatter', x='nb_sold', y='avg_price', title='Number of avocados sold vs. average price')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "'''Super scatter plot! It looks like when more avocados are sold, prices go down. However, this doesn't mean that fewer sales causes higher prices - we can only tell that they're correlated with each other.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "''Avocado supply and demand\n",
    "Scatter plots are ideal for visualizing relationships between numerical variables. In this exercise, you'll compare the number of avocados sold to average price and see if they're at all related. If they're related, you may be able to use one number to predict the other.\n",
    "\n",
    "matplotlib.pyplot has been imported as plt, pandas has been imported as pd, and avocados is available.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Create a scatter plot with nb_sold on the x-axis and avg_price on the y-axis. Title it \"Number of avocados sold vs. average price\".\n",
    "Show the plot.'''\n",
    "\n",
    "# Scatter plot of avg_price vs. nb_sold with title\n",
    "____.____\n",
    "\n",
    "# Show the plot\n",
    "____\n",
    "ChatGPT\n",
    "Here's how you can create the scatter plot of the number of avocados sold versus the average price using Matplotlib and pandas:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the avocados DataFrame is available\n",
    "# Example data for demonstration purposes\n",
    "data = {\n",
    "    'nb_sold': [1500, 1600, 1800, 1900, 2000],\n",
    "    'avg_price': [1.50, 1.60, 1.55, 1.70, 1.65]\n",
    "}\n",
    "avocados = pd.DataFrame(data)\n",
    "\n",
    "# Create a scatter plot with nb_sold on the x-axis and avg_price on the y-axis\n",
    "plt.scatter(avocados['nb_sold'], avocados['avg_price'])\n",
    "\n",
    "# Title the plot\n",
    "plt.title('Number of avocados sold vs. average price')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "Replace the data dictionary with your actual avocados DataFrame. This script will create a scatter plot with the number of avocados sold on the x-axis and the average price on the y-axis, and then display the plot.\n",
    "\n",
    "Incorrect submission\n",
    "Did you call avocados.plot()?\n",
    "ChatGPT\n",
    "You are right, using the plot method from the DataFrame would be more straightforward. Here's how to do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the avocados DataFrame is available\n",
    "# Example data for demonstration purposes\n",
    "data = {\n",
    "    'nb_sold': [1500, 1600, 1800, 1900, 2000],\n",
    "    'avg_price': [1.50, 1.60, 1.55, 1.70, 1.65]\n",
    "}\n",
    "avocados = pd.DataFrame(data)\n",
    "\n",
    "# Create a scatter plot with nb_sold on the x-axis and avg_price on the y-axis\n",
    "avocados.plot(kind='scatter', x='nb_sold', y='avg_price', title='Number of avocados sold vs. average price')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "This will create and display the scatter plot using the plot method from the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "# Histogram of conventional avg_price \n",
    "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist()\n",
    "\n",
    "# Histogram of organic avg_price\n",
    "avocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist()\n",
    "\n",
    "# Add a legend\n",
    "plt.legend([\"conventional\", \"organic\"])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Avocado supply and demand\n",
    "Scatter plots are ideal for visualizing relationships between numerical variables. In this exercise, you'll compare the number of avocados sold to average price and see if they're at all related. If they're related, you may be able to use one number to predict the other.\n",
    "\n",
    "matplotlib.pyplot has been imported as plt, pandas has been imported as pd, and avocados is available.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Create a scatter plot with nb_sold on the x-axis and avg_price on the y-axis. Title it \"Number of avocados sold vs. average price\".\n",
    "Show the plot.'''\n",
    "\n",
    "# Scatter plot of avg_price vs. nb_sold with title\n",
    "____.____\n",
    "\n",
    "# Show the plot\n",
    "____\n",
    "ChatGPT\n",
    "Here's how you can create the scatter plot of the number of avocados sold versus the average price using Matplotlib and pandas:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the avocados DataFrame is available\n",
    "# Example data for demonstration purposes\n",
    "data = {\n",
    "    'nb_sold': [1500, 1600, 1800, 1900, 2000],\n",
    "    'avg_price': [1.50, 1.60, 1.55, 1.70, 1.65]\n",
    "}\n",
    "avocados = pd.DataFrame(data)\n",
    "\n",
    "# Create a scatter plot with nb_sold on the x-axis and avg_price on the y-axis\n",
    "plt.scatter(avocados['nb_sold'], avocados['avg_price'])\n",
    "\n",
    "# Title the plot\n",
    "plt.title('Number of avocados sold vs. average price')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "Replace the data dictionary with your actual avocados DataFrame. This script will create a scatter plot with the number of avocados sold on the x-axis and the average price on the y-axis, and then display the plot.\n",
    "\n",
    "Incorrect submission\n",
    "Did you call avocados.plot()?\n",
    "ChatGPT\n",
    "You are right, using the plot method from the DataFrame would be more straightforward. Here's how to do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the avocados DataFrame is available\n",
    "# Example data for demonstration purposes\n",
    "data = {\n",
    "    'nb_sold': [1500, 1600, 1800, 1900, 2000],\n",
    "    'avg_price': [1.50, 1.60, 1.55, 1.70, 1.65]\n",
    "}\n",
    "avocados = pd.DataFrame(data)\n",
    "\n",
    "# Create a scatter plot with nb_sold on the x-axis and avg_price on the y-axis\n",
    "avocados.plot(kind='scatter', x='nb_sold', y='avg_price', title='Number of avocados sold vs. average price')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "This will create and display the scatter plot using the plot method from the DataFrame.\n",
    "\n",
    "'''Price of conventional vs. organic avocados\n",
    "Creating multiple plots for different subsets of data allows you to compare groups. In this exercise, you'll create multiple histograms to compare the prices of conventional and organic avocados.\n",
    "\n",
    "matplotlib.pyplot has been imported as plt and pandas has been imported as pd.\n",
    "\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "1\n",
    "2\n",
    "3\n",
    "Subset avocados for the \"conventional\" type and create a histogram of the avg_price column.\n",
    "Create a histogram of avg_price for \"organic\" type avocados.\n",
    "Add a legend to your plot, with the names \"conventional\" and \"organic\".\n",
    "Show your plot.\n",
    "\n",
    "'''\n",
    "\n",
    "# Histogram of conventional avg_price \n",
    "avocados[____][____].____\n",
    "\n",
    "# Histogram of organic avg_price\n",
    "avocados[____][____].____\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(____)\n",
    "\n",
    "# Show the plot\n",
    "____\n",
    "ChatGPT\n",
    "Here's how you can create histograms for the average prices of conventional and organic avocados, add a legend, and show the plot:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Example data for demonstration purposes\n",
    "data = {\n",
    "    'type': ['conventional', 'conventional', 'organic', 'organic', 'conventional', 'organic'],\n",
    "    'avg_price': [1.10, 1.20, 1.50, 1.60, 1.15, 1.55]\n",
    "}\n",
    "avocados = pd.DataFrame(data)\n",
    "\n",
    "# Histogram of conventional avg_price\n",
    "avocados[avocados['type'] == 'conventional']['avg_price'].hist(alpha=0.5, label='conventional')\n",
    "\n",
    "# Histogram of organic avg_price\n",
    "avocados[avocados['type'] == 'organic']['avg_price'].hist(alpha=0.5, label='organic')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "In this code:\n",
    "\n",
    "We filter the DataFrame for conventional and organic types and plot their histograms.\n",
    "We set the alpha parameter to make the histograms slightly transparent for better visualization.\n",
    "We add a legend to distinguish between conventional and organic avocados.\n",
    "Finally, we show the plot.\n",
    "\n",
    "2/2\n",
    "\n",
    "Incorrect submission\n",
    "Are you creating a histogram of the average price of conventional avocados?\n",
    "ChatGPT\n",
    "Let's correct this by ensuring we create a histogram for the average price of both conventional and organic avocados correctly.\n",
    "\n",
    "Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the avocados DataFrame is available\n",
    "# Example data for demonstration purposes\n",
    "data = {\n",
    "    'type': ['conventional', 'conventional', 'organic', 'organic', 'conventional', 'organic'],\n",
    "    'avg_price': [1.10, 1.20, 1.50, 1.60, 1.15, 1.55]\n",
    "}\n",
    "avocados = pd.DataFrame(data)\n",
    "\n",
    "# Subset avocados for the \"conventional\" type and create a histogram of the avg_price column\n",
    "avocados[avocados['type'] == 'conventional']['avg_price'].hist(alpha=0.5, label='conventional')\n",
    "\n",
    "# Subset avocados for the \"organic\" type and create a histogram of the avg_price column\n",
    "avocados[avocados['type'] == 'organic']['avg_price'].hist(alpha=0.5, label='organic')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "In this code:\n",
    "\n",
    "We filter the DataFrame for conventional avocados and plot their histogram.\n",
    "We filter the DataFrame for organic avocados and plot their histogram.\n",
    "We add a legend to distinguish between the two histograms.\n",
    "Finally, we show the plot.\n",
    "Hint\n",
    "Subset rows using avocados[\"type\"] == \"conventional\".\n",
    "Remember to select the avg_price column after subsetting and before plotting.\n",
    "Call .hist() to draw a histogram.\n",
    "Your plot won't appear until you display it using a function from plt.\n",
    "ChatGPT\n",
    "Thank you for the hint. Here’s how you can create the histograms correctly by subsetting the data, selecting the avg_price column, and then plotting the histograms:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Example data for demonstration purposes\n",
    "data = {\n",
    "    'type': ['conventional', 'conventional', 'organic', 'organic', 'conventional', 'organic'],\n",
    "    'avg_price': [1.10, 1.20, 1.50, 1.60, 1.15, 1.55]\n",
    "}\n",
    "avocados = pd.DataFrame(data)\n",
    "\n",
    "# Subset avocados for the \"conventional\" type and create a histogram of the avg_price column\n",
    "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist(alpha=0.5, label=\"conventional\")\n",
    "\n",
    "# Subset avocados for the \"organic\" type and create a histogram of the avg_price column\n",
    "avocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist(alpha=0.5, label=\"organic\")\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "In this code:\n",
    "\n",
    "We filter the DataFrame for conventional avocados and select the avg_price column to create a histogram.\n",
    "We filter the DataFrame for organic avocados and select the avg_price column to create a histogram.\n",
    "We add a legend to distinguish between conventional and organic avocados.\n",
    "Finally, we display the plot using plt.show().\n",
    "solution\n",
    "# Histogram of conventional avg_price \n",
    "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist()\n",
    "\n",
    "# Histogram of organic avg_price\n",
    "avocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist()\n",
    "\n",
    "# Add a legend\n",
    "plt.legend([\"conventional\", \"organic\"])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "ChatGPT\n",
    "That looks correct! The histograms for both conventional and organic avocado prices are plotted, and the legend correctly identifies the two groups. Here’s a brief overview of the code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Example data for demonstration purposes\n",
    "data = {\n",
    "    'type': ['conventional', 'conventional', 'organic', 'organic', 'conventional', 'organic'],\n",
    "    'avg_price': [1.10, 1.20, 1.50, 1.60, 1.15, 1.55]\n",
    "}\n",
    "avocados = pd.DataFrame(data)\n",
    "\n",
    "# Histogram of conventional avg_price \n",
    "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist(alpha=0.5, label=\"conventional\")\n",
    "\n",
    "# Histogram of organic avg_price\n",
    "avocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist(alpha=0.5, label=\"organic\")\n",
    "\n",
    "# Add a legend\n",
    "plt.legend([\"conventional\", \"organic\"])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "This approach ensures that the histograms are displayed with the proper labels and that the plot is clearly distinguishable for both avocado types.\n",
    "\n",
    "\n",
    "\n",
    "Sent by you:\n",
    "'''Price of conventional vs. organic avocados\n",
    "Creating multiple plots for different subsets of data allows you to compare groups. In this exercise, you'll create multiple histograms to compare the prices of conventional and organic avocados.\n",
    "\n",
    "matplotlib.pyplot has been imported as plt and pandas has been imported as pd.\n",
    "\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "1\n",
    "2\n",
    "3\n",
    "Subset avocados for the \"conventional\" type and create a histogram of the avg_price column.\n",
    "Create a histogram of avg_price for \"organic\" type avocados.\n",
    "Add a legend to your plot, with the names \"conventional\" and \"organic\".\n",
    "Show your plot.'''\n",
    "\n",
    "# Histogram of conventional avg_price \n",
    "avocados[____][____].____\n",
    "\n",
    "# Histogram of organic avg_price\n",
    "avocados[____][____].____\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(____)\n",
    "\n",
    "# Show the plot\n",
    "____\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "Here’s how you can complete the code to create histograms for the average prices of conventional and organic avocados:\n",
    "\n",
    "Python\n",
    "\n",
    "# Histogram of conventional avg_price \n",
    "avocados[avocados['type'] == 'conventional']['avg_price'].hist(alpha=0.5, label='conventional')\n",
    "\n",
    "# Histogram of organic avg_price\n",
    "avocados[avocados['type'] == 'organic']['avg_price'].hist(alpha=0.5, label='organic')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(['conventional', 'organic'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This code will create overlapping histograms for the average prices of conventional and organic avocados, allowing you to compare their distributions. The alpha parameter makes the histograms semi-transparent so you can see both distributions clearly.\n",
    "\n",
    "You\n",
    "Sent by you:\n",
    "Incorrect submission\n",
    "Are you creating a histogram of the average price of conventional avocados?\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "Yes, the code provided creates a histogram of the average price of conventional avocados. Here’s a more detailed breakdown:\n",
    "\n",
    "Subset avocados for the “conventional” type and create a histogram of the avg_price column:\n",
    "Python\n",
    "\n",
    "avocados[avocados['type'] == 'conventional']['avg_price'].hist(alpha=0.5, label='conventional')\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "Create a histogram of avg_price for “organic” type avocados:\n",
    "Python\n",
    "\n",
    "avocados[avocados['type'] == 'organic']['avg_price'].hist(alpha=0.5, label='organic')\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "Add a legend to your plot, with the names “conventional” and “organic”:\n",
    "Python\n",
    "\n",
    "plt.legend(['conventional', 'organic'])\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "Show your plot:\n",
    "Python\n",
    "\n",
    "plt.show()\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This code will generate overlapping histograms for the average prices of conventional and organic avocados, allowing you to compare their distributions. If you need further adjustments or have any specific requirements, feel free to let me know!\n",
    "\n",
    "You\n",
    "Sent by you:\n",
    "get me the complete code\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "Sure! Here’s the complete code to create histograms for the average prices of conventional and organic avocados:\n",
    "\n",
    "Python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'avocados' is your DataFrame\n",
    "\n",
    "# Histogram of conventional avg_price \n",
    "avocados[avocados['type'] == 'conventional']['avg_price'].hist(alpha=0.5, bins=20, label='conventional')\n",
    "\n",
    "# Histogram of organic avg_price\n",
    "avocados[avocados['type'] == 'organic']['avg_price'].hist(alpha=0.5, bins=20, label='organic')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(['conventional', 'organic'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This code will generate overlapping histograms with 20 bins for the average prices of conventional and organic avocados, allowing you to compare their distributions. The alpha parameter makes the histograms semi-transparent so you can see both distributions clearly.\n",
    "\n",
    "Learn more\n",
    "\n",
    "1\n",
    "\n",
    "github.com\n",
    "2\n",
    "\n",
    "github.com\n",
    "3\n",
    "\n",
    "github.com\n",
    "4\n",
    "\n",
    "github.com\n",
    "You\n",
    "Sent by you:\n",
    "Hint\n",
    "Subset rows using avocados[\"type\"] == \"conventional\".\n",
    "Remember to select the avg_price column after subsetting and before plotting.\n",
    "Call .hist() to draw a histogram.\n",
    "Your plot won't appear until you display it using a function from plt.\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "Got it! Here’s the complete code with the hints incorporated:\n",
    "\n",
    "Python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'avocados' is your DataFrame\n",
    "\n",
    "# Subset avocados for the \"conventional\" type and create a histogram of the avg_price column\n",
    "avocados[avocados['type'] == 'conventional']['avg_price'].hist(alpha=0.5, bins=20, label='conventional')\n",
    "\n",
    "# Subset avocados for the \"organic\" type and create a histogram of the avg_price column\n",
    "avocados[avocados['type'] == 'organic']['avg_price'].hist(alpha=0.5, bins=20, label='organic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'avocados' is your DataFrame\n",
    "\n",
    "# Modify histogram transparency to 0.5 for conventional avg_price\n",
    "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist(alpha=0.5)\n",
    "\n",
    "# Modify histogram transparency to 0.5 for organic avg_price\n",
    "avocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist(alpha=0.5)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend([\"conventional\", \"organic\"])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correct solution\n",
    "# Histogram of conventional avg_price \n",
    "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist()\n",
    "\n",
    "# Histogram of organic avg_price\n",
    "avocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist()\n",
    "\n",
    "# Add a legend\n",
    "plt.legend([\"conventional\", \"organic\"])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "Great! Your solution is correct. Here’s the complete code for clarity:\n",
    "\n",
    "Python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'avocados' is your DataFrame\n",
    "\n",
    "# Histogram of conventional avg_price \n",
    "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist()\n",
    "\n",
    "# Histogram of organic avg_price\n",
    "avocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist()\n",
    "\n",
    "# Add a legend\n",
    "plt.legend([\"conventional\", \"organic\"])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This code will create separate histograms for the average prices of conventional and organic avocados, and display them on the same plot with a legend. If you have any more questions or need further assistance, feel free to ask!\n",
    "\n",
    "Learn more\n",
    "\n",
    "1\n",
    "\n",
    "github.com\n",
    "2\n",
    "\n",
    "github.com\n",
    "3\n",
    "\n",
    "github.com\n",
    "You\n",
    "Sent by you:\n",
    "# Modify your code to adjust the transparency of both histograms to 0.5 to see how much overlap there is between the two distributions.\n",
    "\n",
    "# Modify histogram transparency to 0.5 \n",
    "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist()\n",
    "\n",
    "# Modify histogram transparency to 0.5\n",
    "avocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist()\n",
    "\n",
    "# Add a legend\n",
    "plt.legend([\"conventional\", \"organic\"])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "To adjust the transparency of both histograms to 0.5, you can use the alpha parameter in the hist() function. Here’s the modified code:\n",
    "\n",
    "Python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'avocados' is your DataFrame\n",
    "\n",
    "# Modify histogram transparency to 0.5 for conventional avg_price\n",
    "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist(alpha=0.5)\n",
    "\n",
    "# Modify histogram transparency to 0.5 for organic avg_price\n",
    "avocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist(alpha=0.5)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend([\"conventional\", \"organic\"])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This will make both histograms semi-transparent, allowing you to see how much overlap there is between the two distributions. If you have any more questions or need further adjustments, feel free to ask!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'avocados' is your DataFrame\n",
    "\n",
    "# Modify histogram transparency to 0.5 and bins to 20 for conventional avg_price\n",
    "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist(alpha=0.5, bins=20)\n",
    "\n",
    "# Modify histogram transparency to 0.5 and bins to 20 for organic avg_price\n",
    "avocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist(alpha=0.5, bins=20)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend([\"conventional\", \"organic\"])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "# Great layering! We can see that on average, organic avocados are more expensive than conventional ones, but their price distributions have some overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You\n",
    "Sent by you:\n",
    "# Modify your code to use 20 bins in both histograms.\n",
    "\n",
    "# Modify bins to 20\n",
    "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist(alpha=0.5)\n",
    "\n",
    "# Modify bins to 20\n",
    "avocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist(alpha=0.5)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend([\"conventional\", \"organic\"])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "To modify the code to use 20 bins in both histograms, you can add the bins=20 parameter to the hist() function. Here’s the updated code:\n",
    "\n",
    "Python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'avocados' is your DataFrame\n",
    "\n",
    "# Modify histogram transparency to 0.5 and bins to 20 for conventional avg_price\n",
    "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist(alpha=0.5, bins=20)\n",
    "\n",
    "# Modify histogram transparency to 0.5 and bins to 20 for organic avg_price\n",
    "avocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist(alpha=0.5, bins=20)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend([\"conventional\", \"organic\"])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This code will create histograms with 20 bins for both conventional and organic avocados, with transparency set to 0.5, allowing you to see the overlap between the two distributions. If you need any further adjustments, feel free to ask!\n",
    "\n",
    "Learn more\n",
    "\n",
    "1\n",
    "\n",
    "github.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isna())\n",
    "\n",
    "# Check each column for missing values\n",
    "print(avocados_2016.isna().any())\n",
    "\n",
    "# Bar plot of missing values by variable\n",
    "avocados_2016.isna().sum().plot(kind=\"bar\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Marvelous missing value detection! It looks like there are missing values in the small_sold, large_sold, and xl_sold columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint\n",
    "Use the .isna() method to check each individual value for missingness.\n",
    "The .any() method will take a DataFrame of Booleans and flatten it to indicate if there are any True values in each column.\n",
    "The .sum() method can be used on a DataFrame of Booleans to count the number of True values in each column.\n",
    "Call .plot(), setting kind to \"bar\" to draw a bar plot.\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "It sounds like you’re working with a DataFrame in Python, likely using pandas. Here’s a step-by-step guide to achieve what you’re aiming for:\n",
    "\n",
    "Check for missing values using .isna().\n",
    "Identify columns with any missing values using .any().\n",
    "Count the number of missing values in each column using .sum().\n",
    "Plot the results using .plot() with kind=\"bar\".\n",
    "Here’s a sample code snippet to illustrate these steps:\n",
    "\n",
    "Python\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'A': [1, 2, None, 4],\n",
    "    'B': [None, 2, 3, 4],\n",
    "    'C': [1, None, None, 4]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Check for missing values\n",
    "missing_values = df.isna()\n",
    "\n",
    "# Step 2: Identify columns with any missing values\n",
    "columns_with_missing = missing_values.any()\n",
    "\n",
    "# Step 3: Count the number of missing values in each column\n",
    "missing_count = missing_values.sum()\n",
    "\n",
    "# Step 4: Plot the results\n",
    "missing_count.plot(kind='bar')\n",
    "plt.title('Number of Missing Values per Column')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Number of Missing Values')\n",
    "plt.show()\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This code will create a bar plot showing the number of missing values in each column of your DataFrame. Let me know if you need any further assistance!\n",
    "\n",
    "You\n",
    "Sent by you:\n",
    "correction\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isna())\n",
    "\n",
    "# Check each column for missing values\n",
    "print(avocados_2016.isna().any())\n",
    "\n",
    "# Bar plot of missing values by variable\n",
    "avocados_2016.isna().sum().plot(kind=\"bar\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "Your code looks almost perfect! Here are a couple of minor corrections and improvements:\n",
    "\n",
    "Ensure that avocados_2016 is defined and contains your DataFrame.\n",
    "You might want to add comments for clarity.\n",
    "Here’s the corrected version:\n",
    "\n",
    "Python\n",
    "\n",
    "# Import pandas and matplotlib.pyplot with alias plt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample DataFrame (replace this with your actual DataFrame)\n",
    "avocados_2016 = pd.DataFrame({\n",
    "    'Date': ['2020-01-01', '2020-01-02', '2020-01-03'],\n",
    "    'AveragePrice': [1.00, 1.20, None],\n",
    "    'Total Volume': [15000, None, 20000],\n",
    "    '4046': [None, 3000, 4000],\n",
    "    '4225': [5000, 6000, None],\n",
    "    '4770': [None, None, None]\n",
    "})\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isna())\n",
    "\n",
    "# Check each column for missing values\n",
    "print(avocados_2016.isna().any())\n",
    "\n",
    "# Bar plot of missing values by variable\n",
    "avocados_2016.isna().sum().plot(kind=\"bar\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This code will print the DataFrame showing missing values, indicate which columns have any missing values, and display a bar plot of the number of missing values per column. Let me know if you need any further assistance!\n",
    "\n",
    "\n",
    "'''Finding missing values\n",
    "Missing values are everywhere, and you don't want them interfering with your work. Some functions ignore missing data by default, but that's not always the behavior you might want. Some functions can't handle missing values at all, so these values need to be taken care of before you can use them. If you don't know where your missing values are, or if they exist, you could make mistakes in your analysis. In this exercise, you'll determine if there are missing values in the dataset, and if so, how many.\n",
    "\n",
    "pandas has been imported as pd and avocados_2016, a subset of avocados that contains only sales from 2016, is available.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Print a DataFrame that shows whether each value in avocados_2016 is missing or not.\n",
    "Print a summary that shows whether any value in each column is missing or not.\n",
    "Create a bar plot of the total number of missing values in each column.'''\n",
    "\n",
    "\n",
    "\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(____)\n",
    "\n",
    "# Check each column for missing values\n",
    "print(____)\n",
    "\n",
    "# Bar plot of missing values by variable\n",
    "____\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "ChatGPT\n",
    "To complete the exercise, you'll need to use the appropriate pandas functions to check for missing values and create a bar plot. Here's how you can fill in the blanks:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isnull())\n",
    "\n",
    "# Check each column for missing values\n",
    "print(avocados_2016.isnull().any())\n",
    "\n",
    "# Bar plot of missing values by variable\n",
    "avocados_2016.isnull().sum().plot(kind='bar')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "Here's a breakdown of the code:\n",
    "\n",
    "avocados_2016.isnull() creates a DataFrame of the same shape as avocados_2016 but with True for missing values and False for non-missing values.\n",
    "avocados_2016.isnull().any() returns a Series indicating whether each column contains any missing values.\n",
    "avocados_2016.isnull().sum() calculates the total number of missing values in each column, and .plot(kind='bar') creates a bar plot of these totals.\n",
    "Incorrect submission\n",
    "Did you use the correct method to detect missing values in avocados_2016?\n",
    "ChatGPT\n",
    "To address missing values correctly in pandas, ensure you're using the appropriate methods. Here’s a revised approach:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isnull())\n",
    "\n",
    "# Check if any values in each column are missing\n",
    "print(avocados_2016.isnull().sum())\n",
    "\n",
    "# Bar plot of missing values by column\n",
    "avocados_2016.isnull().sum().plot(kind='bar')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "In this approach:\n",
    "\n",
    "avocados_2016.isnull() returns a DataFrame with boolean values indicating missing data.\n",
    "avocados_2016.isnull().sum() provides a Series with the count of missing values for each column, which is used for the bar plot.\n",
    ".plot(kind='bar') generates a bar plot of the count of missing values by column.\n",
    "Hint\n",
    "Use the .isna() method to check each individual value for missingness.\n",
    "The .any() method will take a DataFrame of Booleans and flatten it to indicate if there are any True values in each column.\n",
    "The .sum() method can be used on a DataFrame of Booleans to count the number of True values in each column.\n",
    "Call .plot(), setting kind to \"bar\" to draw a bar plot.\n",
    "ChatGPT\n",
    "Memory updated\n",
    "Here's how you can complete the exercise using the provided hints:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isna())\n",
    "\n",
    "# Check if any values in each column are missing\n",
    "print(avocados_2016.isna().any())\n",
    "\n",
    "# Count the number of missing values in each column\n",
    "missing_values_count = avocados_2016.isna().sum()\n",
    "\n",
    "# Bar plot of missing values by column\n",
    "missing_values_count.plot(kind='bar')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "This script:\n",
    "\n",
    "Uses isna() to detect missing values.\n",
    "Applies any() to check if there are any missing values in each column.\n",
    "Uses sum() to count the missing values in each column and plots the result as a bar chart.\n",
    "Incorrect submission\n",
    "Did you use .plot() to create a bar plot of the total number of missing values?\n",
    "\n",
    " \n",
    "ChatGPT\n",
    "It looks like the .plot() method needs to be used directly on the result of sum() to create the bar plot. Here’s the corrected code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isna())\n",
    "\n",
    "# Check if any values in each column are missing\n",
    "print(avocados_2016.isna().any())\n",
    "\n",
    "# Count the number of missing values in each column\n",
    "missing_values_count = avocados_2016.isna().sum()\n",
    "\n",
    "# Bar plot of missing values by column\n",
    "missing_values_count.plot(kind='bar')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "Make sure to:\n",
    "\n",
    "Store the result of isna().sum() in a variable (e.g., missing_values_count).\n",
    "Use missing_values_count.plot(kind='bar') to generate the bar plot.\n",
    "Incorrect submission\n",
    "Did you use .plot() to create a bar plot of the total number of missing values?\n",
    "\n",
    " \n",
    "ChatGPT\n",
    "It sounds like there might have been an issue with the way the bar plot was created. Let’s ensure we use the .plot() method correctly.\n",
    "\n",
    "Here’s a revised version:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isna())\n",
    "\n",
    "# Check if any values in each column are missing\n",
    "print(avocados_2016.isna().any())\n",
    "\n",
    "# Count the number of missing values in each column\n",
    "missing_values_count = avocados_2016.isna().sum()\n",
    "\n",
    "# Create a bar plot of the total number of missing values\n",
    "missing_values_count.plot(kind='bar')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "In this code:\n",
    "\n",
    "missing_values_count is a Series with the count of missing values in each column.\n",
    "missing_values_count.plot(kind='bar') creates the bar plot.\n",
    "plt.show() displays the plot.\n",
    "correction\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isna())\n",
    "\n",
    "# Check each column for missing values\n",
    "print(avocados_2016.isna().any())\n",
    "\n",
    "# Bar plot of missing values by variable\n",
    "avocados_2016.isna().sum().plot(kind=\"bar\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "ChatGPT\n",
    "That correction looks good! Here’s a summary of the final code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isna())\n",
    "\n",
    "# Check each column for missing values\n",
    "print(avocados_2016.isna().any())\n",
    "\n",
    "# Bar plot of missing values by variable\n",
    "avocados_2016.isna().sum().plot(kind=\"bar\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "This code effectively:\n",
    "\n",
    "Uses isna() to identify missing values.\n",
    "Uses any() to check if any missing values exist in each column.\n",
    "Counts the missing values in each column and creates a bar plot to visualize the results.\n",
    "\n",
    "'''Print a DataFrame that shows whether each value in avocados_2016 is missing or not.\n",
    "Print a summary that shows whether any value in each column is missing or not.\n",
    "Create a bar plot of the total number of missing values in each column.'''\n",
    "\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(____)\n",
    "\n",
    "# Check each column for missing values\n",
    "print(____)\n",
    "\n",
    "# Bar plot of missing values by variable\n",
    "____\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values\n",
    "avocados_complete = avocados_2016.dropna()\n",
    "\n",
    "# Check if any columns contain missing values\n",
    "print(avocados_complete.isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "'''Finding missing values\n",
    "Missing values are everywhere, and you don't want them interfering with your work. Some functions ignore missing data by default, but that's not always the behavior you might want. Some functions can't handle missing values at all, so these values need to be taken care of before you can use them. If you don't know where your missing values are, or if they exist, you could make mistakes in your analysis. In this exercise, you'll determine if there are missing values in the dataset, and if so, how many.\n",
    "\n",
    "pandas has been imported as pd and avocados_2016, a subset of avocados that contains only sales from 2016, is available.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Print a DataFrame that shows whether each value in avocados_2016 is missing or not.\n",
    "Print a summary that shows whether any value in each column is missing or not.\n",
    "Create a bar plot of the total number of missing values in each column.'''\n",
    "\n",
    "\n",
    "\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(____)\n",
    "\n",
    "# Check each column for missing values\n",
    "print(____)\n",
    "\n",
    "# Bar plot of missing values by variable\n",
    "____\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "ChatGPT\n",
    "To complete the exercise, you'll need to use the appropriate pandas functions to check for missing values and create a bar plot. Here's how you can fill in the blanks:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isnull())\n",
    "\n",
    "# Check each column for missing values\n",
    "print(avocados_2016.isnull().any())\n",
    "\n",
    "# Bar plot of missing values by variable\n",
    "avocados_2016.isnull().sum().plot(kind='bar')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "Here's a breakdown of the code:\n",
    "\n",
    "avocados_2016.isnull() creates a DataFrame of the same shape as avocados_2016 but with True for missing values and False for non-missing values.\n",
    "avocados_2016.isnull().any() returns a Series indicating whether each column contains any missing values.\n",
    "avocados_2016.isnull().sum() calculates the total number of missing values in each column, and .plot(kind='bar') creates a bar plot of these totals.\n",
    "Incorrect submission\n",
    "Did you use the correct method to detect missing values in avocados_2016?\n",
    "ChatGPT\n",
    "To address missing values correctly in pandas, ensure you're using the appropriate methods. Here’s a revised approach:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isnull())\n",
    "\n",
    "# Check if any values in each column are missing\n",
    "print(avocados_2016.isnull().sum())\n",
    "\n",
    "# Bar plot of missing values by column\n",
    "avocados_2016.isnull().sum().plot(kind='bar')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "In this approach:\n",
    "\n",
    "avocados_2016.isnull() returns a DataFrame with boolean values indicating missing data.\n",
    "avocados_2016.isnull().sum() provides a Series with the count of missing values for each column, which is used for the bar plot.\n",
    ".plot(kind='bar') generates a bar plot of the count of missing values by column.\n",
    "Hint\n",
    "Use the .isna() method to check each individual value for missingness.\n",
    "The .any() method will take a DataFrame of Booleans and flatten it to indicate if there are any True values in each column.\n",
    "The .sum() method can be used on a DataFrame of Booleans to count the number of True values in each column.\n",
    "Call .plot(), setting kind to \"bar\" to draw a bar plot.\n",
    "ChatGPT\n",
    "Memory updated\n",
    "Here's how you can complete the exercise using the provided hints:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isna())\n",
    "\n",
    "# Check if any values in each column are missing\n",
    "print(avocados_2016.isna().any())\n",
    "\n",
    "# Count the number of missing values in each column\n",
    "missing_values_count = avocados_2016.isna().sum()\n",
    "\n",
    "# Bar plot of missing values by column\n",
    "missing_values_count.plot(kind='bar')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "This script:\n",
    "\n",
    "Uses isna() to detect missing values.\n",
    "Applies any() to check if there are any missing values in each column.\n",
    "Uses sum() to count the missing values in each column and plots the result as a bar chart.\n",
    "Incorrect submission\n",
    "Did you use .plot() to create a bar plot of the total number of missing values?\n",
    "\n",
    " \n",
    "ChatGPT\n",
    "It looks like the .plot() method needs to be used directly on the result of sum() to create the bar plot. Here’s the corrected code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isna())\n",
    "\n",
    "# Check if any values in each column are missing\n",
    "print(avocados_2016.isna().any())\n",
    "\n",
    "# Count the number of missing values in each column\n",
    "missing_values_count = avocados_2016.isna().sum()\n",
    "\n",
    "# Bar plot of missing values by column\n",
    "missing_values_count.plot(kind='bar')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "Make sure to:\n",
    "\n",
    "Store the result of isna().sum() in a variable (e.g., missing_values_count).\n",
    "Use missing_values_count.plot(kind='bar') to generate the bar plot.\n",
    "Incorrect submission\n",
    "Did you use .plot() to create a bar plot of the total number of missing values?\n",
    "\n",
    " \n",
    "ChatGPT\n",
    "It sounds like there might have been an issue with the way the bar plot was created. Let’s ensure we use the .plot() method correctly.\n",
    "\n",
    "Here’s a revised version:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isna())\n",
    "\n",
    "# Check if any values in each column are missing\n",
    "print(avocados_2016.isna().any())\n",
    "\n",
    "# Count the number of missing values in each column\n",
    "missing_values_count = avocados_2016.isna().sum()\n",
    "\n",
    "# Create a bar plot of the total number of missing values\n",
    "missing_values_count.plot(kind='bar')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "In this code:\n",
    "\n",
    "missing_values_count is a Series with the count of missing values in each column.\n",
    "missing_values_count.plot(kind='bar') creates the bar plot.\n",
    "plt.show() displays the plot.\n",
    "correction\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isna())\n",
    "\n",
    "# Check each column for missing values\n",
    "print(avocados_2016.isna().any())\n",
    "\n",
    "# Bar plot of missing values by variable\n",
    "avocados_2016.isna().sum().plot(kind=\"bar\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "ChatGPT\n",
    "That correction looks good! Here’s a summary of the final code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isna())\n",
    "\n",
    "# Check each column for missing values\n",
    "print(avocados_2016.isna().any())\n",
    "\n",
    "# Bar plot of missing values by variable\n",
    "avocados_2016.isna().sum().plot(kind=\"bar\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "This code effectively:\n",
    "\n",
    "Uses isna() to identify missing values.\n",
    "Uses any() to check if any missing values exist in each column.\n",
    "Counts the missing values in each column and creates a bar plot to visualize the results.\n",
    "'''Removing missing values\n",
    "Now that you know there are some missing values in your DataFrame, you have a few options to deal with them. One way is to remove them from the dataset completely. In this exercise, you'll remove missing values by removing all rows that contain missing values.\n",
    "\n",
    "pandas has been imported as pd and avocados_2016 is available.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Remove the rows of avocados_2016 that contain missing values and store the remaining rows in avocados_complete.\n",
    "Verify that all missing values have been removed from avocados_complete. Calculate each column that has NAs and print.'''\n",
    "\n",
    "\n",
    "# Remove rows with missing values\n",
    "avocados_complete = ____\n",
    "\n",
    "# Check if any columns contain missing values\n",
    "print(____)\n",
    "ChatGPT\n",
    "Here’s how you can complete the exercise:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Remove rows with missing values\n",
    "avocados_complete = avocados_2016.dropna()\n",
    "\n",
    "# Check if any columns contain missing values\n",
    "print(avocados_complete.isna().sum())\n",
    "Explanation:\n",
    "\n",
    "avocados_2016.dropna() removes any rows that contain missing values and stores the cleaned DataFrame in avocados_complete.\n",
    "avocados_complete.isna().sum() checks each column in avocados_complete for missing values and prints the count of missing values in each column.\n",
    "Incorrect submission\n",
    "Did you use use the correct method to print a summary of columns containing missing values?\n",
    "ChatGPT\n",
    "To ensure you accurately print a summary of columns containing missing values after removing rows with missing values, use the .isna() method along with .sum(). Here’s the corrected approach:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Remove rows with missing values\n",
    "avocados_complete = avocados_2016.dropna()\n",
    "\n",
    "# Check if any columns contain missing values in the cleaned DataFrame\n",
    "print(avocados_complete.isna().sum())\n",
    "Here’s a breakdown:\n",
    "\n",
    "avocados_2016.dropna() removes rows with any missing values.\n",
    "avocados_complete.isna().sum() will now show if there are any missing values left in each column of avocados_complete. If the output is all zeros, it means there are no missing values remaining.\n",
    "Remove the rows of avocados_2016 that contain missing values and store the remaining rows in avocados_complete.\n",
    "Verify that all missing values have been removed from avocados_complete. Calculate each column that has NAs and print.\n",
    "ChatGPT\n",
    "To remove rows with missing values and verify that all missing values have been removed from avocados_complete, you can use the following code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Remove rows with missing values\n",
    "avocados_complete = avocados_2016.dropna()\n",
    "\n",
    "# Verify that all missing values have been removed\n",
    "missing_values_summary = avocados_complete.isna().sum()\n",
    "\n",
    "# Print the summary of missing values\n",
    "print(missing_values_summary)\n",
    "This code does the following:\n",
    "\n",
    "dropna() removes rows with any missing values and stores the result in avocados_complete.\n",
    "isna().sum() calculates the number of missing values remaining in each column of avocados_complete.\n",
    "print(missing_values_summary) outputs the number of missing values for each column, which should be zeros if all missing values have been successfully removed.\n",
    "Remove the rows of avocados_2016 that contain missing values and store the remaining rows in avocados_complete.\n",
    "Verify that all missing values have been removed from avocados_complete. Calculate each column that has NAs and print.\n",
    "ChatGPT\n",
    "Here's how to remove rows with missing values from avocados_2016, store the result in avocados_complete, and verify that there are no remaining missing values:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Remove rows with missing values\n",
    "avocados_complete = avocados_2016.dropna()\n",
    "\n",
    "# Check for missing values in the cleaned DataFrame\n",
    "missing_values_summary = avocados_complete.isna().sum()\n",
    "\n",
    "# Print the summary of missing values\n",
    "print(missing_values_summary)\n",
    "Explanation:\n",
    "Remove Rows with Missing Values: avocados_2016.dropna() removes any rows that contain missing values. The result is stored in avocados_complete.\n",
    "Verify Missing Values: avocados_complete.isna().sum() calculates the number of missing values in each column of avocados_complete.\n",
    "Print Summary: print(missing_values_summary) outputs the number of missing values per column. If there are no missing values, all entries in the output will be zeros.\n",
    "correction\n",
    "# Remove rows with missing values\n",
    "avocados_complete = avocados_2016.dropna()\n",
    "\n",
    "# Check if any columns contain missing values\n",
    "print(avocados_complete.isna().any())\n",
    "ChatGPT\n",
    "Your correction correctly checks if any columns in avocados_complete contain missing values after removing rows with missing values. Here’s the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Remove rows with missing values\n",
    "avocados_complete = avocados_2016.dropna()\n",
    "\n",
    "# Check if any columns contain missing values\n",
    "print(avocados_complete.isna().any())\n",
    "Explanation:\n",
    "avocados_2016.dropna() removes all rows with missing values and stores the cleaned DataFrame in avocados_complete.\n",
    "avocados_complete.isna().any() checks if there are any missing values in each column of the cleaned DataFrame and returns a Boolean Series indicating which columns still have missing values (if any). If all values are False, it means there are no missing values left.\n",
    "\n",
    "'''Removing missing values\n",
    "Now that you know there are some missing values in your DataFrame, you have a few options to deal with them. One way is to remove them from the dataset completely. In this exercise, you'll remove missing values by removing all rows that contain missing values.\n",
    "\n",
    "pandas has been imported as pd and avocados_2016 is available.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Remove the rows of avocados_2016 that contain missing values and store the remaining rows in avocados_complete.\n",
    "Verify that all missing values have been removed from avocados_complete. Calculate each column that has NAs and print.'''\n",
    "\n",
    "\n",
    "# Remove rows with missing values\n",
    "avocados_complete = ____\n",
    "\n",
    "# Check if any columns contain missing values\n",
    "print(____)\n",
    "\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "To remove rows with missing values and verify that all missing values have been removed, you can use the following code:\n",
    "\n",
    "Python\n",
    "\n",
    "# Remove rows with missing values\n",
    "avocados_complete = avocados_2016.dropna()\n",
    "\n",
    "# Check if any columns contain missing values\n",
    "print(avocados_complete.isna().sum())\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This code will:\n",
    "\n",
    "Remove all rows that contain any missing values and store the remaining rows in avocados_complete.\n",
    "Print the count of missing values in each column of avocados_complete to verify that all missing values have been removed.\n",
    "Let me know if you need any further assistance!\n",
    "\n",
    "You\n",
    "Sent by you:\n",
    "hints \n",
    "Remove the rows of avocados_2016 that contain missing values and store the remaining rows in avocados_complete.\n",
    "Verify that all missing values have been removed from avocados_complete. Calculate each column that has NAs and print.\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "Sure, here are the hints to help you complete the task:\n",
    "\n",
    "Remove rows with missing values:\n",
    "Use the .dropna() method on your DataFrame to remove rows that contain any missing values.\n",
    "Store the result in a new DataFrame called avocados_complete.\n",
    "Verify that all missing values have been removed:\n",
    "Use the .isna().sum() method on avocados_complete to count the number of missing values in each column.\n",
    "Print the result to ensure there are no missing values left.\n",
    "Here’s a step-by-step breakdown:\n",
    "\n",
    "Remove rows with missing values:\n",
    "Python\n",
    "\n",
    "avocados_complete = avocados_2016.dropna()\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "Verify that all missing values have been removed:\n",
    "Python\n",
    "\n",
    "print(avocados_complete.isna().sum())\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This will help you ensure that all rows with missing values are removed and verify that your new DataFrame avocados_complete has no missing values. Let me know if you need any further assistance!\n",
    "Delightful dropping! Removing observations with missing values is a quick and dirty way to deal with missing data, but this can introduce bias to your data if the values are not missing at random.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the columns with missing values\n",
    "cols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\n",
    "\n",
    "# Create histograms showing the distributions of cols_with_missing\n",
    "avocados_2016[cols_with_missing].hist()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From previous step\n",
    "cols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\n",
    "avocados_2016[cols_with_missing].hist()\n",
    "plt.show()\n",
    "\n",
    "# Fill in missing values with 0\n",
    "avocados_filled = avocados_2016.fillna(0)\n",
    "\n",
    "# Create histograms of the filled columns\n",
    "avocados_filled[cols_with_missing].hist()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Fabulous filling! Notice how the distribution has changed shape after replacing missing values with zeros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Replacing missing values\n",
    "Another way of handling missing values is to replace them all with the same value. For numerical variables, one option is to replace values with 0— you'll do this here. However, when you replace missing values, you make assumptions about what a missing value means. In this case, you will assume that a missing number sold means that no sales for that avocado type were made that week.\n",
    "\n",
    "In this exercise, you'll see how replacing missing values can affect the distribution of a variable using histograms. You can plot histograms for multiple variables at a time as follows:\n",
    "\n",
    "dogs[[\"height_cm\", \"weight_kg\"]].hist()\n",
    "pandas has been imported as pd and matplotlib.pyplot has been imported as plt. The avocados_2016 dataset is available.\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "1\n",
    "2\n",
    "A list has been created, cols_with_missing, containing the names of columns with missing values: \"small_sold\", \"large_sold\", and \"xl_sold\".\n",
    "Create a histogram of those columns.\n",
    "Show the plot.'''\n",
    "\n",
    "# List the columns with missing values\n",
    "cols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\n",
    "\n",
    "# Create histograms showing the distributions cols_with_missing\n",
    "avocados_2016[____].____\n",
    "\n",
    "# Show the plot\n",
    "____\n",
    "ChatGPT\n",
    "Here’s how to create histograms for the columns with missing values and show the plot:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# List the columns with missing values\n",
    "cols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\n",
    "\n",
    "# Create histograms showing the distributions of cols_with_missing\n",
    "avocados_2016[cols_with_missing].hist()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "Explanation:\n",
    "avocados_2016[cols_with_missing] selects the columns specified in cols_with_missing.\n",
    ".hist() creates histograms for these columns.\n",
    "plt.show() displays the histograms.\n",
    "'''Replace the missing values of avocados_2016 with 0s and store the result as avocados_filled.\n",
    "Create a histogram of the cols_with_missing columns of avocados_filled'''\n",
    "\n",
    "\n",
    "# From previous step\n",
    "cols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\n",
    "avocados_2016[cols_with_missing].hist()\n",
    "plt.show()\n",
    "\n",
    "# Fill in missing values with 0\n",
    "avocados_filled = ____\n",
    "\n",
    "# Create histograms of the filled columns\n",
    "____\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "ChatGPT\n",
    "To replace missing values with 0 and create histograms for the specified columns in avocados_filled, you can use the following code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# From previous step\n",
    "cols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\n",
    "avocados_2016[cols_with_missing].hist()\n",
    "plt.show()\n",
    "\n",
    "# Fill in missing values with 0\n",
    "avocados_filled = avocados_2016.fillna(0)\n",
    "\n",
    "# Create histograms of the filled columns\n",
    "avocados_filled[cols_with_missing].hist()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "Explanation:\n",
    "avocados_2016.fillna(0) replaces all missing values in avocados_2016 with 0 and stores the result in avocados_filled.\n",
    "avocados_filled[cols_with_missing].hist() creates histograms for the columns listed in cols_with_missing from the avocados_filled DataFrame.\n",
    "plt.show() displays the histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a list of dictionaries with new data\n",
    "avocados_list = [\n",
    "    {'date': '2019-11-03', 'small_sold': 10376832, 'large_sold': 7835071},\n",
    "    {'date': '2019-11-10', 'small_sold': 10717154, 'large_sold': 8561348},\n",
    "]\n",
    "\n",
    "# Convert list into DataFrame\n",
    "avocados_2019 = pd.DataFrame(avocados_list)\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(avocados_2019)\n",
    "# Lovely work with the list-of-dictionaries! The list-of-dictionaries method creates DataFrames row-by-row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''List of dictionaries\n",
    "You recently got some new avocado data from 2019 that you'd like to put in a DataFrame using the list of dictionaries method. Remember that with this method, you go through the data row by row.\n",
    "\n",
    "date\tsmall_sold\tlarge_sold\n",
    "\"2019-11-03\"\t10376832\t7835071\n",
    "\"2019-11-10\"\t10717154\t8561348\n",
    "pandas as pd is imported.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Create a list of dictionaries with the new data called avocados_list.\n",
    "Convert the list into a DataFrame called avocados_2019.\n",
    "Print your new DataFrame.'''\n",
    "\n",
    "\n",
    "# Create a list of dictionaries with new data\n",
    "avocados_list = [\n",
    "    {____: ____, ____: ____, ____: ____},\n",
    "    {____: ____, ____: ____, ____: ____},\n",
    "]\n",
    "\n",
    "# Convert list into DataFrame\n",
    "avocados_2019 = ____\n",
    "\n",
    "# Print the new DataFrame\n",
    "____\n",
    "ChatGPT\n",
    "Here's how you can complete the task using the list of dictionaries method:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Create a list of dictionaries with new data\n",
    "avocados_list = [\n",
    "    {'date': '2019-11-03', 'small_sold': 10376832, 'large_sold': 7835071},\n",
    "    {'date': '2019-11-10', 'small_sold': 10717154, 'large_sold': 8561348},\n",
    "]\n",
    "\n",
    "# Convert list into DataFrame\n",
    "avocados_2019 = pd.DataFrame(avocados_list)\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(avocados_2019)\n",
    "This code creates the avocados_list with the given data, converts it into a DataFrame using pd.DataFrame(), and prints the DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dictionary of lists with new data\n",
    "avocados_dict = {\n",
    "    \"date\": [\"2019-11-17\", \"2019-12-01\"],\n",
    "    \"small_sold\": [10859987, 9291631],\n",
    "    \"large_sold\": [7674135, 6238096]\n",
    "}\n",
    "\n",
    "# Convert dictionary into DataFrame\n",
    "avocados_2019 = pd.DataFrame(avocados_dict)\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(avocados_2019)\n",
    "# Delightful dictionary-of-lists usage! The list-of-dictionaries method creates DataFrames column-by-column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "'''Dictionary of lists\n",
    "Some more data just came in! This time, you'll use the dictionary of lists method, parsing the data column by column.\n",
    "\n",
    "date\tsmall_sold\tlarge_sold\n",
    "\"2019-11-17\"\t10859987\t7674135\n",
    "\"2019-12-01\"\t9291631\t6238096\n",
    "pandas as pd is imported.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Create a dictionary of lists with the new data called avocados_dict.\n",
    "Convert the dictionary to a DataFrame called avocados_2019.\n",
    "Print your new DataFrame.'''\n",
    "\n",
    "\n",
    "# Create a dictionary of lists with new data\n",
    "avocados_dict = {\n",
    "  \"____\": [____],\n",
    "  \"____\": [____],\n",
    "  \"____\": [____]\n",
    "}\n",
    "\n",
    "# Convert dictionary into DataFrame\n",
    "avocados_2019 = ____\n",
    "\n",
    "# Print the new DataFrame\n",
    "____\n",
    "ChatGPT\n",
    "Here's how you can complete the code to create a dictionary of lists from the given data, convert it into a DataFrame, and print the result:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Create a dictionary of lists with new data\n",
    "avocados_dict = {\n",
    "    \"date\": [\"2019-11-17\", \"2019-12-01\"],\n",
    "    \"small_sold\": [10859987, 9291631],\n",
    "    \"large_sold\": [7674135, 6238096]\n",
    "}\n",
    "\n",
    "# Convert dictionary into DataFrame\n",
    "avocados_2019 = pd.DataFrame(avocados_dict)\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(avocados_2019)\n",
    "This code will produce a DataFrame with the specified columns and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV as DataFrame called airline_bumping\n",
    "airline_bumping = pd.read_csv(\"airline_bumping.csv\")\n",
    "\n",
    "# Take a look at the DataFrame\n",
    "print(airline_bumping.head())\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read CSV as DataFrame called airline_bumping\n",
    "airline_bumping = pd.read_csv(\"airline_bumping.csv\")\n",
    "\n",
    "# Print the first few rows of the DataFrame to understand its structure\n",
    "print(airline_bumping.head())\n",
    "\n",
    "# For each airline, select nb_bumped and total_passengers and calculate the sum\n",
    "airline_totals = airline_bumping.groupby('airline')[['nb_bumped', 'total_passengers']].sum()\n",
    "\n",
    "# Print the result to check the totals\n",
    "print(airline_totals)\n",
    "\n",
    "# Create new column, bumps_per_10k: number of bumps per 10k passengers for each airline\n",
    "airline_totals[\"bumps_per_10k\"] = (airline_totals[\"nb_bumped\"] / airline_totals[\"total_passengers\"]) * 10000\n",
    "\n",
    "# Print the updated DataFrame to check the new column\n",
    "print(airline_totals)\n",
    "\n",
    "# Print airline_totals\n",
    "print(airline_totals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''CSV to DataFrame\n",
    "You work for an airline, and your manager has asked you to do a competitive analysis and see how often passengers flying on other airlines are involuntarily bumped from their flights. You got a CSV file (airline_bumping.csv) from the Department of Transportation containing data on passengers that were involuntarily denied boarding in 2016 and 2017, but it doesn't have the exact numbers you want. In order to figure this out, you'll need to get the CSV into a pandas DataFrame and do some manipulation!\n",
    "\n",
    "pandas is imported for you as pd. \"airline_bumping.csv\" is in your working directory.\n",
    "\n",
    "Instructions 1/4\n",
    "25 XP\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "Read the CSV file \"airline_bumping.csv\" and store it as a DataFrame called airline_bumping.\n",
    "Print the first few rows of airline_bumping.'''\n",
    "\n",
    "# Read CSV as DataFrame called airline_bumping\n",
    "airline_bumping = ____\n",
    "\n",
    "# Take a look at the DataFrame\n",
    "print(____)\n",
    "ChatGPT\n",
    "Here’s how you can complete the instructions:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Read CSV as DataFrame called airline_bumping\n",
    "airline_bumping = pd.read_csv(\"airline_bumping.csv\")\n",
    "\n",
    "# Take a look at the DataFrame\n",
    "print(airline_bumping.head())\n",
    "This will read the CSV file into a DataFrame named airline_bumping and then print the first few rows to help you understand its structure.\n",
    "\n",
    "For each airline group, select the nb_bumped, and total_passengers columns, and calculate the sum (for both years). Store this as airline_totals.\n",
    "ChatGPT\n",
    "To calculate the sum of nb_bumped and total_passengers for each airline group, you can use the groupby method followed by sum. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Group by 'airline_group' and calculate the sum of 'nb_bumped' and 'total_passengers'\n",
    "airline_totals = airline_bumping.groupby('airline_group')[['nb_bumped', 'total_passengers']].sum()\n",
    "\n",
    "# Print the result to check the totals\n",
    "print(airline_totals)\n",
    "This code groups the DataFrame by the airline_group column and then calculates the sum for the nb_bumped and total_passengers columns within each group.\n",
    "\n",
    "# For each airline group, select the nb_bumped, and total_passengers columns, and calculate the sum (for both years). Store this as airline_totals.\n",
    "\n",
    "\n",
    "# From previous step\n",
    "airline_bumping = pd.read_csv(\"airline_bumping.csv\")\n",
    "print(airline_bumping.head())\n",
    "\n",
    "# For each airline, select nb_bumped and total_passengers and sum\n",
    "airline_totals = airline_bumping.groupby(____)[[____]].____\n",
    "ChatGPT\n",
    "Here's the completed code with the appropriate placeholders filled in:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# For each airline, select nb_bumped and total_passengers and sum\n",
    "airline_totals = airline_bumping.groupby('airline_group')[['nb_bumped', 'total_passengers']].sum()\n",
    "This will group the data by the airline_group column and then compute the sum for the nb_bumped and total_passengers columns.\n",
    "\n",
    "Incorrect submission\n",
    "Did you define the variable airline_bumping without errors?\n",
    "\n",
    " \n",
    "ChatGPT\n",
    "It looks like there might be an issue with the CSV file or the data itself. Let’s troubleshoot:\n",
    "\n",
    "Check the CSV File: Ensure that the file airline_bumping.csv is present in the working directory and is correctly formatted.\n",
    "\n",
    "Inspect the DataFrame: Print the columns of airline_bumping to make sure airline_group, nb_bumped, and total_passengers are present.\n",
    "\n",
    "Here's an updated version of the code to help with debugging:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Read CSV as DataFrame called airline_bumping\n",
    "airline_bumping = pd.read_csv(\"airline_bumping.csv\")\n",
    "\n",
    "# Print the columns to check if the expected columns are present\n",
    "print(airline_bumping.columns)\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(airline_bumping.head())\n",
    "\n",
    "# For each airline group, select nb_bumped and total_passengers and sum\n",
    "airline_totals = airline_bumping.groupby('airline_group')[['nb_bumped', 'total_passengers']].sum()\n",
    "\n",
    "# Print the result to check the totals\n",
    "print(airline_totals)\n",
    "Make sure that airline_group, nb_bumped, and total_passengers are the exact column names in your CSV file. If they differ, update the column names accordingly.\n",
    "\n",
    "# Incorrect submission\n",
    "# Did you define the variable airline_bumping without errors?\n",
    "\n",
    " \n",
    "# From previous step\n",
    "airline_bumping = pd.read_csv(\"airline_bumping.csv\")\n",
    "print(airline_bumping.head())\n",
    "\n",
    "# For each airline, select nb_bumped and total_passengers and sum\n",
    "airline_totals = airline_bumping.groupby(____)[[____]].____\n",
    "ChatGPT\n",
    "It seems there might be a syntax or logic issue in your code submission. Let's fill in the placeholders with the correct values:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# For each airline group, select nb_bumped and total_passengers and calculate the sum\n",
    "airline_totals = airline_bumping.groupby('airline_group')[['nb_bumped', 'total_passengers']].sum()\n",
    "Here's what each placeholder should be:\n",
    "\n",
    "____ (first placeholder): 'airline_group' — this is the column by which you want to group the data.\n",
    "____ (second placeholder): ['nb_bumped', 'total_passengers'] — these are the columns you want to sum.\n",
    "____ (third placeholder): .sum() — this computes the sum of the grouped data.\n",
    "Incorrect submission\n",
    "Check your call of airline_bumping.groupby(). Did you correctly specify the first argument? Expected 'airline', but got 'airline_group'\n",
    "ChatGPT\n",
    "Memory updated\n",
    "It looks like the correct column to group by is airline, not airline_group. Here’s the corrected code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# For each airline, select nb_bumped and total_passengers and calculate the sum\n",
    "airline_totals = airline_bumping.groupby('airline')[['nb_bumped', 'total_passengers']].sum()\n",
    "\n",
    "# Print the result to check the totals\n",
    "print(airline_totals)\n",
    "get me the complete code\n",
    "ChatGPT\n",
    "Sure! Here is the complete code from reading the CSV to calculating the totals:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Read CSV as DataFrame called airline_bumping\n",
    "airline_bumping = pd.read_csv(\"airline_bumping.csv\")\n",
    "\n",
    "# Print the first few rows of the DataFrame to understand its structure\n",
    "print(airline_bumping.head())\n",
    "\n",
    "# For each airline, select nb_bumped and total_passengers and calculate the sum\n",
    "airline_totals = airline_bumping.groupby('airline')[['nb_bumped', 'total_passengers']].sum()\n",
    "\n",
    "# Print the result to check the totals\n",
    "print(airline_totals)\n",
    "This code:\n",
    "\n",
    "Imports the pandas library.\n",
    "Reads the CSV file into a DataFrame called airline_bumping.\n",
    "Prints the first few rows to inspect the data.\n",
    "Groups the data by the airline column, calculates the sum of nb_bumped and total_passengers, and stores the result in airline_totals.\n",
    "Prints airline_totals to display the aggregated results.\n",
    "'''Create a new column of airline_totals called bumps_per_10k, which is the number of passengers bumped per 10,000 passengers in 2016 and 2017.'''\n",
    "\n",
    "# From previous steps\n",
    "airline_bumping = pd.read_csv(\"airline_bumping.csv\")\n",
    "print(airline_bumping.head())\n",
    "airline_totals = airline_bumping.groupby(\"airline\")[[\"nb_bumped\", \"total_passengers\"]].sum()\n",
    "\n",
    "# Create new col, bumps_per_10k: no. of bumps per 10k passengers for each airline\n",
    "airline_totals[\"bumps_per_10k\"] = ____ / ____ * 10000\n",
    "ChatGPT\n",
    "To create the new column bumps_per_10k, you'll need to divide the number of bumps (nb_bumped) by the total number of passengers (total_passengers) and then multiply by 10,000. Here’s how you can complete the code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Create new column, bumps_per_10k: number of bumps per 10k passengers for each airline\n",
    "airline_totals[\"bumps_per_10k\"] = (airline_totals[\"nb_bumped\"] / airline_totals[\"total_passengers\"]) * 10000\n",
    "\n",
    "# Print the updated DataFrame to check the new column\n",
    "print(airline_totals)\n",
    "This line of code calculates the number of bumps per 10,000 passengers and adds it as a new column bumps_per_10k to the airline_totals DataFrame.\n",
    "\n",
    "# Print airline_totals to see the results of your manipulations.\n",
    "\n",
    "# From previous steps\n",
    "airline_bumping = pd.read_csv(\"airline_bumping.csv\")\n",
    "print(airline_bumping.head())\n",
    "airline_totals = airline_bumping.groupby(\"airline\")[[\"nb_bumped\", \"total_passengers\"]].sum()\n",
    "airline_totals[\"bumps_per_10k\"] = airline_totals[\"nb_bumped\"] / airline_totals[\"total_passengers\"] * 10000\n",
    "\n",
    "# Print airline_totals\n",
    "____\n",
    "ChatGPT\n",
    "To print the airline_totals DataFrame and see the results of your manipulations, you can use the following line of code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Print airline_totals\n",
    "print(airline_totals)\n",
    "So your complete code will be:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Read CSV as DataFrame called airline_bumping\n",
    "airline_bumping = pd.read_csv(\"airline_bumping.csv\")\n",
    "\n",
    "# Print the first few rows of the DataFrame to understand its structure\n",
    "print(airline_bumping.head())\n",
    "\n",
    "# For each airline, select nb_bumped and total_passengers and calculate the sum\n",
    "airline_totals = airline_bumping.groupby(\"airline\")[[\"nb_bumped\", \"total_passengers\"]].sum()\n",
    "\n",
    "# Create new column, bumps_per_10k: number of bumps per 10k passengers for each airline\n",
    "airline_totals[\"bumps_per_10k\"] = airline_totals[\"nb_bumped\"] / airline_totals[\"total_passengers\"] * 10000\n",
    "\n",
    "# Print airline_totals to see the results of your manipulations\n",
    "print(airline_totals)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming airline_totals is your DataFrame\n",
    "# Sort airline_totals by bumps_per_10k from highest to lowest\n",
    "airline_totals_sorted = airline_totals.sort_values(by='bumps_per_10k', ascending=False)\n",
    "\n",
    "# Print the sorted DataFrame\n",
    "print(airline_totals_sorted)\n",
    "\n",
    "# Save the sorted DataFrame as a CSV called \"airline_totals_sorted.csv\"\n",
    "airline_totals_sorted.to_csv('airline_totals_sorted.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named airline_totals\n",
    "\n",
    "# Create airline_totals_sorted\n",
    "airline_totals_sorted = airline_totals.sort_values(by='bumps_per_10k', ascending=False)\n",
    "\n",
    "# Print airline_totals_sorted\n",
    "print(airline_totals_sorted)\n",
    "\n",
    "# Save as airline_totals_sorted.csv\n",
    "airline_totals_sorted.to_csv('airline_totals_sorted.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct submission\n",
    "import pandas as pd\n",
    "\n",
    "# Create airline_totals_sorted\n",
    "airline_totals_sorted = airline_totals.sort_values(by='bumps_per_10k', ascending=False)\n",
    "\n",
    "# Print airline_totals_sorted\n",
    "print(airline_totals_sorted)\n",
    "\n",
    "# Save as airline_totals_sorted.csv\n",
    "airline_totals_sorted.to_csv('airline_totals_sorted.csv', index=False)\n",
    "\n",
    "# Excellent exporting! Now you can share these insights about your competitors with your team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "'''DataFrame to CSV\n",
    "You're almost there! To make things easier to read, you'll need to sort the data and export it to CSV so that your colleagues can read it.\n",
    "\n",
    "pandas as pd has been imported for you.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Sort airline_totals by the values of bumps_per_10k from highest to lowest, storing as airline_totals_sorted.\n",
    "Print your sorted DataFrame.\n",
    "Save the sorted DataFrame as a CSV called \"airline_totals_sorted.csv\".'''\n",
    "\n",
    "\n",
    "# Create airline_totals_sorted\n",
    "airline_totals_sorted = ____\n",
    "\n",
    "# Print airline_totals_sorted\n",
    "____\n",
    "\n",
    "# Save as airline_totals_sorted.csv\n",
    "____\n",
    "ChatGPT\n",
    "Here’s how you can sort your DataFrame and save it to a CSV file:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named airline_totals\n",
    "\n",
    "# Create airline_totals_sorted\n",
    "airline_totals_sorted = airline_totals.sort_values(by='bumps_per_10k', ascending=False)\n",
    "\n",
    "# Print airline_totals_sorted\n",
    "print(airline_totals_sorted)\n",
    "\n",
    "# Save as airline_totals_sorted.csv\n",
    "airline_totals_sorted.to_csv('airline_totals_sorted.csv', index=False)\n",
    "This code will sort the airline_totals DataFrame by the bumps_per_10k column in descending order, print the sorted DataFrame, and save it to a CSV file named airline_totals_sorted.csv without including the index column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
