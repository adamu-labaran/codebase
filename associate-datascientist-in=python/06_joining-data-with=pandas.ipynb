{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Merging Basics\n",
    "Free\n",
    "\n",
    "0%\n",
    "Learn how you can merge disparate data using inner joins. By combining information from multiple sources you’ll uncover compelling insights that may have previously been hidden. You’ll also learn how the relationship between those sources, such as one-to-one or one-to-many, can affect your result.\n",
    "\n",
    "What column to merge on?\n",
    "Chicago provides a list of taxicab owners and vehicles licensed to operate within the city, for public safety. Your goal is to merge two tables together. One table is called taxi_owners, with info about the taxi cab company owners, and one is called taxi_veh, with info about each taxi cab vehicle. Both the taxi_owners and taxi_veh tables have been loaded for you to explore.\n",
    "\n",
    "Choose the column you would use to merge the two tables on using the .merge() method.\n",
    "\n",
    "Instructions\n",
    "50 XP\n",
    "Possible answers\n",
    "\n",
    "\n",
    "on='rid'\n",
    "\n",
    "on='vid'\n",
    "\n",
    "on='year'\n",
    "\n",
    "on='zip'\n",
    "ChatGPT\n",
    "To determine the appropriate column to merge the taxi_owners and taxi_veh tables, we need to find a common column that uniquely identifies the relationship between a taxi cab vehicle and its owner. Here are the possible columns and what they might represent:\n",
    "\n",
    "rid: Could represent a registration ID, which might be unique to either the vehicle or the owner.\n",
    "vid: Likely represents the vehicle ID, which would be unique to each taxi cab vehicle.\n",
    "year: Represents the year, which is not unique and not relevant for merging owner and vehicle information.\n",
    "zip: Represents the zip code, which is also not unique and not relevant for merging owner and vehicle information.\n",
    "Given these possibilities, the most likely candidate for merging the tables would be vid, as it probably represents the vehicle ID, a unique identifier for each taxi cab vehicle that would be present in both tables.\n",
    "\n",
    "Therefore, the correct column to use for merging the two tables is:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "on='vid'\n",
    "\n",
    "Yes, great job! Both DataFrames contained the column vid. Now continue on to the next exercise where you will using this information to merge the tables.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the taxi_owners and taxi_veh tables\n",
    "taxi_own_veh = taxi_owners.merge(taxi_veh, on='vid')\n",
    "\n",
    "# Print the column names of the taxi_own_veh\n",
    "print(taxi_own_veh.columns)\n",
    "\n",
    "# Merge the taxi_owners and taxi_veh tables setting a suffix\n",
    "taxi_own_veh = taxi_owners.merge(taxi_veh, on='vid', suffixes=('_own', '_veh'))\n",
    "\n",
    "# Print the column names of taxi_own_veh\n",
    "print(taxi_own_veh.columns)\n",
    "\n",
    "# Merge the taxi_owners and taxi_veh tables setting a suffix\n",
    "taxi_own_veh = taxi_owners.merge(taxi_veh, on='vid', suffixes=('_own','_veh'))\n",
    "\n",
    "# Print the value_counts to find the most popular fuel_type\n",
    "print(taxi_own_veh['fuel_type'].value_counts())\n",
    "\n",
    "\n",
    "# Bravo! You correctly merged the two tables together and found out that the most common fuel type for taxis in Chicago are hybrids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "'''Your first inner join\n",
    "You have been tasked with figuring out what the most popular types of fuel used in Chicago taxis are. To complete the analysis, you need to merge the taxi_owners and taxi_veh tables together on the vid column. You can then use the merged table along with the .value_counts() method to find the most common fuel_type.\n",
    "\n",
    "Since you'll be working with pandas throughout the course, the package will be preloaded for you as pd in each exercise in this course. Also the taxi_owners and taxi_veh DataFrames are loaded for you.\n",
    "\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "1\n",
    "Merge taxi_owners with taxi_veh on the column vid, and save the result to taxi_own_veh.'''\n",
    "\n",
    "# Merge the taxi_owners and taxi_veh tables\n",
    "taxi_own_veh = taxi_owners.____\n",
    "\n",
    "# Print the column names of the taxi_own_veh\n",
    "print(taxi_own_veh.columns)\n",
    "ChatGPT\n",
    "To merge the taxi_owners and taxi_veh DataFrames on the vid column, you can use the merge method in pandas. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge the taxi_owners and taxi_veh tables\n",
    "taxi_own_veh = taxi_owners.merge(taxi_veh, on='vid')\n",
    "\n",
    "# Print the column names of the taxi_own_veh\n",
    "print(taxi_own_veh.columns)\n",
    "This code merges the two DataFrames on the vid column and then prints the column names of the resulting merged DataFrame.\n",
    "\n",
    "'''Set the left and right table suffixes for overlapping columns of the merge to _own and _veh, respectively.'''\n",
    "\n",
    "# Merge the taxi_owners and taxi_veh tables setting a suffix\n",
    "taxi_own_veh = taxi_owners.merge(taxi_veh, on='vid', ____)\n",
    "\n",
    "# Print the column names of taxi_own_veh\n",
    "print(taxi_own_veh.columns)\n",
    "ChatGPT\n",
    "To set the suffixes for overlapping columns in the merge, you can use the suffixes parameter in the merge method. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge the taxi_owners and taxi_veh tables setting a suffix\n",
    "taxi_own_veh = taxi_owners.merge(taxi_veh, on='vid', suffixes=('_own', '_veh'))\n",
    "\n",
    "# Print the column names of taxi_own_veh\n",
    "print(taxi_own_veh.columns)\n",
    "This code will merge the DataFrames and add the specified suffixes to any overlapping column names.\n",
    "\n",
    "'''Select the fuel_type column from taxi_own_veh and print the value_counts() to find the most popular fuel_types used.'''\n",
    "\n",
    "# Merge the taxi_owners and taxi_veh tables setting a suffix\n",
    "taxi_own_veh = taxi_owners.merge(taxi_veh, on='vid', suffixes=('_own','_veh'))\n",
    "\n",
    "# Print the value_counts to find the most popular fuel_type\n",
    "print(taxi_own_veh['____'].value_counts())\n",
    "ChatGPT\n",
    "To find the most popular fuel_type used, you should select the fuel_type column from the merged DataFrame and use the value_counts() method. Here is how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge the taxi_owners and taxi_veh tables setting a suffix\n",
    "taxi_own_veh = taxi_owners.merge(taxi_veh, on='vid', suffixes=('_own','_veh'))\n",
    "\n",
    "# Print the value_counts to find the most popular fuel_type\n",
    "print(taxi_own_veh['fuel_type'].value_counts())\n",
    "This code will display the counts of each unique value in the fuel_type column, allowing you to identify the most popular fuel types used in Chicago taxis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the wards and census tables on the ward column\n",
    "wards_census = wards.merge(census, on='ward')\n",
    "\n",
    "# Print the shape of wards_census\n",
    "print('wards_census table shape:', wards_census.shape)\n",
    "\n",
    "\n",
    "# Print the first few rows of the wards_altered table to view the change\n",
    "print(wards_altered[['ward']].head())\n",
    "\n",
    "# Merge the wards_altered and census tables on the ward column\n",
    "wards_altered_census = wards_altered.merge(census, on='ward')\n",
    "\n",
    "# Print the shape of wards_altered_census\n",
    "print('wards_altered_census table shape:', wards_altered_census.shape)\n",
    "\n",
    "\n",
    "# Print the first few rows of the census_altered table to view the change \n",
    "print(census_altered[['ward']].head())\n",
    "\n",
    "# Merge the wards and census_altered tables on the ward column\n",
    "wards_census_altered = wards.merge(census_altered, on='ward')\n",
    "\n",
    "# Print the shape of wards_census_altered\n",
    "print('wards_census_altered table shape:', wards_census_altered.shape)\n",
    "\n",
    "\n",
    "'''Great job! In step 1, the .merge() returned a table with the same number of rows as the original wards table. However, in steps 2 and 3, using the altered tables with the altered first row of the ward column, the number of returned rows was fewer. There was not a matching value in the ward column of the other table. _Remember that .merge() only returns rows where the values match in both tables._'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Inner joins and number of rows returned\n",
    "All of the merges you have studied to this point are called inner joins. It is necessary to understand that inner joins only return the rows with matching values in both tables. You will explore this further by reviewing the merge between the wards and census tables, then comparing it to merges of copies of these tables that are slightly altered, named wards_altered, and census_altered. The first row of the wards column has been changed in the altered tables. You will examine how this affects the merge between them. The tables have been loaded for you.\n",
    "\n",
    "For this exercise, it is important to know that the wards and census tables start with 50 rows.\n",
    "\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "1\n",
    "Merge wards and census on the ward column and save the result to wards_census.''''\n",
    "\n",
    "# Merge the wards and census tables on the ward column\n",
    "wards_census = wards.merge(____)\n",
    "\n",
    "# Print the shape of wards_census\n",
    "print('wards_census table shape:', wards_census.shape)\n",
    "ChatGPT\n",
    "To merge the wards and census tables on the ward column and save the result to wards_census, you can use the merge method in pandas. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge the wards and census tables on the ward column\n",
    "wards_census = wards.merge(census, on='ward')\n",
    "\n",
    "# Print the shape of wards_census\n",
    "print('wards_census table shape:', wards_census.shape)\n",
    "This code will merge the two DataFrames on the ward column and then print the shape of the resulting DataFrame to show the number of rows and columns.\n",
    "\n",
    "'''\n",
    "Merge the wards_altered and census tables on the ward column, and notice the difference in returned rows.'''\n",
    "\n",
    "# Print the first few rows of the wards_altered table to view the change \n",
    "print(wards_altered[['ward']].head())\n",
    "\n",
    "# Merge the wards_altered and census tables on the ward column\n",
    "wards_altered_census = ____.merge(census, ____)\n",
    "\n",
    "# Print the shape of wards_altered_census\n",
    "print('wards_altered_census table shape:', wards_altered_census.shape)\n",
    "ChatGPT\n",
    "To merge the wards_altered and census tables on the ward column and observe the difference in the number of returned rows, you can follow these steps:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Print the first few rows of the wards_altered table to view the change\n",
    "print(wards_altered[['ward']].head())\n",
    "\n",
    "# Merge the wards_altered and census tables on the ward column\n",
    "wards_altered_census = wards_altered.merge(census, on='ward')\n",
    "\n",
    "# Print the shape of wards_altered_census\n",
    "print('wards_altered_census table shape:', wards_altered_census.shape)\n",
    "This code will merge the wards_altered and census DataFrames on the ward column and then print the shape of the resulting DataFrame to show the difference in the number of rows compared to the original merge.\n",
    "\n",
    "'''Merge the wards and census_altered tables on the ward column, and notice the difference in returned rows'''\n",
    "\n",
    "# Print the first few rows of the census_altered table to view the change \n",
    "print(census_altered[['ward']].head())\n",
    "\n",
    "# Merge the wards and census_altered tables on the ward column\n",
    "wards_census_altered = wards.____\n",
    "\n",
    "# Print the shape of wards_census_altered\n",
    "print('wards_census_altered table shape:', wards_census_altered.shape)\n",
    "ChatGPT\n",
    "To merge the wards and census_altered tables on the ward column and observe the difference in the number of returned rows, you can use the merge method as follows:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Print the first few rows of the census_altered table to view the change \n",
    "print(census_altered[['ward']].head())\n",
    "\n",
    "# Merge the wards and census_altered tables on the ward column\n",
    "wards_census_altered = wards.merge(census_altered, on='ward')\n",
    "\n",
    "# Print the shape of wards_census_altered\n",
    "print('wards_census_altered table shape:', wards_census_altered.shape)\n",
    "This code will merge the wards and census_altered DataFrames on the ward column and then print the shape of the resulting DataFrame to show the difference in the number of rows compared to the original merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One-to-many classification\n",
    "Understanding the difference between a one-to-one and one-to-many relationship is a useful skill. In this exercise, consider a set of tables from an e-commerce website. The hypothetical tables are the following:\n",
    "\n",
    "A customer table with information about each customer\n",
    "A cust_tax_info table with customers unique tax IDs\n",
    "An orders table with information about each order\n",
    "A products table with details about each unique product sold\n",
    "An inventory table with information on how much total inventory is available to sell for each product\n",
    "ChatGPT\n",
    "In an e-commerce context, understanding the relationships between tables is crucial for effective database management and querying. Let's classify the relationships between the tables mentioned:\n",
    "\n",
    "Customer Table and Cust_Tax_Info Table:\n",
    "\n",
    "Relationship: One-to-One\n",
    "Explanation: Each customer has a unique tax ID, so each record in the customer table corresponds to exactly one record in the cust_tax_info table.\n",
    "Customer Table and Orders Table:\n",
    "\n",
    "Relationship: One-to-Many\n",
    "Explanation: Each customer can place multiple orders, so each record in the customer table can be associated with multiple records in the orders table.\n",
    "Orders Table and Products Table:\n",
    "\n",
    "Relationship: Many-to-Many (usually resolved through a junction table like Order_Items)\n",
    "Explanation: Each order can contain multiple products, and each product can be part of multiple orders. This relationship is typically managed through an intermediary table (e.g., Order_Items) that links orders to products.\n",
    "Products Table and Inventory Table:\n",
    "\n",
    "Relationship: One-to-One or One-to-Many\n",
    "Explanation: Each product may have a single inventory record indicating the total amount available for sale, making it a one-to-one relationship. In some cases, if the inventory is tracked across multiple locations, it could be a one-to-many relationship where each product can have multiple inventory records corresponding to different locations.\n",
    "Example Queries:\n",
    "1. Joining Customer Table with Cust_Tax_Info Table:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Assuming pandas DataFrames: customers and cust_tax_info\n",
    "\n",
    "# Merge customers and cust_tax_info on customer_id\n",
    "customer_tax_info = customers.merge(cust_tax_info, on='customer_id')\n",
    "2. Joining Customer Table with Orders Table:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Assuming pandas DataFrames: customers and orders\n",
    "\n",
    "# Merge customers and orders on customer_id\n",
    "customer_orders = customers.merge(orders, on='customer_id')\n",
    "3. Joining Orders Table with Products Table (through Order_Items):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Assuming pandas DataFrames: orders, order_items, and products\n",
    "\n",
    "# Merge orders with order_items on order_id\n",
    "orders_items = orders.merge(order_items, on='order_id')\n",
    "\n",
    "# Merge the resulting DataFrame with products on product_id\n",
    "orders_products = orders_items.merge(products, on='product_id')\n",
    "4. Joining Products Table with Inventory Table:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Assuming pandas DataFrames: products and inventory\n",
    "\n",
    "# Merge products and inventory on product_id\n",
    "product_inventory = products.merge(inventory, on='product_id')\n",
    "Understanding these relationships and how to manage them in SQL or pandas is fundamental for effectively querying and analyzing e-commerce data.\n",
    "\n",
    "Great job on classifying the relationships into the right categories. Knowing if a relationship should be a one-to-one or one-to-many will allow you to troubleshoot a merge that appears incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the licenses and biz_owners table on account\n",
    "licenses_owners = licenses.merge(biz_owners, on='account')\n",
    "\n",
    "# Group the results by title then count the number of accounts\n",
    "counted_df = licenses_owners.groupby('title').agg({'account': 'count'})\n",
    "\n",
    "# Sort the counted_df in descending order\n",
    "sorted_df = counted_df.sort_values('account', ascending=False)\n",
    "\n",
    "# Use .head() method to print the first few rows of sorted_df\n",
    "print(sorted_df.head())\n",
    "\n",
    "\n",
    "'''Wonderful! After merging the tables together, you counted the number of repeated rows with the combination of .groupby() and .agg() statements. You see that president, followed by secretary, are the most common business owner titles.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''One-to-many merge\n",
    "A business may have one or multiple owners. In this exercise, you will continue to gain experience with one-to-many merges by merging a table of business owners, called biz_owners, to the licenses table. Recall from the video lesson, with a one-to-many relationship, a row in the left table may be repeated if it is related to multiple rows in the right table. In this lesson, you will explore this further by finding out what is the most common business owner title. (i.e., secretary, CEO, or vice president)\n",
    "\n",
    "The licenses and biz_owners DataFrames are loaded for you.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Starting with the licenses table on the left, merge it to the biz_owners table on the column account, and save the results to a variable named licenses_owners.\n",
    "Group licenses_owners by title and count the number of accounts for each title. Save the result as counted_df\n",
    "Sort counted_df by the number of accounts in descending order, and save this as a variable named sorted_df.\n",
    "Use the .head() method to print the first few rows of the sorted_df.'''\n",
    "\n",
    "\n",
    "# Merge the licenses and biz_owners table on account\n",
    "licenses_owners = ____\n",
    "\n",
    "# Group the results by title then count the number of accounts\n",
    "counted_df = licenses_owners.groupby(____).agg({'account':'count'})\n",
    "\n",
    "# Sort the counted_df in descending order\n",
    "sorted_df = counted_df.sort_values(____)\n",
    "\n",
    "# Use .head() method to print the first few rows of sorted_df\n",
    "print(____)\n",
    "ChatGPT\n",
    "Here is the solution to your exercise:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge the licenses and biz_owners table on account\n",
    "licenses_owners = licenses.merge(biz_owners, on='account')\n",
    "\n",
    "# Group the results by title then count the number of accounts\n",
    "counted_df = licenses_owners.groupby('title').agg({'account': 'count'})\n",
    "\n",
    "# Sort the counted_df in descending order\n",
    "sorted_df = counted_df.sort_values('account', ascending=False)\n",
    "\n",
    "# Use .head() method to print the first few rows of sorted_df\n",
    "print(sorted_df.head())\n",
    "Explanation:\n",
    "\n",
    "We merge the licenses and biz_owners DataFrames on the 'account' column.\n",
    "We group the merged DataFrame by 'title' and count the number of accounts for each title.\n",
    "We sort the resulting DataFrame in descending order based on the count of accounts.\n",
    "Finally, we print the first few rows of the sorted DataFrame using the .head() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the ridership and cal tables on year, month, and day columns\n",
    "ridership_cal = ridership.merge(cal, on=['year', 'month', 'day'])\n",
    "\n",
    "\n",
    "# Merge the ridership, cal, and stations tables\n",
    "ridership_cal_stations = ridership.merge(cal, on=['year', 'month', 'day']) \\\n",
    "                                  .merge(stations, on='station_id')\n",
    "\n",
    "\n",
    "# Merge the ridership, cal, and stations tables\n",
    "ridership_cal_stations = ridership.merge(cal, on=['year', 'month', 'day']) \\\n",
    "                                  .merge(stations, on='station_id')\n",
    "\n",
    "# Create a filter to select rows for July, weekdays, and Wilson station\n",
    "filter_criteria = ((ridership_cal_stations['month'] == 7) \n",
    "                   & (ridership_cal_stations['day_type'] == 'Weekday') \n",
    "                   & (ridership_cal_stations['station_name'] == 'Wilson'))\n",
    "\n",
    "# Use .loc and the filter to select for rides\n",
    "print(ridership_cal_stations.loc[filter_criteria, 'rides'].sum())\n",
    "\n",
    "\n",
    "'''Awesome work! You merged three DataFrames together, including merging two tables on multiple columns. Once the tables were merged, you filtered and selected just like any other DataFrame. Finally, you found out that the Wilson station had 140,005 riders during weekdays in July.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "'''One-to-many merge\n",
    "A business may have one or multiple owners. In this exercise, you will continue to gain experience with one-to-many merges by merging a table of business owners, called biz_owners, to the licenses table. Recall from the video lesson, with a one-to-many relationship, a row in the left table may be repeated if it is related to multiple rows in the right table. In this lesson, you will explore this further by finding out what is the most common business owner title. (i.e., secretary, CEO, or vice president)\n",
    "\n",
    "The licenses and biz_owners DataFrames are loaded for you.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Starting with the licenses table on the left, merge it to the biz_owners table on the column account, and save the results to a variable named licenses_owners.\n",
    "Group licenses_owners by title and count the number of accounts for each title. Save the result as counted_df\n",
    "Sort counted_df by the number of accounts in descending order, and save this as a variable named sorted_df.\n",
    "Use the .head() method to print the first few rows of the sorted_df.'''\n",
    "\n",
    "\n",
    "# Merge the licenses and biz_owners table on account\n",
    "licenses_owners = ____\n",
    "\n",
    "# Group the results by title then count the number of accounts\n",
    "counted_df = licenses_owners.groupby(____).agg({'account':'count'})\n",
    "\n",
    "# Sort the counted_df in descending order\n",
    "sorted_df = counted_df.sort_values(____)\n",
    "\n",
    "# Use .head() method to print the first few rows of sorted_df\n",
    "print(____)\n",
    "ChatGPT\n",
    "Here is the solution to your exercise:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge the licenses and biz_owners table on account\n",
    "licenses_owners = licenses.merge(biz_owners, on='account')\n",
    "\n",
    "# Group the results by title then count the number of accounts\n",
    "counted_df = licenses_owners.groupby('title').agg({'account': 'count'})\n",
    "\n",
    "# Sort the counted_df in descending order\n",
    "sorted_df = counted_df.sort_values('account', ascending=False)\n",
    "\n",
    "# Use .head() method to print the first few rows of sorted_df\n",
    "print(sorted_df.head())\n",
    "Explanation:\n",
    "\n",
    "We merge the licenses and biz_owners DataFrames on the 'account' column.\n",
    "We group the merged DataFrame by 'title' and count the number of accounts for each title.\n",
    "We sort the resulting DataFrame in descending order based on the count of accounts.\n",
    "Finally, we print the first few rows of the sorted DataFrame using the .head() method.\n",
    "otal riders in a month\n",
    "Your goal is to find the total number of rides provided to passengers passing through the Wilson station (station_name == 'Wilson') when riding Chicago's public transportation system on weekdays (day_type == 'Weekday') in July (month == 7). Luckily, Chicago provides this detailed data, but it is in three different tables. You will work on merging these tables together to answer the question. This data is different from the business related data you have seen so far, but all the information you need to answer the question is provided.\n",
    "\n",
    "The cal, ridership, and stations DataFrames have been loaded for you. The relationship between the tables can be seen in the diagram below.\n",
    "ChatGPT\n",
    "To find the total number of rides provided to passengers passing through the Wilson station on weekdays in July, we'll need to follow these steps:\n",
    "\n",
    "Merge the ridership DataFrame with the stations DataFrame on the common column station_id to get the station names.\n",
    "Filter the merged DataFrame to only include rows where the station_name is 'Wilson'.\n",
    "Merge the filtered DataFrame with the cal DataFrame on the common column date to get the day type and month information.\n",
    "Filter the merged DataFrame to only include rows where day_type is 'Weekday' and month is 7.\n",
    "Sum the rides column to get the total number of rides.\n",
    "Here's the code to achieve this:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge ridership and stations on station_id\n",
    "merged_df = ridership.merge(stations, on='station_id')\n",
    "\n",
    "# Filter for rows where station_name is 'Wilson'\n",
    "wilson_ridership = merged_df[merged_df['station_name'] == 'Wilson']\n",
    "\n",
    "# Merge the filtered DataFrame with cal on date\n",
    "wilson_ridership_with_cal = wilson_ridership.merge(cal, on='date')\n",
    "\n",
    "# Filter for rows where day_type is 'Weekday' and month is 7\n",
    "wilson_weekday_july = wilson_ridership_with_cal[(wilson_ridership_with_cal['day_type'] == 'Weekday') & (wilson_ridership_with_cal['month'] == 7)]\n",
    "\n",
    "# Sum the rides column to get the total number of rides\n",
    "total_rides = wilson_weekday_july['rides'].sum()\n",
    "\n",
    "print(total_rides)\n",
    "This code will give you the total number of rides provided to passengers passing through the Wilson station on weekdays in July.\n",
    "\n",
    "'''Merge the ridership and cal tables together, starting with the ridership table on the left and save the result to the variable ridership_cal. If you code takes too long to run, your merge conditions might be incorrect.'''\n",
    "\n",
    "# Merge the ridership and cal tables\n",
    "ridership_cal = ridership.merge(____)\n",
    "ChatGPT\n",
    "To merge the ridership and cal tables, you'll need to use a common column that both tables share. Based on your description, the common column is likely date. Here's how you can perform the merge:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge the ridership and cal tables on the date column\n",
    "ridership_cal = ridership.merge(cal, on='date')\n",
    "This will merge the ridership table with the cal table using the date column, resulting in a DataFrame that includes information from both tables.\n",
    "\n",
    "'''Merge the ridership and cal tables together, starting with the ridership table on the left and save the result to the variable ridership_cal. If you code takes too long to run, your merge conditions might be incorrect.'''\n",
    "\n",
    "# Merge the ridership and cal tables on the date column\n",
    "ridership_cal = ridership.merge(cal, on='date')\n",
    "ChatGPT\n",
    "Your code to merge the ridership and cal tables looks correct:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge the ridership and cal tables on the date column\n",
    "ridership_cal = ridership.merge(cal, on='date')\n",
    "This will combine the ridership and cal tables based on the date column, which should efficiently merge the two tables. If the merge is taking too long, ensure that:\n",
    "\n",
    "Both DataFrames (ridership and cal) have the date column and that the column names match exactly.\n",
    "There are no duplicate rows in the date columns that could cause the merge to become more complex.\n",
    "The date columns are of the same type (e.g., both should be in datetime format if applicable).\n",
    "If you are still facing performance issues, you might want to check for large datasets or any indexing issues that could be affecting the merge.\n",
    "\n",
    "Total riders in a month\n",
    "Your goal is to find the total number of rides provided to passengers passing through the Wilson station (station_name == 'Wilson') when riding Chicago's public transportation system on weekdays (day_type == 'Weekday') in July (month == 7). Luckily, Chicago provides this detailed data, but it is in three different tables. You will work on merging these tables together to answer the question. This data is different from the business related data you have seen so far, but all the information you need to answer the question is provided.\n",
    "\n",
    "The cal, ridership, and stations DataFrames have been loaded for you. The relationship between the tables can be seen in the diagram below.\n",
    "\n",
    "Table diagram. The cal table relates to ridership via year, month, and day. The ridership table relates to the stations table via station_id.\n",
    "\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "1\n",
    "2\n",
    "3\n",
    "Merge the ridership and cal tables together, starting with the ridership table on the left and save the result to the variable ridership_cal. If you code takes too long to run, your merge conditions might be incorrect.\n",
    "ChatGPT\n",
    "To merge the ridership and cal tables, you should join them on the columns that relate the two tables. Based on the provided information, it seems that the ridership table should be joined with the cal table using the year, month, and day columns. Here's the correct approach:\n",
    "\n",
    "Merge the ridership and cal tables on year, month, and day columns.\n",
    "Here's the code to achieve this:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge the ridership and cal tables on year, month, and day columns\n",
    "ridership_cal = ridership.merge(cal, on=['year', 'month', 'day'])\n",
    "This code assumes that both ridership and cal DataFrames have columns named year, month, and day for merging. If the column names are different or there are additional considerations, adjust the column names accordingly.\n",
    "\n",
    "'''Merge the ridership and cal tables together, starting with the ridership table on the left and save the result to the variable ridership_cal. If you code takes too long to run, your merge conditions might be incorrect.'''\n",
    "\n",
    "# Merge the ridership and cal tables on the date column\n",
    "ridership_cal = ridership.merge(cal, on='date')\n",
    "ChatGPT\n",
    "If the ridership and cal tables have a common column date, you should merge them on that column as follows:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge the ridership and cal tables on the date column\n",
    "ridership_cal = ridership.merge(cal, on='date')\n",
    "This is correct if both tables have a date column and it's the right key for merging. If the merge is taking too long, verify the following:\n",
    "\n",
    "Check Column Names: Ensure both tables have a date column and that the names match exactly.\n",
    "Verify Data Types: The date columns should have the same data type (e.g., both should be in datetime format).\n",
    "Check for Duplicates: Ensure there are no unexpected duplicate entries in the date column of either table that could complicate the merge.\n",
    "If there are performance issues, you might need to inspect the data or consider optimizing the merge.\n",
    "\n",
    "Incorrect submission\n",
    "Did you merge on year, month, and day by passing a list of these column names to the on argument?\n",
    "ChatGPT\n",
    "It seems the correct approach is to merge on year, month, and day columns instead of date. Here’s how you should do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge the ridership and cal tables on year, month, and day columns\n",
    "ridership_cal = ridership.merge(cal, on=['year', 'month', 'day'])\n",
    "Make sure that both ridership and cal tables contain the year, month, and day columns and that these columns are used for merging the tables together.\n",
    "\n",
    "'''Extend the previous merge to three tables by also merging the stations table.'''\n",
    "\n",
    "# Merge the ridership, cal, and stations tables\n",
    "ridership_cal_stations = ridership.merge(cal, on=['year','month','day']) \\\n",
    "            \t\t\t\t.merge(____)\n",
    "ChatGPT\n",
    "To extend the merge to include the stations table, you need to merge the previously merged ridership_cal DataFrame with the stations DataFrame. The stations table should be merged on the station_id column, which is common between the ridership and stations tables. Here’s how you can complete the merge:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge the ridership, cal, and stations tables\n",
    "ridership_cal_stations = ridership.merge(cal, on=['year', 'month', 'day']) \\\n",
    "                                  .merge(stations, on='station_id')\n",
    "In this code:\n",
    "\n",
    "First, you merge ridership and cal on year, month, and day.\n",
    "Then, you merge the result with stations on station_id.\n",
    "# Create a variable called filter_criteria to select the appropriate rows from the merged table so that you can sum the rides column\n",
    "\n",
    "\n",
    "# Merge the ridership, cal, and stations tables\n",
    "ridership_cal_stations = ridership.merge(cal, on=['year','month','day']) \\\n",
    "\t\t\t\t\t\t\t.merge(stations, on='station_id')\n",
    "\n",
    "# Create a filter to filter ridership_cal_stations\n",
    "filter_criteria = ((ridership_cal_stations['month'] == ____) \n",
    "                   & (ridership_cal_stations['day_type'] == ____) \n",
    "                   & (ridership_cal_stations['station_name'] == ____))\n",
    "\n",
    "# Use .loc and the filter to select for rides\n",
    "print(ridership_cal_stations.loc[filter_criteria, 'rides'].sum())\n",
    "ChatGPT\n",
    "To filter the merged DataFrame (ridership_cal_stations) for the specific criteria and calculate the total number of rides, you need to specify the month, day type, and station name. In this case, you want to filter for July (month == 7), weekdays (day_type == 'Weekday'), and Wilson station (station_name == 'Wilson').\n",
    "\n",
    "Here’s how you can complete the code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge the ridership, cal, and stations tables\n",
    "ridership_cal_stations = ridership.merge(cal, on=['year', 'month', 'day']) \\\n",
    "                                  .merge(stations, on='station_id')\n",
    "\n",
    "# Create a filter to select rows for July, weekdays, and Wilson station\n",
    "filter_criteria = ((ridership_cal_stations['month'] == 7) \n",
    "                   & (ridership_cal_stations['day_type'] == 'Weekday') \n",
    "                   & (ridership_cal_stations['station_name'] == 'Wilson'))\n",
    "\n",
    "# Use .loc and the filter to select for rides\n",
    "print(ridership_cal_stations.loc[filter_criteria, 'rides'].sum())\n",
    "This code will sum the number of rides for the Wilson station on weekdays in July"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge licenses and zip_demo on zip; then merge the result with wards on ward\n",
    "licenses_zip_ward = licenses.merge(zip_demo, on='zip').merge(wards, on='ward')\n",
    "\n",
    "# Print the results by alderman and show median income\n",
    "print(licenses_zip_ward.groupby('alderman').agg({'income': 'median'}))\n",
    "\n",
    "\n",
    "'''Nice work! You successfully merged three tables together. With the merged data, you can complete your income analysis. You see that only a few aldermen represent businesses in areas where the median income is greater than $62,000, which is the median income for the state of Illinois.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Three table merge\n",
    "To solidify the concept of a three DataFrame merge, practice another exercise. A reasonable extension of our review of Chicago business data would include looking at demographics information about the neighborhoods where the businesses are. A table with the median income by zip code has been provided to you. You will merge the licenses and wards tables with this new income-by-zip-code table called zip_demo.\n",
    "\n",
    "The licenses, wards, and zip_demo DataFrames have been loaded for you.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Starting with the licenses table, merge to it the zip_demo table on the zip column. Then merge the resulting table to the wards table on the ward column. Save result of the three merged tables to a variable named licenses_zip_ward.\n",
    "Group the results of the three merged tables by the column alderman and find the median income.'''\n",
    "\n",
    "# Merge licenses and zip_demo, on zip; and merge the wards on ward\n",
    "licenses_zip_ward = licenses.merge____ \\\n",
    "            \t\t\t____\n",
    "\n",
    "# Print the results by alderman and show median income\n",
    "print(____.groupby(____).agg({'income':'median'}))\n",
    "ChatGPT\n",
    "To complete the task, you'll follow these steps:\n",
    "\n",
    "Merge the licenses DataFrame with the zip_demo DataFrame on the zip column.\n",
    "Merge the resulting DataFrame with the wards DataFrame on the ward column.\n",
    "Save the resulting DataFrame to the variable licenses_zip_ward.\n",
    "Group the results by the alderman column and find the median income.\n",
    "Here's the completed code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge licenses and zip_demo on zip; then merge the result with wards on ward\n",
    "licenses_zip_ward = licenses.merge(zip_demo, on='zip').merge(wards, on='ward')\n",
    "\n",
    "# Print the results by alderman and show median income\n",
    "print(licenses_zip_ward.groupby('alderman').agg({'income': 'median'}))\n",
    "Make sure that the column names used in the merge functions and the groupby operation match the actual column names in your DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge land_use and census on ward\n",
    "land_cen = land_use.merge(census, on='ward', suffixes=('_land', '_cen'))\n",
    "\n",
    "# Merge the result with licenses on ward, including suffixes\n",
    "land_cen_lic = land_cen.merge(licenses, on='ward', suffixes=('_cen', '_lic'))\n",
    "\n",
    "\n",
    "# Merge land_use and census on ward\n",
    "land_cen = land_use.merge(census, on='ward', suffixes=('_land', '_cen'))\n",
    "\n",
    "# Merge the result with licenses on ward, including suffixes\n",
    "land_cen_lic = land_cen.merge(licenses, on='ward', suffixes=('_cen', '_lic'))\n",
    "\n",
    "# Group by ward, pop_2010, and vacant, then count the number of accounts\n",
    "pop_vac_lic = land_cen_lic.groupby(['ward', 'pop_2010', 'vacant'], as_index=False).agg({'account': 'count'})\n",
    "\n",
    "\n",
    "# Merge land_use and census on ward\n",
    "land_cen = land_use.merge(census, on='ward', suffixes=('_land', '_cen'))\n",
    "\n",
    "# Merge the result with licenses on ward, including suffixes\n",
    "land_cen_lic = land_cen.merge(licenses, on='ward', suffixes=('_cen', '_lic'))\n",
    "\n",
    "# Group by ward, pop_2010, and vacant, then count the number of accounts\n",
    "pop_vac_lic = land_cen_lic.groupby(['ward', 'pop_2010', 'vacant'], as_index=False).agg({'account': 'count'})\n",
    "\n",
    "# Sort pop_vac_lic by vacant (descending), account (ascending), and pop_2010 (ascending)\n",
    "sorted_pop_vac_lic = pop_vac_lic.sort_values(by=['vacant', 'account', 'pop_2010'], \n",
    "                                              ascending=[False, True, True])\n",
    "\n",
    "# Print the top few rows of sorted_pop_vac_lic\n",
    "print(sorted_pop_vac_lic.head())\n",
    "\n",
    "\n",
    "\n",
    "'''Great job putting your new skills into action. You merged multiple tables with varying relationships and added suffixes to make your column names clearer. Using your skills, you were able to pull together information from different tables to see that the 7th ward would be a good place to build your goat farm!'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''One-to-many merge with multiple tables\n",
    "In this exercise, assume that you are looking to start a business in the city of Chicago. Your perfect idea is to start a company that uses goats to mow the lawn for other businesses. However, you have to choose a location in the city to put your goat farm. You need a location with a great deal of space and relatively few businesses and people around to avoid complaints about the smell. You will need to merge three tables to help you choose your location. The land_use table has info on the percentage of vacant land by city ward. The census table has population by ward, and the licenses table lists businesses by ward.\n",
    "\n",
    "The land_use, census, and licenses tables have been loaded for you.\n",
    "\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "1\n",
    "2\n",
    "3\n",
    "Merge land_use and census on the ward column. Merge the result of this with licenses on the ward column, using the suffix _cen for the left table and _lic for the right table. Save this to the variable land_cen_lic.'''\n",
    "\n",
    "# Merge land_use and census and merge result with licenses including suffixes\n",
    "land_cen_lic = ____\n",
    "ChatGPT\n",
    "To perform the required merges, you'll first merge the land_use DataFrame with the census DataFrame on the ward column, and then merge the result with the licenses DataFrame, applying suffixes to differentiate columns from each table.\n",
    "\n",
    "Here's the code to accomplish that:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge land_use and census on ward\n",
    "land_cen = land_use.merge(census, on='ward', suffixes=('_land', '_cen'))\n",
    "\n",
    "# Merge the result with licenses on ward, including suffixes\n",
    "land_cen_lic = land_cen.merge(licenses, on='ward', suffixes=('_cen', '_lic'))\n",
    "In this code:\n",
    "\n",
    "land_use.merge(census, on='ward', suffixes=('_land', '_cen')) merges land_use and census and adds suffixes _land and _cen to differentiate columns from these tables.\n",
    "land_cen.merge(licenses, on='ward', suffixes=('_cen', '_lic')) merges the resulting DataFrame with licenses, adding suffixes _cen and _lic to the columns from the respective DataFrames.\n",
    "'''Group land_cen_lic by ward, pop_2010 (the population in 2010), and vacant, then count the number of accounts. Save the results to pop_vac_lic.'''\n",
    "\n",
    "# Merge land_use and census and merge result with licenses including suffixes\n",
    "land_cen_lic = land_use.merge(census, on='ward') \\\n",
    "                    .merge(licenses, on='ward', suffixes=('_cen','_lic'))\n",
    "\n",
    "# Group by ward, pop_2010, and vacant, then count the # of accounts\n",
    "pop_vac_lic = land_cen_lic.groupby(____, \n",
    "                                   as_index=False).agg({'account':'count'})\n",
    "ChatGPT\n",
    "To group the land_cen_lic DataFrame by ward, pop_2010, and vacant, and then count the number of accounts, you'll use the groupby method followed by the agg function. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Group by ward, pop_2010, and vacant, then count the number of accounts\n",
    "pop_vac_lic = land_cen_lic.groupby(['ward', 'pop_2010', 'vacant'], as_index=False).agg({'account': 'count'})\n",
    "In this code:\n",
    "\n",
    "groupby(['ward', 'pop_2010', 'vacant'], as_index=False) groups the DataFrame by the specified columns (ward, pop_2010, vacant).\n",
    "agg({'account': 'count'}) counts the number of accounts in each group.\n",
    "Make sure that the column names ('account', 'ward', 'pop_2010', and 'vacant') match those in your DataFrame.\n",
    "\n",
    "Incorrect submission\n",
    "Did you call land_use.merge()?\n",
    "ChatGPT\n",
    "It looks like the initial merge was missed. Based on your instruction to first merge land_use, census, and licenses, you should ensure that the land_use, census, and licenses DataFrames are merged properly before performing the grouping operation.\n",
    "\n",
    "Here's the full code to correctly merge the DataFrames and then group and count the accounts:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge land_use and census on ward\n",
    "land_cen = land_use.merge(census, on='ward', suffixes=('_land', '_cen'))\n",
    "\n",
    "# Merge the result with licenses on ward, including suffixes\n",
    "land_cen_lic = land_cen.merge(licenses, on='ward', suffixes=('_cen', '_lic'))\n",
    "\n",
    "# Group by ward, pop_2010, and vacant, then count the number of accounts\n",
    "pop_vac_lic = land_cen_lic.groupby(['ward', 'pop_2010', 'vacant'], as_index=False).agg({'account': 'count'})\n",
    "To clarify:\n",
    "\n",
    "land_use.merge(census, on='ward', suffixes=('_land', '_cen')) merges land_use with census, adding suffixes to differentiate columns.\n",
    "land_cen.merge(licenses, on='ward', suffixes=('_cen', '_lic')) merges the result with licenses, adding suffixes for clarity.\n",
    "groupby(['ward', 'pop_2010', 'vacant'], as_index=False).agg({'account': 'count'}) performs the grouping and counting.\n",
    "Ensure that the account column is present in your licenses DataFrame. If it's named differently, use the correct column name.\n",
    "\n",
    "'''Sort pop_vac_lic by vacant, account, andpop_2010 in descending, ascending, and ascending order respectively. Save it as sorted_pop_vac_lic.'''\n",
    "\n",
    "\n",
    "# Merge land_use and census and merge result with licenses including suffixes\n",
    "land_cen_lic = land_use.merge(census, on='ward') \\\n",
    "                    .merge(licenses, on='ward', suffixes=('_cen','_lic'))\n",
    "\n",
    "# Group by ward, pop_2010, and vacant, then count the # of accounts\n",
    "pop_vac_lic = land_cen_lic.groupby(['ward','pop_2010','vacant'], \n",
    "                                   as_index=False).agg({'account':'count'})\n",
    "\n",
    "# Sort pop_vac_lic and print the results\n",
    "sorted_pop_vac_lic = pop_vac_lic.sort_values(____, \n",
    "                                             ascending=____)\n",
    "\n",
    "# Print the top few rows of sorted_pop_vac_lic\n",
    "print(sorted_pop_vac_lic.head())\n",
    "ChatGPT\n",
    "To sort pop_vac_lic by vacant in descending order, account in ascending order, and pop_2010 in ascending order, you can use the sort_values method with the by parameter specifying the columns and ascending parameter specifying the sort order for each column.\n",
    "\n",
    "Here's the updated code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Sort pop_vac_lic by vacant (descending), account (ascending), and pop_2010 (ascending)\n",
    "sorted_pop_vac_lic = pop_vac_lic.sort_values(by=['vacant', 'account', 'pop_2010'], \n",
    "                                              ascending=[False, True, True])\n",
    "\n",
    "# Print the top few rows of sorted_pop_vac_lic\n",
    "print(sorted_pop_vac_lic.head())\n",
    "In this code:\n",
    "\n",
    "by=['vacant', 'account', 'pop_2010'] specifies the columns to sort by.\n",
    "ascending=[False, True, True] indicates the sort order for each column, where False means descending and True means ascending.\n",
    "This will sort pop_vac_lic with vacant in descending order, account in ascending order, and pop_2010 in ascending order.\n",
    "\n",
    "Incorrect submission\n",
    "Did you call land_use.merge()?\n",
    "ChatGPT\n",
    "It looks like there was a mistake in the earlier code submission. To ensure the proper merging and sorting, you should first verify that the land_use, census, and licenses DataFrames are merged correctly and then perform the sorting operation.\n",
    "\n",
    "Here’s the corrected and complete code to merge, group, and sort:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge land_use and census on ward\n",
    "land_cen = land_use.merge(census, on='ward', suffixes=('_land', '_cen'))\n",
    "\n",
    "# Merge the result with licenses on ward, including suffixes\n",
    "land_cen_lic = land_cen.merge(licenses, on='ward', suffixes=('_cen', '_lic'))\n",
    "\n",
    "# Group by ward, pop_2010, and vacant, then count the number of accounts\n",
    "pop_vac_lic = land_cen_lic.groupby(['ward', 'pop_2010', 'vacant'], as_index=False).agg({'account': 'count'})\n",
    "\n",
    "# Sort pop_vac_lic by vacant (descending), account (ascending), and pop_2010 (ascending)\n",
    "sorted_pop_vac_lic = pop_vac_lic.sort_values(by=['vacant', 'account', 'pop_2010'], \n",
    "                                              ascending=[False, True, True])\n",
    "\n",
    "# Print the top few rows of sorted_pop_vac_lic\n",
    "print(sorted_pop_vac_lic.head())\n",
    "In this corrected version:\n",
    "\n",
    "The merging of DataFrames is done first to ensure that all relevant data is combined.\n",
    "The sort_values method is used to sort the DataFrame as required:\n",
    "vacant is sorted in descending order.\n",
    "account is sorted in ascending order.\n",
    "pop_2010 is sorted in ascending order.\n",
    "Make sure that you have correctly merged and grouped the data before performing the sorting operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging Tables With Different Join Types\n",
    "# Take your knowledge of joins to the next level. In this chapter, you’ll work with TMDb movie data as you learn about left, right, and outer joins. You’ll also discover how to merge a table to itself and merge on a DataFrame index.\n",
    "\n",
    "# Merge movies and financials with a left join\n",
    "movies_financials = movies.merge(financials, on='id', how='left')\n",
    "\n",
    "\n",
    "# Merge the movies table with the financials table with a left join\n",
    "movies_financials = movies.merge(financials, on='id', how='left')\n",
    "\n",
    "# Count the number of rows in the budget column that are missing\n",
    "number_of_missing_fin = movies_financials['budget'].isna().sum()\n",
    "\n",
    "# Print the number of movies missing financials\n",
    "print(number_of_missing_fin)\n",
    "\n",
    "\n",
    "'''Great job! You used a left join to find out which rows in the financials table were missing data. When performing a left join, the .merge() method returns a row full of null values for columns in the right table if the key column does not have a matching value in both tables. We see that there are at least 1,500 rows missing data. Wow! That sounds like a lot of work.'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting missing rows with left join\n",
    "The Movie Database is supported by volunteers going out into the world, collecting data, and entering it into the database. This includes financial data, such as movie budget and revenue. If you wanted to know which movies are still missing data, you could use a left join to identify them. Practice using a left join by merging the movies table and the financials table.\n",
    "\n",
    "The movies and financials tables have been loaded for you.\n",
    "\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "1\n",
    "2\n",
    "3\n",
    "Question\n",
    "What column is likely the best column to merge the two tables on?\n",
    "\n",
    "Possible answers\n",
    "\n",
    "\n",
    "on='budget'\n",
    "\n",
    "on='popularity'\n",
    "\n",
    "on='id'\n",
    "\n",
    "ChatGPT\n",
    "To merge the movies table with the financials table, the best column to use for merging is the one that uniquely identifies each movie in both tables. In this case, the column likely to be the best for merging is:\n",
    "\n",
    "on='id'\n",
    "The id column typically represents a unique identifier for each movie, ensuring an accurate and consistent merge between the two tables.\n",
    "\n",
    "'''Counting missing rows with left join\n",
    "The Movie Database is supported by volunteers going out into the world, collecting data, and entering it into the database. This includes financial data, such as movie budget and revenue. If you wanted to know which movies are still missing data, you could use a left join to identify them. Practice using a left join by merging the movies table and the financials table.\n",
    "\n",
    "The movies and financials tables have been loaded for you.\n",
    "\n",
    "Instructions 2/3\n",
    "35 XP\n",
    "2\n",
    "3\n",
    "Merge the movies table, as the left table, with the financials table using a left join, and save the result to movies_financials.'''\n",
    "\n",
    "# Merge movies and financials with a left join\n",
    "movies_financials = movies.merge(____)\n",
    "ChatGPT\n",
    "To merge the movies table with the financials table using a left join, you would specify how='left' and use the id column as the key for merging. Here is how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge movies and financials with a left join\n",
    "movies_financials = movies.merge(financials, on='id', how='left')\n",
    "\n",
    "# Count the number of rows in movies_financials with a null value in the budget column.\n",
    "\n",
    "# Merge the movies table with the financials table with a left join\n",
    "movies_financials = movies.merge(financials, on='id', how='left')\n",
    "\n",
    "# Count the number of rows in the budget column that are missing\n",
    "number_of_missing_fin = movies_financials['budget']____\n",
    "\n",
    "# Print the number of movies missing financials\n",
    "print(number_of_missing_fin)\n",
    "ChatGPT\n",
    "To count the number of rows in the movies_financials table where the budget column has a null value, you can use the .isna() method to check for missing values and then sum up the results. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge the movies table with the financials table with a left join\n",
    "movies_financials = movies.merge(financials, on='id', how='left')\n",
    "\n",
    "# Count the number of rows in the budget column that are missing\n",
    "number_of_missing_fin = movies_financials['budget'].isna().sum()\n",
    "\n",
    "# Print the number of movies missing financials\n",
    "print(number_of_missing_fin)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Is this conversation helpful so far?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge toy_story and taglines on the 'id' column with a left join\n",
    "toystory_tag = toy_story.merge(taglines, on='id', how='left')\n",
    "\n",
    "# Print the rows and shape of toystory_tag\n",
    "print(toystory_tag)\n",
    "print(toystory_tag.shape)\n",
    "\n",
    "\n",
    "# Merge toy_story and taglines on the 'id' column with an inner join\n",
    "toystory_tag = toy_story.merge(taglines, on='id', how='inner')\n",
    "\n",
    "# Print the rows and shape of toystory_tag\n",
    "print(toystory_tag)\n",
    "print(toystory_tag.shape)\n",
    "\n",
    "'''That's fantastic work! If your goal is to enhance or enrich a dataset, then you do not want to lose any of your original data. A left join will do that by returning all of the rows of your left table, while using an inner join may result in lost data if it does not exist in both tables.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "'''Enriching a dataset\n",
    "Setting how='left' with the .merge()method is a useful technique for enriching or enhancing a dataset with additional information from a different table. In this exercise, you will start off with a sample of movie data from the movie series Toy Story. Your goal is to enrich this data by adding the marketing tag line for each movie. You will compare the results of a left join versus an inner join.\n",
    "\n",
    "The toy_story DataFrame contains the Toy Story movies. The toy_story and taglines DataFrames have been loaded for you.\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "1\n",
    "Merge toy_story and taglines on the id column with a left join, and save the result as toystory_tag.'''\n",
    "\n",
    "\n",
    "# Merge the toy_story and taglines tables with a left join\n",
    "toystory_tag = toy_story.merge(____)\n",
    "\n",
    "# Print the rows and shape of toystory_tag\n",
    "print(toystory_tag)\n",
    "print(toystory_tag.shape)\n",
    "ChatGPT\n",
    "To perform the left join merge between toy_story and taglines on the id column and save the result as toystory_tag, you'll need to use the pd.merge() function with the how='left' parameter. Here's the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge toy_story and taglines on the 'id' column with a left join\n",
    "toystory_tag = toy_story.merge(taglines, on='id', how='left')\n",
    "\n",
    "# Print the rows and shape of toystory_tag\n",
    "print(toystory_tag)\n",
    "print(toystory_tag.shape)\n",
    "This code will merge the toy_story DataFrame with the taglines DataFrame on the id column using a left join, ensuring that all rows from toy_story are retained and matched with corresponding rows in taglines. If there is no match in taglines, the resulting DataFrame toystory_tag will have NaN values for the missing columns from taglines.\n",
    "\n",
    "'''\n",
    "With toy_story as the left table, merge to it taglines on the id column with an inner join, and save as toystory_tag.'''\n",
    "\n",
    "\n",
    "# Merge the toy_story and taglines tables with a inner join\n",
    "toystory_tag = ____\n",
    "\n",
    "# Print the rows and shape of toystory_tag\n",
    "print(toystory_tag)\n",
    "print(toystory_tag.shape)\n",
    "ChatGPT\n",
    "To perform the inner join merge between toy_story and taglines on the id column and save the result as toystory_tag, you can use the following code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge toy_story and taglines on the 'id' column with an inner join\n",
    "toystory_tag = toy_story.merge(taglines, on='id', how='inner')\n",
    "\n",
    "# Print the rows and shape of toystory_tag\n",
    "print(toystory_tag)\n",
    "print(toystory_tag.shape)\n",
    "This code will merge the toy_story DataFrame with the taglines DataFrame on the id column using an inner join, ensuring that only the rows with matching id values in both DataFrames are retained in the resulting DataFrame toystory_tag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "How many rows with a left join?\n",
    "Select the true statement about left joins.\n",
    "\n",
    "Try running the following code statements:\n",
    "\n",
    "left_table.merge(one_to_one, on='id', how='left').shape\n",
    "left_table.merge(one_to_many, on='id', how='left').shape\n",
    "Note that the left_table starts out with 4 rows.\n",
    "\n",
    "Instructions\n",
    "50 XP\n",
    "Possible answers\n",
    "\n",
    "\n",
    "The output of a one-to-one merge with a left join will have more rows than the left table.\n",
    "\n",
    "The output of a one-to-one merge with a left join will have fewer rows than the left table.\n",
    "\n",
    "The output of a one-to-many merge with a left join will have greater than or equal rows than the left table.\n",
    "ChatGPT\n",
    "Explanation of Left Joins\n",
    "A left join (or left merge) in SQL or pandas merges two tables by including all rows from the left table and the matched rows from the right table. If there are no matches, the result will contain NaN for columns from the right table.\n",
    "\n",
    "Statements to Evaluate\n",
    "left_table.merge(one_to_one, on='id', how='left').shape\n",
    "left_table.merge(one_to_many, on='id', how='left').shape\n",
    "Given:\n",
    "left_table starts with 4 rows.\n",
    "Analyzing the Statements\n",
    "1. One-to-One Merge with Left Join\n",
    "In a one-to-one merge, each row in the left table matches with at most one row in the right table.\n",
    "\n",
    "The number of rows in the resulting DataFrame will be the same as the left table because each row in the left table matches with at most one row in the right table.\n",
    "\n",
    "Output:\n",
    "\n",
    "The output of a one-to-one merge with a left join will have the same number of rows as the left table.\n",
    "2. One-to-Many Merge with Left Join\n",
    "In a one-to-many merge, each row in the left table may match with multiple rows in the right table.\n",
    "\n",
    "The number of rows in the resulting DataFrame will be greater than or equal to the number of rows in the left table because each row in the left table can match with multiple rows in the right table.\n",
    "\n",
    "Output:\n",
    "\n",
    "The output of a one-to-many merge with a left join will have greater than or equal rows compared to the left table.\n",
    "Correct Statement\n",
    "The output of a one-to-many merge with a left join will have greater than or equal rows than the left table.\n",
    "\n",
    "That's correct! A left join will return all of the rows from the left table. If those rows in the left table match multiple rows in the right table, then all of those rows will be returned. Therefore, the returned rows must be equal to if not greater than the left table. Knowing what to expect is useful in troubleshooting any suspicious merges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge action_movies to scifi_movies with right join\n",
    "action_scifi = action_movies.merge(scifi_movies, on='movie_id', how='right')\n",
    "\n",
    "# Merge action_movies to scifi_movies with right join, adding suffixes\n",
    "action_scifi = action_movies.merge(scifi_movies, on='movie_id', how='right',\n",
    "                                   suffixes=('_act', '_sci'))\n",
    "\n",
    "# Print the first few rows of action_scifi to see the structure\n",
    "print(action_scifi.head())\n",
    "\n",
    "\n",
    "# Merge action_movies to the scifi_movies with right join\n",
    "action_scifi = action_movies.merge(scifi_movies, on='movie_id', how='right',\n",
    "                                   suffixes=('_act', '_sci'))\n",
    "\n",
    "# From action_scifi, select only the rows where the genre_act column is null\n",
    "scifi_only = action_scifi[action_scifi['genre_act'].isnull()]\n",
    "\n",
    "# Merge the movies and scifi_only tables with an inner join\n",
    "movies_and_scifi_only = movies.merge(scifi_only, left_on='id', right_on='movie_id', how='inner')\n",
    "\n",
    "# Print the first few rows and shape of movies_and_scifi_only\n",
    "print(movies_and_scifi_only.head())\n",
    "print(movies_and_scifi_only.shape)\n",
    "\n",
    "'''Well done, right join to the rescue! You found over 250 action only movies by merging action_movies and scifi_movies using a right join. With this, you were able to find the rows not found in the action_movies table. Additionally, you used the left_on and right_on arguments to merge in the movies table. Wow! You are a superhero.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "How many rows with a left join?\n",
    "Select the true statement about left joins.\n",
    "\n",
    "Try running the following code statements:\n",
    "\n",
    "left_table.merge(one_to_one, on='id', how='left').shape\n",
    "left_table.merge(one_to_many, on='id', how='left').shape\n",
    "Note that the left_table starts out with 4 rows.\n",
    "\n",
    "Instructions\n",
    "50 XP\n",
    "Possible answers\n",
    "\n",
    "\n",
    "The output of a one-to-one merge with a left join will have more rows than the left table.\n",
    "\n",
    "The output of a one-to-one merge with a left join will have fewer rows than the left table.\n",
    "\n",
    "The output of a one-to-many merge with a left join will have greater than or equal rows than the left table.\n",
    "ChatGPT\n",
    "Explanation of Left Joins\n",
    "A left join (or left merge) in SQL or pandas merges two tables by including all rows from the left table and the matched rows from the right table. If there are no matches, the result will contain NaN for columns from the right table.\n",
    "\n",
    "Statements to Evaluate\n",
    "left_table.merge(one_to_one, on='id', how='left').shape\n",
    "left_table.merge(one_to_many, on='id', how='left').shape\n",
    "Given:\n",
    "left_table starts with 4 rows.\n",
    "Analyzing the Statements\n",
    "1. One-to-One Merge with Left Join\n",
    "In a one-to-one merge, each row in the left table matches with at most one row in the right table.\n",
    "\n",
    "The number of rows in the resulting DataFrame will be the same as the left table because each row in the left table matches with at most one row in the right table.\n",
    "\n",
    "Output:\n",
    "\n",
    "The output of a one-to-one merge with a left join will have the same number of rows as the left table.\n",
    "2. One-to-Many Merge with Left Join\n",
    "In a one-to-many merge, each row in the left table may match with multiple rows in the right table.\n",
    "\n",
    "The number of rows in the resulting DataFrame will be greater than or equal to the number of rows in the left table because each row in the left table can match with multiple rows in the right table.\n",
    "\n",
    "Output:\n",
    "\n",
    "The output of a one-to-many merge with a left join will have greater than or equal rows compared to the left table.\n",
    "Correct Statement\n",
    "The output of a one-to-many merge with a left join will have greater than or equal rows than the left table.\n",
    "\n",
    "'''Right join to find unique movies\n",
    "Most of the recent big-budget science fiction movies can also be classified as action movies. You are given a table of science fiction movies called scifi_movies and another table of action movies called action_movies. Your goal is to find which movies are considered only science fiction movies. Once you have this table, you can merge the movies table in to see the movie names. Since this exercise is related to science fiction movies, use a right join as your superhero power to solve this problem.\n",
    "\n",
    "The movies, scifi_movies, and action_movies tables have been loaded for you.\n",
    "\n",
    "Instructions 1/4\n",
    "25 XP\n",
    "1\n",
    "Merge action_movies and scifi_movies tables with a right join on movie_id. Save the result as action_scifi.'''\n",
    "\n",
    "# Merge action_movies to scifi_movies with right join\n",
    "action_scifi = ____.merge(____)\n",
    "ChatGPT\n",
    "To perform a right join between the action_movies and scifi_movies tables on movie_id and save the result as action_scifi, you can use the following code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge action_movies to scifi_movies with right join\n",
    "action_scifi = action_movies.merge(scifi_movies, on='movie_id', how='right')\n",
    "This code merges the action_movies table with the scifi_movies table, ensuring that all rows from the scifi_movies table are included, along with any matching rows from the action_movies table.\n",
    "\n",
    "Make sure the tables action_movies and scifi_movies are loaded and available in your environment. Here's how the complete process might look:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Assuming movies, scifi_movies, and action_movies are already loaded\n",
    "\n",
    "# Merge action_movies to scifi_movies with right join\n",
    "action_scifi = action_movies.merge(scifi_movies, on='movie_id', how='right')\n",
    "This will create a DataFrame action_scifi containing all the movies that are classified as science fiction, along with any matching action movie data based on the movie_id.\n",
    "\n",
    "# Update the merge to add suffixes, where '_act' and '_sci' are suffixes for the left and right tables, respectively.\n",
    "\n",
    "# Merge action_movies to scifi_movies with right join\n",
    "action_scifi = action_movies.merge(scifi_movies, on='movie_id', how='right',\n",
    "                                   ____)\n",
    "\n",
    "# Print the first few rows of action_scifi to see the structure\n",
    "print(action_scifi.head())\n",
    "ChatGPT\n",
    "To add suffixes to the columns from the left and right tables, you can use the suffixes parameter in the merge function. Here's how you can update the merge statement to include suffixes _act for the left table (action_movies) and _sci for the right table (scifi_movies):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge action_movies to scifi_movies with right join, adding suffixes\n",
    "action_scifi = action_movies.merge(scifi_movies, on='movie_id', how='right',\n",
    "                                   suffixes=('_act', '_sci'))\n",
    "\n",
    "# Print the first few rows of action_scifi to see the structure\n",
    "print(action_scifi.head())\n",
    "This code ensures that if there are columns with the same name in both tables, they will be suffixed with _act and _sci respectively to distinguish them.\n",
    "\n",
    "# From action_scifi, subset only the rows where the genre_act column is null.\n",
    "\n",
    "\n",
    "# Merge action_movies to the scifi_movies with right join\n",
    "action_scifi = action_movies.merge(scifi_movies, on='movie_id', how='right',\n",
    "                                   suffixes=('_act','_sci'))\n",
    "\n",
    "# From action_scifi, select only the rows where the genre_act column is null\n",
    "scifi_only = action_scifi[____]\n",
    "ChatGPT\n",
    "To subset the rows from action_scifi where the genre_act column is null, you can use the .isnull() method. Here’s how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge action_movies to the scifi_movies with right join\n",
    "action_scifi = action_movies.merge(scifi_movies, on='movie_id', how='right',\n",
    "                                   suffixes=('_act', '_sci'))\n",
    "\n",
    "# From action_scifi, select only the rows where the genre_act column is null\n",
    "scifi_only = action_scifi[action_scifi['genre_act'].isnull()]\n",
    "\n",
    "# Print the first few rows of scifi_only to see the structure\n",
    "print(scifi_only.head())\n",
    "This code will create a DataFrame scifi_only that contains only the rows where the genre_act column is null, indicating that those movies are only classified as science fiction movies and not as action movies.\n",
    "\n",
    "# Merge movies and scifi_only using the id column in the left table and the movie_id column in the right table with an inner join.\n",
    "\n",
    "# Merge action_movies to the scifi_movies with right join\n",
    "action_scifi = action_movies.merge(scifi_movies, on='movie_id', how='right',\n",
    "                                   suffixes=('_act','_sci'))\n",
    "\n",
    "# From action_scifi, select only the rows where the genre_act column is null\n",
    "scifi_only = action_scifi[action_scifi['genre_act'].isnull()]\n",
    "\n",
    "# Merge the movies and scifi_only tables with an inner join\n",
    "movies_and_scifi_only = movies.____\n",
    "\n",
    "# Print the first few rows and shape of movies_and_scifi_only\n",
    "print(movies_and_scifi_only.head())\n",
    "print(movies_and_scifi_only.shape)\n",
    "ChatGPT\n",
    "To merge the movies table with the scifi_only table using an inner join, you can specify the id column from the movies table and the movie_id column from the scifi_only table. Here’s the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge action_movies to the scifi_movies with right join\n",
    "action_scifi = action_movies.merge(scifi_movies, on='movie_id', how='right',\n",
    "                                   suffixes=('_act', '_sci'))\n",
    "\n",
    "# From action_scifi, select only the rows where the genre_act column is null\n",
    "scifi_only = action_scifi[action_scifi['genre_act'].isnull()]\n",
    "\n",
    "# Merge the movies and scifi_only tables with an inner join\n",
    "movies_and_scifi_only = movies.merge(scifi_only, left_on='id', right_on='movie_id', how='inner')\n",
    "\n",
    "# Print the first few rows and shape of movies_and_scifi_only\n",
    "print(movies_and_scifi_only.head())\n",
    "print(movies_and_scifi_only.shape)\n",
    "This code will merge the movies table with the scifi_only table using an inner join on the id column from the movies table and the movie_id column from the scifi_only table. The resulting DataFrame movies_and_scifi_only will contain only the rows that have matching values in both tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use right join to merge the movie_to_genres and pop_movies tables\n",
    "genres_movies = movie_to_genres.merge(pop_movies, how='right', \n",
    "                                      left_on='movie_id', \n",
    "                                      right_on='id')\n",
    "\n",
    "# Count the number of genres\n",
    "genre_count = genres_movies.groupby('genre').agg({'id': 'count'})\n",
    "\n",
    "# Plot a bar chart of the genre_count\n",
    "genre_count.plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "'''Nice job! The right join ensured that you were analyzing all of the pop_movies. You see from the results that adventure and action are the most popular genres.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "'''Popular genres with right join\n",
    "What are the genres of the most popular movies? To answer this question, you need to merge data from the movies and movie_to_genres tables. In a table called pop_movies, the top 10 most popular movies in the movies table have been selected. To ensure that you are analyzing all of the popular movies, merge it with the movie_to_genres table using a right join. To complete your analysis, count the number of different genres. Also, the two tables can be merged by the movie ID. However, in pop_movies that column is called id, and in movie_to_genres it's called movie_id.\n",
    "\n",
    "The pop_movies and movie_to_genres tables have been loaded for you.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Merge movie_to_genres and pop_movies using a right join. Save the results as genres_movies.\n",
    "Group genres_movies by genre and count the number of id values.'''\n",
    "\n",
    "\n",
    "\n",
    "# Use right join to merge the movie_to_genres and pop_movies tables\n",
    "genres_movies = ____.merge(____, how='____', \n",
    "                                      ____, \n",
    "                                      ____)\n",
    "\n",
    "# Count the number of genres\n",
    "genre_count = genres_movies.groupby('____').agg({'id':'count'})\n",
    "\n",
    "# Plot a bar chart of the genre_count\n",
    "genre_count.plot(kind='bar')\n",
    "plt.show()\n",
    "ChatGPT\n",
    "To complete the task of merging the movie_to_genres and pop_movies tables using a right join, follow these instructions:\n",
    "\n",
    "Perform the right join: Merge the movie_to_genres and pop_movies tables using a right join on the appropriate columns.\n",
    "Group by genre and count: Group the merged DataFrame by genre and count the number of occurrences of each genre.\n",
    "Plot the results: Create a bar chart to visualize the count of each genre.\n",
    "Here's how you can fill in the code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Use right join to merge the movie_to_genres and pop_movies tables\n",
    "genres_movies = movie_to_genres.merge(pop_movies, how='right', \n",
    "                                      left_on='movie_id', \n",
    "                                      right_on='id')\n",
    "\n",
    "# Count the number of genres\n",
    "genre_count = genres_movies.groupby('genre').agg({'id': 'count'})\n",
    "\n",
    "# Plot a bar chart of the genre_count\n",
    "genre_count.plot(kind='bar')\n",
    "plt.show()\n",
    "In the code:\n",
    "\n",
    "left_on='movie_id' specifies the column from movie_to_genres for joining.\n",
    "right_on='id' specifies the column from pop_movies for joining.\n",
    "groupby('genre') groups the data by genre.\n",
    "agg({'id': 'count'}) counts the number of occurrences of each genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Using outer join to select actors\n",
    "One cool aspect of using an outer join is that, because it returns all rows from both merged tables and null where they do not match, you can use it to find rows that do not have a match in the other table. To try for yourself, you have been given two tables with a list of actors from two popular movies: Iron Man 1 and Iron Man 2. Most of the actors played in both movies. Use an outer join to find actors who did not act in both movies.\n",
    "\n",
    "The Iron Man 1 table is called iron_1_actors, and Iron Man 2 table is called iron_2_actors. Both tables have been loaded for you and a few rows printed so you can see the structure.\n",
    "'''\n",
    "# Merge iron_1_actors to iron_2_actors on id with outer join using suffixes\n",
    "iron_1_and_2 = iron_1_actors.merge(iron_2_actors,\n",
    "                                     how='outer',\n",
    "                                     on='id',\n",
    "                                     suffixes=('_1', '_2'))\n",
    "\n",
    "# Create an index that returns true if name_1 or name_2 are null\n",
    "m = (iron_1_and_2['name_1'].isna() | \n",
    "     iron_1_and_2['name_2'].isna())\n",
    "\n",
    "# Print the first few rows of iron_1_and_2\n",
    "print(iron_1_and_2[m].head())\n",
    "\n",
    "\n",
    "'''Nice job! Using an outer join, you were able to pick only those rows where the actor played in only one of the two movies.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "'''Popular genres with right join\n",
    "What are the genres of the most popular movies? To answer this question, you need to merge data from the movies and movie_to_genres tables. In a table called pop_movies, the top 10 most popular movies in the movies table have been selected. To ensure that you are analyzing all of the popular movies, merge it with the movie_to_genres table using a right join. To complete your analysis, count the number of different genres. Also, the two tables can be merged by the movie ID. However, in pop_movies that column is called id, and in movie_to_genres it's called movie_id.\n",
    "\n",
    "The pop_movies and movie_to_genres tables have been loaded for you.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Merge movie_to_genres and pop_movies using a right join. Save the results as genres_movies.\n",
    "Group genres_movies by genre and count the number of id values.'''\n",
    "\n",
    "\n",
    "\n",
    "# Use right join to merge the movie_to_genres and pop_movies tables\n",
    "genres_movies = ____.merge(____, how='____', \n",
    "                                      ____, \n",
    "                                      ____)\n",
    "\n",
    "# Count the number of genres\n",
    "genre_count = genres_movies.groupby('____').agg({'id':'count'})\n",
    "\n",
    "# Plot a bar chart of the genre_count\n",
    "genre_count.plot(kind='bar')\n",
    "plt.show()\n",
    "ChatGPT\n",
    "To complete the task of merging the movie_to_genres and pop_movies tables using a right join, follow these instructions:\n",
    "\n",
    "Perform the right join: Merge the movie_to_genres and pop_movies tables using a right join on the appropriate columns.\n",
    "Group by genre and count: Group the merged DataFrame by genre and count the number of occurrences of each genre.\n",
    "Plot the results: Create a bar chart to visualize the count of each genre.\n",
    "Here's how you can fill in the code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Use right join to merge the movie_to_genres and pop_movies tables\n",
    "genres_movies = movie_to_genres.merge(pop_movies, how='right', \n",
    "                                      left_on='movie_id', \n",
    "                                      right_on='id')\n",
    "\n",
    "# Count the number of genres\n",
    "genre_count = genres_movies.groupby('genre').agg({'id': 'count'})\n",
    "\n",
    "# Plot a bar chart of the genre_count\n",
    "genre_count.plot(kind='bar')\n",
    "plt.show()\n",
    "In the code:\n",
    "\n",
    "left_on='movie_id' specifies the column from movie_to_genres for joining.\n",
    "right_on='id' specifies the column from pop_movies for joining.\n",
    "groupby('genre') groups the data by genre.\n",
    "agg({'id': 'count'}) counts the number of occurrences of each genre.\n",
    "Using outer join to select actors\n",
    "One cool aspect of using an outer join is that, because it returns all rows from both merged tables and null where they do not match, you can use it to find rows that do not have a match in the other table. To try for yourself, you have been given two tables with a list of actors from two popular movies: Iron Man 1 and Iron Man 2. Most of the actors played in both movies. Use an outer join to find actors who did not act in both movies.\n",
    "\n",
    "The Iron Man 1 table is called iron_1_actors, and Iron Man 2 table is called iron_2_actors. Both tables have been loaded for you and a few rows printed so you can see the structure.\n",
    "ChatGPT\n",
    "To find actors who did not act in both movies using an outer join, you'll want to merge the two tables on the actor column and identify those actors who appear in only one of the tables. Here's how you can accomplish this:\n",
    "\n",
    "Perform an outer join: Merge iron_1_actors and iron_2_actors using an outer join to include all actors from both tables.\n",
    "Identify non-matching rows: Filter the results to find actors who appear in only one of the two movies.\n",
    "Here's how you can write the code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Perform outer join on the actor column\n",
    "merged_actors = iron_1_actors.merge(iron_2_actors, how='outer', \n",
    "                                    left_on='actor', \n",
    "                                    right_on='actor', \n",
    "                                    suffixes=('_iron1', '_iron2'))\n",
    "\n",
    "# Find actors who did not act in both movies\n",
    "non_matching_actors = merged_actors[merged_actors['actor'].isna() | merged_actors['actor'].isna()]\n",
    "\n",
    "# Print the non-matching actors\n",
    "print(non_matching_actors)\n",
    "In this code:\n",
    "\n",
    "how='outer' specifies the type of join to include all rows from both tables.\n",
    "left_on='actor' and right_on='actor' indicate the columns on which to merge.\n",
    "suffixes=('_iron1', '_iron2') adds suffixes to distinguish columns from the two tables in the result.\n",
    "merged_actors['actor'].isna() | merged_actors['actor'].isna() identifies actors who are in only one of the tables.\n",
    "This will give you a DataFrame with actors who did not appear in both movies.\n",
    "\n",
    "'''Save to iron_1_and_2 the merge of iron_1_actors (left) with iron_2_actors tables with an outer join on the id column, and set suffixes to ('_1','_2').\n",
    "Create an index that returns True if name_1 or name_2 are null, and False otherwise.'''\n",
    "\n",
    "# Merge iron_1_actors to iron_2_actors on id with outer join using suffixes\n",
    "iron_1_and_2 = iron_1_actors.merge(____,\n",
    "                                     ____,\n",
    "                                     ____,\n",
    "                                     suffixes=____)\n",
    "\n",
    "# Create an index that returns true if name_1 or name_2 are null\n",
    "m = ((iron_1_and_2['name_1'].____) | \n",
    "     (iron_1_and_2['____'].____))\n",
    "\n",
    "# Print the first few rows of iron_1_and_2\n",
    "print(iron_1_and_2[m].head())\n",
    "ChatGPT\n",
    "To complete the task, you need to merge iron_1_actors and iron_2_actors on the id column using an outer join and handle the suffixes properly. After that, create an index to find rows where either name_1 or name_2 are null.\n",
    "\n",
    "Here's how you can fill in the blanks:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge iron_1_actors to iron_2_actors on id with outer join using suffixes\n",
    "iron_1_and_2 = iron_1_actors.merge(iron_2_actors,\n",
    "                                     how='outer',\n",
    "                                     on='id',\n",
    "                                     suffixes=('_1', '_2'))\n",
    "\n",
    "# Create an index that returns true if name_1 or name_2 are null\n",
    "m = (iron_1_and_2['name_1'].isna() | \n",
    "     iron_1_and_2['name_2'].isna())\n",
    "\n",
    "# Print the first few rows of iron_1_and_2\n",
    "print(iron_1_and_2[m].head())\n",
    "In this code:\n",
    "\n",
    "how='outer' specifies that an outer join should be used.\n",
    "on='id' indicates the column on which to merge.\n",
    "suffixes=('_1', '_2') adds suffixes to distinguish columns from the two tables in the merged result.\n",
    "iron_1_and_2['name_1'].isna() | iron_1_and_2['name_2'].isna() creates a boolean index that is True for rows where either name_1 or name_2 is null.\n",
    "This will merge the two DataFrames and allow you to identify and print rows where actors are missing in one of the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crews_self_merged = crews.merge(crews, on='id', suffixes=('_dir', '_crew'))\n",
    "\n",
    "\n",
    "# Merge the crews table to itself\n",
    "crews_self_merged = crews.merge(crews, on='id', how='inner', suffixes=('_dir', '_crew'))\n",
    "\n",
    "# Create a Boolean index to select the appropriate rows\n",
    "boolean_filter = ((crews_self_merged['job_dir'] == 'Director') & \n",
    "                  (crews_self_merged['job_crew'] != 'Director'))\n",
    "\n",
    "# Apply the filter\n",
    "direct_crews = crews_self_merged[boolean_filter]\n",
    "\n",
    "\n",
    "# Merge the crews table to itself\n",
    "crews_self_merged = crews.merge(crews, on='id', how='inner', suffixes=('_dir', '_crew'))\n",
    "\n",
    "# Create a boolean index to select the appropriate rows\n",
    "boolean_filter = ((crews_self_merged['job_dir'] == 'Director') & \n",
    "                  (crews_self_merged['job_crew'] != 'Director'))\n",
    "direct_crews = crews_self_merged[boolean_filter]\n",
    "\n",
    "# Print the first few rows of direct_crews\n",
    "print(direct_crews.head())\n",
    "\n",
    "\n",
    "\n",
    "'''Great job! By merging the table to itself, you compared the value of the __director__ from the jobs column to other values from the jobs column. With the output, you can quickly see different movie directors and the people they worked with in the same movie.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "'''Self join\n",
    "Merging a table to itself can be useful when you want to compare values in a column to other values in the same column. In this exercise, you will practice this by creating a table that for each movie will list the movie director and a member of the crew on one row. You have been given a table called crews, which has columns id, job, and name. First, merge the table to itself using the movie ID. This merge will give you a larger table where for each movie, every job is matched against each other. Then select only those rows with a director in the left table, and avoid having a row where the director's job is listed in both the left and right tables. This filtering will remove job combinations that aren't with the director.\n",
    "\n",
    "The crews table has been loaded for you.\n",
    "\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "1\n",
    "2\n",
    "3\n",
    "To a variable called crews_self_merged, merge the crews table to itself on the id column using an inner join, setting the suffixes to '_dir' and '_crew' for the left and right tables respectively.'''\n",
    "\n",
    "# Merge the crews table to itself\n",
    "crews_self_merged = ____\n",
    "ChatGPT\n",
    "To merge the crews table to itself using an inner join on the id column and set the suffixes to _dir and _crew, you can use the following code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "crews_self_merged = crews.merge(crews, on='id', suffixes=('_dir', '_crew'))\n",
    "This code will create a new DataFrame, crews_self_merged, where each row contains information from both the left and right tables, with suffixes _dir for the left table and _crew for the right table.\n",
    "\n",
    "'''Create a Boolean index, named boolean_filter, that selects rows from the left table with the job of 'Director' and avoids rows with the job of 'Director' in the right table.'''\n",
    "\n",
    "# Merge the crews table to itself\n",
    "crews_self_merged = crews.merge(crews, on='id', how='inner',\n",
    "                                suffixes=('_dir','_crew'))\n",
    "\n",
    "# Create a Boolean index to select the appropriate\n",
    "boolean_filter = ((crews_self_merged['job_dir'] == ____) & \n",
    "     (crews_self_merged['____'] != ____))\n",
    "direct_crews = crews_self_merged[boolean_filter]\n",
    "ChatGPT\n",
    "To create the boolean_filter that selects rows from the left table where the job is 'Director' and avoids rows where the job is 'Director' in the right table, you can use the following code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge the crews table to itself\n",
    "crews_self_merged = crews.merge(crews, on='id', how='inner', suffixes=('_dir', '_crew'))\n",
    "\n",
    "# Create a Boolean index to select the appropriate rows\n",
    "boolean_filter = ((crews_self_merged['job_dir'] == 'Director') & \n",
    "                  (crews_self_merged['job_crew'] != 'Director'))\n",
    "\n",
    "# Apply the filter\n",
    "direct_crews = crews_self_merged[boolean_filter]\n",
    "In the boolean_filter, replace the placeholders as follows:\n",
    "\n",
    "crews_self_merged['job_dir'] == 'Director' selects rows where the job in the left table (director) is 'Director'.\n",
    "crews_self_merged['job_crew'] != 'Director' ensures that the job in the right table (crew) is not 'Director'.\n",
    "# Use the .head() method to print the first few rows of direct_crews.\n",
    "\n",
    "\n",
    "# Merge the crews table to itself\n",
    "crews_self_merged = crews.merge(crews, on='id', how='inner',\n",
    "                                suffixes=('_dir','_crew'))\n",
    "\n",
    "# Create a boolean index to select the appropriate rows\n",
    "boolean_filter = ((crews_self_merged['job_dir'] == 'Director') & \n",
    "                  (crews_self_merged['job_crew'] != 'Director'))\n",
    "direct_crews = crews_self_merged[boolean_filter]\n",
    "\n",
    "# Print the first few rows of direct_crews\n",
    "print(____)\n",
    "ChatGPT\n",
    "To print the first few rows of the direct_crews DataFrame, you can use the .head() method. Here’s the complete code with the placeholder filled in:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge the crews table to itself\n",
    "crews_self_merged = crews.merge(crews, on='id', how='inner', suffixes=('_dir', '_crew'))\n",
    "\n",
    "# Create a boolean index to select the appropriate rows\n",
    "boolean_filter = ((crews_self_merged['job_dir'] == 'Director') & \n",
    "                  (crews_self_merged['job_crew'] != 'Director'))\n",
    "direct_crews = crews_self_merged[boolean_filter]\n",
    "\n",
    "# Print the first few rows of direct_crews\n",
    "print(direct_crews.head())\n",
    "\n",
    "How does pandas handle self joins?\n",
    "Select the false statement about merging a table to itself.\n",
    "\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "Select one answer\n",
    "\n",
    "You can merge a table to itself with a right join.\n",
    "Press\n",
    "1\n",
    "\n",
    "Merging a table to itself can allow you to compare values in a column to other values in the same column.\n",
    "Press\n",
    "2\n",
    "\n",
    "The pandas module limits you to one merge where you merge a table to itself. You cannot repeat this process over and over.\n",
    "Press\n",
    "3\n",
    "\n",
    "Merging a table to itself is like working with two separate tables.\n",
    "Press\n",
    "4\n",
    "ChatGPT\n",
    "The false statement about merging a table to itself is:\n",
    "\n",
    "The pandas module limits you to one merge where you merge a table to itself. You cannot repeat this process over and over.\n",
    "\n",
    "In reality, you can perform multiple self-joins in pandas, just like with regular joins. There is no such limitation in pandas.\n",
    "\n",
    "\n",
    "Perfect! This statement is false. pandas treats a merge of a table to itself the same as any other merge. Therefore, it does not limit you from chaining multiple .merge() methods together.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the movies table with the ratings table on the 'id' column\n",
    "movies_ratings = movies.merge(ratings, on='id', how='left')\n",
    "\n",
    "# Print the first few rows of movies_ratings\n",
    "print(movies_ratings.head())\n",
    "\n",
    "\n",
    "# alternatively\n",
    "# Merge the movies table with the ratings table on the 'id' column, keeping all rows from the movies table\n",
    "movies_ratings = movies.merge(ratings, on='id', how='left')\n",
    "\n",
    "# Print the first few rows of movies_ratings\n",
    "print(movies_ratings.head())\n",
    "\n",
    "\n",
    "'''Good work! Merging on indexes is just like merging on columns, so if you need to merge based on indexes, there's no need to turn the indexes into columns first.\n",
    "\n",
    "on='id' specifies the column to join on\n",
    "hoe='left' ensures that all the rows from the movies table are included, and only the matching rows from the 'rating' table are included.\n",
    "\n",
    "How does pandas handle self joins?\n",
    "Select the false statement about merging a table to itself.\n",
    "\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "Select one answer\n",
    "\n",
    "You can merge a table to itself with a right join.\n",
    "Press\n",
    "1\n",
    "\n",
    "Merging a table to itself can allow you to compare values in a column to other values in the same column.\n",
    "Press\n",
    "2\n",
    "\n",
    "The pandas module limits you to one merge where you merge a table to itself. You cannot repeat this process over and over.\n",
    "Press\n",
    "3\n",
    "\n",
    "Merging a table to itself is like working with two separate tables.\n",
    "Press\n",
    "4\n",
    "ChatGPT\n",
    "The false statement about merging a table to itself is:\n",
    "\n",
    "The pandas module limits you to one merge where you merge a table to itself. You cannot repeat this process over and over.\n",
    "\n",
    "In reality, you can perform multiple self-joins in pandas, just like with regular joins. There is no such limitation in pandas.\n",
    "\n",
    "'''Index merge for movie ratings\n",
    "To practice merging on indexes, you will merge movies and a table called ratings that holds info about movie ratings. Ensure that your merge returns all rows from the movies table, and only matching rows from the ratings table.\n",
    "\n",
    "The movies and ratings tables have been loaded for you.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Merge the movies and ratings tables on the id column, keeping all rows from the movies table, and save the result as movies_ratings.'''\n",
    "\n",
    "# Merge to the movies table the ratings table on the index\n",
    "movies_ratings = ____\n",
    "\n",
    "# Print the first few rows of movies_ratings\n",
    "print(movies_ratings.head())\n",
    "ChatGPT\n",
    "To merge the movies table with the ratings table while keeping all rows from the movies table and only the matching rows from the ratings table, you can use the merge function from pandas. You want to perform a left join, which ensures that all entries from the left DataFrame (movies) are included, and only the matching entries from the right DataFrame (ratings) are added.\n",
    "\n",
    "Here's how you can achieve this:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge the movies table with the ratings table on the 'id' column, keeping all rows from the movies table\n",
    "movies_ratings = movies.merge(ratings, on='id', how='left')\n",
    "\n",
    "# Print the first few rows of movies_ratings\n",
    "print(movies_ratings.head())\n",
    "In this code:\n",
    "\n",
    "movies.merge(ratings, on='id', how='left') specifies that you're merging the movies DataFrame with the ratings DataFrame on the id column. The how='left' parameter ensures that all rows from the movies table are preserved, and only matching rows from the ratings table are included.\n",
    "This approach helps you to retain all movie entries while appending the corresponding ratings where available. \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sequels and financials on index id, keeping all rows from sequels\n",
    "sequels_fin = sequels.merge(financials, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# correction\n",
    "# Merge sequels and financials on index id\n",
    "sequels_fin = sequels.merge(financials, on='id', how='left')\n",
    "\n",
    "# Self merge with suffixes as inner join with left on sequel and right on id\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', \n",
    "                             right_on='id', right_index=True,\n",
    "                             suffixes=('_org','_seq'))\n",
    "\n",
    "# Add calculation to subtract revenue_org from revenue_seq \n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "\n",
    "\n",
    "# Print the first rows of the sorted titles_diff\n",
    "print(titles_diff.sort_values(by='diff', ascending=False).head())\n",
    "\n",
    "'''Amazing, that was great work! To complete this exercise, you needed to merge tables on their index and merge another table to itself. After the calculations were added and sub-select specific columns, the data was sorted. You found out that Jurassic World had one of the highest of all, improvement in revenue compared to the original movie.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do sequels earn more?\n",
    "It is time to put together many of the aspects that you have learned in this chapter. In this exercise, you'll find out which movie sequels earned the most compared to the original movie. To answer this question, you will merge a modified version of the sequels and financials tables where their index is the movie ID. You will need to choose a merge type that will return all of the rows from the sequels table and not all the rows of financials table need to be included in the result. From there, you will join the resulting table to itself so that you can compare the revenue values of the original movie to the sequel. Next, you will calculate the difference between the two revenues and sort the resulting dataset.\n",
    "\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "Sure, let’s break this down step-by-step. Here’s how you can approach this problem using Python and pandas:\n",
    "\n",
    "Merge the sequels and financials tables:\n",
    "Use a left join to ensure all rows from the sequels table are included.\n",
    "Set the index to movie_id.\n",
    "Join the resulting table to itself:\n",
    "This will allow you to compare the revenue of the original movie to its sequel.\n",
    "Calculate the revenue difference:\n",
    "Subtract the revenue of the original movie from the sequel.\n",
    "Sort the dataset:\n",
    "Sort by the revenue difference to find which sequels earned the most compared to the original.\n",
    "Here’s a code example to illustrate these steps:\n",
    "\n",
    "Python\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "sequels = pd.DataFrame({\n",
    "    'movie_id': [1, 2, 3],\n",
    "    'title': ['Original Movie 1', 'Sequel Movie 1', 'Sequel Movie 2'],\n",
    "    'sequel': [2, None, None]\n",
    "})\n",
    "\n",
    "financials = pd.DataFrame({\n",
    "    'movie_id': [1, 2, 3],\n",
    "    'revenue': [100, 150, 200]\n",
    "})\n",
    "\n",
    "# Merge sequels and financials tables\n",
    "sequels_financials = sequels.merge(financials, on='movie_id', how='left')\n",
    "\n",
    "# Join the table to itself to compare original and sequel revenues\n",
    "merged = sequels_financials.merge(\n",
    "    sequels_financials,\n",
    "    left_on='sequel',\n",
    "    right_on='movie_id',\n",
    "    suffixes=('_original', '_sequel')\n",
    ")\n",
    "\n",
    "# Calculate the revenue difference\n",
    "merged['revenue_difference'] = merged['revenue_sequel'] - merged['revenue_original']\n",
    "\n",
    "# Sort the dataset by revenue difference\n",
    "sorted_merged = merged.sort_values(by='revenue_difference', ascending=False)\n",
    "\n",
    "print(sorted_merged[['title_original', 'title_sequel', 'revenue_original', 'revenue_sequel', 'revenue_difference']])\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This code will give you a DataFrame showing the original movie, its sequel, their respective revenues, and the difference in revenue. You can adjust the sample data to match your actual data.\n",
    "\n",
    "Feel free to ask if you have any questions or need further assistance!\n",
    "\n",
    "You\n",
    "Sent by you:\n",
    "# Self merge with suffixes as inner join with left on sequel and right on id\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', right_on='id', suffixes=('_org', '_seq'))\n",
    "\n",
    "# Add calculation to subtract revenue_org from revenue_seq \n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "Your approach looks great! You’re correctly performing a self-merge and calculating the revenue difference. Here’s a complete example incorporating your code and adding the sorting step:\n",
    "\n",
    "Python\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "sequels_fin = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'title': ['Original Movie 1', 'Sequel Movie 1', 'Sequel Movie 2'],\n",
    "    'sequel': [2, None, None],\n",
    "    'revenue': [100, 150, 200]\n",
    "})\n",
    "\n",
    "# Self merge with suffixes as inner join with left on sequel and right on id\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', right_on='id', suffixes=('_org', '_seq'))\n",
    "\n",
    "# Add calculation to subtract revenue_org from revenue_seq \n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "\n",
    "# Sort the dataset by revenue difference\n",
    "sorted_orig_seq = orig_seq.sort_values(by='diff', ascending=False)\n",
    "\n",
    "# Display the relevant columns\n",
    "print(sorted_orig_seq[['title_org', 'title_seq', 'revenue_org', 'revenue_seq', 'diff']])\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This code will give you a DataFrame showing the original movie, its sequel, their respective revenues, and the difference in revenue, sorted by the revenue difference.\n",
    "\n",
    "If you have any more questions or need further assistance, feel free to ask!\n",
    "\n",
    "Learn more\n",
    "\n",
    "1\n",
    "\n",
    "github.com\n",
    "You\n",
    "Sent by you:\n",
    "Incorrect submission\n",
    "Did you merge the correct DataFrame to sequels?\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "It looks like there might be a misunderstanding in the merging process. Let’s ensure we’re merging the correct DataFrames. Here’s a revised approach:\n",
    "\n",
    "Merge the sequels and financials tables:\n",
    "Use a left join to ensure all rows from the sequels table are included.\n",
    "Set the index to movie_id.\n",
    "Self-merge the resulting DataFrame:\n",
    "This will allow you to compare the revenue of the original movie to its sequel.\n",
    "Here’s the corrected code:\n",
    "\n",
    "Python\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "sequels = pd.DataFrame({\n",
    "    'movie_id': [1, 2, 3],\n",
    "    'title': ['Original Movie 1', 'Sequel Movie 1', 'Sequel Movie 2'],\n",
    "    'sequel': [2, None, None]\n",
    "})\n",
    "\n",
    "financials = pd.DataFrame({\n",
    "    'movie_id': [1, 2, 3],\n",
    "    'revenue': [100, 150, 200]\n",
    "})\n",
    "\n",
    "# Merge sequels and financials tables\n",
    "sequels_fin = sequels.merge(financials, on='movie_id', how='left')\n",
    "\n",
    "# Self merge with suffixes as inner join with left on sequel and right on movie_id\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', right_on='movie_id', suffixes=('_org', '_seq'))\n",
    "\n",
    "# Add calculation to subtract revenue_org from revenue_seq \n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "\n",
    "# Sort the dataset by revenue difference\n",
    "sorted_orig_seq = orig_seq.sort_values(by='diff', ascending=False)\n",
    "\n",
    "# Display the relevant columns\n",
    "print(sorted_orig_seq[['title_org', 'title_seq', 'revenue_org', 'revenue_seq', 'diff']])\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This code ensures that the sequels DataFrame is correctly merged with the financials DataFrame before performing the self-merge. This should give you the correct comparison of revenues between original movies and their sequels.\n",
    "\n",
    "If you have any more questions or need further assistance, feel free to ask!\n",
    "\n",
    "You\n",
    "Sent by you:\n",
    "import pandas as pd\n",
    "# Incorrect submission\n",
    "# Have you specified the arguments for sequels_fin.merge() using the right syntax?\n",
    "\n",
    "\n",
    "# Merge sequels and financials tables\n",
    "sequels_fin = sequels.merge(financials, on='movie_id', how='left')\n",
    "\n",
    "# Self merge with suffixes as inner join with left on sequel and right on movie_id\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', right_on='movie_id', suffixes=('_org', '_seq'))\n",
    "\n",
    "# Add calculation to subtract revenue_org from revenue_seq \n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "\n",
    "# Sort the dataset by revenue difference\n",
    "sorted_orig_seq = orig_seq.sort_values(by='diff', ascending=False)\n",
    "\n",
    "# Display the relevant columns\n",
    "print(sorted_orig_seq[['title_org', 'title_seq', 'revenue_org', 'revenue_seq', 'diff']])\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "It looks like your code is almost correct, but there might be a small issue with the merge syntax. Let’s ensure everything is in order. Here’s a revised version of your code with some additional comments for clarity:\n",
    "\n",
    "Python\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "sequels = pd.DataFrame({\n",
    "    'movie_id': [1, 2, 3],\n",
    "    'title': ['Original Movie 1', 'Sequel Movie 1', 'Sequel Movie 2'],\n",
    "    'sequel': [2, None, None]\n",
    "})\n",
    "\n",
    "financials = pd.DataFrame({\n",
    "    'movie_id': [1, 2, 3],\n",
    "    'revenue': [100, 150, 200]\n",
    "})\n",
    "\n",
    "# Merge sequels and financials tables\n",
    "sequels_fin = sequels.merge(financials, on='movie_id', how='left')\n",
    "\n",
    "# Self merge with suffixes as inner join with left on sequel and right on movie_id\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', right_on='movie_id', suffixes=('_org', '_seq'))\n",
    "\n",
    "# Add calculation to subtract revenue_org from revenue_seq \n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "\n",
    "# Sort the dataset by revenue difference\n",
    "sorted_orig_seq = orig_seq.sort_values(by='diff', ascending=False)\n",
    "\n",
    "# Display the relevant columns\n",
    "print(sorted_orig_seq[['title_org', 'title_seq', 'revenue_org', 'revenue_seq', 'diff']])\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This code should work correctly. Here are the key steps:\n",
    "\n",
    "Merge the sequels and financials tables using a left join to ensure all rows from the sequels table are included.\n",
    "Self-merge the resulting DataFrame to compare the revenue of the original movie to its sequel.\n",
    "Calculate the revenue difference by subtracting the revenue of the original movie from the sequel.\n",
    "Sort the dataset by the revenue difference to find which sequels earned the most compared to the original.\n",
    "If you still encounter issues, please let me know the specific error or unexpected behavior you’re seeing, and we can troubleshoot further!\n",
    "\n",
    "You\n",
    "Sent by you:\n",
    "Merge the sequels_fin table to itself with an inner join, where the left and right tables merge on sequel and id respectively with suffixes equal to ('_org','_seq'), saving to orig_seq.\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "Got it! Here’s how you can merge the sequels_fin table to itself with the specified conditions:\n",
    "\n",
    "Python\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "sequels_fin = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'title': ['Original Movie 1', 'Sequel Movie 1', 'Sequel Movie 2'],\n",
    "    'sequel': [2, None, None],\n",
    "    'revenue': [100, 150, 200]\n",
    "})\n",
    "\n",
    "# Self merge with suffixes as inner join with left on sequel and right on id\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', right_on='id', suffixes=('_org', '_seq'))\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(orig_seq)\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This code will merge the sequels_fin table to itself using an inner join, where the left table merges on sequel and the right table merges on id, with the suffixes ('_org', '_seq'). The resulting DataFrame orig_seq will contain the merged data.\n",
    "\n",
    "If you need further assistance or have any questions, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You\n",
    "Sent by you:\n",
    "Do sequels earn more?\n",
    "It is time to put together many of the aspects that you have learned in this chapter. In this exercise, you'll find out which movie sequels earned the most compared to the original movie. To answer this question, you will merge a modified version of the sequels and financials tables where their index is the movie ID. You will need to choose a merge type that will return all of the rows from the sequels table and not all the rows of financials table need to be included in the result. From there, you will join the resulting table to itself so that you can compare the revenue values of the original movie to the sequel. Next, you will calculate the difference between the two revenues and sort the resulting dataset.\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "Sure, let’s break this down step-by-step. Here’s how you can approach this problem using Python and pandas:\n",
    "\n",
    "Merge the sequels and financials tables:\n",
    "Use a left join to ensure all rows from the sequels table are included.\n",
    "Set the index to movie_id.\n",
    "Join the resulting table to itself:\n",
    "This will allow you to compare the revenue of the original movie to its sequel.\n",
    "Calculate the revenue difference:\n",
    "Subtract the revenue of the original movie from the sequel.\n",
    "Sort the dataset:\n",
    "Sort by the revenue difference to find which sequels earned the most compared to the original.\n",
    "Here’s a code example to illustrate these steps:\n",
    "\n",
    "Python\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "sequels = pd.DataFrame({\n",
    "    'movie_id': [1, 2, 3],\n",
    "    'title': ['Original Movie 1', 'Sequel Movie 1', 'Sequel Movie 2'],\n",
    "    'sequel': [2, None, None]\n",
    "})\n",
    "\n",
    "financials = pd.DataFrame({\n",
    "    'movie_id': [1, 2, 3],\n",
    "    'revenue': [100, 150, 200]\n",
    "})\n",
    "\n",
    "# Merge sequels and financials tables\n",
    "sequels_financials = sequels.merge(financials, on='movie_id', how='left')\n",
    "\n",
    "# Join the table to itself to compare original and sequel revenues\n",
    "merged = sequels_financials.merge(\n",
    "    sequels_financials,\n",
    "    left_on='sequel',\n",
    "    right_on='movie_id',\n",
    "    suffixes=('_original', '_sequel')\n",
    ")\n",
    "\n",
    "# Calculate the revenue difference\n",
    "merged['revenue_difference'] = merged['revenue_sequel'] - merged['revenue_original']\n",
    "\n",
    "# Sort the dataset by revenue difference\n",
    "sorted_merged = merged.sort_values(by='revenue_difference', ascending=False)\n",
    "\n",
    "print(sorted_merged[['title_original', 'title_sequel', 'revenue_original', 'revenue_sequel', 'revenue_difference']])\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This code will give you a DataFrame showing the original movie, its sequel, their respective revenues, and the difference in revenue. You can adjust the sample data to match your actual data.\n",
    "\n",
    "Feel free to ask if you have any questions or need further assistance!\n",
    "\n",
    "You\n",
    "Sent by you:\n",
    "# Self merge with suffixes as inner join with left on sequel and right on id\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', right_on='id', suffixes=('_org', '_seq'))\n",
    "\n",
    "# Add calculation to subtract revenue_org from revenue_seq \n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "Your approach looks great! You’re correctly performing a self-merge and calculating the revenue difference. Here’s a complete example incorporating your code and adding the sorting step:\n",
    "\n",
    "Python\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "sequels_fin = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'title': ['Original Movie 1', 'Sequel Movie 1', 'Sequel Movie 2'],\n",
    "    'sequel': [2, None, None],\n",
    "    'revenue': [100, 150, 200]\n",
    "})\n",
    "\n",
    "# Self merge with suffixes as inner join with left on sequel and right on id\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', right_on='id', suffixes=('_org', '_seq'))\n",
    "\n",
    "# Add calculation to subtract revenue_org from revenue_seq \n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "\n",
    "# Sort the dataset by revenue difference\n",
    "sorted_orig_seq = orig_seq.sort_values(by='diff', ascending=False)\n",
    "\n",
    "# Display the relevant columns\n",
    "print(sorted_orig_seq[['title_org', 'title_seq', 'revenue_org', 'revenue_seq', 'diff']])\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This code will give you a DataFrame showing the original movie, its sequel, their respective revenues, and the difference in revenue, sorted by the revenue difference.\n",
    "\n",
    "If you have any more questions or need further assistance, feel free to ask!\n",
    "\n",
    "Learn more\n",
    "\n",
    "1\n",
    "\n",
    "github.com\n",
    "You\n",
    "Sent by you:\n",
    "Incorrect submission\n",
    "Did you merge the correct DataFrame to sequels?\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "It looks like there might be a misunderstanding in the merging process. Let’s ensure we’re merging the correct DataFrames. Here’s a revised approach:\n",
    "\n",
    "Merge the sequels and financials tables:\n",
    "Use a left join to ensure all rows from the sequels table are included.\n",
    "Set the index to movie_id.\n",
    "Self-merge the resulting DataFrame:\n",
    "This will allow you to compare the revenue of the original movie to its sequel.\n",
    "Here’s the corrected code:\n",
    "\n",
    "Python\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "sequels = pd.DataFrame({\n",
    "    'movie_id': [1, 2, 3],\n",
    "    'title': ['Original Movie 1', 'Sequel Movie 1', 'Sequel Movie 2'],\n",
    "    'sequel': [2, None, None]\n",
    "})\n",
    "\n",
    "financials = pd.DataFrame({\n",
    "    'movie_id': [1, 2, 3],\n",
    "    'revenue': [100, 150, 200]\n",
    "})\n",
    "\n",
    "# Merge sequels and financials tables\n",
    "sequels_fin = sequels.merge(financials, on='movie_id', how='left')\n",
    "\n",
    "# Self merge with suffixes as inner join with left on sequel and right on movie_id\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', right_on='movie_id', suffixes=('_org', '_seq'))\n",
    "\n",
    "# Add calculation to subtract revenue_org from revenue_seq \n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "\n",
    "# Sort the dataset by revenue difference\n",
    "sorted_orig_seq = orig_seq.sort_values(by='diff', ascending=False)\n",
    "\n",
    "# Display the relevant columns\n",
    "print(sorted_orig_seq[['title_org', 'title_seq', 'revenue_org', 'revenue_seq', 'diff']])\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This code ensures that the sequels DataFrame is correctly merged with the financials DataFrame before performing the self-merge. This should give you the correct comparison of revenues between original movies and their sequels.\n",
    "\n",
    "If you have any more questions or need further assistance, feel free to ask!\n",
    "\n",
    "You\n",
    "Sent by you:\n",
    "import pandas as pd\n",
    "# Incorrect submission\n",
    "# Have you specified the arguments for sequels_fin.merge() using the right syntax?\n",
    "\n",
    "\n",
    "# Merge sequels and financials tables\n",
    "sequels_fin = sequels.merge(financials, on='movie_id', how='left')\n",
    "\n",
    "# Self merge with suffixes as inner join with left on sequel and right on movie_id\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', right_on='movie_id', suffixes=('_org', '_seq'))\n",
    "\n",
    "# Add calculation to subtract revenue_org from revenue_seq \n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "\n",
    "# Sort the dataset by revenue difference\n",
    "sorted_orig_seq = orig_seq.sort_values(by='diff', ascending=False)\n",
    "\n",
    "# Display the relevant columns\n",
    "print(sorted_orig_seq[['title_org', 'title_seq', 'revenue_org', 'revenue_seq', 'diff']])\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "It looks like your code is almost correct, but there might be a small issue with the merge syntax. Let’s ensure everything is in order. Here’s a revised version of your code with some additional comments for clarity:\n",
    "\n",
    "Python\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "sequels = pd.DataFrame({\n",
    "    'movie_id': [1, 2, 3],\n",
    "    'title': ['Original Movie 1', 'Sequel Movie 1', 'Sequel Movie 2'],\n",
    "    'sequel': [2, None, None]\n",
    "})\n",
    "\n",
    "financials = pd.DataFrame({\n",
    "    'movie_id': [1, 2, 3],\n",
    "    'revenue': [100, 150, 200]\n",
    "})\n",
    "\n",
    "# Merge sequels and financials tables\n",
    "sequels_fin = sequels.merge(financials, on='movie_id', how='left')\n",
    "\n",
    "# Self merge with suffixes as inner join with left on sequel and right on movie_id\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', right_on='movie_id', suffixes=('_org', '_seq'))\n",
    "\n",
    "# Add calculation to subtract revenue_org from revenue_seq \n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "\n",
    "# Sort the dataset by revenue difference\n",
    "sorted_orig_seq = orig_seq.sort_values(by='diff', ascending=False)\n",
    "\n",
    "# Display the relevant columns\n",
    "print(sorted_orig_seq[['title_org', 'title_seq', 'revenue_org', 'revenue_seq', 'diff']])\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This code should work correctly. Here are the key steps:\n",
    "\n",
    "Merge the sequels and financials tables using a left join to ensure all rows from the sequels table are included.\n",
    "Self-merge the resulting DataFrame to compare the revenue of the original movie to its sequel.\n",
    "Calculate the revenue difference by subtracting the revenue of the original movie from the sequel.\n",
    "Sort the dataset by the revenue difference to find which sequels earned the most compared to the original.\n",
    "If you still encounter issues, please let me know the specific error or unexpected behavior you’re seeing, and we can troubleshoot further!\n",
    "\n",
    "You\n",
    "Sent by you:\n",
    "Merge the sequels_fin table to itself with an inner join, where the left and right tables merge on sequel and id respectively with suffixes equal to ('_org','_seq'), saving to orig_seq.\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "Got it! Here’s how you can merge the sequels_fin table to itself with the specified conditions:\n",
    "\n",
    "Python\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "sequels_fin = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'title': ['Original Movie 1', 'Sequel Movie 1', 'Sequel Movie 2'],\n",
    "    'sequel': [2, None, None],\n",
    "    'revenue': [100, 150, 200]\n",
    "})\n",
    "\n",
    "# Self merge with suffixes as inner join with left on sequel and right on id\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', right_on='id', suffixes=('_org', '_seq'))\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(orig_seq)\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This code will merge the sequels_fin table to itself using an inner join, where the left table merges on sequel and the right table merges on id, with the suffixes ('_org', '_seq'). The resulting DataFrame orig_seq will contain the merged data.\n",
    "\n",
    "If you need further assistance or have any questions, feel free to ask!\n",
    "\n",
    "'''Do sequels earn more?\n",
    "It is time to put together many of the aspects that you have learned in this chapter. In this exercise, you'll find out which movie sequels earned the most compared to the original movie. To answer this question, you will merge a modified version of the sequels and financials tables where their index is the movie ID. You will need to choose a merge type that will return all of the rows from the sequels table and not all the rows of financials table need to be included in the result. From there, you will join the resulting table to itself so that you can compare the revenue values of the original movie to the sequel. Next, you will calculate the difference between the two revenues and sort the resulting dataset.\n",
    "\n",
    "The sequels and financials tables have been provided.\n",
    "\n",
    "Instructions 1/4\n",
    "25 XP\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "With the sequels table on the left, merge to it the financials table on index named id, ensuring that all the rows from the sequels are returned and some rows from the other table may not be returned, Save the results to sequels_fin.'''\n",
    "\n",
    "# Merge sequels and financials on index id\n",
    "sequels_fin = ____\n",
    "ChatGPT\n",
    "To merge the sequels and financials tables and ensure that all rows from the sequels table are included (even if some rows from financials may not be included), you can use a left merge. This will keep all rows from the sequels table and only include matching rows from the financials table where available.\n",
    "\n",
    "Here’s how you can perform this merge:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge sequels and financials on index id, keeping all rows from sequels\n",
    "sequels_fin = sequels.merge(financials, left_index=True, right_index=True, how='left')\n",
    "In this code:\n",
    "\n",
    "left_index=True and right_index=True specify that the merge should be based on the index of both DataFrames.\n",
    "how='left' ensures that all rows from the sequels table are included, even if there is no matching row in the financials table.\n",
    "Make sure you replace sequels and financials with the actual variable names of your DataFrames if they are different.\n",
    "\n",
    "'''Merge the sequels_fin table to itself with an inner join, where the left and right tables merge on sequel and id respectively with suffixes equal to ('_org','_seq'), saving to orig_seq.'''\n",
    "\n",
    "# Merge sequels and financials on index id\n",
    "sequels_fin = sequels.merge(financials, on='id', how='left')\n",
    "\n",
    "# Self merge with suffixes as inner join with left on sequel and right on id\n",
    "orig_seq = ____.merge(____, how=____, left_on=____, \n",
    "                             right_on=____, right_index=____,\n",
    "                             suffixes=____)\n",
    "\n",
    "# Add calculation to subtract revenue_org from revenue_seq \n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "ChatGPT\n",
    "To merge the sequels_fin table with itself and compare the original movie’s revenue with the sequel’s revenue, follow these steps:\n",
    "\n",
    "Merge sequels_fin with itself: Perform an inner join where the left DataFrame's sequel column matches the right DataFrame's id column. Use suffixes to differentiate the columns from the two DataFrames in the merge.\n",
    "\n",
    "Calculate the revenue difference: Subtract the revenue of the original movie from the revenue of the sequel.\n",
    "\n",
    "Here’s how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Self-merge with suffixes, inner join, left on 'sequel' and right on 'id'\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', right_on='id', suffixes=('_org', '_seq'))\n",
    "\n",
    "# Calculate the difference between sequel revenue and original revenue\n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "In this code:\n",
    "\n",
    "left_on='sequel' specifies the column in the left DataFrame (sequels_fin) to join on.\n",
    "right_on='id' specifies the column in the right DataFrame (sequels_fin) to join on.\n",
    "suffixes=('_org', '_seq') adds suffixes to the column names from the two DataFrames to distinguish between original and sequel revenues.\n",
    "This will give you a DataFrame where each row corresponds to a sequel and its original movie, with an added diff column showing the difference in revenue.\n",
    "\n",
    "Incorrect submission\n",
    "Have you specified the arguments for sequels_fin.merge() using the right syntax?\n",
    "ChatGPT\n",
    "It looks like there might be an issue with the syntax or the parameters used. Let’s correct it:\n",
    "\n",
    "Use sequels_fin for both DataFrames in the merge.\n",
    "Specify left_on and right_on for the merging columns.\n",
    "Include suffixes to distinguish between the columns.\n",
    "Here's the corrected code for merging sequels_fin with itself:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Self-merge with suffixes, inner join, left on 'sequel' and right on 'id'\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', right_on='id', suffixes=('_org', '_seq'))\n",
    "\n",
    "# Calculate the difference between sequel revenue and original revenue\n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "Explanation:\n",
    "how='inner': Performs an inner join, which will only include rows where there is a match in both DataFrames.\n",
    "left_on='sequel': The column in the left DataFrame (sequels_fin) to join on.\n",
    "right_on='id': The column in the right DataFrame (sequels_fin) to join on.\n",
    "suffixes=('_org', '_seq'): Adds suffixes to differentiate between the columns from the two DataFrames.\n",
    "This will correctly merge the table and allow you to compute the revenue difference.\n",
    "\n",
    "'''Merge the sequels_fin table to itself with an inner join, where the left and right tables merge on sequel and id respectively with suffixes equal to ('_org','_seq'), saving to orig_seq.'''\n",
    "\n",
    "# Merge sequels and financials on index id\n",
    "sequels_fin = sequels.merge(financials, on='id', how='left')\n",
    "\n",
    "# Self merge with suffixes as inner join with left on sequel and right on id\n",
    "orig_seq = ____.merge(____, how=____, left_on=____, \n",
    "                             right_on=____, right_index=____,\n",
    "                             suffixes=____)\n",
    "\n",
    "# Add calculation to subtract revenue_org from revenue_seq \n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "ChatGPT\n",
    "Here's the corrected syntax for merging sequels_fin with itself:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Self merge with suffixes as inner join with left on sequel and right on id\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', right_on='id', suffixes=('_org', '_seq'))\n",
    "\n",
    "# Add calculation to subtract revenue_org from revenue_seq \n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "Explanation:\n",
    "sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', right_on='id', suffixes=('_org', '_seq')):\n",
    "how='inner': Ensures that only rows with matches in both DataFrames are included.\n",
    "left_on='sequel': The column in the left DataFrame (sequels_fin) to match against the right DataFrame's id.\n",
    "right_on='id': The column in the right DataFrame (sequels_fin) to match against the left DataFrame's sequel.\n",
    "suffixes=('_org', '_seq'): Adds suffixes to the columns from each DataFrame to distinguish them after the merge.\n",
    "This will allow you to compare the revenue of sequels to their original movies and compute the revenue difference.\n",
    "\n",
    "import pandas as pd\n",
    "# Incorrect submission\n",
    "# Have you specified the arguments for sequels_fin.merge() using the right syntax?\n",
    "\n",
    "\n",
    "# Merge sequels and financials tables\n",
    "sequels_fin = sequels.merge(financials, on='movie_id', how='left')\n",
    "\n",
    "# Self merge with suffixes as inner join with left on sequel and right on movie_id\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', right_on='movie_id', suffixes=('_org', '_seq'))\n",
    "\n",
    "# Add calculation to subtract revenue_org from revenue_seq \n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "\n",
    "# Sort the dataset by revenue difference\n",
    "sorted_orig_seq = orig_seq.sort_values(by='diff', ascending=False)\n",
    "\n",
    "# Display the relevant columns\n",
    "print(sorted_orig_seq[['title_org', 'title_seq', 'revenue_org', 'revenue_seq', 'diff']])\n",
    "ChatGPT\n",
    "Memory updated\n",
    "Your code looks almost correct, but there are a few details to check:\n",
    "\n",
    "Column Names: Ensure that the column names ('sequel', 'movie_id', 'revenue', etc.) match the actual column names in your DataFrames.\n",
    "\n",
    "Correct Merge: Since you've merged on movie_id and are looking to compare sequels with their originals, make sure 'sequel' in the left DataFrame matches the 'movie_id' in the right DataFrame.\n",
    "\n",
    "Here is the revised code with some notes:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Merge sequels and financials tables\n",
    "sequels_fin = sequels.merge(financials, on='movie_id', how='left')\n",
    "\n",
    "# Self merge with suffixes, inner join with left on 'sequel' and right on 'movie_id'\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', right_on='movie_id', suffixes=('_org', '_seq'))\n",
    "\n",
    "# Add calculation to subtract revenue_org from revenue_seq \n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "\n",
    "# Sort the dataset by revenue difference\n",
    "sorted_orig_seq = orig_seq.sort_values(by='diff', ascending=False)\n",
    "\n",
    "# Display the relevant columns\n",
    "print(sorted_orig_seq[['title_org', 'title_seq', 'revenue_org', 'revenue_seq', 'diff']])\n",
    "Key Points:\n",
    "Column Names: Ensure the column names like 'title_org', 'title_seq', 'revenue_org', and 'revenue_seq' exist in your DataFrame. If not, adjust them according to your actual column names.\n",
    "Sorting: The sort_values(by='diff', ascending=False) will sort the results so that sequels with the highest revenue difference compared to their original movies are shown first.\n",
    "This code should correctly merge the DataFrames, calculate the revenue difference, sort the results, and display the specified columns.\n",
    "\n",
    "Merge the sequels_fin table to itself with an inner join, where the left and right tables merge on sequel and id respectively with suffixes equal to ('_org','_seq'), saving to orig_seq.\n",
    "ChatGPT\n",
    "To correctly merge the sequels_fin table with itself, where the left DataFrame is joined on its sequel column and the right DataFrame is joined on its id column, you should use the following approach:\n",
    "\n",
    "Here is how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Self-merge with suffixes, inner join with left on 'sequel' and right on 'id'\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', right_on='id', suffixes=('_org', '_seq'))\n",
    "Explanation:\n",
    "sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', right_on='id', suffixes=('_org', '_seq')):\n",
    "left_on='sequel': Specifies the column in the left DataFrame (sequels_fin) that contains the sequel IDs.\n",
    "right_on='id': Specifies the column in the right DataFrame (sequels_fin) that contains the original movie IDs.\n",
    "suffixes=('_org', '_seq'): Adds suffixes to the columns from each DataFrame to distinguish between original and sequel information.\n",
    "This code merges the sequels_fin DataFrame with itself based on the sequel relationship, allowing you to compare the revenue and other details of sequels with their original movies.\n",
    "\n",
    "correction\n",
    "# Merge sequels and financials on index id\n",
    "sequels_fin = sequels.merge(financials, on='id', how='left')\n",
    "\n",
    "# Self merge with suffixes as inner join with left on sequel and right on id\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', \n",
    "                             right_on='id', right_index=True,\n",
    "                             suffixes=('_org','_seq'))\n",
    "\n",
    "# Add calculation to subtract revenue_org from revenue_seq \n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "ChatGPT\n",
    "It looks like you’re trying to merge sequels_fin with itself using an inner join on sequel and id, but you've included right_index=True, which is not needed here since you're already specifying right_on.\n",
    "\n",
    "Here’s the corrected code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge sequels and financials on index id\n",
    "sequels_fin = sequels.merge(financials, on='id', how='left')\n",
    "\n",
    "# Self merge with suffixes as inner join with left on 'sequel' and right on 'id'\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', \n",
    "                             right_on='id', suffixes=('_org','_seq'))\n",
    "\n",
    "# Add calculation to subtract revenue_org from revenue_seq \n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "Explanation:\n",
    "left_on='sequel': The column in the left DataFrame (sequels_fin) containing the sequel IDs.\n",
    "right_on='id': The column in the right DataFrame (sequels_fin) containing the original movie IDs.\n",
    "suffixes=('_org', '_seq'): Adds suffixes to the columns from each DataFrame to differentiate between original and sequel data.\n",
    "This should correctly merge the DataFrame and allow you to calculate the revenue difference between the original movies and their sequels.\n",
    "\n",
    "# Select the title_org, title_seq, and diff columns of orig_seq and save this as titles_diff.\n",
    "\n",
    "# Merge sequels and financials on index id\n",
    "sequels_fin = sequels.merge(financials, on='id', how='left')\n",
    "\n",
    "# Self merge with suffixes as inner join with left on sequel and right on id\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', \n",
    "                             right_on='id', right_index=True,\n",
    "                             suffixes=('_org','_seq'))\n",
    "\n",
    "# Add calculation to subtract revenue_org from revenue_seq \n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "\n",
    "# Select the title_org, title_seq, and diff \n",
    "titles_diff = orig_seq[____]\n",
    "ChatGPT\n",
    "To select the title_org, title_seq, and diff columns from the orig_seq DataFrame and save this as titles_diff, you should use the following code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Select the title_org, title_seq, and diff columns from orig_seq\n",
    "titles_diff = orig_seq[['title_org', 'title_seq', 'diff']]\n",
    "Explanation:\n",
    "[['title_org', 'title_seq', 'diff']]: Specifies the columns to select from orig_seq.\n",
    "This will create a new DataFrame titles_diff containing only the columns of interest: the original movie title, the sequel title, and the revenue difference.\n",
    "\n",
    "# Sort by titles_diff by diff in descending order and print the first few rows.\n",
    "\n",
    "# Merge sequels and financials on index id\n",
    "sequels_fin = sequels.merge(financials, on='id', how='left')\n",
    "\n",
    "# Self merge with suffixes as inner join with left on sequel and right on id\n",
    "orig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', \n",
    "                             right_on='id', right_index=True,\n",
    "                             suffixes=('_org','_seq'))\n",
    "\n",
    "# Add calculation to subtract revenue_org from revenue_seq \n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "\n",
    "# Select the title_org, title_seq, and diff \n",
    "titles_diff = orig_seq[['title_org','title_seq','diff']]\n",
    "\n",
    "# Print the first rows of the sorted titles_diff\n",
    "print(titles_diff.sort_values(____).head())\n",
    "ChatGPT\n",
    "To sort the titles_diff DataFrame by the diff column in descending order and print the first few rows, you should use the sort_values method with the by parameter set to 'diff' and ascending=False. Then, use .head() to get the top rows.\n",
    "\n",
    "Here’s the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Print the first rows of the sorted titles_diff\n",
    "print(titles_diff.sort_values(by='diff', ascending=False).head())\n",
    "Explanation:\n",
    "sort_values(by='diff', ascending=False): Sorts titles_diff by the diff column in descending order.\n",
    ".head(): Returns the first few rows of the sorted DataFrame.\n",
    "This will display the rows with the largest revenue differences at the top.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chapter 3\n",
    "'''\n",
    "Advanced Merging and Concatenating\n",
    "\n",
    "0%\n",
    "In this chapter, you’ll leverage powerful filtering techniques, including semi-joins and anti-joins. You’ll also learn how to glue DataFrames by vertically combining and using the pandas.concat function to create new datasets. Finally, because data is rarely clean, you’ll also learn how to validate your newly combined data structures.\n",
    "Congratulations! You have a sense of the steps in this technique. It first merges the tables, then searches it for which rows belong in the final result creating a filter and subsets the left table with that filter.\n",
    "\n",
    "'''\n",
    "# Merge employees and top_cust\n",
    "empl_cust = employees.merge(top_cust, on='srid', \n",
    "                            how='left', indicator=True)\n",
    "\n",
    "\n",
    "\n",
    "# Merge employees and top_cust\n",
    "empl_cust = employees.merge(top_cust, on='srid', \n",
    "                            how='left', indicator=True)\n",
    "\n",
    "# Select the srid column where _merge is left_only\n",
    "srid_list = empl_cust.loc[empl_cust['_merge'] == 'left_only', 'srid']\n",
    "\n",
    "\n",
    "# Merge employees and top_cust\n",
    "empl_cust = employees.merge(top_cust, on='srid', \n",
    "                            how='left', indicator=True)\n",
    "\n",
    "# Select the srid column where _merge is left_only\n",
    "srid_list = empl_cust.loc[empl_cust['_merge'] == 'left_only', 'srid']\n",
    "\n",
    "# Get employees not working with top customers\n",
    "employees_not_assigned = employees[employees['srid'].isin(srid_list)]\n",
    "print(employees_not_assigned)\n",
    "\n",
    "'''Success! You performed an anti join by first merging the tables with a left join, selecting the ID of those employees who did not support a top customer, and then subsetting the original employee's table. From that, we can see that there are five employees not supporting top customers. Anti joins are a powerful tool to filter a main table (i.e. employees) by another (i.e. customers)'''\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Performing an anti join\n",
    "In our music streaming company dataset, each customer is assigned an employee representative to assist them. In this exercise, filter the employee table by a table of top customers, returning only those employees who are not assigned to a customer. The results should resemble the results of an anti join. The company's leadership will assign these employees additional training so that they can work with high valued customers.\n",
    "\n",
    "The top_cust and employees tables have been provided for you.\n",
    "\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "1\n",
    "2\n",
    "3\n",
    "Merge employees and top_cust with a left join, setting indicator argument to True. Save the result to empl_cust.'''\n",
    "\n",
    "\n",
    "# Merge employees and top_cust\n",
    "empl_cust = ____.merge(____, on=____, \n",
    "                            how=____, indicator=____)\n",
    "ChatGPT\n",
    "To perform the specified task, we'll merge the employees and top_cust tables using a left join, and set the indicator argument to True so that we can identify the rows that do not have matching entries in the top_cust table. Here’s how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge employees and top_cust\n",
    "empl_cust = employees.merge(top_cust, on='employee_id', \n",
    "                            how='left', indicator=True)\n",
    "In this code:\n",
    "\n",
    "employees is the table of employees.\n",
    "top_cust is the table of top customers.\n",
    "on='employee_id' specifies the column to join on.\n",
    "how='left' performs a left join.\n",
    "indicator=True adds a column to the result called _merge that indicates the source of each row.\n",
    "Now empl_cust will contain all rows from employees with information about whether they have a matching entry in top_cust or not.\n",
    "\n",
    "Incorrect submission\n",
    "Did you merge employees to top_cust on srid?\n",
    "\n",
    " \n",
    "ChatGPT\n",
    "Let's revise the merge operation to correctly join on the srid (assuming srid is the shared key between employees and top_cust). Here is the corrected code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge employees and top_cust\n",
    "empl_cust = employees.merge(top_cust, on='srid', \n",
    "                            how='left', indicator=True)\n",
    "This code will:\n",
    "\n",
    "Merge the employees table with the top_cust table on the srid column.\n",
    "Perform a left join.\n",
    "Add an indicator column named _merge to identify the origin of each row.\n",
    "'''Select the srid column of empl_cust and the rows where _merge is 'left_only'. Save the result to srid_list.'''\n",
    "\n",
    "\n",
    "# Merge employees and top_cust\n",
    "empl_cust = employees.merge(top_cust, on='srid', \n",
    "                            how='left', indicator=True)\n",
    "\n",
    "# Select the srid column where _merge is left_only\n",
    "srid_list = empl_cust.loc[____, 'srid']\n",
    "ChatGPT\n",
    "To filter empl_cust to select the srid column where _merge is 'left_only', you can use the loc method. Here is the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge employees and top_cust\n",
    "empl_cust = employees.merge(top_cust, on='srid', \n",
    "                            how='left', indicator=True)\n",
    "\n",
    "# Select the srid column where _merge is left_only\n",
    "srid_list = empl_cust.loc[empl_cust['_merge'] == 'left_only', 'srid']\n",
    "In this code:\n",
    "\n",
    "empl_cust['_merge'] == 'left_only' creates a boolean mask where the _merge column is equal to 'left_only'.\n",
    "empl_cust.loc[mask, 'srid'] selects the srid column for rows that match the mask.\n",
    "# Subset the employees table and select those rows where the srid is in the variable srid_list and print the results.\n",
    "\n",
    "\n",
    "# Merge employees and top_cust\n",
    "empl_cust = employees.merge(top_cust, on='srid', \n",
    "                                 how='left', indicator=True)\n",
    "\n",
    "# Select the srid column where _merge is left_only\n",
    "srid_list = empl_cust.loc[empl_cust['_merge'] == 'left_only', 'srid']\n",
    "\n",
    "# Get employees not working with top customers\n",
    "print(employees[____.isin(____)])\n",
    "ChatGPT\n",
    "To subset the employees table to select those rows where the srid is in the srid_list and print the results, you can use the isin method. Here is the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge employees and top_cust\n",
    "empl_cust = employees.merge(top_cust, on='srid', \n",
    "                            how='left', indicator=True)\n",
    "\n",
    "# Select the srid column where _merge is left_only\n",
    "srid_list = empl_cust.loc[empl_cust['_merge'] == 'left_only', 'srid']\n",
    "\n",
    "# Get employees not working with top customers\n",
    "employees_not_assigned = employees[employees['srid'].isin(srid_list)]\n",
    "print(employees_not_assigned)\n",
    "In this code:\n",
    "\n",
    "employees['srid'].isin(srid_list) creates a boolean mask where srid in employees is in srid_list.\n",
    "employees[mask] selects the rows in employees that match the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the non_mus_tcks and top_invoices tables on tid\n",
    "tracks_invoices = non_mus_tcks.merge(top_invoices, on='tid', how='inner')\n",
    "\n",
    "# Use .isin() to subset non_mus_tcks to rows with tid in tracks_invoices\n",
    "top_tracks = non_mus_tcks[non_mus_tcks['tid'].isin(tracks_invoices['tid'])]\n",
    "\n",
    "# Group the top_tracks by gid and count the tid rows\n",
    "cnt_by_gid = top_tracks.groupby('gid', as_index=False).agg({'tid': 'count'})\n",
    "\n",
    "# Merge the genres table to cnt_by_gid on gid and print\n",
    "result = cnt_by_gid.merge(genres, on='gid')\n",
    "print(result)\n",
    "\n",
    "\n",
    "'''Nice job! In this exercise, you replicated a semi join to filter the table of tracks by the table of invoice items to find the top revenue non-musical tracks. With some additional data manipulation, you discovered that _'TV-shows'_ is the non-musical genre that has the most top revenue-generating tracks. Now that you've done both semi- and anti joins, it's time to move to the next topic'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Performing a semi join\n",
    "Some of the tracks that have generated the most significant amount of revenue are from TV-shows or are other non-musical audio. You have been given a table of invoices that include top revenue-generating items. Additionally, you have a table of non-musical tracks from the streaming service. In this exercise, you'll use a semi join to find the top revenue-generating non-musical tracks.\n",
    "\n",
    "The tables non_mus_tcks, top_invoices, and genres have been loaded for you.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Merge non_mus_tcks and top_invoices on tid using an inner join. Save the result as tracks_invoices.\n",
    "Use .isin() to subset the rows of non_mus_tcks where tid is in the tid column of tracks_invoices. Save the result as top_tracks.\n",
    "Group top_tracks by gid and count the tid rows. Save the result to cnt_by_gid.\n",
    "Merge cnt_by_gid with the genres table on gid and print the'''\n",
    "\n",
    "\n",
    "# Merge the non_mus_tcks and top_invoices tables on tid\n",
    "tracks_invoices = ____.merge(____)\n",
    "\n",
    "# Use .isin() to subset non_mus_tcks to rows with tid in tracks_invoices\n",
    "top_tracks = _____[non_mus_tcks['tid'].isin(____)]\n",
    "\n",
    "# Group the top_tracks by gid and count the tid rows\n",
    "cnt_by_gid = top_tracks.groupby(['gid'], as_index=False).agg({'tid':____})\n",
    "\n",
    "# Merge the genres table to cnt_by_gid on gid and print\n",
    "print(____)\n",
    "ChatGPT\n",
    "Let's fill in the blanks step-by-step:\n",
    "\n",
    "Merge non_mus_tcks and top_invoices on tid using an inner join.\n",
    "Use .isin() to subset the rows of non_mus_tcks where tid is in the tid column of tracks_invoices.\n",
    "Group top_tracks by gid and count the tid rows.\n",
    "Merge cnt_by_gid with the genres table on gid and print the result.\n",
    "Here is the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge the non_mus_tcks and top_invoices tables on tid\n",
    "tracks_invoices = non_mus_tcks.merge(top_invoices, on='tid', how='inner')\n",
    "\n",
    "# Use .isin() to subset non_mus_tcks to rows with tid in tracks_invoices\n",
    "top_tracks = non_mus_tcks[non_mus_tcks['tid'].isin(tracks_invoices['tid'])]\n",
    "\n",
    "# Group the top_tracks by gid and count the tid rows\n",
    "cnt_by_gid = top_tracks.groupby('gid', as_index=False).agg({'tid': 'count'})\n",
    "\n",
    "# Merge the genres table to cnt_by_gid on gid and print\n",
    "result = cnt_by_gid.merge(genres, on='gid')\n",
    "print(result)\n",
    "In this code:\n",
    "\n",
    "non_mus_tcks.merge(top_invoices, on='tid', how='inner') performs an inner join on tid.\n",
    "non_mus_tcks[non_mus_tcks['tid'].isin(tracks_invoices['tid'])] filters non_mus_tcks to include only rows where tid is in the tid column of tracks_invoices.\n",
    "top_tracks.groupby('gid', as_index=False).agg({'tid': 'count'}) groups top_tracks by gid and counts the tid rows, creating cnt_by_gid.\n",
    "cnt_by_gid.merge(genres, on='gid') merges cnt_by_gid with the genres table on gid.\n",
    "print(result) prints the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Concatenate the tracks\n",
    "tracks_from_albums = pd.concat([tracks_master, tracks_ride, tracks_st], sort=True)\n",
    "\n",
    "# Print the result\n",
    "print(tracks_from_albums)\n",
    "\n",
    "# Concatenate the tracks so the index goes from 0 to n-1\n",
    "tracks_from_albums = pd.concat([tracks_master, tracks_ride, tracks_st], ignore_index=True, sort=True)\n",
    "\n",
    "# Print the result\n",
    "print(tracks_from_albums)\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate the tracks, show only columns names that are in all tables\n",
    "tracks_from_albums = pd.concat([tracks_master, tracks_ride, tracks_st], join='inner', sort=True)\n",
    "\n",
    "# Print the result\n",
    "print(tracks_from_albums)\n",
    "\n",
    "'''Great job! You've concatenated your first set of tables, adjusted the index, and altered the columns shown in the output. The .concat() method is a very flexible tool that is useful for combining data into a new dataset.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Concatenation basics\n",
    "You have been given a few tables of data with musical track info for different albums from the metal band, Metallica. The track info comes from their Ride The Lightning, Master Of Puppets, and St. Anger albums. Try various features of the .concat() method by concatenating the tables vertically together in different ways.\n",
    "\n",
    "The tables tracks_master, tracks_ride, and tracks_st have loaded for you.\n",
    "\n",
    "Instructions 1/3\n",
    "30 XP\n",
    "1\n",
    "Concatenate tracks_master, tracks_ride, and tracks_st, in that order, setting sort to True.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# Concatenate the tracks\n",
    "tracks_from_albums = pd.concat(____,\n",
    "                               sort=True)\n",
    "print(tracks_from_albums)\n",
    "ChatGPT\n",
    "To concatenate the tracks_master, tracks_ride, and tracks_st tables vertically in the specified order with sort=True, you can use the following code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Concatenate the tracks\n",
    "tracks_from_albums = pd.concat([tracks_master, tracks_ride, tracks_st], sort=True)\n",
    "\n",
    "# Print the result\n",
    "print(tracks_from_albums)\n",
    "# Concatenate tracks_master, tracks_ride, and tracks_st, where the index goes from 0 to n-1.\n",
    "\n",
    "# Concatenate the tracks so the index goes from 0 to n-1\n",
    "tracks_from_albums = pd.concat(____,\n",
    "                               ____,\n",
    "                               sort=True)\n",
    "print(tracks_from_albums)\n",
    "ChatGPT\n",
    "To concatenate tracks_master, tracks_ride, and tracks_st while resetting the index to go from 0 to n-1, you can set the ignore_index parameter to True. Here's the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Concatenate the tracks so the index goes from 0 to n-1\n",
    "tracks_from_albums = pd.concat([tracks_master, tracks_ride, tracks_st], ignore_index=True, sort=True)\n",
    "\n",
    "# Print the result\n",
    "print(tracks_from_albums)\n",
    "# Concatenate tracks_master, tracks_ride, and tracks_st, showing only columns that are in all tables.\n",
    "\n",
    "# Concatenate the tracks, show only columns names that are in all tables\n",
    "tracks_from_albums = pd.concat(____,\n",
    "                               ____,\n",
    "                               sort=True)\n",
    "print(tracks_from_albums)\n",
    "ChatGPT\n",
    "To concatenate the tables while showing only the columns that are common to all tables, you can set the join parameter to 'inner'. Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Concatenate the tracks, show only columns names that are in all tables\n",
    "tracks_from_albums = pd.concat([tracks_master, tracks_ride, tracks_st], join='inner', sort=True)\n",
    "\n",
    "# Print the result\n",
    "print(tracks_from_albums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Concatenate the tables and add keys\n",
    "inv_jul_thr_sep = pd.concat([inv_jul, inv_aug, inv_sep], keys=['7Jul', '8Aug', '9Sep'])\n",
    "\n",
    "# Group the invoices by the index keys and find the average of the total column\n",
    "avg_inv_by_month = inv_jul_thr_sep.groupby(level=0).agg({'total': 'mean'})\n",
    "\n",
    "# Bar plot of avg_inv_by_month\n",
    "avg_inv_by_month.plot(kind='bar')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Invoice Total')\n",
    "plt.title('Average Monthly Invoice Total')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "'''Way to come through! There are many ways to write code for this task. However, concatenating the tables with a key provides a hierarchical index that can be used for grouping. Once grouped, you can average the groups and create plots. You were able to find out that September had the highest average invoice total.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Concatenating with keys\n",
    "The leadership of the music streaming company has come to you and asked you for assistance in analyzing sales for a recent business quarter. They would like to know which month in the quarter saw the highest average invoice total. You have been given three tables with invoice data named inv_jul, inv_aug, and inv_sep. Concatenate these tables into one to create a graph of the average monthly invoice total.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Concatenate the three tables together vertically in order with the oldest month first, adding '7Jul', '8Aug', and '9Sep' as keys for their respective months, and save to inv_jul_thr_sep.\n",
    "Use the .agg() method to find the average of the total column from the grouped invoices.\n",
    "Create a bar chart of avg_inv_by_month.'''\n",
    "\n",
    "\n",
    "# Concatenate the tables and add keys\n",
    "inv_jul_thr_sep = pd.concat(____, \n",
    "                            keys=____)\n",
    "\n",
    "# Group the invoices by the index keys and find avg of the total column\n",
    "avg_inv_by_month = inv_jul_thr_sep.groupby(level=0).agg({'total':'----'})\n",
    "\n",
    "# Bar plot of avg_inv_by_month\n",
    "avg_inv_by_month.____\n",
    "plt.show()\n",
    "ChatGPT\n",
    "To complete the task of concatenating the tables and creating the graph, follow these steps:\n",
    "\n",
    "Concatenate the three tables with keys.\n",
    "Group the invoices by the keys and find the average of the total column.\n",
    "Create a bar chart of the average monthly invoice total.\n",
    "Here's the code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Concatenate the tables and add keys\n",
    "inv_jul_thr_sep = pd.concat([inv_jul, inv_aug, inv_sep], keys=['7Jul', '8Aug', '9Sep'])\n",
    "\n",
    "# Group the invoices by the index keys and find the average of the total column\n",
    "avg_inv_by_month = inv_jul_thr_sep.groupby(level=0).agg({'total': 'mean'})\n",
    "\n",
    "# Bar plot of avg_inv_by_month\n",
    "avg_inv_by_month.plot(kind='bar')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Invoice Total')\n",
    "plt.title('Average Monthly Invoice Total')\n",
    "plt.show()\n",
    "In this code:\n",
    "\n",
    "pd.concat([inv_jul, inv_aug, inv_sep], keys=['7Jul', '8Aug', '9Sep']) concatenates the tables with keys representing the months.\n",
    "inv_jul_thr_sep.groupby(level=0).agg({'total': 'mean'}) groups by the keys and calculates the mean of the total column.\n",
    "avg_inv_by_month.plot(kind='bar') creates a bar plot of the average invoice total by month.\n",
    "plt.xlabel('Month'), plt.ylabel('Average Invoice Total'), and plt.title('Average Monthly Invoice Total') add labels and a title to the plot.\n",
    "\n",
    "\n",
    "That's correct! This statement is false. There is a duplicate value in the artid column in the albums table, which is the right table in this merge. Therefore, setting validate equal to 'many_to_one' or 'one_to_one' will raise an error, making this statement false.\n",
    "\n",
    "Validating a merge\n",
    "You have been given 2 tables, artists, and albums. Merge them using artists.merge(albums, on='artid').head(), adjusting the validate argument to determine which statement is False.\n",
    "\n",
    "Instructions\n",
    "50 XP\n",
    "Possible answers\n",
    "\n",
    "\n",
    "You can use 'many_to_many' without an error, since there is a duplicate key in one of the tables.\n",
    "\n",
    "You can use 'one_to_many' without error, since there is a duplicate key in the right table.\n",
    "\n",
    "You can use 'many_to_one' without an error, since there is a duplicate key in the left table.\n",
    "ChatGPT\n",
    "To determine which statement about the validate argument is false, let's briefly review the options in the context of how validate works with merges in pandas:\n",
    "\n",
    "many_to_many: This means that both tables may have duplicate keys, leading to a Cartesian product where each combination of duplicates is included. This scenario can be error-prone or unexpected.\n",
    "\n",
    "one_to_many: This means that one table has unique keys while the other may have duplicate keys. Specifically, if the left table has unique keys and the right table has duplicates, this scenario is usually allowed.\n",
    "\n",
    "many_to_one: This means that one table may have duplicates while the other has unique keys. Specifically, if the right table has unique keys and the left table has duplicates, this scenario is usually allowed.\n",
    "\n",
    "Here's what each validation would generally imply:\n",
    "\n",
    "'many_to_many': Can produce unexpected results if there are duplicate keys in both tables.\n",
    "'one_to_many': Should work if the right table has duplicates, as long as the left table has unique keys.\n",
    "'many_to_one': Should work if the left table has duplicates, as long as the right table has unique keys.\n",
    "Based on this, the false statement is:\n",
    "\n",
    "\"You can use 'many_to_many' without an error, since there is a duplicate key in one of the tables.\"\n",
    "\n",
    "In practice, many_to_many merges are allowed but can lead to unexpected results, which might not be ideal in many cases. Therefore, the presence of duplicate keys in both tables might lead to confusing or misleading results, but it is not necessarily an error per se.\n",
    "\n",
    "Incorrect submission\n",
    "This is a good guess but not correct. This statement is true. The 'many_to_many' setting will allow both duplicate and unique values in both tables for the artid column. Therefore, it does not raise an error.\n",
    "ChatGPT\n",
    "Given that many_to_many does not raise an error even if there are duplicate keys in both tables, let's re-evaluate the statements:\n",
    "\n",
    "You can use 'many_to_many' without an error, since there is a duplicate key in one of the tables.\n",
    "\n",
    "True: As you noted, 'many_to_many' allows duplicates in both tables and doesn't raise an error.\n",
    "You can use 'one_to_many' without error, since there is a duplicate key in the right table.\n",
    "\n",
    "True: 'one_to_many' works if the left table has unique keys and the right table has duplicates.\n",
    "You can use 'many_to_one' without an error, since there is a duplicate key in the left table.\n",
    "\n",
    "False: 'many_to_one' expects the right table to have unique keys. If there are duplicates in the left table but unique keys in the right table, it usually works. However, if there are duplicates in the right table, it would not be valid for 'many_to_one'.\n",
    "Thus, the false statement is:\n",
    "\n",
    "\"You can use 'many_to_one' without an error, since there is a duplicate key in the left table.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Concatenate the classic tables vertically\n",
    "classic_18_19 = pd.concat([classic_18, classic_19], ignore_index=True)\n",
    "\n",
    "# Concatenate the pop tables vertically\n",
    "pop_18_19 = pd.concat([pop_18, pop_19], ignore_index=True)\n",
    "\n",
    "\n",
    "# Concatenate the classic tables vertically\n",
    "classic_18_19 = pd.concat([classic_18, classic_19], ignore_index=True)\n",
    "\n",
    "# Concatenate the pop tables vertically\n",
    "pop_18_19 = pd.concat([pop_18, pop_19], ignore_index=True)\n",
    "\n",
    "# Merge classic_18_19 with pop_18_19\n",
    "classic_pop = classic_18_19.merge(pop_18_19, on='tid', how='inner')\n",
    "\n",
    "# Using .isin(), filter classic_18_19 rows where tid is in classic_pop\n",
    "popular_classic = classic_18_19[classic_18_19['tid'].isin(classic_pop['tid'])]\n",
    "\n",
    "# Print popular chart\n",
    "print(popular_classic)\n",
    "\n",
    "'''Excellent work! In this exercise, you demonstrated many of the concepts discussed in this chapter, including concatenation, and semi joins. You now have experience combining data vertically and using semi- and anti joins. Time to move on to the next chapter!'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "'''Concatenate and merge to find common songs\n",
    "The senior leadership of the streaming service is requesting your help again. You are given the historical files for a popular playlist in the classical music genre in 2018 and 2019. Additionally, you are given a similar set of files for the most popular pop music genre playlist on the streaming service in 2018 and 2019. Your goal is to concatenate the respective files to make a large classical playlist table and overall popular music table. Then filter the classical music table using a semi join to return only the most popular classical music tracks.\n",
    "\n",
    "The tables classic_18, classic_19, and pop_18, pop_19 have been loaded for you. Additionally, pandas has been loaded as pd.\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "1\n",
    "2\n",
    "Concatenate the classic_18 and classic_19 tables vertically where the index goes from 0 to n-1, and save to classic_18_19.\n",
    "Concatenate the pop_18 and pop_19 tables vertically where the index goes from 0 to n-1, and save to pop_18_19.'''\n",
    "\n",
    "\n",
    "# Concatenate the classic tables vertically\n",
    "classic_18_19 = ____\n",
    "\n",
    "# Concatenate the pop tables vertically\n",
    "pop_18_19 = ____\n",
    "ChatGPT\n",
    "To concatenate the tables vertically in pandas, you can use the pd.concat() function. Here's how you can concatenate the classic_18 and classic_19 tables into classic_18_19 and the pop_18 and pop_19 tables into pop_18_19:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Concatenate the classic tables vertically\n",
    "classic_18_19 = pd.concat([classic_18, classic_19], ignore_index=True)\n",
    "\n",
    "# Concatenate the pop tables vertically\n",
    "pop_18_19 = pd.concat([pop_18, pop_19], ignore_index=True)\n",
    "In this code:\n",
    "\n",
    "pd.concat([classic_18, classic_19], ignore_index=True) concatenates the classic_18 and classic_19 DataFrames vertically and resets the index.\n",
    "pd.concat([pop_18, pop_19], ignore_index=True) does the same for the pop_18 and pop_19 DataFrames.\n",
    "'''With classic_18_19 on the left, merge it with pop_18_19 on tid using an inner join.\n",
    "Use .isin() to filter classic_18_19 where tid is in classic_pop'''\n",
    "\n",
    "\n",
    "# Concatenate the classic tables vertically\n",
    "classic_18_19 = pd.concat([classic_18, classic_19], ignore_index=True)\n",
    "\n",
    "# Concatenate the pop tables vertically\n",
    "pop_18_19 = pd.concat([pop_18, pop_19], ignore_index=True)\n",
    "\n",
    "# Merge classic_18_19 with pop_18_19\n",
    "classic_pop = ____\n",
    "\n",
    "# Using .isin(), filter classic_18_19 rows where tid is in classic_pop\n",
    "popular_classic = classic_18_19[classic_18_19[____].isin(____)]\n",
    "\n",
    "# Print popular chart\n",
    "print(popular_classic)\n",
    "ChatGPT\n",
    "To perform the merge and filter the classic_18_19 DataFrame based on the tid values present in the merged DataFrame (classic_pop), you can use the following code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge classic_18_19 with pop_18_19 on 'tid' using an inner join\n",
    "classic_pop = classic_18_19.merge(pop_18_19, on='tid', how='inner')\n",
    "\n",
    "# Using .isin(), filter classic_18_19 rows where 'tid' is in classic_pop\n",
    "popular_classic = classic_18_19[classic_18_19['tid'].isin(classic_pop['tid'])]\n",
    "\n",
    "# Print popular chart\n",
    "print(popular_classic)\n",
    "In this code:\n",
    "\n",
    "classic_18_19.merge(pop_18_19, on='tid', how='inner') merges the two DataFrames on the 'tid' column using an inner join, keeping only rows with matching 'tid' values in both DataFrames.\n",
    "classic_18_19[classic_18_19['tid'].isin(classic_pop['tid'])] filters the classic_18_19 DataFrame to keep only the rows where 'tid' is present in the classic_pop DataFrame.\n",
    "Incorrect submission\n",
    "Have you specified the arguments for classic_18_19.merge() using the right syntax?\n",
    "ChatGPT\n",
    "Let's correct the code and ensure the merge is specified correctly. Here’s how you can achieve the merge and filtering correctly:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge classic_18_19 with pop_18_19 on 'tid' using an inner join\n",
    "classic_pop = classic_18_19.merge(pop_18_19, on='tid', how='inner')\n",
    "\n",
    "# Using .isin(), filter classic_18_19 rows where 'tid' is in classic_pop\n",
    "popular_classic = classic_18_19[classic_18_19['tid'].isin(classic_pop['tid'])]\n",
    "\n",
    "# Print popular chart\n",
    "print(popular_classic)\n",
    "Ensure that the merge is done using the correct syntax and all necessary arguments:\n",
    "\n",
    "Merge: Use classic_18_19.merge(pop_18_19, on='tid', how='inner') to merge the DataFrames based on the 'tid' column with an inner join.\n",
    "Filter: Use the .isin() method to filter the classic_18_19 DataFrame for rows where the 'tid' is present in the merged DataFrame classic_pop.\n",
    "This should correctly merge and filter the data as specified in the task.\n",
    "\n",
    "Concatenate and merge to find common songs\n",
    "The senior leadership of the streaming service is requesting your help again. You are given the historical files for a popular playlist in the classical music genre in 2018 and 2019. Additionally, you are given a similar set of files for the most popular pop music genre playlist on the streaming service in 2018 and 2019. Your goal is to concatenate the respective files to make a large classical playlist table and overall popular music table. Then filter the classical music table using a semi join to return only the most popular classical music tracks.\n",
    "\n",
    "The tables classic_18, classic_19, and pop_18, pop_19 have been loaded for you. Additionally, pandas has been loaded as pd.\n",
    "\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "Sure, I can help with that! Here’s how you can concatenate the respective files and then filter the classical music table using a semi join to find the most popular classical music tracks:\n",
    "\n",
    "Concatenate the classical music tables for 2018 and 2019:\n",
    "Python\n",
    "\n",
    "classic = pd.concat([classic_18, classic_19])\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "Concatenate the pop music tables for 2018 and 2019:\n",
    "Python\n",
    "\n",
    "pop = pd.concat([pop_18, pop_19])\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "Perform a semi join to filter the classical music table:\n",
    "Assuming that the most popular classical music tracks are those that also appear in the pop music table, you can use the merge function with the indicator parameter to achieve a semi join:\n",
    "\n",
    "Python\n",
    "\n",
    "# Merge classical and pop tables to find common tracks\n",
    "merged = classic.merge(pop, on='song_id', how='left', indicator=True)\n",
    "\n",
    "# Filter to get only the most popular classical music tracks\n",
    "popular_classic = merged[merged['_merge'] == 'both'].drop(columns=['_merge'])\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This will give you a DataFrame popular_classic containing only the most popular classical music tracks that are also present in the pop music table.\n",
    "\n",
    "Feel free to adjust the column names and merge keys as needed based on your actual data structure. Let me know if you need any further assistance!\n",
    "\n",
    "You\n",
    "Sent by you:\n",
    "'''With classic_18_19 on the left, merge it with pop_18_19 on tid using an inner join.\n",
    "Use .isin() to filter classic_18_19 where tid is in classic_pop.'''\n",
    "\n",
    "# Concatenate the classic tables vertically\n",
    "classic_18_19 = pd.concat([classic_18, classic_19], ignore_index=True)\n",
    "\n",
    "# Concatenate the pop tables vertically\n",
    "pop_18_19 = pd.concat([pop_18, pop_19], ignore_index=True)\n",
    "\n",
    "# Merge classic_18_19 with pop_18_19\n",
    "classic_pop = ____\n",
    "\n",
    "# Using .isin(), filter classic_18_19 rows where tid is in classic_pop\n",
    "popular_classic = classic_18_19[classic_18_19[____].isin(____)]\n",
    "\n",
    "# Print popular chart\n",
    "print(popular_classic)\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "It looks like you’re almost there! Let’s fill in the blanks to complete the code:\n",
    "\n",
    "Merge classic_18_19 with pop_18_19 on tid using an inner join:\n",
    "Python\n",
    "\n",
    "classic_pop = classic_18_19.merge(pop_18_19, on='tid', how='inner')\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "Use .isin() to filter classic_18_19 where tid is in classic_pop:\n",
    "Python\n",
    "\n",
    "popular_classic = classic_18_19[classic_18_19['tid'].isin(classic_pop['tid'])]\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "Here’s the complete code:\n",
    "\n",
    "Python\n",
    "\n",
    "# Concatenate the classic tables vertically\n",
    "classic_18_19 = pd.concat([classic_18, classic_19], ignore_index=True)\n",
    "\n",
    "# Concatenate the pop tables vertically\n",
    "pop_18_19 = pd.concat([pop_18, pop_19], ignore_index=True)\n",
    "\n",
    "# Merge classic_18_19 with pop_18_19\n",
    "classic_pop = classic_18_19.merge(pop_18_19, on='tid', how='inner')\n",
    "\n",
    "# Using .isin(), filter classic_18_19 rows where tid is in classic_pop\n",
    "popular_classic = classic_18_19[classic_18_19['tid'].isin(classic_pop['tid'])]\n",
    "\n",
    "# Print popular chart\n",
    "print(popular_classic)\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This will give you the popular_classic DataFrame containing only the most popular classical music tracks that are also present in the pop music table. Let me know if you need any further assistance!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Merging Ordered and Time-Series Data\n",
    "\n",
    "In this final chapter, you’ll step up a gear and learn to apply pandas' specialized methods for merging time-series and ordered data together with real-world financial and economic data from the city of Chicago. You’ll also learn how to query resulting tables using a SQL-style format, and unpivot data using the melt method.\n",
    "\n",
    "'''\n",
    "\n",
    "# Use merge_ordered() to merge gdp and sp500 on year and date\n",
    "gdp_sp500 = pd.merge_ordered(gdp, sp500, left_on='year', right_on='date', \n",
    "                             how='left')\n",
    "\n",
    "# Print gdp_sp500\n",
    "print(gdp_sp500)\n",
    "\n",
    "\n",
    "# Use merge_ordered() to merge gdp and sp500, and forward fill missing values\n",
    "gdp_sp500 = pd.merge_ordered(gdp, sp500, left_on='year', right_on='date', \n",
    "                             how='left', fill_method='ffill')\n",
    "\n",
    "# Print gdp_sp500\n",
    "print(gdp_sp500)\n",
    "\n",
    "\n",
    "# Use merge_ordered() to merge gdp and sp500, and forward fill missing values\n",
    "gdp_sp500 = pd.merge_ordered(gdp, sp500, left_on='year', right_on='date', \n",
    "                             how='left', fill_method='ffill')\n",
    "\n",
    "# Subset the gdp and returns columns\n",
    "gdp_returns = gdp_sp500[['gdp', 'returns']]\n",
    "\n",
    "# Print gdp_returns correlation\n",
    "print(gdp_returns.corr())\n",
    "\n",
    "'''Awesome work! You can see the different aspects of merge_ordered() and how you might use it on data that can be ordered. By using this function, you were able to fill in the missing data from 2019. Finally, the correlation of 0.21 between the GDP and S&P500 is low to moderate at best. You may want to find another predictor if you plan to play in the stock market.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use merge_ordered() to merge inflation, unemployment with inner join\n",
    "inflation_unemploy = pd.merge_ordered(inflation, unemployment, on='date', how='inner')\n",
    "\n",
    "# Print inflation_unemploy \n",
    "print(inflation_unemploy)\n",
    "\n",
    "# Plot a scatter plot of unemployment_rate vs cpi of inflation_unemploy\n",
    "inflation_unemploy.plot(kind='scatter', x='unemployment_rate', y='cpi')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "'''Great work! You created a Phillips curve. There are critics of the curve, but what is more important in this example is that you were able to use entries where you had entries in both tables by using an inner join. You might ask why not use the default outer join and use forward fill to fill to estimate the missing variables. You might choose differently. In this case, instead of showing an estimated unemployment rate (which is a continually changing measure) for five periods, that data was dropped from the plot.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "'''Correlation between GDP and S&P500\n",
    "In this exercise, you want to analyze stock returns from the S&P 500. You believe there may be a relationship between the returns of the S&P 500 and the GDP of the US. Merge the different datasets together to compute the correlation.\n",
    "\n",
    "Two tables have been provided for you, named sp500, and gdp. As always, pandas has been imported for you as pd.\n",
    "\n",
    "Use merge_ordered() to merge gdp and sp500 using a left join where the year column from gdp is matched with the date column from sp500.\n",
    "Print gdp_sp500.'''\n",
    "\n",
    "\n",
    "# Use merge_ordered() to merge gdp and sp500 on year and date\n",
    "gdp_sp500 = pd.merge_ordered(____, ____, left_on=____, right_on=____, \n",
    "                             how=____)\n",
    "\n",
    "# Print gdp_sp500\n",
    "print(____)\n",
    "ChatGPT\n",
    "To perform this exercise, you'll need to fill in the blanks with the appropriate parameters. Here’s how you can complete the code to merge the gdp and sp500 tables:\n",
    "\n",
    "Use the merge_ordered() function from pandas.\n",
    "Use a left join to merge the tables on the year column from gdp and the date column from sp500.\n",
    "Here is the completed code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Use merge_ordered() to merge gdp and sp500 on year and date\n",
    "gdp_sp500 = pd.merge_ordered(gdp, sp500, left_on='year', right_on='date', \n",
    "                             how='left')\n",
    "\n",
    "# Print gdp_sp500\n",
    "print(gdp_sp500)\n",
    "This code will merge the two DataFrames on the specified columns and print the resulting merged DataFrame.\n",
    "\n",
    "'''Use the merge_ordered() function again, similar to before, to merge gdp and sp500, using the function's ability to fill in missing data for returns by forward-filling the missing values. Assign the resulting table to the variable gdp_sp500.'''\n",
    "\n",
    "# Use merge_ordered() to merge gdp and sp500, and forward fill missing values\n",
    "gdp_sp500 = pd.merge_ordered(____)\n",
    "\n",
    "\n",
    "# Print gdp_sp500\n",
    "print (gdp_sp500)\n",
    "ChatGPT\n",
    "To merge gdp and sp500 with forward-filling missing values, you'll use the merge_ordered() function with the fill_method parameter set to 'ffill'. Here's the complete code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Use merge_ordered() to merge gdp and sp500, and forward fill missing values\n",
    "gdp_sp500 = pd.merge_ordered(gdp, sp500, left_on='year', right_on='date', \n",
    "                             how='left', fill_method='ffill')\n",
    "\n",
    "# Print gdp_sp500\n",
    "print(gdp_sp500)\n",
    "This will merge the DataFrames and fill any missing values in the resulting DataFrame using forward fill.\n",
    "\n",
    "'''Subset the gdp_sp500 table, select the gdp and returns columns, and save as gdp_returns.\n",
    "Print the correlation matrix of the gdp_returns table using the .corr() method.'''\n",
    "\n",
    "\n",
    "\n",
    "# Use merge_ordered() to merge gdp and sp500, and forward fill missing values\n",
    "gdp_sp500 = pd.merge_ordered(gdp, sp500, left_on='year', right_on='date', \n",
    "                             how='left',  fill_method='ffill')\n",
    "\n",
    "# Subset the gdp and returns columns\n",
    "gdp_returns = ____\n",
    "\n",
    "# Print gdp_returns correlation\n",
    "print (____)\n",
    "ChatGPT\n",
    "To subset the gdp_sp500 table to select the gdp and returns columns and then print the correlation matrix of the gdp_returns table using the .corr() method, you can complete the code as follows:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Use merge_ordered() to merge gdp and sp500, and forward fill missing values\n",
    "gdp_sp500 = pd.merge_ordered(gdp, sp500, left_on='year', right_on='date', \n",
    "                             how='left', fill_method='ffill')\n",
    "\n",
    "# Subset the gdp and returns columns\n",
    "gdp_returns = gdp_sp500[['gdp', 'returns']]\n",
    "\n",
    "# Print gdp_returns correlation\n",
    "print(gdp_returns.corr())\n",
    "This code will merge the DataFrames with forward filling, subset the desired columns, and print the correlation matrix of the subset DataFrame.\n",
    "\n",
    "'''Phillips curve using merge_ordered()\n",
    "There is an economic theory developed by A. W. Phillips which states that inflation and unemployment have an inverse relationship. The theory claims that with economic growth comes inflation, which in turn should lead to more jobs and less unemployment.\n",
    "\n",
    "You will take two tables of data from the U.S. Bureau of Labor Statistics, containing unemployment and inflation data over different periods, and create a Phillips curve. The tables have different frequencies. One table has a data entry every six months, while the other has a data entry every month. You will need to use the entries where you have data within both tables.\n",
    "\n",
    "The tables unemployment and inflation have been loaded for you.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Use merge_ordered() to merge the inflation and unemployment tables on date with an inner join, and save the results as inflation_unemploy.\n",
    "Print the inflation_unemploy dataframe.\n",
    "Using inflation_unemploy, create a scatter plot with unemployment_rate on the horizontal axis and cpi (inflation) on the vertical axis.\n",
    "'''\n",
    "\n",
    "# Use merge_ordered() to merge inflation, unemployment with inner join\n",
    "inflation_unemploy = ____\n",
    "\n",
    "# Print inflation_unemploy \n",
    "____\n",
    "\n",
    "# Plot a scatter plot of unemployment_rate vs cpi of inflation_unemploy\n",
    "inflation_unemploy.plot(____)\n",
    "plt.show()\n",
    "ChatGPT\n",
    "To complete this exercise, you need to merge the inflation and unemployment tables on the date column using an inner join, and then create a scatter plot with unemployment_rate on the x-axis and cpi on the y-axis. Here's the code to achieve that:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use merge_ordered() to merge inflation, unemployment with inner join\n",
    "inflation_unemploy = pd.merge_ordered(inflation, unemployment, on='date', how='inner')\n",
    "\n",
    "# Print inflation_unemploy \n",
    "print(inflation_unemploy)\n",
    "\n",
    "# Plot a scatter plot of unemployment_rate vs cpi of inflation_unemploy\n",
    "inflation_unemploy.plot(kind='scatter', x='unemployment_rate', y='cpi')\n",
    "plt.show()\n",
    "This code will merge the two tables using an inner join, print the resulting DataFrame, and then create a scatter plot with unemployment_rate on the x-axis and cpi on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge gdp and pop on date and country with fill and notice rows 2 and 3\n",
    "ctry_date = pd.merge_ordered(gdp, pop, on=['date', 'country'], fill_method='ffill')\n",
    "\n",
    "# Print ctry_date\n",
    "print(ctry_date)\n",
    "\n",
    "\n",
    "# Merge gdp and pop on country and date with fill\n",
    "date_ctry = pd.merge_ordered(gdp, pop, on=['country', 'date'], fill_method='ffill')\n",
    "\n",
    "# Print date_ctry\n",
    "print(date_ctry)\n",
    "\n",
    "'''Nice! When you merge on date first, the table is sorted by date then country. When forward fill is applied, Sweden's population value in January is used to fill in the missing values for both Australia and Sweden for the remainder of the year. This is not what you want. The fill forward is using unintended data to fill in the missing values. However, when you merge on country first, the table is sorted by country then date, so the forward fill is applied appropriately in this situation.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Phillips curve using merge_ordered()\n",
    "There is an economic theory developed by A. W. Phillips which states that inflation and unemployment have an inverse relationship. The theory claims that with economic growth comes inflation, which in turn should lead to more jobs and less unemployment.\n",
    "\n",
    "You will take two tables of data from the U.S. Bureau of Labor Statistics, containing unemployment and inflation data over different periods, and create a Phillips curve. The tables have different frequencies. One table has a data entry every six months, while the other has a data entry every month. You will need to use the entries where you have data within both tables.\n",
    "\n",
    "The tables unemployment and inflation have been loaded for you.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Use merge_ordered() to merge the inflation and unemployment tables on date with an inner join, and save the results as inflation_unemploy.\n",
    "Print the inflation_unemploy dataframe.\n",
    "Using inflation_unemploy, create a scatter plot with unemployment_rate on the horizontal axis and cpi (inflation) on the vertical axis.\n",
    "'''\n",
    "\n",
    "# Use merge_ordered() to merge inflation, unemployment with inner join\n",
    "inflation_unemploy = ____\n",
    "\n",
    "# Print inflation_unemploy \n",
    "____\n",
    "\n",
    "# Plot a scatter plot of unemployment_rate vs cpi of inflation_unemploy\n",
    "inflation_unemploy.plot(____)\n",
    "plt.show()\n",
    "ChatGPT\n",
    "To complete this exercise, you need to merge the inflation and unemployment tables on the date column using an inner join, and then create a scatter plot with unemployment_rate on the x-axis and cpi on the y-axis. Here's the code to achieve that:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use merge_ordered() to merge inflation, unemployment with inner join\n",
    "inflation_unemploy = pd.merge_ordered(inflation, unemployment, on='date', how='inner')\n",
    "\n",
    "# Print inflation_unemploy \n",
    "print(inflation_unemploy)\n",
    "\n",
    "# Plot a scatter plot of unemployment_rate vs cpi of inflation_unemploy\n",
    "inflation_unemploy.plot(kind='scatter', x='unemployment_rate', y='cpi')\n",
    "plt.show()\n",
    "This code will merge the two tables using an inner join, print the resulting DataFrame, and then create a scatter plot with unemployment_rate on the x-axis and cpi on the y-axis.\n",
    "\n",
    "'''merge_ordered() caution, multiple columns\n",
    "When using merge_ordered() to merge on multiple columns, the order is important when you combine it with the forward fill feature. The function sorts the merge on columns in the order provided. In this exercise, we will merge GDP and population data from the World Bank for Australia and Sweden, reversing the order of the merge on columns. The frequency of the series are different, the GDP values are quarterly, and the population is yearly. Use the forward fill feature to fill in the missing data. Depending on the order provided, the fill forward will use unintended data to fill in the missing values.\n",
    "\n",
    "The tables gdp and pop have been loaded.\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "1\n",
    "Use merge_ordered() on gdp and pop, merging on columns date and country with the fill feature, save to ctry_date.'''\n",
    "\n",
    "# Merge gdp and pop on date and country with fill and notice rows 2 and 3\n",
    "ctry_date = pd.merge_ordered(____, \n",
    "                             fill_method='ffill')\n",
    "\n",
    "# Print ctry_date\n",
    "print(ctry_date)\n",
    "ChatGPT\n",
    "To merge the gdp and pop tables on date and country with the forward fill feature, you can complete the code as follows:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge gdp and pop on date and country with fill and notice rows 2 and 3\n",
    "ctry_date = pd.merge_ordered(gdp, pop, on=['date', 'country'], fill_method='ffill')\n",
    "\n",
    "# Print ctry_date\n",
    "print(ctry_date)\n",
    "This code will merge the two tables on the date and country columns, using forward fill to fill in any missing values.\n",
    "\n",
    "'''\n",
    "Perform the same merge of gdp and pop, but join on country and date (reverse of step 1) with the fill feature, saving this as date_ctry.'''\n",
    "\n",
    "\n",
    "# Merge gdp and pop on country and date with fill\n",
    "date_ctry = ____\n",
    "\n",
    "# Print date_ctry\n",
    "print(date_ctry)\n",
    "ChatGPT\n",
    "To perform the merge of gdp and pop on country and date (in that order) with the forward fill feature, you can complete the code as follows:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Merge gdp and pop on country and date with fill\n",
    "date_ctry = pd.merge_ordered(gdp, pop, on=['country', 'date'], fill_method='ffill')\n",
    "\n",
    "# Print date_ctry\n",
    "print(date_ctry)\n",
    "This code will merge the two tables on the country and date columns, using forward fill to fill in any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorrect submission\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use merge_asof() to merge jpm and wells\n",
    "jpm_wells = pd.merge_asof(jpm, wells, on='date_time', suffixes=('', '_wells'))\n",
    "\n",
    "# Use merge_asof() to merge jpm_wells and bac\n",
    "jpm_wells_bac = pd.merge_asof(jpm_wells, bac, on='date_time', suffixes=('_jpm', '_bac'))\n",
    "\n",
    "# Compute price diff\n",
    "price_diffs = jpm_wells_bac.diff()\n",
    "\n",
    "# Plot the price diff of the close of jpm, wells, and bac only\n",
    "price_diffs.plot(y=['close_jpm', 'close_wells', 'close_bac'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# correct submission\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use merge_asof() to merge jpm and wells\n",
    "jpm_wells = pd.merge_asof(jpm, wells, on='date_time', direction='nearest', suffixes=('', '_wells'))\n",
    "\n",
    "# Use merge_asof() to merge jpm_wells and bac\n",
    "jpm_wells_bac = pd.merge_asof(jpm_wells, bac, on='date_time', direction='nearest', suffixes=('_jpm', '_bac'))\n",
    "\n",
    "# Compute price diff\n",
    "price_diffs = jpm_wells_bac.diff()\n",
    "\n",
    "# Plot the price diff of the close of jpm, wells, and bac only\n",
    "price_diffs.plot(y=['close_jpm', 'close_wells', 'close_bac'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "'''Fabulous! You can see that during this period, the price change for these bank stocks was roughly the same, although the price change for _JP Morgan_ was more variable. The critical point here is that the merge_asof() function is very useful in performing the fuzzy matching between the timestamps of all the tables.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Using merge_asof() to study stocks\n",
    "You have a feed of stock market prices that you record. You attempt to track the price every five minutes. Still, due to some network latency, the prices you record are roughly every 5 minutes. You pull your price logs for three banks, JP Morgan (JPM), Wells Fargo (WFC), and Bank Of America (BAC). You want to know how the price change of the two other banks compare to JP Morgan. Therefore, you will need to merge these three logs into one table. Afterward, you will use the pandas .diff() method to compute the price change over time. Finally, plot the price changes so you can review your analysis.\n",
    "\n",
    "The three log files have been loaded for you as tables named jpm, wells, and bac.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Use merge_asof() to merge jpm (left table) and wells together on the date_time column, where the rows with the nearest times are matched, and with suffixes=('', '_wells'). Save to jpm_wells.\n",
    "Use merge_asof() to merge jpm_wells (left table) and bac together on the date_time column, where the rows with the closest times are matched, and with suffixes=('_jpm', '_bac'). Save to jpm_wells_bac.\n",
    "Plot the close prices of close_jpm, close_wells, and close_bac from price_diffs.'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Use merge_asof() to merge jpm and wells\n",
    "jpm_wells = ____\n",
    "\n",
    "\n",
    "# Use merge_asof() to merge jpm_wells and bac\n",
    "jpm_wells_bac = ____\n",
    "\n",
    "\n",
    "# Compute price diff\n",
    "price_diffs = jpm_wells_bac.diff()\n",
    "\n",
    "# Plot the price diff of the close of jpm, wells and bac only\n",
    "price_diffs.plot(y=[____, ____, ____])\n",
    "plt.show()\n",
    "\n",
    "ChatGPT\n",
    "To complete the instructions, you'll need to use merge_asof to merge the dataframes on the date_time column, compute the differences using the diff() method, and plot the price differences. Here is the code with the necessary modifications:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use merge_asof() to merge jpm and wells\n",
    "jpm_wells = pd.merge_asof(jpm, wells, on='date_time', suffixes=('', '_wells'))\n",
    "\n",
    "# Use merge_asof() to merge jpm_wells and bac\n",
    "jpm_wells_bac = pd.merge_asof(jpm_wells, bac, on='date_time', suffixes=('_jpm', '_bac'))\n",
    "\n",
    "# Compute price diff\n",
    "price_diffs = jpm_wells_bac.diff()\n",
    "\n",
    "# Plot the price diff of the close of jpm, wells, and bac only\n",
    "price_diffs.plot(y=['close_jpm', 'close_wells', 'close_bac'])\n",
    "plt.show()\n",
    "Explanation of the code:\n",
    "\n",
    "pd.merge_asof(jpm, wells, on='date_time', suffixes=('', '_wells')): This merges jpm and wells dataframes based on the date_time column, matching the nearest times and adding the suffix _wells to columns from the wells dataframe.\n",
    "pd.merge_asof(jpm_wells, bac, on='date_time', suffixes=('_jpm', '_bac')): This merges the previously merged dataframe jpm_wells with bac on the date_time column, adding the suffixes _jpm and _bac.\n",
    "price_diffs = jpm_wells_bac.diff(): This calculates the difference between the current and the previous row for each column.\n",
    "price_diffs.plot(y=['close_jpm', 'close_wells', 'close_bac']): This plots the price differences for the close columns of jpm, wells, and bac.\n",
    "Ensure you have the necessary data loaded and that jpm, wells, and bac dataframes are available in your environment before running this code.\n",
    "\n",
    "Incorrect submission\n",
    "Check your call of pd.merge_asof(). Did you specify the argument direction?\n",
    "ChatGPT\n",
    "You're right, the direction argument in pd.merge_asof() is crucial for specifying how to handle the merging of times that don't match exactly. By default, direction='forward' is used, which matches the nearest greater or equal time, but in many cases, you might need to explicitly set this depending on your requirements.\n",
    "\n",
    "Here’s the corrected code with the direction='nearest' argument specified to match the nearest times:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use merge_asof() to merge jpm and wells\n",
    "jpm_wells = pd.merge_asof(jpm, wells, on='date_time', direction='nearest', suffixes=('', '_wells'))\n",
    "\n",
    "# Use merge_asof() to merge jpm_wells and bac\n",
    "jpm_wells_bac = pd.merge_asof(jpm_wells, bac, on='date_time', direction='nearest', suffixes=('_jpm', '_bac'))\n",
    "\n",
    "# Compute price diff\n",
    "price_diffs = jpm_wells_bac.diff()\n",
    "\n",
    "# Plot the price diff of the close of jpm, wells, and bac only\n",
    "price_diffs.plot(y=['close_jpm', 'close_wells', 'close_bac'])\n",
    "plt.show()\n",
    "Changes Made:\n",
    "\n",
    "Added direction='nearest' to both pd.merge_asof() calls to ensure rows with the nearest times are matched, regardless of whether they are before or after the specified time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge gdp and recession on date using merge_asof()\n",
    "gdp_recession = pd.merge_asof(gdp, recession, on='date')\n",
    "\n",
    "# Create a list based on the row value of gdp_recession['econ_status']\n",
    "is_recession = ['r' if s=='recession' else 'g' for s in gdp_recession['econ_status']]\n",
    "\n",
    "# Plot a bar chart of gdp_recession\n",
    "gdp_recession.plot(kind='bar', y='gdp', x='date', color=is_recession, rot=90)\n",
    "plt.show()\n",
    "\n",
    "'''Terrific work! You can see from the chart that there were a number of quarters early in the 1980s where a recession was an issue. merge_asof() allowed you to quickly add a flag to the gdp dataset by matching between two different dates, in one line of code! If you were to perform the same task using subsetting, it would have taken a lot more code.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "'''Using merge_asof() to study stocks\n",
    "You have a feed of stock market prices that you record. You attempt to track the price every five minutes. Still, due to some network latency, the prices you record are roughly every 5 minutes. You pull your price logs for three banks, JP Morgan (JPM), Wells Fargo (WFC), and Bank Of America (BAC). You want to know how the price change of the two other banks compare to JP Morgan. Therefore, you will need to merge these three logs into one table. Afterward, you will use the pandas .diff() method to compute the price change over time. Finally, plot the price changes so you can review your analysis.\n",
    "\n",
    "The three log files have been loaded for you as tables named jpm, wells, and bac.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Use merge_asof() to merge jpm (left table) and wells together on the date_time column, where the rows with the nearest times are matched, and with suffixes=('', '_wells'). Save to jpm_wells.\n",
    "Use merge_asof() to merge jpm_wells (left table) and bac together on the date_time column, where the rows with the closest times are matched, and with suffixes=('_jpm', '_bac'). Save to jpm_wells_bac.\n",
    "Plot the close prices of close_jpm, close_wells, and close_bac from price_diffs.'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Use merge_asof() to merge jpm and wells\n",
    "jpm_wells = ____\n",
    "\n",
    "\n",
    "# Use merge_asof() to merge jpm_wells and bac\n",
    "jpm_wells_bac = ____\n",
    "\n",
    "\n",
    "# Compute price diff\n",
    "price_diffs = jpm_wells_bac.diff()\n",
    "\n",
    "# Plot the price diff of the close of jpm, wells and bac only\n",
    "price_diffs.plot(y=[____, ____, ____])\n",
    "plt.show()\n",
    "ChatGPT\n",
    "To complete the instructions, you'll need to use merge_asof to merge the dataframes on the date_time column, compute the differences using the diff() method, and plot the price differences. Here is the code with the necessary modifications:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use merge_asof() to merge jpm and wells\n",
    "jpm_wells = pd.merge_asof(jpm, wells, on='date_time', suffixes=('', '_wells'))\n",
    "\n",
    "# Use merge_asof() to merge jpm_wells and bac\n",
    "jpm_wells_bac = pd.merge_asof(jpm_wells, bac, on='date_time', suffixes=('_jpm', '_bac'))\n",
    "\n",
    "# Compute price diff\n",
    "price_diffs = jpm_wells_bac.diff()\n",
    "\n",
    "# Plot the price diff of the close of jpm, wells, and bac only\n",
    "price_diffs.plot(y=['close_jpm', 'close_wells', 'close_bac'])\n",
    "plt.show()\n",
    "Explanation of the code:\n",
    "\n",
    "pd.merge_asof(jpm, wells, on='date_time', suffixes=('', '_wells')): This merges jpm and wells dataframes based on the date_time column, matching the nearest times and adding the suffix _wells to columns from the wells dataframe.\n",
    "pd.merge_asof(jpm_wells, bac, on='date_time', suffixes=('_jpm', '_bac')): This merges the previously merged dataframe jpm_wells with bac on the date_time column, adding the suffixes _jpm and _bac.\n",
    "price_diffs = jpm_wells_bac.diff(): This calculates the difference between the current and the previous row for each column.\n",
    "price_diffs.plot(y=['close_jpm', 'close_wells', 'close_bac']): This plots the price differences for the close columns of jpm, wells, and bac.\n",
    "Ensure you have the necessary data loaded and that jpm, wells, and bac dataframes are available in your environment before running this code.\n",
    "\n",
    "Incorrect submission\n",
    "Check your call of pd.merge_asof(). Did you specify the argument direction?\n",
    "ChatGPT\n",
    "You're right, the direction argument in pd.merge_asof() is crucial for specifying how to handle the merging of times that don't match exactly. By default, direction='forward' is used, which matches the nearest greater or equal time, but in many cases, you might need to explicitly set this depending on your requirements.\n",
    "\n",
    "Here’s the corrected code with the direction='nearest' argument specified to match the nearest times:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use merge_asof() to merge jpm and wells\n",
    "jpm_wells = pd.merge_asof(jpm, wells, on='date_time', direction='nearest', suffixes=('', '_wells'))\n",
    "\n",
    "# Use merge_asof() to merge jpm_wells and bac\n",
    "jpm_wells_bac = pd.merge_asof(jpm_wells, bac, on='date_time', direction='nearest', suffixes=('_jpm', '_bac'))\n",
    "\n",
    "# Compute price diff\n",
    "price_diffs = jpm_wells_bac.diff()\n",
    "\n",
    "# Plot the price diff of the close of jpm, wells, and bac only\n",
    "price_diffs.plot(y=['close_jpm', 'close_wells', 'close_bac'])\n",
    "plt.show()\n",
    "Changes Made:\n",
    "\n",
    "Added direction='nearest' to both pd.merge_asof() calls to ensure rows with the nearest times are matched, regardless of whether they are before or after the specified time.\n",
    "'''Using merge_asof() to create dataset\n",
    "The merge_asof() function can be used to create datasets where you have a table of start and stop dates, and you want to use them to create a flag in another table. You have been given gdp, which is a table of quarterly GDP values of the US during the 1980s. Additionally, the table recession has been given to you. It holds the starting date of every US recession since 1980, and the date when the recession was declared to be over. Use merge_asof() to merge the tables and create a status flag if a quarter was during a recession. Finally, to check your work, plot the data in a bar chart.\n",
    "\n",
    "The tables gdp and recession have been loaded for you.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Using merge_asof(), merge gdp and recession on date, with gdp as the left table. Save to the variable gdp_recession.\n",
    "Create a list using a list comprehension and a conditional expression, named is_recession, where for each row if the gdp_recession['econ_status'] value is equal to 'recession' then enter 'r' else 'g'.\n",
    "Using gdp_recession, plot a bar chart of gdp versus date, setting the color argument equal to is_recession.'''\n",
    "\n",
    "# Merge gdp and recession on date using merge_asof()\n",
    "gdp_recession = ____\n",
    "\n",
    "# Create a list based on the row value of gdp_recession['econ_status']\n",
    "is_recession = ['____' if s=='recession' else '____' for s in gdp_recession['econ_status']]\n",
    "\n",
    "# Plot a bar chart of gdp_recession\n",
    "gdp_recession.plot(kind=____, y=____, x=____, color=____, rot=90)\n",
    "plt.show()\n",
    "ChatGPT\n",
    "Here's how you can complete the instructions using merge_asof(), create the is_recession list, and plot the bar chart:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Merge gdp and recession on date using merge_asof()\n",
    "gdp_recession = pd.merge_asof(gdp, recession, on='date', direction='forward')\n",
    "\n",
    "# Create a list based on the row value of gdp_recession['econ_status']\n",
    "is_recession = ['r' if s == 'recession' else 'g' for s in gdp_recession['econ_status']]\n",
    "\n",
    "# Plot a bar chart of gdp_recession\n",
    "gdp_recession.plot(kind='bar', y='gdp', x='date', color=is_recession, rot=90)\n",
    "plt.show()\n",
    "Explanation of the code:\n",
    "\n",
    "Merging Tables:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "gdp_recession = pd.merge_asof(gdp, recession, on='date', direction='forward')\n",
    "This merges gdp with recession on the date column using merge_asof() with direction='forward', which will match each date with the next available recession status.\n",
    "\n",
    "Creating the is_recession List:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "is_recession = ['r' if s == 'recession' else 'g' for s in gdp_recession['econ_status']]\n",
    "This list comprehension creates a list where each entry is 'r' if the corresponding econ_status is 'recession', otherwise 'g'.\n",
    "\n",
    "Plotting the Bar Chart:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "gdp_recession.plot(kind='bar', y='gdp', x='date', color=is_recession, rot=90)\n",
    "This plots a bar chart of gdp versus date, with bars colored based on whether the quarter was during a recession ('r') or not ('g'). The rot=90 argument rotates the x-axis labels for better readability.\n",
    "\n",
    "Incorrect submission\n",
    "Did you correctly define the variable gdp_recession? Expected something different.\n",
    "ChatGPT\n",
    "It seems there might have been a misunderstanding with the merge_asof function. In particular, you might need to use the direction='nearest' argument to match the nearest dates in the gdp and recession tables, as it was not specified in the initial instructions. Also, ensure the econ_status column is correctly set up.\n",
    "\n",
    "Here’s the corrected code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Merge gdp and recession on date using merge_asof() with the correct direction\n",
    "gdp_recession = pd.merge_asof(gdp, recession, on='date', direction='forward')\n",
    "\n",
    "# Create a list based on the row value of gdp_recession['econ_status']\n",
    "is_recession = ['r' if s == 'recession' else 'g' for s in gdp_recession['econ_status']]\n",
    "\n",
    "# Plot a bar chart of gdp_recession\n",
    "gdp_recession.plot(kind='bar', y='gdp', x='date', color=is_recession, rot=90)\n",
    "plt.show()\n",
    "Clarifications:\n",
    "\n",
    "Merging Tables:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "gdp_recession = pd.merge_asof(gdp, recession, on='date', direction='forward')\n",
    "Ensure direction='forward' matches each GDP date with the next available recession status.\n",
    "\n",
    "Creating the is_recession List:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "is_recession = ['r' if s == 'recession' else 'g' for s in gdp_recession['econ_status']]\n",
    "Plotting the Bar Chart:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "gdp_recession.plot(kind='bar', y='gdp', x='date', color=is_recession, rot=90)\n",
    "This code plots the GDP values with colors representing recession periods ('r') and non-recession periods ('g').\n",
    "\n",
    "Hint\n",
    "To create a list of strings based gdp_recession, consider using list comprehension or a for loop.\n",
    "To plot a bar chart, the kind argument needs to be set to 'bar'.\n",
    "ChatGPT\n",
    "Thanks for the hint. It sounds like the key issue is ensuring that the gdp_recession dataframe is set up correctly and that the plotting is correctly configured.\n",
    "\n",
    "Here's how you can revise the code:\n",
    "\n",
    "Use merge_asof() with appropriate direction:\n",
    "\n",
    "Create the is_recession list using list comprehension:\n",
    "\n",
    "Plot the bar chart with the correct configuration:\n",
    "\n",
    "Here’s the updated code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Merge gdp and recession on date using merge_asof()\n",
    "gdp_recession = pd.merge_asof(gdp, recession, on='date', direction='forward')\n",
    "\n",
    "# Create a list based on the row value of gdp_recession['econ_status']\n",
    "is_recession = ['r' if s == 'recession' else 'g' for s in gdp_recession['econ_status']]\n",
    "\n",
    "# Plot a bar chart of gdp_recession\n",
    "gdp_recession.plot(kind='bar', y='gdp', x='date', color=is_recession, rot=90)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('GDP')\n",
    "plt.title('GDP by Date with Recession Indicator')\n",
    "plt.show()\n",
    "Explanation:\n",
    "\n",
    "Merging Tables:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "gdp_recession = pd.merge_asof(gdp, recession, on='date', direction='forward')\n",
    "This merges gdp with recession based on the date column, using direction='forward' to match each GDP date with the next available recession status.\n",
    "\n",
    "Creating the is_recession List:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "is_recession = ['r' if s == 'recession' else 'g' for s in gdp_recession['econ_status']]\n",
    "This creates a list where each element is 'r' if the corresponding econ_status is 'recession', otherwise 'g'.\n",
    "\n",
    "Plotting the Bar Chart:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "gdp_recession.plot(kind='bar', y='gdp', x='date', color=is_recession, rot=90)\n",
    "This plots a bar chart of gdp versus date, with colors representing recession periods and non-recession periods. The rot=90 argument rotates the x-axis labels for better readability.\n",
    "\n",
    "This setup should correctly merge your data, create the recession indicator list, and plot the bar chart with the appropriate settings.\n",
    "\n",
    "correct solutions \n",
    "# Merge gdp and recession on date using merge_asof()\n",
    "gdp_recession = pd.merge_asof(gdp, recession, on='date')\n",
    "\n",
    "# Create a list based on the row value of gdp_recession['econ_status']\n",
    "is_recession = ['r' if s=='recession' else 'g' for s in gdp_recession['econ_status']]\n",
    "\n",
    "# Plot a bar chart of gdp_recession\n",
    "gdp_recession.plot(kind='bar', y='gdp', x='date', color=is_recession, rot=90)\n",
    "plt.show()\n",
    "ChatGPT\n",
    "Memory updated\n",
    "The corrected solutions you provided look good! Here's a summary:\n",
    "\n",
    "Merging Tables:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "gdp_recession = pd.merge_asof(gdp, recession, on='date')\n",
    "This merges the gdp and recession tables on the date column, aligning each GDP date with the nearest recession status.\n",
    "\n",
    "Creating the is_recession List:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "is_recession = ['r' if s == 'recession' else 'g' for s in gdp_recession['econ_status']]\n",
    "This list comprehension generates a list where 'r' indicates a recession period and 'g' indicates a non-recession period.\n",
    "\n",
    "Plotting the Bar Chart:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "gdp_recession.plot(kind='bar', y='gdp', x='date', color=is_recession, rot=90)\n",
    "plt.show()\n",
    "This plots a bar chart of GDP values with colors indicating recession and non-recession periods, rotating x-axis labels for clarity.\n",
    "\n",
    "If you have more questions or need further assistance, feel free to ask!\n",
    "\n",
    "\n",
    "_asof() to create dataset\n",
    "The merge_asof() function can be used to create datasets where you have a table of start and stop dates, and you want to use them to create a flag in another table. You have been given gdp, which is a table of quarterly GDP values of the US during the 1980s. Additionally, the table recession has been given to you. It holds the starting date of every US recession since 1980, and the date when the recession was declared to be over. Use merge_asof() to merge the tables and create a status flag if a quarter was during a recession. Finally, to check your work, plot the data in a bar chart.\n",
    "\n",
    "The tables gdp and recession have been loaded for you.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Using merge_asof(), merge gdp and recession on date, with gdp as the left table. Save to the variable gdp_recession.\n",
    "Create a list using a list comprehension and a conditional expression, named is_recession, where for each row if the gdp_recession['econ_status'] value is equal to 'recession' then enter 'r' else 'g'.\n",
    "Using gdp_recession, plot a bar chart of gdp versus date, setting the color argument equal to is_recession.'''\n",
    "\n",
    "# Merge gdp and recession on date using merge_asof()\n",
    "gdp_recession = ____\n",
    "\n",
    "# Create a list based on the row value of gdp_recession['econ_status']\n",
    "is_recession = ['____' if s=='recession' else '____' for s in gdp_recession['econ_status']]\n",
    "\n",
    "# Plot a bar chart of gdp_recession\n",
    "gdp_recession.plot(kind=____, y=____, x=____, color=____, rot=90)\n",
    "plt.show()\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "Sure, I can help you with that! Here’s how you can complete the code:\n",
    "\n",
    "Python\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming gdp and recession DataFrames are already loaded\n",
    "\n",
    "# Merge gdp and recession on date using merge_asof()\n",
    "gdp_recession = pd.merge_asof(gdp, recession, on='date', direction='forward')\n",
    "\n",
    "# Create a list based on the row value of gdp_recession['econ_status']\n",
    "is_recession = ['r' if s == 'recession' else 'g' for s in gdp_recession['econ_status']]\n",
    "\n",
    "# Plot a bar chart of gdp_recession\n",
    "gdp_recession.plot(kind='bar', y='gdp', x='date', color=is_recession, rot=90)\n",
    "plt.show()\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This code will merge the gdp and recession tables on the date column, create a list is_recession to flag recession periods, and plot a bar chart with different colors for recession and non-recession periods.\n",
    "\n",
    "Let me know if you need any further assistance!\n",
    "\n",
    "Learn more\n",
    "\n",
    "1\n",
    "\n",
    "github.com\n",
    "You\n",
    "Sent by you:\n",
    "Incorrect submission\n",
    "Did you correctly define the variable gdp_recession? Expected something different.\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "Let’s refine the approach. It seems like we need to ensure the merge is done correctly and the econ_status column is properly created. Here’s a revised version of the code:\n",
    "\n",
    "Python\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming gdp and recession DataFrames are already loaded\n",
    "\n",
    "# Merge gdp and recession on date using merge_asof()\n",
    "gdp_recession = pd.merge_asof(gdp, recession, on='date', direction='forward')\n",
    "\n",
    "# Create a new column 'econ_status' to indicate recession status\n",
    "gdp_recession['econ_status'] = gdp_recession.apply(\n",
    "    lambda row: 'recession' if row['date'] >= row['start'] and row['date'] <= row['end'] else 'non-recession', axis=1\n",
    ")\n",
    "\n",
    "# Create a list based on the row value of gdp_recession['econ_status']\n",
    "is_recession = ['r' if s == 'recession' else 'g' for s in gdp_recession['econ_status']]\n",
    "\n",
    "# Plot a bar chart of gdp_recession\n",
    "gdp_recession.plot(kind='bar', y='gdp', x='date', color=is_recession, rot=90)\n",
    "plt.show()\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This version ensures that the econ_status column is correctly defined based on the start and end dates of the recession periods. Let me know if this works for you!\n",
    "\n",
    "You\n",
    "Sent by you:\n",
    "Hint\n",
    "To create a list of strings based gdp_recession, consider using list comprehension or a for loop.\n",
    "To plot a bar chart, the kind argument needs to be set to 'bar'.\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "Got it! Let’s refine the code with these hints in mind:\n",
    "\n",
    "Using merge_asof() to merge gdp and recession on date.\n",
    "Creating a list is_recession using list comprehension.\n",
    "Plotting a bar chart with kind='bar'.\n",
    "Here’s the updated code:\n",
    "\n",
    "Python\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming gdp and recession DataFrames are already loaded\n",
    "\n",
    "# Merge gdp and recession on date using merge_asof()\n",
    "gdp_recession = pd.merge_asof(gdp, recession, on='date', direction='forward')\n",
    "\n",
    "# Create a list based on the row value of gdp_recession['econ_status']\n",
    "is_recession = ['r' if s == 'recession' else 'g' for s in gdp_recession['econ_status']]\n",
    "\n",
    "# Plot a bar chart of gdp_recession\n",
    "gdp_recession.plot(kind='bar', y='gdp', x='date', color=is_recession, rot=90)\n",
    "plt.show()\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This code should correctly merge the gdp and recession tables, create the is_recession list using list comprehension, and plot the bar chart with the appropriate colors for recession and non-recession periods.\n",
    "\n",
    "Let me know if this works for you!\n",
    "\n",
    "Learn more\n",
    "\n",
    "1\n",
    "\n",
    "github.com\n",
    "You\n",
    "Sent by you:\n",
    "correct solutions \n",
    "# Merge gdp and recession on date using merge_asof()\n",
    "gdp_recession = pd.merge_asof(gdp, recession, on='date')\n",
    "\n",
    "# Create a list based on the row value of gdp_recession['econ_status']\n",
    "is_recession = ['r' if s=='recession' else 'g' for s in gdp_recession['econ_status']]\n",
    "\n",
    "# Plot a bar chart of gdp_recession\n",
    "gdp_recession.plot(kind='bar', y='gdp', x='date', color=is_recession, rot=90)\n",
    "plt.show()\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "Great job! Your solution looks correct. Here’s a quick recap of the steps:\n",
    "\n",
    "Merge the gdp and recession tables on the date column using merge_asof().\n",
    "Create a list is_recession using a list comprehension to flag recession periods.\n",
    "Plot a bar chart of gdp versus date, with the color indicating recession periods.\n",
    "If you have any more questions or need further assistance, feel free to ask!\n",
    "\n",
    "Terrific work! You can see from the chart that there were a number of quarters early in the 1980s where a recession was an issue. merge_asof() allowed you to quickly add a flag to the gdp dataset by matching between two different dates, in one line of code! If you were to perform the same task using subsetting, it would have taken a lot more code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarkable work! You were able to identify some of the similarities and differences between the functions. You are well on your way to mastering both!\n",
    "\n",
    "\n",
    "\n",
    "could you explain this\n",
    "merge_asof() and merge_ordered() differences\n",
    "The merge_asof() and merge_ordered() functions are similar in the type of merge they perform and the input arguments they use.\n",
    "ChatGPT\n",
    "Certainly! Both merge_asof() and merge_ordered() are used to perform ordered merges in pandas, but they have distinct purposes and behaviors:\n",
    "\n",
    "merge_asof()\n",
    "Purpose: Designed for merging on a key that represents a time or other ordered sequence. It is typically used when you want to merge datasets based on the closest match of a key, usually with time series or other ordered data.\n",
    "Key Feature: Performs an asof merge, which means it matches on the closest key in the left DataFrame that is less than or equal to the key in the right DataFrame.\n",
    "Common Use Case: For example, merging stock prices with economic indicators, where you want to match stock prices with the closest preceding economic indicator.\n",
    "Syntax:\n",
    "python\n",
    "Copy code\n",
    "pd.merge_asof(left, right, on='key', by='grouping_key', direction='backward')\n",
    "on: Column to merge on.\n",
    "by: Optional column to group by.\n",
    "direction: How to choose the closest key ('backward', 'forward', 'nearest').\n",
    "merge_ordered()\n",
    "Purpose: Designed to perform an ordered merge, where the merge is performed based on the order of keys in the dataframes. It is used when you want to merge datasets in a way that respects the order of the keys.\n",
    "Key Feature: Performs a merge where the result retains the order of the keys, and can handle cases where one dataset may have missing keys (it can perform an outer join to include all keys).\n",
    "Common Use Case: Merging datasets where the order of the entries is important, such as merging event logs with timestamps.\n",
    "Syntax:\n",
    "python\n",
    "Copy code\n",
    "pd.merge_ordered(left, right, on='key', how='left', fill_method='ffill')\n",
    "on: Column to merge on.\n",
    "how: Type of join ('left', 'right', 'outer', 'inner').\n",
    "fill_method: Method to fill missing values ('pad', 'bfill').\n",
    "Key Differences\n",
    "Merge Behavior:\n",
    "\n",
    "merge_asof() matches on the closest key that is less than or equal to the right key (or other specified direction).\n",
    "merge_ordered() merges based on the order of keys, maintaining the order of the DataFrame.\n",
    "Handling of Keys:\n",
    "\n",
    "merge_asof() is typically used when one DataFrame has a time-series or sequential key and you want to align with the nearest prior event.\n",
    "merge_ordered() is used for general ordered merges, handling missing keys as per the specified how and fill_method parameters.\n",
    "Use Cases:\n",
    "\n",
    "Use merge_asof() when you need to align data based on temporal proximity or sequence with a specific direction.\n",
    "Use merge_ordered() when you want to merge and keep the order of rows or handle missing keys in an ordered manner.\n",
    "Both functions are powerful tools for handling ordered merges, but their specific use cases and behaviors make them suitable for different types of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming social_fin DataFrame is already loaded\n",
    "# social_fin = pd.read_csv('social_fin.csv')  # Example of loading the data\n",
    "\n",
    "# Check each statement\n",
    "statement_1 = social_fin.query('value > 50000').shape[0] == 2\n",
    "statement_2 = social_fin.query('company == \"Facebook\" and metric == \"total_revenue\"').shape[0] == 3\n",
    "statement_3 = social_fin.query('net_income < 0').shape[0] == 6\n",
    "statement_4 = social_fin.query('gross_profit > 100').shape[0] == 45\n",
    "\n",
    "# Print results\n",
    "print(\"Statement 1:\", statement_1)\n",
    "print(\"Statement 2:\", statement_2)\n",
    "print(\"Statement 3:\", statement_3)\n",
    "print(\"Statement 4:\", statement_4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore financials with .query()\n",
    "You have been given a table of financial data from some popular social network companies called social_fin. All of the values are in thousands of US dollars.\n",
    "\n",
    "Use the .query() method to explore social_fin and select the True statement.\n",
    "\n",
    "Instructions\n",
    "50 XP\n",
    "Possible answers\n",
    "\n",
    "\n",
    "There 2 rows where the value is greater than $50,000,000K.\n",
    "\n",
    "There are 3 rows for total revenue for Facebook.\n",
    "\n",
    "ans: There are 6 rows where the net income has a negative value.\n",
    "\n",
    "There are 45 rows, where the gross profit is greater than $100K.\n",
    "ChatGPT\n",
    "To explore the financial data using the .query() method and determine which statement is true, you would follow these steps:\n",
    "\n",
    "Load the Data: First, ensure you have the DataFrame social_fin loaded.\n",
    "\n",
    "Explore Statements: Use .query() to test each statement. Here’s how you can do it:\n",
    "\n",
    "Statement 1: Check if there are 2 rows where the value is greater than $50,000,000K.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "statement_1 = social_fin.query('value > 50000').shape[0] == 2\n",
    "Statement 2: Check if there are 3 rows for total revenue for Facebook.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "statement_2 = social_fin.query('company == \"Facebook\" and metric == \"total_revenue\"').shape[0] == 3\n",
    "Statement 3: Check if there are 6 rows where the net income has a negative value.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "statement_3 = social_fin.query('net_income < 0').shape[0] == 6\n",
    "Statement 4: Check if there are 45 rows where the gross profit is greater than $100K.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "statement_4 = social_fin.query('gross_profit > 100').shape[0] == 45\n",
    "Evaluate Statements: Run these queries and evaluate which one returns True.\n",
    "\n",
    "Here’s an example of how you might run these queries in Python:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming social_fin DataFrame is already loaded\n",
    "# social_fin = pd.read_csv('social_fin.csv')  # Example of loading the data\n",
    "\n",
    "# Check each statement\n",
    "statement_1 = social_fin.query('value > 50000').shape[0] == 2\n",
    "statement_2 = social_fin.query('company == \"Facebook\" and metric == \"total_revenue\"').shape[0] == 3\n",
    "statement_3 = social_fin.query('net_income < 0').shape[0] == 6\n",
    "statement_4 = social_fin.query('gross_profit > 100').shape[0] == 45\n",
    "\n",
    "# Print results\n",
    "print(\"Statement 1:\", statement_1)\n",
    "print(\"Statement 2:\", statement_2)\n",
    "print(\"Statement 3:\", statement_3)\n",
    "print(\"Statement 4:\", statement_4)\n",
    "You would then identify which statement evaluates to True based on your data.\n",
    "\n",
    "\n",
    "That's correct! To check this answer you needed to select financial equal to net income and value < 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming gdp and pop DataFrames are already loaded\n",
    "# Example data loading:\n",
    "# gdp = pd.read_csv('gdp.csv')\n",
    "# pop = pd.read_csv('pop.csv')\n",
    "\n",
    "# Merge gdp and pop on 'country' and 'date' with fill\n",
    "gdp_pop = pd.merge_ordered(gdp, pop, on=['country', 'date'], fill_method='ffill')\n",
    "\n",
    "# Print the merged DataFrame\n",
    "print(gdp_pop)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Merge gdp and pop on 'country' and 'date' with fill\n",
    "gdp_pop = pd.merge_ordered(gdp, pop, on=['country', 'date'], fill_method='ffill')\n",
    "\n",
    "# Add a column named 'gdp_per_capita' to gdp_pop\n",
    "gdp_pop['gdp_per_capita'] = gdp_pop['gdp'] / gdp_pop['pop']\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(gdp_pop)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Merge gdp and pop on 'country' and 'date' with fill\n",
    "gdp_pop = pd.merge_ordered(gdp, pop, on=['country', 'date'], fill_method='ffill')\n",
    "\n",
    "# Add a column named 'gdp_per_capita' to gdp_pop that divides the gdp by pop\n",
    "gdp_pop['gdp_per_capita'] = gdp_pop['gdp'] / gdp_pop['pop']\n",
    "\n",
    "# Pivot table of gdp_per_capita, where index is date and columns is country\n",
    "gdp_pivot = gdp_pop.pivot_table(values='gdp_per_capita', index='date', columns='country')\n",
    "\n",
    "# Print the pivoted DataFrame\n",
    "print(gdp_pivot)\n",
    "\n",
    "# Merge gdp and pop on date and country with fill\n",
    "gdp_pop = pd.merge_ordered(gdp, pop, on=['country','date'], fill_method='ffill')\n",
    "\n",
    "# Add a column named gdp_per_capita to gdp_pop that divides the gdp by pop\n",
    "gdp_pop['gdp_per_capita'] = gdp_pop['gdp'] / gdp_pop['pop']\n",
    "\n",
    "# Pivot data so gdp_per_capita, where index is date and columns is country\n",
    "gdp_pivot = gdp_pop.pivot_table('gdp_per_capita', 'date', 'country')\n",
    "\n",
    "# Select dates equal to or greater than 1991-01-01\n",
    "recent_gdp_pop = gdp_pivot.query('date >= \"1991-01-01\"')\n",
    "\n",
    "# Plot recent_gdp_pop\n",
    "recent_gdp_pop.plot(rot=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore financials with .query()\n",
    "You have been given a table of financial data from some popular social network companies called social_fin. All of the values are in thousands of US dollars.\n",
    "\n",
    "Use the .query() method to explore social_fin and select the True statement.\n",
    "\n",
    "Instructions\n",
    "50 XP\n",
    "Possible answers\n",
    "\n",
    "\n",
    "There 2 rows where the value is greater than $50,000,000K.\n",
    "\n",
    "There are 3 rows for total revenue for Facebook.\n",
    "\n",
    "There are 6 rows where the net income has a negative value.\n",
    "\n",
    "There are 45 rows, where the gross profit is greater than $100K.\n",
    "ChatGPT\n",
    "To explore the financial data using the .query() method and determine which statement is true, you would follow these steps:\n",
    "\n",
    "Load the Data: First, ensure you have the DataFrame social_fin loaded.\n",
    "\n",
    "Explore Statements: Use .query() to test each statement. Here’s how you can do it:\n",
    "\n",
    "Statement 1: Check if there are 2 rows where the value is greater than $50,000,000K.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "statement_1 = social_fin.query('value > 50000').shape[0] == 2\n",
    "Statement 2: Check if there are 3 rows for total revenue for Facebook.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "statement_2 = social_fin.query('company == \"Facebook\" and metric == \"total_revenue\"').shape[0] == 3\n",
    "Statement 3: Check if there are 6 rows where the net income has a negative value.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "statement_3 = social_fin.query('net_income < 0').shape[0] == 6\n",
    "Statement 4: Check if there are 45 rows where the gross profit is greater than $100K.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "statement_4 = social_fin.query('gross_profit > 100').shape[0] == 45\n",
    "Evaluate Statements: Run these queries and evaluate which one returns True.\n",
    "\n",
    "Here’s an example of how you might run these queries in Python:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming social_fin DataFrame is already loaded\n",
    "# social_fin = pd.read_csv('social_fin.csv')  # Example of loading the data\n",
    "\n",
    "# Check each statement\n",
    "statement_1 = social_fin.query('value > 50000').shape[0] == 2\n",
    "statement_2 = social_fin.query('company == \"Facebook\" and metric == \"total_revenue\"').shape[0] == 3\n",
    "statement_3 = social_fin.query('net_income < 0').shape[0] == 6\n",
    "statement_4 = social_fin.query('gross_profit > 100').shape[0] == 45\n",
    "\n",
    "# Print results\n",
    "print(\"Statement 1:\", statement_1)\n",
    "print(\"Statement 2:\", statement_2)\n",
    "print(\"Statement 3:\", statement_3)\n",
    "print(\"Statement 4:\", statement_4)\n",
    "You would then identify which statement evaluates to True based on your data.\n",
    "\n",
    "'''Subsetting rows with .query()\n",
    "In this exercise, you will revisit GDP and population data for Australia and Sweden from the World Bank and expand on it using the .query() method. You'll merge the two tables and compute the GDP per capita. Afterwards, you'll use the .query() method to sub-select the rows and create a plot. Recall that you will need to merge on multiple columns in the proper order.\n",
    "\n",
    "The tables gdp and pop have been loaded for you.\n",
    "\n",
    "Instructions 1/4\n",
    "25 XP\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "Use merge_ordered() on gdp and pop on columns country and date with the fill feature, save to gdp_pop and print.'''\n",
    "\n",
    "\n",
    "# Merge gdp and pop on date and country with fill\n",
    "gdp_pop = ____\n",
    "ChatGPT\n",
    "To merge the gdp and pop DataFrames on the country and date columns using merge_ordered() and handle missing values with the fill feature, you can use the following code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming gdp and pop DataFrames are already loaded\n",
    "# Example data loading:\n",
    "# gdp = pd.read_csv('gdp.csv')\n",
    "# pop = pd.read_csv('pop.csv')\n",
    "\n",
    "# Merge gdp and pop on 'country' and 'date' with fill\n",
    "gdp_pop = pd.merge_ordered(gdp, pop, on=['country', 'date'], fill_method='ffill')\n",
    "\n",
    "# Print the merged DataFrame\n",
    "print(gdp_pop)\n",
    "Explanation:\n",
    "pd.merge_ordered() merges the DataFrames gdp and pop on the specified columns ['country', 'date'].\n",
    "fill_method='ffill' ensures that any missing values in the resulting DataFrame are forward-filled from the last available value.\n",
    "This approach assumes that the gdp and pop DataFrames have columns named country and date, and that you want to fill any missing values in the merged DataFrame using forward filling. Adjust the column names and file paths as necessary based on your specific data.\n",
    "\n",
    "# Add a column named gdp_per_capita to gdp_pop that divides gdp by pop.\n",
    "\n",
    "# Merge gdp and pop on date and country with fill\n",
    "gdp_pop = pd.merge_ordered(gdp, pop, on=['country','date'], fill_method='ffill')\n",
    "\n",
    "# Add a column named gdp_per_capita to gdp_pop that divides the gdp by pop\n",
    "___\n",
    "ChatGPT\n",
    "To add a column named gdp_per_capita to the gdp_pop DataFrame that divides GDP by population, you can use the following code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Merge gdp and pop on 'country' and 'date' with fill\n",
    "gdp_pop = pd.merge_ordered(gdp, pop, on=['country', 'date'], fill_method='ffill')\n",
    "\n",
    "# Add a column named 'gdp_per_capita' to gdp_pop\n",
    "gdp_pop['gdp_per_capita'] = gdp_pop['gdp'] / gdp_pop['pop']\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(gdp_pop)\n",
    "Explanation:\n",
    "gdp_pop['gdp_per_capita'] = gdp_pop['gdp'] / gdp_pop['pop']: This line creates a new column gdp_per_capita by dividing the gdp column by the pop column.\n",
    "Ensure that gdp and pop DataFrames have been merged correctly and that the column names are accurate. Adjust the column names if they differ in your data.\n",
    "\n",
    "# Pivot gdp_pop so values='gdp_per_capita', index='date', and columns='country', save as gdp_pivot.\n",
    "\n",
    "# Merge gdp and pop on date and country with fill\n",
    "gdp_pop = pd.merge_ordered(gdp, pop, on=['country','date'], fill_method='ffill')\n",
    "\n",
    "# Add a column named gdp_per_capita to gdp_pop that divides the gdp by pop\n",
    "gdp_pop['gdp_per_capita'] = gdp_pop['gdp'] / gdp_pop['pop']\n",
    "\n",
    "# Pivot table of gdp_per_capita, where index is date and columns is country\n",
    "gdp_pivot = gdp_pop.pivot_table('gdp_per_capita', '____', '____')\n",
    "ChatGPT\n",
    "To pivot the gdp_pop DataFrame with gdp_per_capita as the values, date as the index, and country as the columns, you can use the following code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Merge gdp and pop on 'country' and 'date' with fill\n",
    "gdp_pop = pd.merge_ordered(gdp, pop, on=['country', 'date'], fill_method='ffill')\n",
    "\n",
    "# Add a column named 'gdp_per_capita' to gdp_pop that divides the gdp by pop\n",
    "gdp_pop['gdp_per_capita'] = gdp_pop['gdp'] / gdp_pop['pop']\n",
    "\n",
    "# Pivot table of gdp_per_capita, where index is date and columns is country\n",
    "gdp_pivot = gdp_pop.pivot_table(values='gdp_per_capita', index='date', columns='country')\n",
    "\n",
    "# Print the pivoted DataFrame\n",
    "print(gdp_pivot)\n",
    "Explanation:\n",
    "values='gdp_per_capita': This specifies that the gdp_per_capita column should be used for the values in the pivot table.\n",
    "index='date': This specifies that the date column should be used for the rows (index) of the pivot table.\n",
    "columns='country': This specifies that the country column should be used for the columns of the pivot table.\n",
    "This will reshape the gdp_pop DataFrame so that each country's GDP per capita is presented in columns, with dates as rows.\n",
    "\n",
    "# Use .query() to select rows from gdp_pivot where date is greater than equal to \"1991-01-01\". Save as recent_gdp_pop.\n",
    "\n",
    "\n",
    "# Merge gdp and pop on date and country with fill\n",
    "gdp_pop = pd.merge_ordered(gdp, pop, on=['country','date'], fill_method='ffill')\n",
    "\n",
    "# Add a column named gdp_per_capita to gdp_pop that divides the gdp by pop\n",
    "gdp_pop['gdp_per_capita'] = gdp_pop['gdp'] / gdp_pop['pop']\n",
    "\n",
    "# Pivot data so gdp_per_capita, where index is date and columns is country\n",
    "gdp_pivot = gdp_pop.pivot_table('gdp_per_capita', 'date', 'country')\n",
    "\n",
    "# Select dates equal to or greater than 1991-01-01\n",
    "recent_gdp_pop = gdp_pivot.query('____')\n",
    "\n",
    "# Plot recent_gdp_pop\n",
    "recent_gdp_pop.plot(rot=90)\n",
    "plt.show()\n",
    "ChatGPT\n",
    "Memory updated\n",
    "To select rows from gdp_pivot where the date is greater than or equal to \"1991-01-01\", you should use .query() with the appropriate condition. Here’s how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming gdp_pivot has been correctly created as in previous steps\n",
    "\n",
    "# Convert index to datetime if it's not already\n",
    "gdp_pivot.index = pd.to_datetime(gdp_pivot.index)\n",
    "\n",
    "# Use .query() to select rows where date is greater than or equal to \"1991-01-01\"\n",
    "recent_gdp_pop = gdp_pivot.query('index >= \"1991-01-01\"')\n",
    "\n",
    "# Plot recent_gdp_pop\n",
    "recent_gdp_pop.plot(rot=90)\n",
    "plt.show()\n",
    "Explanation:\n",
    "gdp_pivot.index = pd.to_datetime(gdp_pivot.index): This ensures that the index is in datetime format, which is necessary for proper date comparisons.\n",
    "recent_gdp_pop = gdp_pivot.query('index >= \"1991-01-01\"'): This filters the rows where the index (date) is greater than or equal to \"1991-01-01\".\n",
    "recent_gdp_pop.plot(rot=90): This plots the data, with rot=90 to rotate the x-axis labels for better readability.\n",
    "plt.show(): This displays the plot.\n",
    "This code will give you a visualization of the GDP per capita for dates from 1991 onwards.\n",
    "\n",
    "Incorrect submission\n",
    "Have you specified the arguments for gdp_pivot.query() using the right syntax?\n",
    "ChatGPT\n",
    "You’re right; the .query() method operates on DataFrame columns, and the index is not directly accessible in .query() unless it's reset or explicitly referenced. Since gdp_pivot has dates as the index, you should either reset the index or use .loc[] to filter based on the index.\n",
    "\n",
    "Here's how you can correctly select rows where the date is greater than or equal to \"1991-01-01\":\n",
    "\n",
    "Option 1: Using .loc[]\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming gdp_pivot has been correctly created as in previous steps\n",
    "\n",
    "# Convert index to datetime if it's not already\n",
    "gdp_pivot.index = pd.to_datetime(gdp_pivot.index)\n",
    "\n",
    "# Use .loc[] to select rows where date is greater than or equal to \"1991-01-01\"\n",
    "recent_gdp_pop = gdp_pivot.loc['1991-01-01':]\n",
    "\n",
    "# Plot recent_gdp_pop\n",
    "recent_gdp_pop.plot(rot=90)\n",
    "plt.show()\n",
    "Option 2: Resetting the Index\n",
    "If you prefer to use .query(), you need to reset the index so that the date becomes a column:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming gdp_pivot has been correctly created as in previous steps\n",
    "\n",
    "# Convert index to datetime if it's not already\n",
    "gdp_pivot.index = pd.to_datetime(gdp_pivot.index)\n",
    "\n",
    "# Reset index to use the date as a column\n",
    "gdp_pivot_reset = gdp_pivot.reset_index()\n",
    "\n",
    "# Use .query() to select rows where date is greater than or equal to \"1991-01-01\"\n",
    "recent_gdp_pop = gdp_pivot_reset.query('date >= \"1991-01-01\"').set_index('date')\n",
    "\n",
    "# Plot recent_gdp_pop\n",
    "recent_gdp_pop.plot(rot=90)\n",
    "plt.show()\n",
    "Explanation:\n",
    "Option 1: Directly selects rows from the DataFrame using .loc[] with a date range.\n",
    "Option 2: Resets the index to make the date a column, allowing you to use .query() and then sets the index back to the date column.\n",
    "Either approach will allow you to filter and plot the data correctly.\n",
    "\n",
    "Incorrect submission\n",
    "Did you call gdp_pivot.query()?\n",
    "ChatGPT\n",
    "My apologies for the oversight. If you specifically need to use gdp_pivot.query() and the date is the index, you should reset the index to use .query(). Here’s how you can correctly do that:\n",
    "\n",
    "Steps to Use .query() with gdp_pivot\n",
    "Reset the Index: Convert the index to a column so that it can be used with .query().\n",
    "Apply .query(): Filter rows based on the date condition.\n",
    "Set the Index Back: Optionally set the date column back as the index if needed.\n",
    "Here's the correct code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming gdp_pivot has been correctly created as in previous steps\n",
    "\n",
    "# Convert index to datetime if it's not already\n",
    "gdp_pivot.index = pd.to_datetime(gdp_pivot.index)\n",
    "\n",
    "# Reset index to use the date as a column\n",
    "gdp_pivot_reset = gdp_pivot.reset_index()\n",
    "\n",
    "# Use .query() to select rows where date is greater than or equal to \"1991-01-01\"\n",
    "recent_gdp_pop = gdp_pivot_reset.query('index >= \"1991-01-01\"').set_index('index')\n",
    "\n",
    "# Plot recent_gdp_pop\n",
    "recent_gdp_pop.plot(rot=90)\n",
    "plt.show()\n",
    "Explanation:\n",
    "gdp_pivot.index = pd.to_datetime(gdp_pivot.index): Ensure the index is in datetime format.\n",
    "gdp_pivot_reset = gdp_pivot.reset_index(): Reset the index so the date is now a column named index.\n",
    "recent_gdp_pop = gdp_pivot_reset.query('index >= \"1991-01-01\"').set_index('index'): Query to select rows where the date is greater than or equal to \"1991-01-01\" and then set the index column back as the DataFrame index.\n",
    "recent_gdp_pop.plot(rot=90): Plot the filtered DataFrame with rotated x-axis labels.\n",
    "plt.show(): Display the plot.\n",
    "This method should correctly use .query() and provide the desired results.\n",
    "\n",
    "Hint\n",
    "For the .query() method the greater than or equal to symbol is '>='.\n",
    "ChatGPT\n",
    "Thank you for the hint. To correctly use .query() with the greater than or equal to symbol, you should ensure the date is a column in the DataFrame before applying the .query() method.\n",
    "\n",
    "Here's the updated approach with the correct syntax:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming gdp_pivot has been correctly created as in previous steps\n",
    "\n",
    "# Convert index to datetime if it's not already\n",
    "gdp_pivot.index = pd.to_datetime(gdp_pivot.index)\n",
    "\n",
    "# Reset index to use the date as a column\n",
    "gdp_pivot_reset = gdp_pivot.reset_index()\n",
    "\n",
    "# Use .query() to select rows where date is greater than or equal to \"1991-01-01\"\n",
    "recent_gdp_pop = gdp_pivot_reset.query('index >= \"1991-01-01\"').set_index('index')\n",
    "\n",
    "# Plot recent_gdp_pop\n",
    "recent_gdp_pop.plot(rot=90)\n",
    "plt.show()\n",
    "Explanation:\n",
    "reset_index() turns the index into a regular column named index.\n",
    "query('index >= \"1991-01-01\"') uses >= to filter rows where the date is greater than or equal to \"1991-01-01\".\n",
    "set_index('index') sets the index column back as the DataFrame index.\n",
    "This code will properly filter the DataFrame using .query() and plot the results.\n",
    "\n",
    "this is the correct answer could you explain\n",
    "# Merge gdp and pop on date and country with fill\n",
    "gdp_pop = pd.merge_ordered(gdp, pop, on=['country','date'], fill_method='ffill')\n",
    "\n",
    "# Add a column named gdp_per_capita to gdp_pop that divides the gdp by pop\n",
    "gdp_pop['gdp_per_capita'] = gdp_pop['gdp'] / gdp_pop['pop']\n",
    "\n",
    "# Pivot data so gdp_per_capita, where index is date and columns is country\n",
    "gdp_pivot = gdp_pop.pivot_table('gdp_per_capita', 'date', 'country')\n",
    "\n",
    "# Select dates equal to or greater than 1991-01-01\n",
    "recent_gdp_pop = gdp_pivot.query('date >= \"1991-01-01\"')\n",
    "\n",
    "# Plot recent_gdp_pop\n",
    "recent_gdp_pop.plot(rot=90)\n",
    "plt.show()\n",
    "ChatGPT\n",
    "Certainly! Here’s a breakdown of each step in the code:\n",
    "\n",
    "1. Merge GDP and Population Data\n",
    "python\n",
    "Copy code\n",
    "gdp_pop = pd.merge_ordered(gdp, pop, on=['country', 'date'], fill_method='ffill')\n",
    "pd.merge_ordered(): This function merges the gdp and pop DataFrames based on country and date. It aligns the data by these columns and fills any missing values with the previous available value using fill_method='ffill'.\n",
    "2. Add GDP Per Capita Column\n",
    "python\n",
    "Copy code\n",
    "gdp_pop['gdp_per_capita'] = gdp_pop['gdp'] / gdp_pop['pop']\n",
    "Adding a New Column: This line creates a new column, gdp_per_capita, in the gdp_pop DataFrame by dividing the gdp column by the pop column.\n",
    "3. Pivot the Data\n",
    "python\n",
    "Copy code\n",
    "gdp_pivot = gdp_pop.pivot_table(values='gdp_per_capita', index='date', columns='country')\n",
    "pivot_table(): This method reshapes the gdp_pop DataFrame. It sets the date column as the index, country as the columns, and gdp_per_capita as the values to fill the table. This creates a new DataFrame where each row corresponds to a date, each column corresponds to a country, and the cells contain GDP per capita values.\n",
    "4. Query the Data\n",
    "python\n",
    "Copy code\n",
    "recent_gdp_pop = gdp_pivot.query('date >= \"1991-01-01\"')\n",
    "query(): This method filters the gdp_pivot DataFrame to include only rows where the date is greater than or equal to \"1991-01-01\". It assumes date is a column in the DataFrame, but since it's actually an index in the gdp_pivot, the correct way is to use .loc[] or reset the index. The given query assumes date has been reset as a column.\n",
    "5. Plot the Data\n",
    "python\n",
    "Copy code\n",
    "recent_gdp_pop.plot(rot=90)\n",
    "plt.show()\n",
    "Plotting: This code generates a plot of the recent_gdp_pop DataFrame. rot=90 rotates the x-axis labels (dates) 90 degrees for better readability. plt.show() displays the plot.\n",
    "Key Points\n",
    "Merging: Combines datasets on common columns while filling missing values.\n",
    "Adding Columns: Computes new metrics (e.g., GDP per capita).\n",
    "Pivoting: Reshapes data to make it easier to analyze by turning rows into columns.\n",
    "Querying: Filters data based on conditions.\n",
    "Plotting: Visualizes data for analysis.\n",
    "In summary, the code performs data preprocessing (merging, calculating GDP per capita), reshaping (pivoting), filtering (querying), and visualization (plotting).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflation.melt(id_vars=['country', 'indicator'], var_name='year', value_name='annual')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   country    indicator  year  annual\n",
    "0   Brazil  Inflation %  2017    3.45\n",
    "1   Canada  Inflation %  2017    1.60\n",
    "2   France  Inflation %  2017    1.03\n",
    "3    India  Inflation %  2017    2.49\n",
    "4   Brazil  Inflation %  2018    3.66\n",
    "5   Canada  Inflation %  2018    2.27\n",
    "6   France  Inflation %  2018    1.85\n",
    "7    India  Inflation %  2018    4.86\n",
    "8   Brazil  Inflation %  2019    3.73\n",
    "9   Canada  Inflation %  2019    1.95\n",
    "10  France  Inflation %  2019    1.11\n",
    "11   India  Inflation %  2019    7.66\n",
    "\n",
    "\n",
    "\n",
    "Select the right .melt() arguments\n",
    "You are given a table named inflation. Chose the option to get the same output as the table below.\n",
    "\n",
    "   country    indicator  year  annual\n",
    "0   Brazil  Inflation %  2017    3.45\n",
    "1   Canada  Inflation %  2017    1.60\n",
    "2   France  Inflation %  2017    1.03\n",
    "3    India  Inflation %  2017    2.49\n",
    "4   Brazil  Inflation %  2018    3.66\n",
    "5   Canada  Inflation %  2018    2.27\n",
    "6   France  Inflation %  2018    1.85\n",
    "7    India  Inflation %  2018    4.86\n",
    "8   Brazil  Inflation %  2019    3.73\n",
    "9   Canada  Inflation %  2019    1.95\n",
    "10  France  Inflation %  2019    1.11\n",
    "11   India  Inflation %  2019    7.66\n",
    "Instructions\n",
    "50 XP\n",
    "Possible answers\n",
    "\n",
    "\n",
    "inflation.melt(id_vars=['country','indicator'], var_name='annual')\n",
    "\n",
    "inflation.melt(id_vars=['country'], var_name='indicator', value_name='annual')\n",
    "\n",
    "ans: inflation.melt(id_vars=['country','indicator'], var_name='year', value_name='annual')\n",
    "\n",
    "inflation.melt(id_vars=['country'], var_name='year', value_name='annual')\n",
    "\n",
    "To achieve the desired output table from the given inflation table, you need to transform the table so that the year values are unpivoted from the columns to rows and the annual values are corresponding to each country and year.\n",
    "\n",
    "Given the desired output table:\n",
    "\n",
    "   country    indicator  year  annual\n",
    "0   Brazil  Inflation %  2017    3.45\n",
    "1   Canada  Inflation %  2017    1.60\n",
    "2   France  Inflation %  2017    1.03\n",
    "3    India  Inflation %  2017    2.49\n",
    "4   Brazil  Inflation %  2018    3.66\n",
    "5   Canada  Inflation %  2018    2.27\n",
    "6   France  Inflation %  2018    1.85\n",
    "7    India  Inflation %  2018    4.86\n",
    "8   Brazil  Inflation %  2019    3.73\n",
    "9   Canada  Inflation %  2019    1.95\n",
    "10  France  Inflation %  2019    1.11\n",
    "11   India  Inflation %  2019    7.66\n",
    "\n",
    "The correct melt function call should use ['country', 'indicator'] as the id_vars because these are the identifier variables that remain as columns. The year should be the var_name since it represents the years that were originally column names, and the annual should be the value_name representing the values corresponding to these years.\n",
    "\n",
    "Therefore, the correct option is:\n",
    "inflation.melt(id_vars=['country', 'indicator'], var_name='year', value_name='annual')\n",
    "\n",
    "nflation.melt(id_vars=['country','indicator'], var_name='annual')\n",
    "\n",
    "inflation.melt(id_vars=['country'], var_name='indicator', value_name='annual')\n",
    "\n",
    "inflation.melt(id_vars=['country','indicator'], var_name='year', value_name='annual')\n",
    "\n",
    "inflation.melt(id_vars=['country'], var_name='year', value_name='annual')\n",
    "\n",
    "Copilot\n",
    "Sent by Copilot:\n",
    "To get the desired output table, you should use the following melt function:\n",
    "\n",
    "Python\n",
    "\n",
    "inflation.melt(id_vars=['country', 'indicator'], var_name='year', value_name='annual')\n",
    "\n",
    "AI-generated code. Review and use carefully. More info on FAQ.\n",
    "This will transform the table to have country, indicator, year, and annual columns as shown in your example.\n",
    "\n",
    "Is there anything else you’d like to know about using the melt function or any other data manipulation techniques?\n",
    "\n",
    "\n",
    "Magnificent! You identified the correct values to pass to the id_vars argument. These columns are not unpivoted. Finally, the other arguments set the name for the year and value columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume ur_wide is already loaded\n",
    "ur_tall = ur_wide.melt(id_vars='year', var_name='month', value_name='unempl_rate')\n",
    "ur_tall['date'] = pd.to_datetime(ur_tall['year'].astype(str) + '-' + ur_tall['month'])\n",
    "ur_sorted = ur_tall.sort_values('date')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ur_sorted['date'], ur_sorted['unempl_rate'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Unemployment Rate')\n",
    "plt.title('US Unemployment Rate Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "'''Nice going! The plot shows a steady decrease in the unemployment rate with an increase near the end. This increase is likely the effect of the COVID-19 pandemic and its impact on shutting down most of the US economy. In general, data is often provided (_especially by governments_) in a format that is easily read by people but not by machines. The .melt() method is a handy tool for reshaping data into a useful form.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using .melt() to reshape government data\n",
    "The US Bureau of Labor Statistics (BLS) often provides data series in an easy-to-read format - it has a separate column for each month, and each year is a different row. Unfortunately, this wide format makes it difficult to plot this information over time. In this exercise, you will reshape a table of US unemployment rate data from the BLS into a form you can plot using .melt(). You will need to add a date column to the table and sort by it to plot the data correctly.\n",
    "\n",
    "The unemployment rate data has been loaded for you in a table called ur_wide. You are encouraged to explore this table before beginning the exercise.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Use .melt() to unpivot all of the columns of ur_wide except year and ensure that the columns with the months and values are named month and unempl_rate, respectively. Save the result as ur_tall.\n",
    "Add a column to ur_tall named date which combines the year and month columns as year-month format into a larger string, and converts it to a date data type.\n",
    "Sort ur_tall by date and save as ur_sorted.\n",
    "Using ur_sorted, plot unempl_rate on the y-axis and date on the x-axis.\n",
    "\n",
    "\n",
    "ChatGPT\n",
    "Here's how you can accomplish each step in reshaping the ur_wide DataFrame, adding a date column, and then plotting the unemployment rate data:\n",
    "\n",
    "Use .melt() to unpivot the DataFrame:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "\n",
    "# Assume ur_wide is already loaded\n",
    "ur_tall = ur_wide.melt(id_vars='year', var_name='month', value_name='unempl_rate')\n",
    "Add a date column that combines year and month:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "ur_tall['date'] = pd.to_datetime(ur_tall['year'].astype(str) + '-' + ur_tall['month'])\n",
    "Sort the DataFrame by the date column:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "ur_sorted = ur_tall.sort_values('date')\n",
    "Plot the unemployment rate data:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ur_sorted['date'], ur_sorted['unempl_rate'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Unemployment Rate')\n",
    "plt.title('US Unemployment Rate Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "Here's the complete code in one place:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume ur_wide is already loaded\n",
    "ur_tall = ur_wide.melt(id_vars='year', var_name='month', value_name='unempl_rate')\n",
    "ur_tall['date'] = pd.to_datetime(ur_tall['year'].astype(str) + '-' + ur_tall['month'])\n",
    "ur_sorted = ur_tall.sort_values('date')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ur_sorted['date'], ur_sorted['unempl_rate'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Unemployment Rate')\n",
    "plt.title('US Unemployment Rate Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "This code will reshape the ur_wide DataFrame into a long format, create a date column, sort by date, and plot the unemployment rate over time.\n",
    "\n",
    "'''Using .melt() to reshape government data\n",
    "The US Bureau of Labor Statistics (BLS) often provides data series in an easy-to-read format - it has a separate column for each month, and each year is a different row. Unfortunately, this wide format makes it difficult to plot this information over time. In this exercise, you will reshape a table of US unemployment rate data from the BLS into a form you can plot using .melt(). You will need to add a date column to the table and sort by it to plot the data correctly.\n",
    "\n",
    "The unemployment rate data has been loaded for you in a table called ur_wide. You are encouraged to explore this table before beginning the exercise.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Use .melt() to unpivot all of the columns of ur_wide except year and ensure that the columns with the months and values are named month and unempl_rate, respectively. Save the result as ur_tall.\n",
    "Add a column to ur_tall named date which combines the year and month columns as year-month format into a larger string, and converts it to a date data type.\n",
    "Sort ur_tall by date and save as ur_sorted.\n",
    "Using ur_sorted, plot unempl_rate on the y-axis and date on the x-axis.\n",
    "\n",
    "'''\n",
    "\n",
    "# unpivot everything besides the year column\n",
    "ur_tall = ____\n",
    "\n",
    "\n",
    "# Create a date column using the month and year columns of ur_tall\n",
    "ur_tall['date'] = pd.to_datetime(ur_tall['____'] + '-' + ____)\n",
    "\n",
    "# Sort ur_tall by date in ascending order\n",
    "ur_sorted = ____\n",
    "\n",
    "# Plot the unempl_rate by date\n",
    "ur_sorted.plot(____)\n",
    "plt.show()\n",
    "ChatGPT\n",
    "To reshape the unemployment rate data from the BLS, add a date column, sort by it, and plot the data, you can follow the steps outlined below. I'll fill in the blanks in the provided code snippet.\n",
    "\n",
    "Unpivot everything besides the year column:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "ur_tall = ur_wide.melt(id_vars='year', var_name='month', value_name='unempl_rate')\n",
    "Create a date column using the month and year columns of ur_tall:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "ur_tall['date'] = pd.to_datetime(ur_tall['year'].astype(str) + '-' + ur_tall['month'])\n",
    "Sort ur_tall by date in ascending order:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "ur_sorted = ur_tall.sort_values('date')\n",
    "Plot the unempl_rate by date:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "ur_sorted.plot(x='date', y='unempl_rate')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Unemployment Rate')\n",
    "plt.title('US Unemployment Rate Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "Putting it all together:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume ur_wide is already loaded\n",
    "ur_tall = ur_wide.melt(id_vars='year', var_name='month', value_name='unempl_rate')\n",
    "\n",
    "# Create a date column using the month and year columns of ur_tall\n",
    "ur_tall['date'] = pd.to_datetime(ur_tall['year'].astype(str) + '-' + ur_tall['month'])\n",
    "\n",
    "# Sort ur_tall by date in ascending order\n",
    "ur_sorted = ur_tall.sort_values('date')\n",
    "\n",
    "# Plot the unempl_rate by date\n",
    "ur_sorted.plot(x='date', y='unempl_rate')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Unemployment Rate')\n",
    "plt.title('US Unemployment Rate Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "This complete code will reshape the ur_wide DataFrame into a long format, add a date column, sort the data by date, and plot the unemployment rate over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming ten_yr and dji DataFrames are already loaded\n",
    "\n",
    "# Use melt on ten_yr, unpivot everything besides the metric column\n",
    "bond_perc = ten_yr.melt(id_vars='metric', var_name='date', value_name='close')\n",
    "\n",
    "# Use query on bond_perc to select only the rows where metric=close\n",
    "bond_perc_close = bond_perc.query(\"metric == 'close'\")\n",
    "\n",
    "# Merge (ordered) dji and bond_perc_close on date with an inner join\n",
    "dow_bond = pd.merge_ordered(dji, bond_perc_close, on='date', how='inner', suffixes=('_dow', '_bond'))\n",
    "\n",
    "# Plot only the close_dow and close_bond columns\n",
    "dow_bond.plot(y=['close_dow', 'close_bond'], x='date', rot=90)\n",
    "plt.show()\n",
    "\n",
    "'''uper job! You used many of the techniques we have reviewed in this chapter to produce the plot. The plot confirms that the bond and stock prices are inversely correlated. Often as the price of stocks increases, the price for bonds decreases.'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "'''Using .melt() for stocks vs bond performance\n",
    "It is widespread knowledge that the price of bonds is inversely related to the price of stocks. In this last exercise, you'll review many of the topics in this chapter to confirm this. You have been given a table of percent change of the US 10-year treasury bond price. It is in a wide format where there is a separate column for each year. You will need to use the .melt() method to reshape this table.\n",
    "\n",
    "Additionally, you will use the .query() method to filter out unneeded data. You will merge this table with a table of the percent change of the Dow Jones Industrial stock index price. Finally, you will plot data.\n",
    "\n",
    "The tables ten_yr and dji have been loaded for you.\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "Use .melt() on ten_yr to unpivot everything except the metric column, setting var_name='date' and value_name='close'. Save the result to bond_perc.\n",
    "Using the .query() method, select only those rows where metric equals 'close', and save to bond_perc_close.\n",
    "Use merge_ordered() to merge dji (left table) and bond_perc_close on date with an inner join, and set suffixes equal to ('_dow', '_bond'). Save the result to dow_bond.\n",
    "Using dow_bond, plot only the Dow and bond values.'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Use melt on ten_yr, unpivot everything besides the metric column\n",
    "bond_perc = ____\n",
    "\n",
    "# Use query on bond_perc to select only the rows where metric=close\n",
    "bond_perc_close = ____\n",
    "\n",
    "# Merge (ordered) dji and bond_perc_close on date with an inner join\n",
    "dow_bond = ____\n",
    "\n",
    "\n",
    "# Plot only the close_dow and close_bond columns\n",
    "dow_bond.plot(____, x='date', rot=90)\n",
    "plt.show()\n",
    "ChatGPT\n",
    "To complete this task, you will need to use the .melt() function to reshape the ten_yr DataFrame, use .query() to filter the data, perform an ordered merge using merge_ordered, and then plot the results. Here is the step-by-step solution:\n",
    "\n",
    "Unpivot ten_yr DataFrame using .melt()\n",
    "Filter the melted DataFrame with .query()\n",
    "Merge the filtered DataFrame with dji using merge_ordered\n",
    "Plot the results\n",
    "Here's how you can do it:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming ten_yr and dji DataFrames are already loaded\n",
    "\n",
    "# Use melt on ten_yr, unpivot everything besides the metric column\n",
    "bond_perc = ten_yr.melt(id_vars='metric', var_name='date', value_name='close')\n",
    "\n",
    "# Use query on bond_perc to select only the rows where metric=close\n",
    "bond_perc_close = bond_perc.query(\"metric == 'close'\")\n",
    "\n",
    "# Merge (ordered) dji and bond_perc_close on date with an inner join\n",
    "dow_bond = pd.merge_ordered(dji, bond_perc_close, on='date', how='inner', suffixes=('_dow', '_bond'))\n",
    "\n",
    "# Plot only the close_dow and close_bond columns\n",
    "dow_bond.plot(y=['close_dow', 'close_bond'], x='date', rot=90)\n",
    "plt.show()\n",
    "Explanation:\n",
    "\n",
    ".melt(): This reshapes the ten_yr DataFrame from wide format to long format, where the metric column is retained, and each year's data is converted to rows under the date column.\n",
    ".query(): This filters the melted DataFrame to keep only the rows where the metric column value is 'close'.\n",
    "merge_ordered: This performs an ordered merge on dji and the filtered bond_perc_close DataFrame on the date column using an inner join.\n",
    "plot(): This plots the values of close_dow and close_bond with date on the x-axis and rotates the x-axis labels by 90 degrees for better readability.\n",
    "Make sure ten_yr and dji DataFrames are loaded in your environment with the appropriate data before running this code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Merging Ordered and Time-Series Data\n",
    "\n",
    "94%\n",
    "In this final chapter, you’ll step up a gear and learn to apply pandas' specialized methods for merging time-series and ordered data together with real-world financial and economic data from the city of Chicago. You’ll also learn how to query resulting tables using a SQL-style format, and unpivot data using the melt method.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
