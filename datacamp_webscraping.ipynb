{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import the function urlretrieve from the subpackage urllib.request.\n",
    "Assign the URL of the file to the variable url.\n",
    "Use the function urlretrieve() to save the file locally as 'winequality-red.csv'.\n",
    "Execute the remaining code to load 'winequality-red.csv' in a pandas DataFrame and to print its head to the shell.\n",
    "\n",
    "'''\n",
    "\n",
    "# Import package\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Save file locally\n",
    "urlretrieve(url, 'winequality-red.csv')\n",
    "\n",
    "# Read file into a DataFrame and print its head\n",
    "df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Opening and reading flat files from the web\n",
    "You have just imported a file from the web, saved it locally and loaded it into a DataFrame. If you just wanted to load a file from the web into a DataFrame without first saving it locally, you can do that easily using pandas. In particular, you can use the function pd.read_csv() with the URL as the first argument and the separator sep as the second argument.\n",
    "\n",
    "The URL of the file, once again, is\n",
    "\n",
    "'https://assets.datacamp.com/production/course_1606/datasets/wineq\n",
    "'''\n",
    "\n",
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Read file into a DataFrame: df\n",
    "df = pd.read_csv(url, sep=';')\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Plot first column of df\n",
    "df.iloc[:, 0].hist()\n",
    "plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Assign the URL of the file to the variable url.\n",
    "Read the file in url into a dictionary xls using pd.read_excel() recalling that, in order to import all sheets you need to pass None to the argument sheet_name.\n",
    "Print the names of the sheets in the Excel spreadsheet; these will be the keys of the dictionary xls.\n",
    "Print the head of the first sheet using the sheet name, not the index of the sheet! The sheet name is '1700'\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assign the URL of the file to the variable url\n",
    "url = \"https://assets.datacamp.com/course/importing_data_into_r/latitude.xls\"  \n",
    "# Replace with the actual URL of your Excel file\n",
    "\n",
    "# Read in all sheets of the Excel file into a dictionary xls\n",
    "xls = pd.read_excel(url, sheet_name=None)  # Pass None to read all sheets\n",
    "\n",
    "# Print the names of the sheets in the Excel spreadsheet\n",
    "# print(\"Sheet names:\", list(xls.keys()))\n",
    "print(xls.keys())\n",
    "\n",
    "# Print the head of the first sheet using the sheet name '1700'\n",
    "try:\n",
    "  first_sheet_name = \"1700\"\n",
    "  print(f\"\\nHead of sheet '{first_sheet_name}':\")\n",
    "  print(xls[first_sheet_name].head())\n",
    "except KeyError:\n",
    "  print(f\"Sheet named '{first_sheet_name}' not found in the Excel file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import the functions urlopen and Request from the subpackage urllib.request.\n",
    "Package the request to the url \"https://campus.datacamp.com/courses/1606/4135?ex=2\" using the function Request() and assign it to request.\n",
    "Send the request and catch the response in the variable response with the function urlopen().\n",
    "Run the rest of the code to see the datatype of response and to close the connection!\n",
    "'''\n",
    "\n",
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# This packages the request: request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Print the datatype of response\n",
    "print(type(response))\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Send the request and catch the response in the variable response with the function urlopen(), as in the previous exercise.\n",
    "Extract the response using the read() method and store the result in the variable html.\n",
    "Print the string html.\n",
    "Hit submit to perform all of the above and to close the response: be tidy!\n",
    "'''\n",
    "\n",
    "\n",
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# This packages the request: request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response = urlopen(request)\n",
    "\n",
    "# Extract the response using the read() method\n",
    "html = response.read()\n",
    "\n",
    "# Print the string html\n",
    "print(html)\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import the package requests.\n",
    "Assign the URL of interest to the variable url.\n",
    "Package the request to the URL, send the request and catch the response with a single function requests.get(), assigning the response to the variable r.\n",
    "Use the text attribute of the object r to return the HTML of the webpage as a string; store the result in a variable text.\n",
    "Hit submit to print the HTML of the webpage.\n",
    "'''\n",
    "\n",
    "\n",
    "# Import the requests package\n",
    "import requests\n",
    "\n",
    "# Specify the URL\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "\n",
    "# Package the request, send the request, and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response: text\n",
    "text = r.text\n",
    "\n",
    "# Print the HTML\n",
    "print(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import the function BeautifulSoup from the package bs4.\n",
    "Assign the URL of interest to the variable url.\n",
    "Package the request to the URL, send the request and catch the response with a single function requests.get(), assigning the response to the variable r.\n",
    "Use the text attribute of the object r to return the HTML of the webpage as a string; store the result in a variable html_doc.\n",
    "Create a BeautifulSoup object soup from the resulting HTML using the function BeautifulSoup().\n",
    "Use the method prettify() on soup and assign the result to pretty_soup.\n",
    "Hit submit to print to prettified HTML to your shell!\n",
    "'''\n",
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Prettify the BeautifulSoup object: pretty_soup\n",
    "pretty_soup = soup.prettify()\n",
    "\n",
    "# Print the prettified HTML\n",
    "print(pretty_soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "In the sample code, the HTML response object html_doc has already been created: your first task is to Soupify it using the function BeautifulSoup() and to assign the resulting soup to the variable soup.\n",
    "Extract the title from the HTML soup soup using the attribute title and assign the result to guido_title.\n",
    "Print the title of Guido's webpage to the shell using the print() function.\n",
    "Extract the text from the HTML soup soup using the method get_text() and assign to guido_text.\n",
    "Hit submit to print the text from Guido's webpage to the shell.\n",
    "'''\n",
    "'''\n",
    "Hint\n",
    "Pass the HTML response object as an argument to BeautifulSoup().\n",
    "You can access the title attribute of the object soup by executing soup.title.\n",
    "The object that contains the title of Guido's webpage is guido_title; pass this as an argument to print().\n",
    "Use the method get_text() on the HTML soup soup by executing soup.get_text().\n",
    "You don't have to modify the code to print the text from Guido's webpage.\n",
    "'''\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Get the title of Guido's webpage\n",
    "guido_title = soup.title.string\n",
    "\n",
    "# Print the title of Guido's webpage to the shell\n",
    "print(\"Title of Guido's webpage:\", guido_title)\n",
    "\n",
    "# Get Guido's text\n",
    "guido_text = soup.get_text()\n",
    "\n",
    "# Print Guido's text to the shell\n",
    "print(\"Text from Guido's webpage:\")\n",
    "print(guido_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Get the title of Guido's webpage: guido_title\n",
    "guido_title = soup.title\n",
    "\n",
    "# Print the title of Guido's webpage to the shell\n",
    "print(guido_title)\n",
    "\n",
    "# Get Guido's text: guido_text\n",
    "guido_text = soup.get_text()\n",
    "\n",
    "# Print Guido's text to the shell\n",
    "print(guido_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Use the method find_all() to find all hyperlinks in soup, remembering that hyperlinks are defined by the HTML tag <a> but passed to find_all() without angle brackets; store the result in the variable a_tags.\n",
    "The variable a_tags is a results set: your job now is to enumerate over it, using a for loop and to print the actual URLs of the hyperlinks; to do this, for every element link in a_tags, you want to print() link.get('href')\n",
    "'''\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# Print the title of Guido's webpage\n",
    "print(soup.title)\n",
    "\n",
    "# Find all 'a' tags (which define hyperlinks): a_tags\n",
    "a_tags = soup.find_all('a')\n",
    "\n",
    "# Print the URLs to the shell\n",
    "for link in a_tags:\n",
    "    print(link.get('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load the JSON 'a_movie.json' into the variable json_data within the context provided by the with statement. To do so, use the function json.load() within the context manager.\n",
    "Use a for loop to print all key-value pairs in the dictionary json_data. Recall that you can access a value in a dictionary using the syntax: dictionary[key]\n",
    "'''\n",
    "\n",
    "# Load JSON: json_data\n",
    "with open(\"a_movie.json\") as json_file:\n",
    "  json_data = json.load(json_file)\n",
    "\n",
    "# Print each key-value pair in json_data\n",
    "for k in json_data.keys():\n",
    "  print(k + ': ', json_data[k])  # Access value using dictionary syntax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import the requests package.\n",
    "Assign to the variable url the URL of interest in order to query 'http://www.omdbapi.com' for the data corresponding to the movie The Social Network. The query string should have two arguments: apikey=72bc447a and t=the+social+network. You can combine them as follows: apikey=72bc447a&t=the+social+network.\n",
    "Print the text of the response object r by using its text attribute and passing the result to the print() function.\n",
    "'''\n",
    "# Import requests package\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'http://www.omdbapi.com/?apikey=72bc447a&t=the+social+network'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Print the text of the response\n",
    "print(r.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pass the variable url to the requests.get() function in order to send the relevant request and catch the response, assigning the resultant response message to the variable r.\n",
    "Apply the json() method to the response object r and store the resulting dictionary in the variable json_data.\n",
    "Hit submit to print the key-value pairs of the dictionary json_data to the shell.\n",
    "'''\n",
    "\n",
    "'''\n",
    "Pass the variable url to the requests.get() function in order to send the relevant request and catch the response, assigning the resultant response message to the variable r.\n",
    "Apply the json() method to the response object r and store the resulting dictionary in the variable json_data.\n",
    "Hit submit to print the key-value pairs of the dictionary json_data to the shell.\n",
    "'''\n",
    "\n",
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'http://www.omdbapi.com/?apikey=72bc447a&t=social+network'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = r.json()  # Access the JSON data as a dictionary\n",
    "\n",
    "# Print each key-value pair in json_data\n",
    "for k in json_data.keys():\n",
    "  print(k + ': ', json_data[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Assign the relevant URL to the variable url.\n",
    "Apply the json() method to the response object r and store the resulting dictionary in the variable json_data.\n",
    "The variable pizza_extract holds the HTML of an extract from Wikipedia's Pizza page as a string; use the function print() to print this string to the shell.\n",
    "'''\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = r.json()\n",
    "\n",
    "# Print the Wikipedia page extract\n",
    "pizza_extract = json_data['query']['pages']['24768']['extract']\n",
    "print(pizza_extract)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store credentials in relevant variables\n",
    "consumer_key = \"nZ6EA0FxZ293SxGNg8g8aP0HM\"\n",
    "consumer_secret = \"fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i\"\n",
    "access_token = \"1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy\"\n",
    "access_token_secret = \"X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx\"\n",
    "\n",
    "# Create your Stream object with credentials\n",
    "stream = tweepy.Stream(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "\n",
    "# Filter your Stream variable\n",
    "stream.filter(track=[\"clinton\", \"trump\", \"sanders\", \"cruz\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
